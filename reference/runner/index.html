
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for Speechain Toolkit">
      
      
      
      
        <link rel="prev" href="../pyscripts/wavlen_dist_visualizer/">
      
      
        <link rel="next" href="../snapshooter/">
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.46">
    
    
      
        <title>runner - Speechain</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/code_select.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#runner" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Speechain" class="md-header__button md-logo" aria-label="Speechain" data-md-component="logo">
      
  <img src="../../img/speechain.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Speechain
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              runner
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/bagustris/speechain" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Speechain" class="md-nav__button md-logo" aria-label="Speechain" data-md-component="logo">
      
  <img src="../../img/speechain.png" alt="logo">

    </a>
    Speechain
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/bagustris/speechain" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../asr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASR
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    criterion
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            criterion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/accuracy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    accuracy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/att_guid/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    att_guid
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/bce_logits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    bce_logits
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/cross_entropy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    cross_entropy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/ctc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctc
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/error_rate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    error_rate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/fbeta_score/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    fbeta_score
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/least_error/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    least_error
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../criterion/perplexity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    perplexity
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../dataset/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    dataset
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            dataset
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dataset/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dataset/speech_text/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech_text
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    infer_func
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            infer_func
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../infer_func/beam_search/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    beam_search
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../infer_func/ctc_decoding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ctc_decoding
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../infer_func/tts_decoding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tts_decoding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_4" >
        
          
          <label class="md-nav__link" for="__nav_5_4" id="__nav_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    iterator
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_4">
            <span class="md-nav__icon md-icon"></span>
            iterator
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../iterator/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../iterator/block/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    block
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
        
          
          <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    model
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            model
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model/ar_asr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ar_asr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model/ar_tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ar_tts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model/lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    lm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model/nar_tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nar_tts
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6" >
        
          
          <label class="md-nav__link" for="__nav_5_6" id="__nav_5_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    module
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6">
            <span class="md-nav__icon md-icon"></span>
            module
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_2" >
        
          
          <label class="md-nav__link" for="__nav_5_6_2" id="__nav_5_6_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    augment
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_2">
            <span class="md-nav__icon md-icon"></span>
            augment
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/augment/specaug/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    specaug
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_3" >
        
          
          <label class="md-nav__link" for="__nav_5_6_3" id="__nav_5_6_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    conformer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_3">
            <span class="md-nav__icon md-icon"></span>
            conformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/conformer/attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/conformer/encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/conformer/pos_enc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pos_enc
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_4" >
        
          
          <label class="md-nav__link" for="__nav_5_6_4" id="__nav_5_6_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    decoder
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_4">
            <span class="md-nav__icon md-icon"></span>
            decoder
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/decoder/ar_asr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ar_asr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/decoder/ar_tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ar_tts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/decoder/nar_tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nar_tts
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_5" >
        
          
          <label class="md-nav__link" for="__nav_5_6_5" id="__nav_5_6_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    encoder
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_5">
            <span class="md-nav__icon md-icon"></span>
            encoder
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/encoder/asr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    asr
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/encoder/tts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tts
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_6" >
        
          
          <label class="md-nav__link" for="__nav_5_6_6" id="__nav_5_6_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    frontend
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_6">
            <span class="md-nav__icon md-icon"></span>
            frontend
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/frontend/delta_feat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    delta_feat
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/frontend/linear2mel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear2mel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/frontend/speech2linear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech2linear
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/frontend/speech2mel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    speech2mel
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_7" >
        
          
          <label class="md-nav__link" for="__nav_5_6_7" id="__nav_5_6_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    norm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_7">
            <span class="md-nav__icon md-icon"></span>
            norm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/norm/feat_norm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feat_norm
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_8" >
        
          
          <label class="md-nav__link" for="__nav_5_6_8" id="__nav_5_6_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    postnet
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_8">
            <span class="md-nav__icon md-icon"></span>
            postnet
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/postnet/conv1d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    conv1d
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/postnet/token/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    token
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_9" >
        
          
          <label class="md-nav__link" for="__nav_5_6_9" id="__nav_5_6_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    prenet
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_9">
            <span class="md-nav__icon md-icon"></span>
            prenet
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/conv1d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    conv1d
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/conv2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    conv2d
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/embed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    embed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/linear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/spk_embed/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    spk_embed
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/prenet/var_pred/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    var_pred
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_10" >
        
          
          <label class="md-nav__link" for="__nav_5_6_10" id="__nav_5_6_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    standalone
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_10">
            <span class="md-nav__icon md-icon"></span>
            standalone
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/standalone/lm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    lm
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_6_11" >
        
          
          <label class="md-nav__link" for="__nav_5_6_11" id="__nav_5_6_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    transformer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_6_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_6_11">
            <span class="md-nav__icon md-icon"></span>
            transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/transformer/attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/transformer/decoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    decoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/transformer/encoder/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    encoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/transformer/feed_forward/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feed_forward
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module/transformer/pos_enc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    pos_enc
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../monitor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    monitor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_8" >
        
          
          <label class="md-nav__link" for="__nav_5_8" id="__nav_5_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    optim_sche
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_8">
            <span class="md-nav__icon md-icon"></span>
            optim_sche
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optim_sche/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optim_sche/exp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    exp
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optim_sche/noam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    noam
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_9" >
        
          
          <label class="md-nav__link" for="__nav_5_9" id="__nav_5_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    pyscripts
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_9">
            <span class="md-nav__icon md-icon"></span>
            pyscripts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/empty_file_checker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    empty_file_checker
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/folder_summarizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    folder_summarizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/model_para_renamer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model_para_renamer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/phn_duaration_visualizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    phn_duaration_visualizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/text_dist_visualizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_dist_visualizer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pyscripts/wavlen_dist_visualizer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    wavlen_dist_visualizer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    runner
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    runner
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#runner" class="md-nav__link">
    <span class="md-ellipsis">
      runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#runner.Runner" class="md-nav__link">
    <span class="md-ellipsis">
      Runner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Runner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#runner.Runner.add_parse" class="md-nav__link">
    <span class="md-ellipsis">
      add_parse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_iterators" class="md-nav__link">
    <span class="md-ellipsis">
      build_iterators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_model" class="md-nav__link">
    <span class="md-ellipsis">
      build_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_optim_sches" class="md-nav__link">
    <span class="md-ellipsis">
      build_optim_sches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.dict_transform" class="md-nav__link">
    <span class="md-ellipsis">
      dict_transform
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.gather_all_iter_ascii" class="md-nav__link">
    <span class="md-ellipsis">
      gather_all_iter_ascii
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.main_worker" class="md-nav__link">
    <span class="md-ellipsis">
      main_worker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.parse" class="md-nav__link">
    <span class="md-ellipsis">
      parse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.resume" class="md-nav__link">
    <span class="md-ellipsis">
      resume
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.run" class="md-nav__link">
    <span class="md-ellipsis">
      run
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.set_random_seeds" class="md-nav__link">
    <span class="md-ellipsis">
      set_random_seeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.test" class="md-nav__link">
    <span class="md-ellipsis">
      test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../snapshooter/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    snapshooter
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_12" >
        
          
          <label class="md-nav__link" for="__nav_5_12" id="__nav_5_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    tokenizer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_12">
            <span class="md-nav__icon md-icon"></span>
            tokenizer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenizer/abs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    abs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenizer/char/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    char
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenizer/g2p/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    g2p
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenizer/sp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sp
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_13" >
        
          
          <label class="md-nav__link" for="__nav_5_13" id="__nav_5_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    utilbox
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_13">
            <span class="md-nav__icon md-icon"></span>
            utilbox
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/data_loading_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data_loading_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/data_saving_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    data_saving_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/dump_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    dump_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/eval_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    eval_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/feat_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feat_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/import_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    import_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/log_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    log_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/md_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    md_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/regex_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    regex_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/sb_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    sb_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/spk_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    spk_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/tensor_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    tensor_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/text_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    text_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/train_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    train_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/type_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    type_util
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilbox/yaml_util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    yaml_util
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#runner" class="md-nav__link">
    <span class="md-ellipsis">
      runner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#runner.Runner" class="md-nav__link">
    <span class="md-ellipsis">
      Runner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Runner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#runner.Runner.add_parse" class="md-nav__link">
    <span class="md-ellipsis">
      add_parse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_iterators" class="md-nav__link">
    <span class="md-ellipsis">
      build_iterators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_model" class="md-nav__link">
    <span class="md-ellipsis">
      build_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.build_optim_sches" class="md-nav__link">
    <span class="md-ellipsis">
      build_optim_sches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.dict_transform" class="md-nav__link">
    <span class="md-ellipsis">
      dict_transform
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.gather_all_iter_ascii" class="md-nav__link">
    <span class="md-ellipsis">
      gather_all_iter_ascii
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.main_worker" class="md-nav__link">
    <span class="md-ellipsis">
      main_worker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.parse" class="md-nav__link">
    <span class="md-ellipsis">
      parse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.resume" class="md-nav__link">
    <span class="md-ellipsis">
      resume
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.run" class="md-nav__link">
    <span class="md-ellipsis">
      run
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.set_random_seeds" class="md-nav__link">
    <span class="md-ellipsis">
      set_random_seeds
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.test" class="md-nav__link">
    <span class="md-ellipsis">
      test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runner.Runner.train" class="md-nav__link">
    <span class="md-ellipsis">
      train
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>runner</h1>

<div class="doc doc-object doc-module">



<a id="runner"></a>
    <div class="doc doc-contents first">

        <p>Author: Heli Qi
Affiliation: NAIST
Date: 2022.07</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="runner.Runner" class="doc doc-heading">
            <code>Runner</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code>object</code></p>


        <p>Runner is the entrance of our toolkit. This static class is made up of several static functions. The
whole pipeline is done by all static functions step by step. The reason why the functions are all static is to
prevent Runner from becoming the God class after Inheritance.
If you are interested in this topic, please refer to https://wiki.c2.com/?GodClass for more details.</p>
<p>In this class, we provide an overridable interface add_parse() that enables users to add more arguments they
would like their runners to have.</p>
<p>Basically, we don't recommend users to override the other functions in this class for robustness.
However, in case that the existing functions cannot meet your research requirements, you can override them in your
own runners to fit your specific needs. If it happens, we would appreciate it a lot if you could open an issue
and let us know.</p>
<p>Wish you have a happy usage journey in this toolkit ^_^!</p>






              <details class="quote">
                <summary>Source code in <code>speechain/runner.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  38</span>
<span class="normal">  39</span>
<span class="normal">  40</span>
<span class="normal">  41</span>
<span class="normal">  42</span>
<span class="normal">  43</span>
<span class="normal">  44</span>
<span class="normal">  45</span>
<span class="normal">  46</span>
<span class="normal">  47</span>
<span class="normal">  48</span>
<span class="normal">  49</span>
<span class="normal">  50</span>
<span class="normal">  51</span>
<span class="normal">  52</span>
<span class="normal">  53</span>
<span class="normal">  54</span>
<span class="normal">  55</span>
<span class="normal">  56</span>
<span class="normal">  57</span>
<span class="normal">  58</span>
<span class="normal">  59</span>
<span class="normal">  60</span>
<span class="normal">  61</span>
<span class="normal">  62</span>
<span class="normal">  63</span>
<span class="normal">  64</span>
<span class="normal">  65</span>
<span class="normal">  66</span>
<span class="normal">  67</span>
<span class="normal">  68</span>
<span class="normal">  69</span>
<span class="normal">  70</span>
<span class="normal">  71</span>
<span class="normal">  72</span>
<span class="normal">  73</span>
<span class="normal">  74</span>
<span class="normal">  75</span>
<span class="normal">  76</span>
<span class="normal">  77</span>
<span class="normal">  78</span>
<span class="normal">  79</span>
<span class="normal">  80</span>
<span class="normal">  81</span>
<span class="normal">  82</span>
<span class="normal">  83</span>
<span class="normal">  84</span>
<span class="normal">  85</span>
<span class="normal">  86</span>
<span class="normal">  87</span>
<span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Runner</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runner is the entrance of our toolkit. This static class is made up of several static functions. The</span>
<span class="sd">    whole pipeline is done by all static functions step by step. The reason why the functions are all static is to</span>
<span class="sd">    prevent Runner from becoming the God class after Inheritance.</span>
<span class="sd">    If you are interested in this topic, please refer to https://wiki.c2.com/?GodClass for more details.</span>

<span class="sd">    In this class, we provide an overridable interface add_parse() that enables users to add more arguments they</span>
<span class="sd">    would like their runners to have.</span>

<span class="sd">    Basically, we don&#39;t recommend users to override the other functions in this class for robustness.</span>
<span class="sd">    However, in case that the existing functions cannot meet your research requirements, you can override them in your</span>
<span class="sd">    own runners to fit your specific needs. If it happens, we would appreciate it a lot if you could open an issue</span>
<span class="sd">    and let us know.</span>

<span class="sd">    Wish you have a happy usage journey in this toolkit ^_^!</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">add_parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The interface where users can add their own arguments.</span>

<span class="sd">        Args:</span>
<span class="sd">            parser: argparse.ArgumentParser</span>
<span class="sd">                The name space where you want to add your arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            parser: argparse.ArgumentParser</span>
<span class="sd">                The name space containing your arguments.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">parser</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The static function that outputs all the default arguments for the runner.</span>

<span class="sd">        Returns:</span>
<span class="sd">            a Dict containing the key-value pairs of all arguments</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>

        <span class="c1"># All-in-one configuration setting</span>
        <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--config&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
            <span class="c1"># default=None,</span>
            <span class="n">default</span><span class="o">=</span><span class="s2">&quot;recipes/asr/librispeech/train-clean-100/exp_cfg/100-bpe5k_conformer-medium_lr2e-3.yaml&quot;</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the all-in-one experiment configuration file. You can write all the arguments in this &quot;</span>
            <span class="s2">&quot;all-in-one file instead of giving them to `runner.py` by command lines.&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Experimental environment</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 1: Calculation and System Backend&quot;</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--seed&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Initial random seed for the experiment. (default: 0)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--cudnn_enabled&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--cudnn_benchmark&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn.benchmark. &quot;</span>
            <span class="s2">&quot;When True, the process of model training will be speed up and the model performance may improve &quot;</span>
            <span class="s2">&quot;somewhat. But your results will become less reproducible. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--cudnn_deterministic&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn.deterministic. &quot;</span>
            <span class="s2">&quot;This will improve the reproducibility of your experiments. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--train_num_workers&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
            <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
            <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
            <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
            <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
            <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--valid_num_workers&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
            <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
            <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
            <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
            <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
            <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--test_num_workers&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
            <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
            <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
            <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
            <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
            <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--pin_memory&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate `pin_memory` for the Dataloader of each epoch. &quot;</span>
            <span class="s2">&quot;If True, the pinned memory in the dataloaders will be activated and the data loading will be further &quot;</span>
            <span class="s2">&quot;speed up. &quot;</span>
            <span class="s2">&quot;pin_memory=True is often used together with non_blocking=True. Note that this combination requires a &quot;</span>
            <span class="s2">&quot;large amount of memory and CPU cores. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--non_blocking&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate `non_blocking` when transferring data from the memory to GPUs. &quot;</span>
            <span class="s2">&quot;If True, the process of model training will be speed up. &quot;</span>
            <span class="s2">&quot;non_blocking=True is often used together with pin_memory=True. Note that this combination requires a &quot;</span>
            <span class="s2">&quot;large amount of memory and CPU cores. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># gradient descent related</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
            <span class="s2">&quot;Group 2: Gradient Calculation and Back-Propagation&quot;</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--use_amp&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether activate AMP (Automatic Mixed Precision) during the back-propagation. &quot;</span>
            <span class="s2">&quot;If True, the GPU consumption of your model will be smaller so that you can include more data &quot;</span>
            <span class="s2">&quot;instances in a single batch. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--grad_clip&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Gradient clipping threshold during the back-propagation. (default: 5.0)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--grad_norm_type&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Normalization type used when clipping the gradients. (default: 2.0)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--accum_grad&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of gradient accumulation steps. &quot;</span>
            <span class="s2">&quot;To mimic the gradients calculated by large batches with only a small amount of GPUs, please raise &quot;</span>
            <span class="s2">&quot;this argument. &quot;</span>
            <span class="s2">&quot;The virtual batch size will become (accum_grad * the actual batch size). &quot;</span>
            <span class="s2">&quot;Note that the model trained by accum_grad is not identical to the one actually trained by large &quot;</span>
            <span class="s2">&quot;batches because of the different randomness in each training step and the existence of BatchNorm. &quot;</span>
            <span class="s2">&quot;(default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--ft_factor&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The finetuing factor used to scale down learning rates during the parameter optimization. &quot;</span>
            <span class="s2">&quot;If `ft_factor` is smaller than 1.0, the learning rates will be proportionally decreased without &quot;</span>
            <span class="s2">&quot;changing its scheduling strategy. Usually, ft_factor could be set from 0.1 to 0.5 depending on your &quot;</span>
            <span class="s2">&quot;finetuning scenarios. (default: 1.0)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># multi-GPU distributed training</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 3: Multi-GPU Distribution&quot;</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--dist_backend&quot;</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Communication backend for multi-GPU distribution. &quot;</span>
            <span class="s2">&quot;If you are using NVIDIA GPUs, we recommend you set this argument to &#39;nccl&#39;. (default: nccl)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--dist_url&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="s2">&quot;tcp://127.0.0.1&quot;</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Communication URL for multi-GPU distribution. &quot;</span>
            <span class="s2">&quot;The default value is &#39;tcp://127.0.0.1&#39; for single-node distributed training and an idle port will be &quot;</span>
            <span class="s2">&quot;automatically selected. &quot;</span>
            <span class="s2">&quot;The port number cannot be set manually, which means that the argument &#39;tcp://127.0.0.1:xxxxx&#39; will &quot;</span>
            <span class="s2">&quot;have the same effect with &#39;tcp://127.0.0.1&#39;. &quot;</span>
            <span class="s2">&quot;If you want to train your model on multiple nodes, please set dist_url=&#39;env://&#39; &quot;</span>
            <span class="s2">&quot;(Note: multi-node model distribution is still in beta). &quot;</span>
            <span class="s2">&quot;In this case, env values of &#39;MASTER_PORT&#39;, &#39;MASTER_ADDR&#39;, &#39;WORLD_SIZE&#39;, and &#39;RANK&#39; are referred in &quot;</span>
            <span class="s2">&quot;the command line.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--world_size&quot;</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of nodes for model distribution. &quot;</span>
            <span class="s2">&quot;This argument is fixed to 1. Currently, we don&#39;t recommend you to modify its value.&quot;</span>
            <span class="s2">&quot;If you want to conduct multi-node model distribution, please give `world_size` by `WORLD_SIZE=XXX` &quot;</span>
            <span class="s2">&quot;in your terminal (Note: multi-node model distribution is still in beta).&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--rank&quot;</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The global rank of the current node for model distribution. &quot;</span>
            <span class="s2">&quot;This argument is fixed to 0. Currently, we don&#39;t recommend you to modify its value.&quot;</span>
            <span class="s2">&quot;If you want to conduct multi-node model distribution, please give `rank` by `RANK=XXX` in your &quot;</span>
            <span class="s2">&quot;terminal (Note: multi-node model distribution is still in beta).&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--ngpu&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of GPUs used to run your experiment. &quot;</span>
            <span class="s2">&quot;If ngpu is larger than 1, multi-GPU model distribution will be activated. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--gpus&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2none</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This argument specifies the GPUs used to run your experiment. &quot;</span>
            <span class="s2">&quot;If you want to specify multiple GPUs, please give this argument in the form of &#39;x,x,x&#39; &quot;</span>
            <span class="s2">&quot;where different GPUs are separated by a comma (please don&#39;t end this argument with &#39;,&#39;). &quot;</span>
            <span class="s2">&quot;Of course, you could also specify your target GPUs by `CUDA_VISIBLE_DEVICES` in the terminal.&quot;</span>
            <span class="s2">&quot;If this argument is not given, the framework will automatically select `ngpu` idle GPUs. &quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--same_proc_seed&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to set the same initial random seed for all the GPU processes in DDP mode. &quot;</span>
            <span class="s2">&quot;The different random seeds can prevent model distribution from the process homogeneity, &quot;</span>
            <span class="s2">&quot;e.g., different GPU processes may have the same on-the-fly data augmentation strategy &quot;</span>
            <span class="s2">&quot;(noise addition, SpecAugment, ...) if they have the same initial random seed. &quot;</span>
            <span class="s2">&quot;Note: please set this argument to True if you want to use random data selection for your dataloaders &quot;</span>
            <span class="s2">&quot;in the DDP mode. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--ignore_train_exception&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to ignore the exceptions happening during training and validation. &quot;</span>
            <span class="s2">&quot;If set to True, your training would not be interrupted by some nonfatal errors, such as occasional &quot;</span>
            <span class="s2">&quot;&#39;RuntimeError: CUDA Out of memory&#39;, and etc. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--ignore_test_exception&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to ignore the exceptions happening during testing. &quot;</span>
            <span class="s2">&quot;If set to True, your testing would not be interrupted by some nonfatal errors, such as occasional &quot;</span>
            <span class="s2">&quot;&#39;RuntimeError: CUDA Out of memory&#39;, and etc. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--enable_syncbatchnorm&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to process the model by &#39;torch.nn.SyncBatchNorm.convert_sync_batchnorm&#39; for multi-GPU &quot;</span>
            <span class="s2">&quot;distributed training. Sometimes your training may be stuck at some points or terminate without being &quot;</span>
            <span class="s2">&quot;notified of any errors in the multi-GPU distributed mode. If that happens, you can disable &quot;</span>
            <span class="s2">&quot;SyncBatchNorm and debug your codes. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Training monitoring</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 4: Model Training&quot;</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--train_result_path&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Where to place all the experiment folder that contains all the result files. &quot;</span>
            <span class="s2">&quot;If not given, `train_result_path` wil be automatically initialized by your input `config`. &quot;</span>
            <span class="s2">&quot;For example, if your input `config` is &quot;</span>
            <span class="s2">&quot;</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp_cfg/XXXXX.yaml, your `train_result_path` &quot;</span>
            <span class="s2">&quot;will be automatically initialized to `</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp/`.&quot;</span>
            <span class="s2">&quot;(default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--attach_config_folder_to_path&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to attach an additional folder named by your input `--config` at the end of your input &quot;</span>
            <span class="s2">&quot;`train_result_path`. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--train&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to go through the model training branch. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--dry_run&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to turn on the dry-running mode. &quot;</span>
            <span class="s2">&quot;In this mode, only the data loading will be done to see its speed and robustness. &quot;</span>
            <span class="s2">&quot;Model calculation and parameter optimization will be skipped. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--no_optim&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to turn on the no-optimization mode. &quot;</span>
            <span class="s2">&quot;In this mode, only the data loading and model calculation will be done to see their speed, &quot;</span>
            <span class="s2">&quot;robustness, and memory consumption. (default: False) &quot;</span>
            <span class="s2">&quot;(Note: &#39;dry_run&#39; has the higher priority than &#39;no_optim&#39;. It means that the model calculation will &quot;</span>
            <span class="s2">&quot;be skipped if you give both &#39;--dry_run True&#39; and &#39;--no_optim True&#39;.) &quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--resume&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to resume your model training or testing experiment from the checkpoints. &quot;</span>
            <span class="s2">&quot;If True, there must be .pth checkpoint files of your existing experiment in `train_result_path` or &quot;</span>
            <span class="s2">&quot;`test_result_path`. This argument is shared by the training and testing branches. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--start_epoch&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The starting epoch of your experiments. This argument will be automatically initialized by your &quot;</span>
            <span class="s2">&quot;checkpoint files if `--resume` is given. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--num_epochs&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of training epochs of your experiments. (default: 1000)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--valid_per_epochs&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The interval of going through the validation phase during training. &quot;</span>
            <span class="s2">&quot;If not given, validation will be done right after parameter optimization in each epoch. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--report_per_steps&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The interval of reporting step information logs during model training or testing. &quot;</span>
            <span class="s2">&quot;Positive integers mean the absolute reporting intervals that a step report will be made after each &quot;</span>
            <span class="s2">&quot;&#39;report_per_steps&#39; steps; &quot;</span>
            <span class="s2">&quot;Negative integers mean the relative reporting intervals that there will be -&#39;report_per_steps&#39; &quot;</span>
            <span class="s2">&quot;reports in each epoch. &quot;</span>
            <span class="s2">&quot;If not given, there will be default 10 reports in each epoch. &quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--best_model_selection&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The ways of selecting the best models. This argument should be given as a list of quad-tuples, i.e., &quot;</span>
            <span class="s2">&quot;(&#39;metric_group&#39;, &#39;metric_name&#39;, &#39;metric_mode&#39;, &#39;model_number&#39;). &quot;</span>
            <span class="s2">&quot;&#39;metric_group&#39; can be either &#39;train&#39; or &#39;valid&#39; which indicates the group the metric belongs to; &quot;</span>
            <span class="s2">&quot;&#39;metric_name&#39; is the name of the metric you select; &quot;</span>
            <span class="s2">&quot;&#39;metric_mode&#39; can be either &#39;min&#39; or &#39;max&#39; which indicates how to select the models by this metric; &quot;</span>
            <span class="s2">&quot;&#39;model_number&#39; indicates how many best models will be saved by this metric. &quot;</span>
            <span class="s2">&quot;Note: the metric of the first tuple in the list will be used to do early-stopping for model training.&quot;</span>
            <span class="s2">&quot;(default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--early_stopping_patience&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of epochs when the model doesn&#39;t improve its performance before stopping the &quot;</span>
            <span class="s2">&quot;model training. If not given, early-stopping will not be adapted. (default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--early_stopping_threshold&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The threshold to refresh the early-stopping status in the monitor during model training. &quot;</span>
            <span class="s2">&quot;Positive float numbers in (0.0, 1.0) mean the relative threshold over the current best performance. &quot;</span>
            <span class="s2">&quot;Negative float numbers main the absolute threshold over the current best performance. &quot;</span>
            <span class="s2">&quot;early_stopping_threshold=0 means no early-stopping threshold is applied to the current best &quot;</span>
            <span class="s2">&quot;performance when deciding whether to refresh the status. (default: 0.005)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--last_model_number&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of models saved for the last several epochs. &quot;</span>
            <span class="s2">&quot;This argument cannot be lower than 1 otherwise the training will not be able to resume. &quot;</span>
            <span class="s2">&quot;(default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Training Snapshotting</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
            <span class="s2">&quot;Group 5: Real-time Model Visualization Snapshotting&quot;</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--monitor_snapshot_conf&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The configuration given to `matploblib.plot()` in `{SPEECHAIN_ROOT/speechain/snapshooter.py}` to &quot;</span>
            <span class="s2">&quot;plot curve figures for real-time model visualization during model training. &quot;</span>
            <span class="s2">&quot;This argument should be given in the form of a Dict. (default: an empty Dict)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--visual_snapshot_number&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of the validation data instances used to make snapshots made during model visualization. &quot;</span>
            <span class="s2">&quot;This argument should be smaller than the number of your validation data instances. &quot;</span>
            <span class="s2">&quot;(default: 0)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--visual_snapshot_interval&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The snapshotting interval of model visualization during model training. &quot;</span>
            <span class="s2">&quot;This argument should be a positive integer which means that model visualization will be done once &quot;</span>
            <span class="s2">&quot;in every `visual_snapshot_interval` epochs. (default: 5)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Testing</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 6: Model Testing&quot;</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--test_result_path&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Where to place all the result files generated during model testing. &quot;</span>
            <span class="s2">&quot;If not given, `test_result_path` wil be automatically initialized by your input `train_result_path` &quot;</span>
            <span class="s2">&quot;and `test_model`. For example, if your `train_result_path` is &quot;</span>
            <span class="s2">&quot;`</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp`, and `test_model` is `MMMMM`, &quot;</span>
            <span class="s2">&quot;then your `test_result_path` will be automatically initialized to &quot;</span>
            <span class="s2">&quot;`</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp/XXXXX/MMMMM/` where &#39;XXXXX&#39; is the name of &quot;</span>
            <span class="s2">&quot;your configuration file given by `--config`.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--test&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to go through the model testing branch. (default: False)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--test_model&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The names of the model you want to evaluate during model testing. &quot;</span>
            <span class="s2">&quot;If given, `</span><span class="si">{train_result_path}</span><span class="s2">/XXXXX/model/</span><span class="si">{test_model}</span><span class="s2">.pth` will be used to initialize the parameters &quot;</span>
            <span class="s2">&quot;of the Model object. If you only want to evaluate multiple models in one job, please give the &quot;</span>
            <span class="s2">&quot;strings of their names in a List. (default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--attach_model_folder_when_test&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to attach an additional sub-folder named by your input `--test_model` in the testing &quot;</span>
            <span class="s2">&quot;result folder. (default: True)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--bad_cases_selection&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The selection methods of the top-N bad cases during model testing. &quot;</span>
            <span class="s2">&quot;This argument should be given as a list of tri-tuples &quot;</span>
            <span class="s2">&quot;(&#39;selection_metric&#39;, &#39;selection_mode&#39;, &#39;case_number&#39;). &quot;</span>
            <span class="s2">&quot;For example, (&#39;wer&#39;, &#39;max&#39;, 50) means 50 testing waveforms with the largest WER will be selected. &quot;</span>
            <span class="s2">&quot;Multiple tuples can be given to present different sets of top-n bad cases. (default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--saving_proc_num&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of daemon processes used to save data generated during testing to the disk. (default: 1)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Experiment configuration</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
            <span class="s2">&quot;Group 7: Experiment .yaml Configuration File&quot;</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--data_cfg&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the configuration file for data loading and batching. &quot;</span>
            <span class="s2">&quot;This argument is required for both model training and testing.&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--train_cfg&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the configuration file for model construction and parameter optimization. &quot;</span>
            <span class="s2">&quot;This argument is required for both model training (both &#39;model&#39; and &#39;optim_sche&#39; need to be given) &quot;</span>
            <span class="s2">&quot;and testing (only &#39;model&#39; needs to be given).&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--infer_cfg&quot;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The configuration file for model inference during model testing. &quot;</span>
            <span class="s2">&quot;This argument is required for model testing.&quot;</span>
            <span class="s2">&quot;For more details about how to give infer_cfg, please refer to the handbook.md. (default: None)&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Add customized arguments if needed</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">add_parse</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">build_iterators</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]</span> <span class="ow">or</span> <span class="n">Iterator</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This static function builds all iterators used in the experiment. The configuration of iterators is given in</span>
<span class="sd">        your specified &#39;data_cfg&#39;.</span>

<span class="sd">        The iterators are returned as a dictionary where the first-level keys indicate different iterator groups:</span>
<span class="sd">        &#39;train&#39;, &#39;valid&#39;, and &#39;test&#39;. The second-level keys in each group indicates the iterators belonging to the</span>
<span class="sd">        group. In the value of each second-level key, there are two third-level keys: &#39;type&#39; and &#39;conf&#39;. &#39;type&#39;</span>
<span class="sd">        indicates the iterator type and &#39;conf&#39; indicates the iterator configuration. For more details, please refer</span>
<span class="sd">        to ./speechain/iterator/README.md</span>

<span class="sd">        Args:</span>
<span class="sd">            data_cfg: Dict</span>
<span class="sd">                The dictionary containing all the information to initialize the iterators</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The arguments of the runner in this experiment.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The dictionary of the iterators of all groups (train, valid, test).</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">recur_iterator_init</span><span class="p">(</span><span class="n">_data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">_dset</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">_data_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
                <span class="n">iterator_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span><span class="s2">&quot;speechain.iterator.&quot;</span> <span class="o">+</span> <span class="n">_data_cfg</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">])</span>
                <span class="k">return</span> <span class="n">iterator_class</span><span class="p">(</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
                    <span class="n">ngpu</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span>
                    <span class="n">num_workers</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_dset</span><span class="si">}</span><span class="s2">_num_workers&quot;</span><span class="p">),</span>
                    <span class="n">pin_memory</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
                    <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">_data_cfg</span><span class="p">[</span><span class="s2">&quot;conf&quot;</span><span class="p">],</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="n">key</span><span class="p">:</span> <span class="n">recur_iterator_init</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_dset</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>

        <span class="k">def</span> <span class="nf">recur_batch_num_init</span><span class="p">(</span><span class="n">_iterators</span><span class="p">:</span> <span class="n">Dict</span> <span class="ow">or</span> <span class="n">Iterator</span><span class="p">):</span>
            <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_iterators</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">_iterators</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sub_leaf_flag</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                    <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
                <span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">_iterators</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">sub_leaf_flag</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">{</span>
                        <span class="n">key</span><span class="p">:</span> <span class="n">recur_batch_num_init</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>

        <span class="k">def</span> <span class="nf">flatten_dict_to_list</span><span class="p">(</span><span class="n">_input</span><span class="p">:</span> <span class="n">Dict</span> <span class="ow">or</span> <span class="nb">int</span> <span class="ow">or</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
            <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_input</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">_input</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_input</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">_input</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_output</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_input</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">_output</span> <span class="o">+=</span> <span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">_output</span>

        <span class="c1"># get the target groups of the current experiment</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;valid&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;If args.train is set to True, please give &#39;train&#39; and &#39;valid&#39; as first-level keys of data_cfg.&quot;</span>
            <span class="n">dset_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;If args.test is set to True, please give &#39;test&#39; as first-level keys of data_cfg.&quot;</span>
            <span class="n">dset_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please set either args.train or args.test to True!&quot;</span><span class="p">)</span>

        <span class="c1"># recursively initialize all the iterators in the Dict</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="k">else</span> <span class="s2">&quot;test&quot;</span>
        <span class="n">iterators</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">dset</span><span class="p">:</span> <span class="n">recur_iterator_init</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">[</span><span class="n">dset</span><span class="p">],</span> <span class="n">dset</span><span class="p">)</span> <span class="k">for</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">dset_keys</span>
        <span class="p">}</span>
        <span class="n">batch_nums</span> <span class="o">=</span> <span class="n">recur_batch_num_init</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="n">mode</span><span class="p">])</span>

        <span class="c1"># set the relative reporting interval during training or testing</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_reports_per_epoch</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">10</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">min</span><span class="p">(</span><span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">))</span> <span class="o">//</span> <span class="n">_reports_per_epoch</span>
            <span class="p">)</span>
        <span class="c1"># check the absolute reporting interval during training and testing</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">)</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;If args.report_per_steps is given as a positive integer, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it should be smaller than the minimal </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> batch number (</span><span class="si">{</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">)</span><span class="si">}</span><span class="s2">). &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;But got report_per_steps=</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>

            <span class="c1"># in case that report_per_steps is given as a float number</span>
            <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">iterators</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Model</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This static function builds the model used in the experiment. The configuration of the model is given in</span>
<span class="sd">        the value of the &#39;model&#39; key in your specified &#39;model_cfg&#39;.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_cfg: Dict</span>
<span class="sd">                Model Configuration</span>
<span class="sd">            args:</span>
<span class="sd">            device:</span>

<span class="sd">        Returns:</span>
<span class="sd">            The target Model object initialized by your given configuration</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;model_type&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="s2">&quot;Please specify the model_type!&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;module_conf&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="s2">&quot;Please specify the module_conf!&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;criterion_conf&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;criterion_conf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">model_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span><span class="s2">&quot;speechain.model.&quot;</span> <span class="o">+</span> <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">model_class</span><span class="p">(</span>
            <span class="n">model_conf</span><span class="o">=</span><span class="p">(</span>
                <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;model_conf&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;model_conf&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="n">module_conf</span><span class="o">=</span><span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;module_conf&quot;</span><span class="p">],</span>
            <span class="n">criterion_conf</span><span class="o">=</span><span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;criterion_conf&quot;</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">result_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
            <span class="n">non_blocking</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">non_blocking</span><span class="p">,</span>
            <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">build_optim_sches</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span> <span class="n">optim_sche_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OptimScheduler</span><span class="p">]</span> <span class="ow">or</span> <span class="n">OptimScheduler</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This static function builds the OptimSchedulers used in the pipeline. The configuration of the</span>
<span class="sd">        OptimSchedulers is given in the value of &#39;optim_sches&#39; key in your specified &#39;train_cfg&#39;.</span>

<span class="sd">        This function must be done after DDP wrapping because we need to make sure that the model parameters received</span>
<span class="sd">        by the optimizer in each process are identical. With the identical model parameters, it&#39;s safe to consider that</span>
<span class="sd">        the optimizer parameters are also identical.</span>

<span class="sd">        Args:</span>
<span class="sd">            model: Model</span>
<span class="sd">                The initialized model.</span>
<span class="sd">            optim_sche_cfg: Dict</span>
<span class="sd">                OptimScheduler Configuration</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The Dict of the initialized OptimSchedulers.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># single-optimizer scenario</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optim_sche_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">optim_sche_cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">main</span><span class="o">=</span><span class="n">optim_sche_cfg</span><span class="p">)</span>

        <span class="n">optim_sches</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">optim_sche_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span>
                <span class="s2">&quot;speechain.optim_sche.&quot;</span> <span class="o">+</span> <span class="n">optim_sche</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">optim_sches</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optim_sche_class</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
                <span class="n">use_amp</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_amp</span><span class="p">,</span>
                <span class="n">accum_grad</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">accum_grad</span><span class="p">,</span>
                <span class="n">ft_factor</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ft_factor</span><span class="p">,</span>
                <span class="n">grad_clip</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">,</span>
                <span class="n">grad_norm_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">grad_norm_type</span><span class="p">,</span>
                <span class="o">**</span><span class="n">optim_sche</span><span class="p">[</span><span class="s2">&quot;conf&quot;</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="c1"># multi-optimizer scenario</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optim_sches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># adjust whether there are parameter overlapping among updated_modules of all the OptimSchedulers</span>
            <span class="n">is_all_para</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">updated_modules</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="c1"># updated_modules of all the OptimSchedulers cannot be None at the same time</span>
            <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">is_all_para</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">is_all_para</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># collect the updated_modules of all the OptimScheduler</span>
                <span class="n">para_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">updated_modules</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
                <span class="c1"># adjust whether there are redundant keys</span>
                <span class="n">para_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">para_list</span><span class="p">)</span>
                <span class="c1"># there is parameter overlapping if there are redundant keys</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">para_set</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">para_list</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span>

        <span class="c1"># resuming from an existing checkpoint</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                    <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">optim_sches</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optim_sches&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;No checkpoint is found in </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;The training process will start from scratch.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">optim_sches</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">resume</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span> <span class="n">monitor</span><span class="p">:</span> <span class="n">TrainValidMonitor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        load the model parameters to the current process. This operation is necessary in our toolkit because we need to</span>
<span class="sd">        make sure that the models in all the processes have the same buffer and parameter tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The input arguments.</span>
<span class="sd">            model: Model</span>
<span class="sd">                The model to be trained.</span>
<span class="sd">            monitor: TrainValidMonitor</span>
<span class="sd">                The train-valid monitor used to monitor the training phase</span>

<span class="sd">        Returns:</span>
<span class="sd">            The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting</span>
<span class="sd">            epoch will be loaded from the checkpoint; otherwise, 1 will be returned.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># start the training from the existing checkpoint</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
            <span class="c1"># load the existing checkpoint</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># load the latest training epoch</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;start_epoch&quot;</span><span class="p">]</span>
            <span class="c1"># for compatibility with old versions</span>
            <span class="k">if</span> <span class="s2">&quot;latest_model&quot;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;latest_model&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">,</span> <span class="s2">&quot;latest.pth&quot;</span><span class="p">),</span>
                        <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># loading the monitor</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># for compatibility with old versions</span>
                <span class="k">if</span> <span class="s2">&quot;monitor&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                        <span class="nb">dict</span><span class="p">(</span>
                            <span class="n">train_monitor</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;train_monitor&quot;</span><span class="p">],</span>
                            <span class="n">valid_monitor</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;valid_monitor&quot;</span><span class="p">],</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;monitor&quot;</span><span class="p">])</span>
                <span class="c1"># info logging</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The training process resumes from the epoch no.</span><span class="si">{</span><span class="n">start_epoch</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># start the training from scratch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">start_epoch</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">dict_transform</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">src_dict</span><span class="p">,</span> <span class="n">transform_func</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            src_dict:</span>
<span class="sd">            transform_func:</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Multi-dataloader</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">src_dict</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">transform_func</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">src_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="c1"># Single-dataloader</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">transform_func</span><span class="p">(</span><span class="n">src_dict</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">measure_time</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">monitor</span><span class="p">:</span> <span class="n">Monitor</span><span class="p">):</span>
        <span class="nd">@contextmanager</span>
        <span class="k">def</span> <span class="nf">empty_context</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">yield</span>

        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">empty_context</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">monitor</span><span class="o">.</span><span class="n">measure_time</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">is_empty_batch</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">input_batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="c1"># Single-dataloader case</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="c1"># Multi-dataloader case</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">sub_value</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sub_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="k">return</span> <span class="kc">True</span>
                    <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span>
        <span class="n">data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="n">iterators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]]</span> <span class="ow">or</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span>
        <span class="n">optim_sches</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OptimScheduler</span><span class="p">]</span> <span class="ow">or</span> <span class="n">OptimScheduler</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">,</span>
        <span class="n">monitor</span><span class="p">:</span> <span class="n">TrainValidMonitor</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The input arguments.</span>
<span class="sd">            data_cfg: Dict</span>
<span class="sd">                The data loading configuration. Used to initialize the iterator for model visualization.</span>
<span class="sd">            iterators: Dict</span>
<span class="sd">                The dictionary that contains all the iterators for training and validation.</span>
<span class="sd">            model: Model</span>
<span class="sd">                The model to be trained.</span>
<span class="sd">            optim_sches: Dict</span>
<span class="sd">                The dictionary that contains all the OptimSchedulers used to update the model parameters.</span>
<span class="sd">            logger:</span>

<span class="sd">            monitor: TrainValidMonitor</span>
<span class="sd">                The wrapper class for a training monitor and a validation monitor.</span>
<span class="sd">                The training monitor controls the training process of the model and generates the real-time logging</span>
<span class="sd">                information.</span>
<span class="sd">                The validation monitor controls the validation process of the model and generates the real-time</span>
<span class="sd">                logging information.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">&lt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span>
        <span class="p">),</span> <span class="s2">&quot;Your given start_epoch is larger than your given num_epochs!&quot;</span>

        <span class="c1"># --- checking the data lengths of all training iterators --- #</span>
        <span class="c1"># multiple dataloaders scenario</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="n">train_batch_nums</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="p">)</span>
            <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">train_batch_nums</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_batch_nums</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Your training iterators have different batch numbers: </span><span class="si">{</span><span class="n">train_batch_nums</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;The actual batch number during training is set to </span><span class="si">{</span><span class="n">min_train_batch_num</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>
        <span class="c1"># single dataloader scenario</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">Iterator</span><span class="p">):</span>
            <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please don&#39;t nest data_cfg[&#39;train&#39;] more than twice!&quot;</span><span class="p">)</span>

        <span class="c1"># --- checking the data lengths of all validation iterators --- #</span>
        <span class="c1"># multiple dataloaders scenario</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="n">valid_batch_nums</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="p">)</span>
            <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">valid_batch_nums</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_batch_nums</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Your validation iterators have different batch numbers: </span><span class="si">{</span><span class="n">valid_batch_nums</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;The actual batch number during validation is set to </span><span class="si">{</span><span class="n">min_valid_batch_num</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>
        <span class="c1"># single dataloader scenario</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="n">Iterator</span><span class="p">):</span>
            <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please don&#39;t nest data_cfg[&#39;valid&#39;] more than twice!&quot;</span><span class="p">)</span>

        <span class="c1"># synchronize the batch numbers across all the distributed processes</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">_world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
            <span class="c1"># make sure that all processes have the same number of training steps</span>
            <span class="n">_all_batch_num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_world_size</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                <span class="n">_all_batch_num</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">min_train_batch_num</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="n">_all_batch_num</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># make sure that all processes have the same number of validation steps</span>
            <span class="n">_all_batch_num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_world_size</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                <span class="n">_all_batch_num</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">min_valid_batch_num</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="n">_all_batch_num</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># --- Initialize the iterator for model visualization --- #</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">_valid_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_valid_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
                    <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">_valid_keys</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">_valid_keys</span>
                <span class="p">):</span>
                    <span class="n">visual_iterator</span> <span class="o">=</span> <span class="n">Iterator</span><span class="p">(</span>
                        <span class="n">dataset_type</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span><span class="s2">&quot;dataset_type&quot;</span><span class="p">],</span>
                        <span class="n">dataset_conf</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span><span class="s2">&quot;dataset_conf&quot;</span><span class="p">],</span>
                        <span class="n">batches_per_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">,</span>
                        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">ngpu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">distributed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">is_descending</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">visual_domain</span> <span class="o">=</span> <span class="n">_valid_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;There are multiple sub-Dict in your given data_cfg[&#39;valid&#39;]. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;The one named </span><span class="si">{</span><span class="n">visual_domain</span><span class="si">}</span><span class="s2"> is used to initialize the visualization iterator.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">visual_iterator</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">visual_domain</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">(</span>
                            <span class="n">dataset_type</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="n">visual_domain</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span>
                                <span class="s2">&quot;dataset_type&quot;</span>
                            <span class="p">],</span>
                            <span class="n">dataset_conf</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="n">visual_domain</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span>
                                <span class="s2">&quot;dataset_conf&quot;</span>
                            <span class="p">],</span>
                            <span class="n">batches_per_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">,</span>
                            <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">ngpu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">distributed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">is_descending</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">visual_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">visual_iterator</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># loop each epoch until the end</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># update the random seeds for the current epoch to keep in line with the dataloaders</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">set_random_seeds</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">epoch</span><span class="p">)</span>
            <span class="c1"># start the current training epoch</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">start_train_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="c1"># initialize all the training dataloaders</span>
            <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="c1"># --- Training Stage --- #</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="c1"># loop all the training batches</span>
            <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_train_batch_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">step_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">min_train_batch_num</span><span class="p">)</span>

                <span class="c1"># --- data loading part --- #</span>
                <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                    <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
                <span class="p">)(</span><span class="s2">&quot;data_load_time&quot;</span><span class="p">):</span>
                    <span class="n">train_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                        <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                    <span class="p">)</span>
                    <span class="c1"># single-GPU case, directly skip the current step when meeting an empty batch</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                        <span class="c1"># skip the empty validation batch</span>
                        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">):</span>
                            <span class="k">continue</span>

                    <span class="c1"># multi-GPU case, scatter the skip flag to all nodes</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">skip_flag_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
                            <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">):</span>
                            <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        <span class="c1"># as long as one node meets an empty batch, all nodes will simultaneously skip the current step</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                            <span class="n">skip_flag_list</span><span class="p">,</span> <span class="n">skip_flag</span>
                        <span class="p">)</span>
                        <span class="k">if</span> <span class="n">skip_flag_list</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                            <span class="k">continue</span>

                <span class="c1"># forward the batch to get the training criteria and optimize the model</span>
                <span class="n">train_metrics</span><span class="p">,</span> <span class="n">optim_lr</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
                <span class="c1"># whether to skip the model forward part and model optimization part</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span><span class="p">:</span>
                    <span class="c1"># --- model forward part --- #</span>
                    <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_amp</span><span class="p">):</span>
                        <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                            <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
                        <span class="p">)(</span><span class="s2">&quot;model_forward_time&quot;</span><span class="p">):</span>
                            <span class="k">try</span><span class="p">:</span>
                                <span class="n">losses</span><span class="p">,</span> <span class="n">train_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                                    <span class="n">batch_data</span><span class="o">=</span><span class="n">train_batch</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span>
                                <span class="p">)</span>
                            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_train_exception</span><span class="p">:</span>
                                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                        <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                        <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> training step will be skipped!&quot;</span>
                                    <span class="p">)</span>
                                    <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                            <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                            <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> training step will be skipped!&quot;</span>
                                        <span class="p">)</span>
                                    <span class="k">continue</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="k">raise</span> <span class="n">e</span>

                    <span class="c1"># whether to skip the model optimization part</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">no_optim</span><span class="p">:</span>
                        <span class="c1"># --- loss backward and optimization part --- #</span>
                        <span class="n">optim_lr</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
                        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="n">optim_sche</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
                                <span class="n">losses</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
                                <span class="n">time_func</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                                    <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
                                <span class="p">),</span>
                                <span class="n">optim_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                                <span class="n">step_num</span><span class="o">=</span><span class="n">step_num</span><span class="p">,</span>
                                <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
                                <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
                            <span class="p">)</span>
                            <span class="n">optim_lr</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optim_sche</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>

                <span class="c1"># log the information of the current training step</span>
                <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span>
                        <span class="n">step_num</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">optim_lr</span><span class="o">=</span><span class="n">optim_lr</span><span class="p">,</span> <span class="n">train_metrics</span><span class="o">=</span><span class="n">train_metrics</span>
                    <span class="p">)</span>

            <span class="c1"># finish the current training epoch</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">finish_train_epoch</span><span class="p">()</span>

            <span class="c1"># --- Validation Stage --- #</span>
            <span class="c1"># start the validation part of the current epoch</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">start_valid_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

            <span class="n">valid_flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">valid_per_epochs</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">valid_flag</span><span class="p">:</span>
                <span class="c1"># initialize all the validation dataloaders</span>
                <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                    <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
                <span class="p">)</span>

                <span class="c1"># make sure that no gradient appears during validation</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                    <span class="c1"># loop all validation batches</span>
                    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_valid_batch_num</span><span class="p">):</span>
                        <span class="c1"># --- data loading part --- #</span>
                        <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                            <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">valid_monitor</span>
                        <span class="p">)(</span><span class="s2">&quot;data_load_time&quot;</span><span class="p">):</span>
                            <span class="n">valid_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                                <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                            <span class="p">)</span>
                            <span class="c1"># single-GPU case, directly skip the current step when meeting an empty batch</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                                <span class="c1"># skip the empty validation batch</span>
                                <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">valid_batch</span><span class="p">):</span>
                                    <span class="k">continue</span>
                            <span class="c1"># multi-GPU case, scatter the skip flag to all nodes</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">skip_flag_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
                                    <span class="p">[</span>
                                        <span class="kc">False</span>
                                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
                                            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
                                        <span class="p">)</span>
                                    <span class="p">]</span>
                                <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                                <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">valid_batch</span><span class="p">):</span>
                                    <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                                        <span class="n">model</span><span class="o">.</span><span class="n">device</span>
                                    <span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                                        <span class="n">model</span><span class="o">.</span><span class="n">device</span>
                                    <span class="p">)</span>
                                <span class="c1"># as long as one node meets an empty batch,</span>
                                <span class="c1"># all nodes will skip the current step at the same time</span>
                                <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                                    <span class="n">skip_flag_list</span><span class="p">,</span> <span class="n">skip_flag</span>
                                <span class="p">)</span>
                                <span class="k">if</span> <span class="n">skip_flag_list</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                                    <span class="k">continue</span>

                        <span class="c1"># forward the batch to get the validation criteria</span>
                        <span class="n">valid_metrics</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="c1"># whether to skip the model forward part</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span><span class="p">:</span>
                            <span class="c1"># --- model forward part --- #</span>
                            <span class="c1"># with autocast(enabled=args.use_amp) is not used here for accurate validation</span>
                            <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                                <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">valid_monitor</span>
                            <span class="p">)(</span><span class="s2">&quot;model_forward_time&quot;</span><span class="p">):</span>
                                <span class="k">try</span><span class="p">:</span>
                                    <span class="n">valid_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_data</span><span class="o">=</span><span class="n">valid_batch</span><span class="p">)</span>
                                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                                    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_train_exception</span><span class="p">:</span>
                                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                            <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                            <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> validation step will be skipped!&quot;</span>
                                        <span class="p">)</span>
                                        <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                                <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                                <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> validation step will be skipped!&quot;</span>
                                            <span class="p">)</span>
                                        <span class="k">continue</span>
                                    <span class="k">else</span><span class="p">:</span>
                                        <span class="k">raise</span> <span class="n">e</span>

                        <span class="c1"># no step log for the validation step</span>
                        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">monitor</span><span class="o">.</span><span class="n">valid_step</span><span class="p">(</span><span class="n">valid_metrics</span><span class="o">=</span><span class="n">valid_metrics</span><span class="p">)</span>

            <span class="c1"># --- Visualization Stage --- #</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_interval</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="c1"># make sure that all processes go through the validation phase smoothly</span>
                <span class="k">if</span> <span class="n">visual_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">visual_iterator</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                        <span class="n">visual_domain</span> <span class="o">=</span> <span class="kc">None</span>
                        <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="o">.</span><span class="n">build_loader</span><span class="p">()</span>
                        <span class="n">visual_indices</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">visual_domain</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">visual_iterator</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="p">[</span>
                            <span class="n">visual_domain</span>
                        <span class="p">]</span><span class="o">.</span><span class="n">build_loader</span><span class="p">()</span>
                        <span class="n">visual_indices</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="p">[</span>
                            <span class="n">visual_domain</span>
                        <span class="p">]</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>

                    <span class="c1"># make sure that no gradient appears during validation</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                    <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">visual_dataloader</span><span class="p">)</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">):</span>
                            <span class="n">visual_sample</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">visual_dataloader</span><span class="p">)</span>
                            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">visual_sample</span><span class="p">):</span>
                                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;The visual sample </span><span class="si">{</span><span class="n">visual_indices</span><span class="p">[</span><span class="n">step</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> is empty, &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;so its visualization is skipped!&quot;</span>
                                <span class="p">)</span>
                                <span class="k">continue</span>
                            <span class="c1"># feed the current sample to the model</span>
                            <span class="n">monitor</span><span class="o">.</span><span class="n">valid_model_snapshot</span><span class="p">(</span>
                                <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
                                <span class="n">domain</span><span class="o">=</span><span class="n">visual_domain</span><span class="p">,</span>
                                <span class="n">sample_index</span><span class="o">=</span><span class="n">visual_indices</span><span class="p">[</span><span class="n">step</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                <span class="n">used_sample</span><span class="o">=</span><span class="n">visual_sample</span><span class="p">,</span>
                            <span class="p">)</span>
                <span class="c1"># synchronize all the GPU processes at the end of the visualization stage</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

            <span class="c1"># finish_valid_epoch() should be called before checkpoint saving</span>
            <span class="n">finish_valid_flag</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">finish_valid_flag</span> <span class="o">=</span> <span class="n">monitor</span><span class="o">.</span><span class="n">finish_valid_epoch</span><span class="p">(</span>
                    <span class="n">valid_flag</span><span class="o">=</span><span class="n">valid_flag</span><span class="p">,</span> <span class="n">valid_per_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">valid_per_epochs</span>
                <span class="p">)</span>

            <span class="c1"># store the checkpoint of the current epoch for later resuming</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">no_optim</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;start_epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="s2">&quot;latest_model&quot;</span><span class="p">:</span> <span class="p">(</span>
                                <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span>
                                <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                            <span class="p">),</span>
                            <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="n">monitor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                            <span class="s2">&quot;optim_sches&quot;</span><span class="p">:</span> <span class="p">{</span>
                                <span class="n">name</span><span class="p">:</span> <span class="n">o</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                            <span class="p">},</span>
                        <span class="p">},</span>
                        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                    <span class="p">)</span>

            <span class="c1"># early-stopping checking for single-GPU</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">finish_valid_flag</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># early-stopping checking for multi-GPU</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="n">stop_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">flag_list</span> <span class="o">=</span> <span class="kc">None</span>

                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">finish_valid_flag</span><span class="p">:</span>
                        <span class="n">stop_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">flag_list</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">stop_flag</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
                    <span class="p">]</span>

                <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stop_flag</span><span class="p">,</span> <span class="n">flag_list</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">stop_flag</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
                    <span class="k">break</span>

        <span class="c1"># check whether all the monitor queues become empty in every minute</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">wait_empty_queues</span><span class="p">()</span>

        <span class="c1"># synchronize all the GPU processes at the end</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span>
        <span class="n">test_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">iterators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The input arguments.</span>
<span class="sd">            iterators: Dict</span>
<span class="sd">                The dictionary that contains all the iterators for training and validation.</span>
<span class="sd">            test_model: Model</span>
<span class="sd">                The model to be trained.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># parse infer_cfg depending on different situations</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="n">load_yaml</span><span class="p">(</span>
                    <span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">}</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">infer_cfg_dict</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">cfg</span><span class="p">))</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                    <span class="n">cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                    <span class="n">infer_cfg_dict</span><span class="p">[</span>
                        <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">cfg</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="s2">&quot;If infer_cfg is given in the form of a List, &quot;</span>
                        <span class="s2">&quot;it must be either a List[str] or a List[Dict]!&quot;</span>
                    <span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;shared_args&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="ow">and</span> <span class="s2">&quot;exclu_args&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;exclu_args&quot;</span><span class="p">],</span> <span class="n">List</span>
                <span class="p">),</span> <span class="p">(</span>
                    <span class="s2">&quot;If infer_cfg is given by &#39;shared_args&#39; and &#39;exclu_args&#39;, &quot;</span>
                    <span class="s2">&quot;infer_cfg[&#39;shared_args&#39;] must be a Dict and infer_cfg[&#39;exclu_args&#39;] must be a List.&quot;</span>
                <span class="p">)</span>
                <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;exclu_args&quot;</span><span class="p">]:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">),</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">for</span> <span class="n">cfg_key</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">cfg_key</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Find a duplicate argument </span><span class="si">{</span><span class="n">cfg_key</span><span class="si">}</span><span class="s2"> in both &#39;shared_args&#39; and &#39;exclu_args&#39;!&quot;</span>
                            <span class="p">)</span>

                    <span class="n">cfg</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">])</span>
                    <span class="n">cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                    <span class="n">infer_cfg_dict</span><span class="p">[</span>
                        <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">cfg</span>

            <span class="k">elif</span> <span class="p">(</span>
                <span class="s2">&quot;shared_args&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="ow">and</span> <span class="s2">&quot;exclu_args&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">default_inference</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                        <span class="nb">sorted</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="p">)</span>
                    <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                            <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
                        <span class="p">):</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span>
                    <span class="p">}</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;If infer_cfg is given in the form of a Dict, &quot;</span>
                    <span class="s2">&quot;&#39;shared_args&#39; and &#39;exclu_args&#39; must be or not be in the key list at the same time!&quot;</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">default_inference</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;infer_cfg must be given in the form of a string, a List, or a Dict!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># loop each test configuration</span>
        <span class="k">for</span> <span class="n">infer_cfg_name</span><span class="p">,</span> <span class="n">infer_cfg</span> <span class="ow">in</span> <span class="n">infer_cfg_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># configuration-specific result path</span>
            <span class="n">test_result_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span>
                    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="ow">is</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span>
                <span class="p">),</span>
                <span class="n">infer_cfg_name</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># load the existing testing configuration for resuming</span>
            <span class="n">infer_cfg_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="s2">&quot;infer_cfg.yaml&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">):</span>
                <span class="n">infer_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">))</span>

            <span class="c1"># save the testing configuration file to infer_cfg_path</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">infer_cfg</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                        <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># unlike training and validation, the testing iterators are looped one by one</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># replace the slash with a percent symbol</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;%&quot;</span><span class="p">)</span>
                <span class="c1"># add the identity symbol to the path for multi-GPU testing</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">attach_model_folder_when_test</span><span class="p">:</span>
                    <span class="n">test_dset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">test_model</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">test_dset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="n">test_rank_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dset_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;rank</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">_tmp&quot;</span><span class="p">)</span>
                <span class="n">logger</span> <span class="o">=</span> <span class="n">logger_stdout_file</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>

                <span class="c1"># initialize top-n bad case presentation</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bad_cases_selection</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="s2">&quot;There is no configuration of topN bad case selection in either your input &quot;</span>
                            <span class="s2">&quot;arguments or default values of your selected model. &quot;</span>
                            <span class="s2">&quot;So there will not be any reports about topN bad cases.&quot;</span>
                        <span class="p">)</span>
                <span class="c1"># the main testing process</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The configuration of topN bad case selection in the current testing process is </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># initialize the testing monitor</span>
                <span class="n">monitor</span> <span class="o">=</span> <span class="n">TestMonitor</span><span class="p">(</span>
                    <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">result_path</span><span class="o">=</span><span class="n">test_dset_path</span>
                <span class="p">)</span>

                <span class="c1"># check the resuming status</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
                    <span class="c1"># loading the existed checkpoint</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">test_checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">)</span>
                        <span class="p">)</span>
                        <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">test_checkpoint</span><span class="p">[</span><span class="s2">&quot;monitor&quot;</span><span class="p">])</span>
                        <span class="n">start_step</span> <span class="o">=</span> <span class="n">test_checkpoint</span><span class="p">[</span><span class="s2">&quot;start_step&quot;</span><span class="p">]</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;The testing process resumes from the step no.</span><span class="si">{</span><span class="n">start_step</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="p">)</span>
                    <span class="c1"># checkpoint does not exist</span>
                    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                        <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;No checkpoint is found in </span><span class="si">{</span><span class="n">test_rank_path</span><span class="si">}</span><span class="s2">. &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;The testing process will start from scratch. &quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;The testing process will start from scratch. &quot;</span><span class="p">)</span>

                <span class="c1"># initialize the dataloaders from the given starting point</span>
                <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                    <span class="n">iterator</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">start_step</span><span class="o">=</span><span class="n">start_step</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="n">test_indices</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                    <span class="n">iterator</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="c1"># if there are multiple dataloaders for the current testing set,</span>
                <span class="c1"># the sample indices of the first element will be used to make the reports</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_indices</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                    <span class="n">test_indices</span> <span class="o">=</span> <span class="n">test_indices</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">test_indices</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]]</span>
                <span class="c1"># report the total number of testing steps needed to be done</span>
                <span class="n">total_step_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_indices</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Totally </span><span class="si">{</span><span class="n">total_step_num</span><span class="si">}</span><span class="s2"> testing steps.&quot;</span><span class="p">)</span>

                <span class="c1"># make sure that no gradient appears during testing</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">(</span><span class="n">total_step_num</span><span class="o">=</span><span class="n">total_step_num</span><span class="p">)</span>
                    <span class="c1"># iterate the testing batches</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step_num</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">start_step</span><span class="p">:</span>
                            <span class="k">continue</span>

                        <span class="c1"># only fetch the testing data right before decoding and evaluation</span>
                        <span class="n">test_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                            <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                        <span class="p">)</span>
                        <span class="c1"># skip the empty testing batch</span>
                        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">test_batch</span><span class="p">):</span>
                            <span class="k">continue</span>
                        <span class="c1"># evaluate the current testing batch and get the evaluation results</span>
                        <span class="k">try</span><span class="p">:</span>
                            <span class="n">test_results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                                <span class="n">test_batch</span><span class="o">=</span><span class="n">test_batch</span><span class="p">,</span> <span class="n">infer_conf</span><span class="o">=</span><span class="n">infer_cfg</span>
                            <span class="p">)</span>
                        <span class="c1"># skip the current step if encounter an error (any kind)</span>
                        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_test_exception</span><span class="p">:</span>
                                <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;0&#39;</span><span class="si">}</span><span class="s2"> meets the error &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2"> at step no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;Indices of the involved testing samples in this step is </span><span class="si">{</span><span class="n">test_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                                <span class="p">)</span>
                                <span class="k">continue</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="n">e</span>
                        <span class="c1"># record evaluation results</span>
                        <span class="n">monitor</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
                            <span class="n">step_num</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="n">test_results</span><span class="o">=</span><span class="n">test_results</span><span class="p">,</span>
                            <span class="n">test_index</span><span class="o">=</span><span class="n">test_indices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="p">)</span>

                        <span class="c1"># reduce the number of IO operations to speed up the testing</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
                        <span class="p">)</span> <span class="o">%</span> <span class="n">monitor</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">total_step_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                            <span class="c1"># save the checkpoint of the current step for both resuming and multi-GPU evaluation</span>
                            <span class="c1"># the iteration conditions of the test dataloader will also be saved for resuming</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                                <span class="nb">dict</span><span class="p">(</span><span class="n">start_step</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()),</span>
                                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                            <span class="p">)</span>

                <span class="c1"># waiting for the data saving daemon process to finish before calling finish_epoch()</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">wait_empty_queues</span><span class="p">()</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># obtain the group information of the current iterator</span>
                    <span class="n">group_info</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">):</span>
                        <span class="c1"># Dict[str, Dict[str, str]]</span>
                        <span class="n">group_info</span> <span class="o">=</span> <span class="n">iterator</span><span class="o">.</span><span class="n">get_group_info</span><span class="p">()</span>
                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                        <span class="c1"># List[Dict[str, Dict[str, str]]]</span>
                        <span class="n">group_info_list</span> <span class="o">=</span> <span class="p">[</span>
                            <span class="n">value</span><span class="o">.</span><span class="n">get_group_info</span><span class="p">()</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">iterator</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                        <span class="p">]</span>
                        <span class="k">for</span> <span class="n">group_dict</span> <span class="ow">in</span> <span class="n">group_info_list</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">group_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="n">group_info</span> <span class="o">=</span> <span class="n">group_dict</span>
                                <span class="k">break</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span>

                    <span class="c1"># finish the evaluation and store the results to the disk</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">finish_epoch</span><span class="p">(</span><span class="n">meta_info</span><span class="o">=</span><span class="n">group_info</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">set_random_seeds</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set random seeds for python environment, numpy environment and torch environment</span>

<span class="sd">        Note:</span>
<span class="sd">            1. torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),</span>
<span class="sd">                so it is not necessary to be included here.</span>
<span class="sd">            2. torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on</span>
<span class="sd">                different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONHASHSEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">gather_all_iter_ascii</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">iterator</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            iterator:</span>
<span class="sd">            device:</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># turn the message into ASCII codes and gather the codes length</span>
        <span class="n">_iter_asc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">iterator</span><span class="p">)])</span>
        <span class="n">_iter_asc_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">_iter_asc</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">_iter_asc_lens</span><span class="p">,</span> <span class="n">_iter_asc_len</span><span class="p">)</span>

        <span class="c1"># padding the ASCII codes to the same length and gather them</span>
        <span class="k">if</span> <span class="n">_iter_asc_len</span> <span class="o">&lt;</span> <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">():</span>
            <span class="n">_iter_asc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">_iter_asc</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                        <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">_iter_asc_len</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">_iter_ascs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(),</span> <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">_iter_asc</span> <span class="k">for</span> <span class="n">_iter_asc</span> <span class="ow">in</span> <span class="n">_iter_ascs</span><span class="p">],</span> <span class="n">_iter_asc_lens</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">main_worker</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">gpu</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The main body of a process on one GPU.</span>

<span class="sd">        Args:</span>
<span class="sd">            gpu:</span>
<span class="sd">            args:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># --- 0. Random Seed Preparation --- #</span>
        <span class="c1"># set different random seeds for the different GPU processes in DDP mode to avoid the process homogeneity</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">same_proc_seed</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="o">+=</span> <span class="n">gpu</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">set_random_seeds</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># --- 1. Experimental Reproducibility Preparation --- #</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_enabled</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_benchmark</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_deterministic</span>
        <span class="c1"># torch.use_deterministic_algorithms(torch.backends.cudnn.deterministic)</span>
        <span class="c1"># For more details about &#39;CUBLAS_WORKSPACE_CONFIG&#39;,</span>
        <span class="c1"># please refer to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility</span>
        <span class="k">if</span> <span class="n">V</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">V</span><span class="p">(</span><span class="s2">&quot;10.2&quot;</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUBLAS_WORKSPACE_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;:4096:8&quot;</span>

        <span class="c1"># --- 2. DDP Model Distribution Initialization --- #</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="c1"># load the global node rank from the os environment in the multi-node setting</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">==</span> <span class="s2">&quot;env://&quot;</span><span class="p">:</span>
                <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Here, the rank is turned from the local node-level rank to the global process-level rank</span>
                <span class="c1"># the input argument &#39;gpu&#39; is the local rank of the current process in the specific node</span>
                <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">+</span> <span class="n">gpu</span>
            <span class="c1"># initialize the distributed environment, connections among all the processes are established here</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
                <span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span>
                <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="p">,</span>
                <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># --- 3. Experimental Environment Logging --- #</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_config_split</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_config_split</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># automatically decide the result path if not given</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">_config_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;If you want to automatically generate train_result_path, please give the configuration by &#39;--config&#39;.&quot;</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">_config_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
                <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;exp&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_config_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
            <span class="p">)</span>
        <span class="c1"># attach a folder named by args.config to the end of your given result path</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">attach_config_folder_to_path</span><span class="p">:</span>
            <span class="c1"># if `--config` is given, attach the name of exp_cfg to the end of train_result_path</span>
            <span class="k">if</span> <span class="n">_config_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_config_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="p">)</span>

        <span class="c1"># initialize the logger and save current script command</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">logger_stdout_file</span><span class="p">(</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
            <span class="s2">&quot;train&quot;</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
            <span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># logging the beginning info of the experiment</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current script command: </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">xi</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">xi</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Multi-GPU distribution information: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;backend=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="si">}</span><span class="s2">, init_method=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;nnode=</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">)</span><span class="si">}</span><span class="s2">, ngpu_per_node=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;used_gpus=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># initialize the computational equipments</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;CUDA is not available! It fails to conduct GPU training.&quot;</span>
        <span class="c1"># args.gpu is the GPU used in the current process while args.gpus are all the available GPUss</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpu</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="n">gpu</span><span class="p">]</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Used GPU in the master process: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># --- 4. Configuration Loading --- #</span>
        <span class="c1"># resume from an existing checkpoint, loading the old data and train configurations</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
            <span class="c1"># loading the existing data and train configurations</span>
            <span class="c1"># But the input data configuration has higher priority than the existing one</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data_cfg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">))</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                    <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span>
                    <span class="nb">open</span><span class="p">(</span>
                        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                            <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;train&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;test&#39;</span><span class="si">}</span><span class="s2">_data_cfg.yaml&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="c1"># training configuration will be loaded from the existing file</span>
            <span class="n">train_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span>
                <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;train_cfg.yaml&quot;</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="c1"># start from scratch, loading the new data and train configurations</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;Please specify a data configuration file and a train configuration file!&quot;</span>
            <span class="n">data_cfg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">))</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span>
            <span class="p">)</span>
            <span class="n">train_cfg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">))</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span>
            <span class="p">)</span>

        <span class="c1"># --- 5. Data Iterator Initialization --- #</span>
        <span class="n">iterators</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_iterators</span><span class="p">(</span><span class="n">data_cfg</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

        <span class="c1"># logging the information of the iterators</span>
        <span class="n">_iter_message</span> <span class="o">=</span> <span class="s2">&quot;The information of the iterators:&quot;</span>
        <span class="k">for</span> <span class="n">dset</span><span class="p">,</span> <span class="n">iters</span> <span class="ow">in</span> <span class="n">iterators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># single iterator for the current dataset</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">):</span>
                <span class="c1"># gather the iterator message from all the process in the multi-GPU distributed training mode</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                    <span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">gather_all_iter_ascii</span><span class="p">(</span>
                        <span class="n">iters</span><span class="p">,</span> <span class="n">device</span>
                    <span class="p">)</span>

                    <span class="c1"># recover the codes from all the processes back to the text</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">asc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">):</span>
                        <span class="n">_iter_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                            <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">asc</span><span class="p">[:</span> <span class="n">_iter_asc_lens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
                        <span class="p">)</span>
                        <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set of the rank no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">_iter_text</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="c1"># directly report the message in the single-GPU mode</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set: </span><span class="si">{</span><span class="n">iters</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># multiple iterators for the current dataset</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="c1"># gather the iterator message from all the process in the multi-GPU distributed training mode</span>
                    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                        <span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">gather_all_iter_ascii</span><span class="p">(</span>
                            <span class="n">iterator</span><span class="p">,</span> <span class="n">device</span>
                        <span class="p">)</span>

                        <span class="c1"># recover the codes from all the processes back to the text</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">asc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">):</span>
                            <span class="n">_iter_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                                <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">asc</span><span class="p">[:</span> <span class="n">_iter_asc_lens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
                            <span class="p">)</span>
                            <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set of the rank no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">_iter_text</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="c1"># directly report the message in the single-GPU mode</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">_iter_message</span> <span class="o">+=</span> <span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set: </span><span class="si">{</span><span class="n">iterator</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">_iter_message</span><span class="p">)</span>

        <span class="c1"># --- 6. Model Initialization --- #</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">train_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Please fill in the &#39;model&#39; tag of your given train_cfg!&quot;</span>
        <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_model</span><span class="p">(</span><span class="n">train_cfg</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">model_summary</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>

        <span class="c1"># for the process of single-GPU training or the rank 0 process of multi-GPUs training</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># dumping all the configuration files into train_result_path for resuming</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;exp_cfg.yaml&quot;</span><span class="p">),</span>
                <span class="s2">&quot;w&quot;</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;train&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;test&#39;</span><span class="si">}</span><span class="s2">_data_cfg.yaml&quot;</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="s2">&quot;w&quot;</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;train_cfg.yaml&quot;</span><span class="p">),</span>
                <span class="s2">&quot;w&quot;</span><span class="p">,</span>
                <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">train_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># --- 7.1. Model Training Branch --- #</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
            <span class="c1"># initialize the Monitor for training and validation</span>
            <span class="n">monitor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="kc">None</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span>
                <span class="k">else</span> <span class="n">TrainValidMonitor</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># loading the model from the existing checkpoint for resuming the training process</span>
            <span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">resume</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">)</span>

            <span class="c1"># DDP Wrapping of the model must be done after model checkpoint loading</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">enable_syncbatchnorm</span><span class="p">:</span>
                    <span class="c1"># turn the batchnorm layers into the sync counterparts</span>
                    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
                <span class="c1"># Here the model buffers and parameters of the master process are broadcast to the other processes</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span>
                <span class="p">)</span>

            <span class="c1"># initialize the OptimSchedulers after DDP wrapping (including optimization resuming)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="s2">&quot;optim_sches&quot;</span> <span class="ow">in</span> <span class="n">train_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;Please fill in the &#39;optim_sches&#39; tag!&quot;</span>
            <span class="n">optim_sches</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_optim_sches</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_sche_cfg</span><span class="o">=</span><span class="n">train_cfg</span><span class="p">[</span><span class="s2">&quot;optim_sches&quot;</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span>
            <span class="p">)</span>

            <span class="c1"># logging the information of the optimschedulers</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> OptimScheduler: </span><span class="si">{</span><span class="n">optim_sche</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># start the training process</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
                <span class="n">data_cfg</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">,</span>
                <span class="n">iterators</span><span class="o">=</span><span class="n">iterators</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">optim_sches</span><span class="o">=</span><span class="n">optim_sches</span><span class="p">,</span>
                <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
                <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># --- 7.2. Model Testing Branch --- #</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">test_model</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">]</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>

            <span class="c1"># loop each model to be tested</span>
            <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">:</span>
                <span class="c1"># get the path of the target model parameters</span>
                <span class="n">_models_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">)</span>
                <span class="c1"># for compatibility with the older version</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.mdl&quot;</span><span class="p">)):</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.mdl&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">)):</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.pth&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">model_name</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not found!&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># load the target model parameters</span>
                <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

                <span class="c1"># start the testing process</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">test</span><span class="p">(</span>
                    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">test_model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">iterators</span><span class="o">=</span><span class="n">iterators</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;train and test in args cannot be False at the same time!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># --- 8. Release Computational Resource --- #</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The beginning of a experiment branch (training or testing).</span>
<span class="sd">        This function decides the single-GPU or multi-GPU training sub-branch.</span>

<span class="sd">        Args:</span>
<span class="sd">            args: argparse.Namespace</span>
<span class="sd">                The input arguments for the experiment.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This block is for safely calling torch.cuda API in the main process</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="c1"># --- 1. Initialization of the used GPUs in the current experiment --- #</span>
        <span class="c1"># &#39;CUDA_VISIBLE_DEVICES&#39; has the higher priority than the argument &#39;gpus&#39;</span>
        <span class="k">if</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span>

        <span class="c1"># if &#39;CUDA_VISIBLE_DEVICES&#39; is not given, initialize it by args.gpus</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">g</span>
                        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>
                        <span class="k">if</span> <span class="n">g</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>

        <span class="c1"># if both &#39;CUDA_VISIBLE_DEVICES&#39; and args.gpus are not given, automatically select available GPUs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">get_idle_gpu</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">)</span>
            <span class="c1"># make sure that GPU no.0 is the first GPU if it is selected</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">gpu</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">]))</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>

        <span class="c1"># convert the GPU absolute number to the GPU relative index to fit &#39;CUDA_VISIBLE_DEVICES&#39;</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))]</span>

        <span class="c1"># check the GPU configuration</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The visible GPUs </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2"> are fewer than the GPUs you would like to use </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">! &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Please use the argument &#39;--gpus&#39; to directly specify your target GPUs.&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># --- 2. Initialization of DDP distribution pipeline --- #</span>
        <span class="c1"># get the world_size from the command line, world_size here means the number of nodes</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">==</span> <span class="s2">&quot;env://&quot;</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">])</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Multi-node DDP distributed training is not supported now.....&quot;</span>
            <span class="p">)</span>

        <span class="c1"># distributed is set to true if multiple GPUs are specified or multiple nodes are specified</span>
        <span class="c1"># args.world_size &gt; 1 means multi-node distribution while args.ngpu &gt; 1 means multi-GPU distribution</span>
        <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span>

        <span class="c1"># multi-GPU distributed training and testing</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># check whether the input number of GPUs is valid</span>
            <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="n">ngpus_per_node</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Your input args.ngpu (</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">) is larger than the GPUs you have on your machine &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">ngpus_per_node</span><span class="si">}</span><span class="s2">). Currently, the real args.ngpu becomes </span><span class="si">{</span><span class="n">ngpus_per_node</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
                <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">=</span> <span class="n">ngpus_per_node</span>
            <span class="c1"># here world_size becomes the total number of processes on all nodes</span>
            <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span>

            <span class="c1"># automatic port selection if no specified port (only one &#39;:&#39; in args.dist_url)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;:</span><span class="si">{</span><span class="n">get_idle_port</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span>

            <span class="c1"># run one process on each GPU</span>
            <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">main_worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">,))</span>

        <span class="c1"># single-GPU training and testing</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">List</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Your input args.ngpu </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2"> is more than one. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Currently, the GPU no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> will be used.&quot;</span>
                <span class="p">)</span>
                <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">main_worker</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

        <span class="c1"># CPU testing with the multiprocessing strategy</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Multiprocessing CPU testing part has not been implemented yet......&quot;</span>
            <span class="p">)</span>

        <span class="c1"># CPU training is not supported</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Our toolkit doesn&#39;t support CPU training. Please specify a number of GPUs......&quot;</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The preparation area of Runner where the configuration is parsed and converted into code-friendly format.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># --- 0. Get the Command Line Arguments --- #</span>
        <span class="n">args</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>

        <span class="c1"># --- 1. Read the Non-Config Arguments from the Command Line --- #</span>
        <span class="c1"># Currently, &#39;world_size&#39; and &#39;rank&#39; are not provided to users to set</span>
        <span class="n">given_args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;world_size&quot;</span><span class="p">,</span> <span class="s2">&quot;rank&quot;</span><span class="p">]</span>
        <span class="c1"># The arguments that users give in the command line should not be refreshed by the argument &#39;--config&#39;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">):</span>
                <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>

        <span class="c1"># check the train and test flags</span>
        <span class="k">if</span> <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">given_args</span> <span class="ow">and</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">^</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;A running job can only conduct either training process or testing process, &quot;</span>
                <span class="s2">&quot;so args.train and args.test cannot be True at the same time. &quot;</span>
                <span class="s2">&quot;If you want to conduct training and testing sequentially, &quot;</span>
                <span class="s2">&quot;please make two running jobs where the first job has args.train=True and args.test=False and &quot;</span>
                <span class="s2">&quot;the second job has args.train=False and args.test=True.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
            <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span>
        <span class="k">elif</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
            <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span>

        <span class="c1"># the command &#39;CUDA_VISIBLE_DEVICES&#39; has the higher priority than the argument &#39;gpus&#39;</span>
        <span class="k">if</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;gpus&quot;</span><span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># --- 2. Overwrite the Arguments by &#39;--config&#39; --- #</span>
        <span class="c1"># overwrite args from the args.config</span>
        <span class="c1"># Note: the ones given in the command line has the higher priority than args.config</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
                    <span class="c1"># remove the port number in &#39;dist_url&#39; if given</span>
                    <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;dist_url&quot;</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mi">3</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                            <span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;:&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                    <span class="c1"># skip the existing &#39;report_per_steps&#39; (either use default value or give it in the command line)</span>
                    <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;report_per_steps&quot;</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="c1"># set the argument from config to args</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

        <span class="c1"># make sure that all the paths are absolute paths</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
                <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">parse_path_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">cfg</span>
                    <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span>
                <span class="p">]</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;infer_cfg should be either a string, a List, or a Dict, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but got type(args.infer_cfg)=</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># --- 3. Start the Experimental Pipeline --- #</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">^</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;A running job can only conduct either training process or testing process, &quot;</span>
            <span class="s2">&quot;so args.train and args.test cannot be True at the same time. &quot;</span>
            <span class="s2">&quot;If you want to conduct training and testing sequentially, &quot;</span>
            <span class="s2">&quot;please make two running jobs where the first job has args.train=True and args.test=False and &quot;</span>
            <span class="s2">&quot;the second job has args.train=False and args.test=True.&quot;</span>
        <span class="p">)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="runner.Runner.add_parse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_parse</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The interface where users can add their own arguments.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>parser</code>
            </td>
            <td>
                  <code><span title="argparse.ArgumentParser">ArgumentParser</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.ArgumentParser
The name space where you want to add your arguments.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>parser</code></td>            <td>
                  <code><span title="argparse.ArgumentParser">ArgumentParser</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.ArgumentParser
The name space containing your arguments.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">add_parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The interface where users can add their own arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">        parser: argparse.ArgumentParser</span>
<span class="sd">            The name space where you want to add your arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        parser: argparse.ArgumentParser</span>
<span class="sd">            The name space containing your arguments.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.build_iterators" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">build_iterators</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>This static function builds all iterators used in the experiment. The configuration of iterators is given in
your specified 'data_cfg'.</p>
<p>The iterators are returned as a dictionary where the first-level keys indicate different iterator groups:
'train', 'valid', and 'test'. The second-level keys in each group indicates the iterators belonging to the
group. In the value of each second-level key, there are two third-level keys: 'type' and 'conf'. 'type'
indicates the iterator type and 'conf' indicates the iterator configuration. For more details, please refer
to ./speechain/iterator/README.md</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>data_cfg</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
The dictionary containing all the information to initialize the iterators</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The arguments of the runner in this experiment.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[str, <span title="speechain.iterator.abs.Iterator">Iterator</span>] or <span title="speechain.iterator.abs.Iterator">Iterator</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The dictionary of the iterators of all groups (train, valid, test).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">build_iterators</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]</span> <span class="ow">or</span> <span class="n">Iterator</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This static function builds all iterators used in the experiment. The configuration of iterators is given in</span>
<span class="sd">    your specified &#39;data_cfg&#39;.</span>

<span class="sd">    The iterators are returned as a dictionary where the first-level keys indicate different iterator groups:</span>
<span class="sd">    &#39;train&#39;, &#39;valid&#39;, and &#39;test&#39;. The second-level keys in each group indicates the iterators belonging to the</span>
<span class="sd">    group. In the value of each second-level key, there are two third-level keys: &#39;type&#39; and &#39;conf&#39;. &#39;type&#39;</span>
<span class="sd">    indicates the iterator type and &#39;conf&#39; indicates the iterator configuration. For more details, please refer</span>
<span class="sd">    to ./speechain/iterator/README.md</span>

<span class="sd">    Args:</span>
<span class="sd">        data_cfg: Dict</span>
<span class="sd">            The dictionary containing all the information to initialize the iterators</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The arguments of the runner in this experiment.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The dictionary of the iterators of all groups (train, valid, test).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">recur_iterator_init</span><span class="p">(</span><span class="n">_data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">_dset</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">_data_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
            <span class="n">iterator_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span><span class="s2">&quot;speechain.iterator.&quot;</span> <span class="o">+</span> <span class="n">_data_cfg</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">iterator_class</span><span class="p">(</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
                <span class="n">ngpu</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span>
                <span class="n">num_workers</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_dset</span><span class="si">}</span><span class="s2">_num_workers&quot;</span><span class="p">),</span>
                <span class="n">pin_memory</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
                <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
                <span class="o">**</span><span class="n">_data_cfg</span><span class="p">[</span><span class="s2">&quot;conf&quot;</span><span class="p">],</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="n">key</span><span class="p">:</span> <span class="n">recur_iterator_init</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_dset</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_data_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>

    <span class="k">def</span> <span class="nf">recur_batch_num_init</span><span class="p">(</span><span class="n">_iterators</span><span class="p">:</span> <span class="n">Dict</span> <span class="ow">or</span> <span class="n">Iterator</span><span class="p">):</span>
        <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_iterators</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">_iterators</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sub_leaf_flag</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">_iterators</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">sub_leaf_flag</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="n">key</span><span class="p">:</span> <span class="n">recur_batch_num_init</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_iterators</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>

    <span class="k">def</span> <span class="nf">flatten_dict_to_list</span><span class="p">(</span><span class="n">_input</span><span class="p">:</span> <span class="n">Dict</span> <span class="ow">or</span> <span class="nb">int</span> <span class="ow">or</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
        <span class="n">leaf_flag</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_input</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">leaf_flag</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">_input</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">_input</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">_input</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_output</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">_input</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">_output</span> <span class="o">+=</span> <span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_output</span>

    <span class="c1"># get the target groups of the current experiment</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;valid&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;If args.train is set to True, please give &#39;train&#39; and &#39;valid&#39; as first-level keys of data_cfg.&quot;</span>
        <span class="n">dset_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">data_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;If args.test is set to True, please give &#39;test&#39; as first-level keys of data_cfg.&quot;</span>
        <span class="n">dset_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please set either args.train or args.test to True!&quot;</span><span class="p">)</span>

    <span class="c1"># recursively initialize all the iterators in the Dict</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="k">else</span> <span class="s2">&quot;test&quot;</span>
    <span class="n">iterators</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">dset</span><span class="p">:</span> <span class="n">recur_iterator_init</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">[</span><span class="n">dset</span><span class="p">],</span> <span class="n">dset</span><span class="p">)</span> <span class="k">for</span> <span class="n">dset</span> <span class="ow">in</span> <span class="n">dset_keys</span>
    <span class="p">}</span>
    <span class="n">batch_nums</span> <span class="o">=</span> <span class="n">recur_batch_num_init</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="n">mode</span><span class="p">])</span>

    <span class="c1"># set the relative reporting interval during training or testing</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_reports_per_epoch</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">10</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="o">-</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">min</span><span class="p">(</span><span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">))</span> <span class="o">//</span> <span class="n">_reports_per_epoch</span>
        <span class="p">)</span>
    <span class="c1"># check the absolute reporting interval during training and testing</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">min</span><span class="p">(</span>
            <span class="n">flatten_dict_to_list</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">)</span>
        <span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;If args.report_per_steps is given as a positive integer, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;it should be smaller than the minimal </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2"> batch number (</span><span class="si">{</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_nums</span><span class="p">)</span><span class="si">}</span><span class="s2">). &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;But got report_per_steps=</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span><span class="si">}</span><span class="s2">!&quot;</span>
        <span class="p">)</span>

        <span class="c1"># in case that report_per_steps is given as a float number</span>
        <span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">report_per_steps</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">iterators</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.build_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">build_model</span><span class="p">(</span><span class="n">model_cfg</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>This static function builds the model used in the experiment. The configuration of the model is given in
the value of the 'model' key in your specified 'model_cfg'.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model_cfg</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
Model Configuration</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="speechain.model.abs.Model">Model</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The target Model object initialized by your given configuration</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">model_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Model</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This static function builds the model used in the experiment. The configuration of the model is given in</span>
<span class="sd">    the value of the &#39;model&#39; key in your specified &#39;model_cfg&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_cfg: Dict</span>
<span class="sd">            Model Configuration</span>
<span class="sd">        args:</span>
<span class="sd">        device:</span>

<span class="sd">    Returns:</span>
<span class="sd">        The target Model object initialized by your given configuration</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="s2">&quot;model_type&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="s2">&quot;Please specify the model_type!&quot;</span>
    <span class="k">assert</span> <span class="s2">&quot;module_conf&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="s2">&quot;Please specify the module_conf!&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;criterion_conf&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;criterion_conf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">model_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span><span class="s2">&quot;speechain.model.&quot;</span> <span class="o">+</span> <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;model_type&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model_class</span><span class="p">(</span>
        <span class="n">model_conf</span><span class="o">=</span><span class="p">(</span>
            <span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;model_conf&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;model_conf&quot;</span> <span class="ow">in</span> <span class="n">model_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="p">),</span>
        <span class="n">module_conf</span><span class="o">=</span><span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;module_conf&quot;</span><span class="p">],</span>
        <span class="n">criterion_conf</span><span class="o">=</span><span class="n">model_cfg</span><span class="p">[</span><span class="s2">&quot;criterion_conf&quot;</span><span class="p">],</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">result_path</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">non_blocking</span><span class="p">,</span>
        <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.build_optim_sches" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">build_optim_sches</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_sche_cfg</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>This static function builds the OptimSchedulers used in the pipeline. The configuration of the
OptimSchedulers is given in the value of 'optim_sches' key in your specified 'train_cfg'.</p>
<p>This function must be done after DDP wrapping because we need to make sure that the model parameters received
by the optimizer in each process are identical. With the identical model parameters, it's safe to consider that
the optimizer parameters are also identical.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code><span title="speechain.model.abs.Model">Model</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Model
The initialized model.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>optim_sche_cfg</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
OptimScheduler Configuration</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="speechain.optim_sche.abs.OptimScheduler">OptimScheduler</span>] or <span title="speechain.optim_sche.abs.OptimScheduler">OptimScheduler</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The Dict of the initialized OptimSchedulers.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">build_optim_sches</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span> <span class="n">optim_sche_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OptimScheduler</span><span class="p">]</span> <span class="ow">or</span> <span class="n">OptimScheduler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This static function builds the OptimSchedulers used in the pipeline. The configuration of the</span>
<span class="sd">    OptimSchedulers is given in the value of &#39;optim_sches&#39; key in your specified &#39;train_cfg&#39;.</span>

<span class="sd">    This function must be done after DDP wrapping because we need to make sure that the model parameters received</span>
<span class="sd">    by the optimizer in each process are identical. With the identical model parameters, it&#39;s safe to consider that</span>
<span class="sd">    the optimizer parameters are also identical.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: Model</span>
<span class="sd">            The initialized model.</span>
<span class="sd">        optim_sche_cfg: Dict</span>
<span class="sd">            OptimScheduler Configuration</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The Dict of the initialized OptimSchedulers.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># single-optimizer scenario</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optim_sche_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
        <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="n">optim_sche_cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">main</span><span class="o">=</span><span class="n">optim_sche_cfg</span><span class="p">)</span>

    <span class="n">optim_sches</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sche_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">optim_sche_class</span> <span class="o">=</span> <span class="n">import_class</span><span class="p">(</span>
            <span class="s2">&quot;speechain.optim_sche.&quot;</span> <span class="o">+</span> <span class="n">optim_sche</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">optim_sches</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optim_sche_class</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">distributed</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
            <span class="n">use_amp</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_amp</span><span class="p">,</span>
            <span class="n">accum_grad</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">accum_grad</span><span class="p">,</span>
            <span class="n">ft_factor</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ft_factor</span><span class="p">,</span>
            <span class="n">grad_clip</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">,</span>
            <span class="n">grad_norm_type</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">grad_norm_type</span><span class="p">,</span>
            <span class="o">**</span><span class="n">optim_sche</span><span class="p">[</span><span class="s2">&quot;conf&quot;</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="c1"># multi-optimizer scenario</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">optim_sches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># adjust whether there are parameter overlapping among updated_modules of all the OptimSchedulers</span>
        <span class="n">is_all_para</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">updated_modules</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="c1"># updated_modules of all the OptimSchedulers cannot be None at the same time</span>
        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">is_all_para</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">is_all_para</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># collect the updated_modules of all the OptimScheduler</span>
            <span class="n">para_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">o</span><span class="o">.</span><span class="n">updated_modules</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="c1"># adjust whether there are redundant keys</span>
            <span class="n">para_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">para_list</span><span class="p">)</span>
            <span class="c1"># there is parameter overlapping if there are redundant keys</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">para_set</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">para_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span>

    <span class="c1"># resuming from an existing checkpoint</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">optim_sches</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optim_sches&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">])</span>
        <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;No checkpoint is found in </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The training process will start from scratch.&quot;</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">optim_sches</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.dict_transform" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dict_transform</span><span class="p">(</span><span class="n">src_dict</span><span class="p">,</span> <span class="n">transform_func</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>src_dict</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transform_func</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:</p>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">dict_transform</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">src_dict</span><span class="p">,</span> <span class="n">transform_func</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        src_dict:</span>
<span class="sd">        transform_func:</span>

<span class="sd">    Returns:</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Multi-dataloader</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">src_dict</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">transform_func</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">src_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="c1"># Single-dataloader</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">transform_func</span><span class="p">(</span><span class="n">src_dict</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.gather_all_iter_ascii" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gather_all_iter_ascii</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>iterator</code>
            </td>
            <td>
                  <code><span title="speechain.iterator.abs.Iterator">Iterator</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:</p>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">gather_all_iter_ascii</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">iterator</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        iterator:</span>
<span class="sd">        device:</span>

<span class="sd">    Returns:</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># turn the message into ASCII codes and gather the codes length</span>
    <span class="n">_iter_asc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">iterator</span><span class="p">)])</span>
    <span class="n">_iter_asc_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">_iter_asc</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
    <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">_iter_asc_lens</span><span class="p">,</span> <span class="n">_iter_asc_len</span><span class="p">)</span>

    <span class="c1"># padding the ASCII codes to the same length and gather them</span>
    <span class="k">if</span> <span class="n">_iter_asc_len</span> <span class="o">&lt;</span> <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">():</span>
        <span class="n">_iter_asc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">_iter_asc</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">_iter_asc_len</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">_iter_ascs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(),</span> <span class="n">_iter_asc_lens</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">_iter_asc</span> <span class="k">for</span> <span class="n">_iter_asc</span> <span class="ow">in</span> <span class="n">_iter_ascs</span><span class="p">],</span> <span class="n">_iter_asc_lens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.main" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The beginning of a experiment branch (training or testing).
This function decides the single-GPU or multi-GPU training sub-branch.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The input arguments for the experiment.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The beginning of a experiment branch (training or testing).</span>
<span class="sd">    This function decides the single-GPU or multi-GPU training sub-branch.</span>

<span class="sd">    Args:</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The input arguments for the experiment.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This block is for safely calling torch.cuda API in the main process</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="c1"># --- 1. Initialization of the used GPUs in the current experiment --- #</span>
    <span class="c1"># &#39;CUDA_VISIBLE_DEVICES&#39; has the higher priority than the argument &#39;gpus&#39;</span>
    <span class="k">if</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span>

    <span class="c1"># if &#39;CUDA_VISIBLE_DEVICES&#39; is not given, initialize it by args.gpus</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">g</span>
                    <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>
                    <span class="k">if</span> <span class="n">g</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>

    <span class="c1"># if both &#39;CUDA_VISIBLE_DEVICES&#39; and args.gpus are not given, automatically select available GPUs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">get_idle_gpu</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">)</span>
        <span class="c1"># make sure that GPU no.0 is the first GPU if it is selected</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">sorted</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">gpu</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">]))</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>

    <span class="c1"># convert the GPU absolute number to the GPU relative index to fit &#39;CUDA_VISIBLE_DEVICES&#39;</span>
    <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))]</span>

    <span class="c1"># check the GPU configuration</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;The visible GPUs </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2"> are fewer than the GPUs you would like to use </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">! &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Please use the argument &#39;--gpus&#39; to directly specify your target GPUs.&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># --- 2. Initialization of DDP distribution pipeline --- #</span>
    <span class="c1"># get the world_size from the command line, world_size here means the number of nodes</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">==</span> <span class="s2">&quot;env://&quot;</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">])</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Multi-node DDP distributed training is not supported now.....&quot;</span>
        <span class="p">)</span>

    <span class="c1"># distributed is set to true if multiple GPUs are specified or multiple nodes are specified</span>
    <span class="c1"># args.world_size &gt; 1 means multi-node distribution while args.ngpu &gt; 1 means multi-GPU distribution</span>
    <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span>

    <span class="c1"># multi-GPU distributed training and testing</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># check whether the input number of GPUs is valid</span>
        <span class="n">ngpus_per_node</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="n">ngpus_per_node</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Your input args.ngpu (</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">) is larger than the GPUs you have on your machine &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">ngpus_per_node</span><span class="si">}</span><span class="s2">). Currently, the real args.ngpu becomes </span><span class="si">{</span><span class="n">ngpus_per_node</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">=</span> <span class="n">ngpus_per_node</span>
        <span class="c1"># here world_size becomes the total number of processes on all nodes</span>
        <span class="n">args</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span>

        <span class="c1"># automatic port selection if no specified port (only one &#39;:&#39; in args.dist_url)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;:</span><span class="si">{</span><span class="n">get_idle_port</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span>

        <span class="c1"># run one process on each GPU</span>
        <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">main_worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">,))</span>

    <span class="c1"># single-GPU training and testing</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">List</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Your input args.ngpu </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2"> is more than one. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Currently, the GPU no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> will be used.&quot;</span>
            <span class="p">)</span>
            <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">main_worker</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="c1"># CPU testing with the multiprocessing strategy</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Multiprocessing CPU testing part has not been implemented yet......&quot;</span>
        <span class="p">)</span>

    <span class="c1"># CPU training is not supported</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Our toolkit doesn&#39;t support CPU training. Please specify a number of GPUs......&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.main_worker" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">main_worker</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The main body of a process on one GPU.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>gpu</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">main_worker</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">gpu</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The main body of a process on one GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        gpu:</span>
<span class="sd">        args:</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- 0. Random Seed Preparation --- #</span>
    <span class="c1"># set different random seeds for the different GPU processes in DDP mode to avoid the process homogeneity</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">same_proc_seed</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="o">+=</span> <span class="n">gpu</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">set_random_seeds</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># --- 1. Experimental Reproducibility Preparation --- #</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_enabled</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_benchmark</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cudnn_deterministic</span>
    <span class="c1"># torch.use_deterministic_algorithms(torch.backends.cudnn.deterministic)</span>
    <span class="c1"># For more details about &#39;CUBLAS_WORKSPACE_CONFIG&#39;,</span>
    <span class="c1"># please refer to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility</span>
    <span class="k">if</span> <span class="n">V</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">V</span><span class="p">(</span><span class="s2">&quot;10.2&quot;</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUBLAS_WORKSPACE_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;:4096:8&quot;</span>

    <span class="c1"># --- 2. DDP Model Distribution Initialization --- #</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
        <span class="c1"># load the global node rank from the os environment in the multi-node setting</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dist_url</span> <span class="o">==</span> <span class="s2">&quot;env://&quot;</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Here, the rank is turned from the local node-level rank to the global process-level rank</span>
            <span class="c1"># the input argument &#39;gpu&#39; is the local rank of the current process in the specific node</span>
            <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">+</span> <span class="n">gpu</span>
        <span class="c1"># initialize the distributed environment, connections among all the processes are established here</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="p">,</span>
            <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># --- 3. Experimental Environment Logging --- #</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_config_split</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_config_split</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># automatically decide the result path if not given</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">_config_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;If you want to automatically generate train_result_path, please give the configuration by &#39;--config&#39;.&quot;</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="s2">&quot;/&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">_config_split</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;exp&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_config_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="p">)</span>
    <span class="c1"># attach a folder named by args.config to the end of your given result path</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">attach_config_folder_to_path</span><span class="p">:</span>
        <span class="c1"># if `--config` is given, attach the name of exp_cfg to the end of train_result_path</span>
        <span class="k">if</span> <span class="n">_config_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_config_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>

    <span class="c1"># initialize the logger and save current script command</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">logger_stdout_file</span><span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
        <span class="s2">&quot;train&quot;</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span>
        <span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># logging the beginning info of the experiment</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current script command: </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">xi</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">xi</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Multi-GPU distribution information: &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;backend=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">dist_backend</span><span class="si">}</span><span class="s2">, init_method=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">dist_url</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;nnode=</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="p">)</span><span class="si">}</span><span class="s2">, ngpu_per_node=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ngpu</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;used_gpus=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># initialize the computational equipments</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="p">),</span> <span class="s2">&quot;CUDA is not available! It fails to conduct GPU training.&quot;</span>
    <span class="c1"># args.gpu is the GPU used in the current process while args.gpus are all the available GPUss</span>
    <span class="n">args</span><span class="o">.</span><span class="n">gpu</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span><span class="p">[</span><span class="n">gpu</span><span class="p">]</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ngpu</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">gpus</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Used GPU in the master process: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># --- 4. Configuration Loading --- #</span>
    <span class="c1"># resume from an existing checkpoint, loading the old data and train configurations</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
        <span class="c1"># loading the existing data and train configurations</span>
        <span class="c1"># But the input data configuration has higher priority than the existing one</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">data_cfg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">))</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span>
                <span class="nb">open</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;train&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;test&#39;</span><span class="si">}</span><span class="s2">_data_cfg.yaml&quot;</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># training configuration will be loaded from the existing file</span>
        <span class="n">train_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span>
            <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;train_cfg.yaml&quot;</span><span class="p">))</span>
        <span class="p">)</span>
    <span class="c1"># start from scratch, loading the new data and train configurations</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Please specify a data configuration file and a train configuration file!&quot;</span>
        <span class="n">data_cfg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span>
        <span class="p">)</span>
        <span class="n">train_cfg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span>
        <span class="p">)</span>

    <span class="c1"># --- 5. Data Iterator Initialization --- #</span>
    <span class="n">iterators</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_iterators</span><span class="p">(</span><span class="n">data_cfg</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

    <span class="c1"># logging the information of the iterators</span>
    <span class="n">_iter_message</span> <span class="o">=</span> <span class="s2">&quot;The information of the iterators:&quot;</span>
    <span class="k">for</span> <span class="n">dset</span><span class="p">,</span> <span class="n">iters</span> <span class="ow">in</span> <span class="n">iterators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># single iterator for the current dataset</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">):</span>
            <span class="c1"># gather the iterator message from all the process in the multi-GPU distributed training mode</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">gather_all_iter_ascii</span><span class="p">(</span>
                    <span class="n">iters</span><span class="p">,</span> <span class="n">device</span>
                <span class="p">)</span>

                <span class="c1"># recover the codes from all the processes back to the text</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">asc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">):</span>
                    <span class="n">_iter_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">asc</span><span class="p">[:</span> <span class="n">_iter_asc_lens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
                    <span class="p">)</span>
                    <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set of the rank no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">_iter_text</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="c1"># directly report the message in the single-GPU mode</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set: </span><span class="si">{</span><span class="n">iters</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># multiple iterators for the current dataset</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iters</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># gather the iterator message from all the process in the multi-GPU distributed training mode</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                    <span class="n">_iter_ascs</span><span class="p">,</span> <span class="n">_iter_asc_lens</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">gather_all_iter_ascii</span><span class="p">(</span>
                        <span class="n">iterator</span><span class="p">,</span> <span class="n">device</span>
                    <span class="p">)</span>

                    <span class="c1"># recover the codes from all the processes back to the text</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">asc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_iter_ascs</span><span class="p">):</span>
                        <span class="n">_iter_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                            <span class="p">[</span><span class="nb">chr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">asc</span><span class="p">[:</span> <span class="n">_iter_asc_lens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
                        <span class="p">)</span>
                        <span class="n">_iter_message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set of the rank no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">_iter_text</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="c1"># directly report the message in the single-GPU mode</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_iter_message</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> iterator in the </span><span class="si">{</span><span class="n">dset</span><span class="si">}</span><span class="s2"> set: </span><span class="si">{</span><span class="n">iterator</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">_iter_message</span><span class="p">)</span>

    <span class="c1"># --- 6. Model Initialization --- #</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">train_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">),</span> <span class="s2">&quot;Please fill in the &#39;model&#39; tag of your given train_cfg!&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_model</span><span class="p">(</span><span class="n">train_cfg</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">model_summary</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>

    <span class="c1"># for the process of single-GPU training or the rank 0 process of multi-GPUs training</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># dumping all the configuration files into train_result_path for resuming</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;exp_cfg.yaml&quot;</span><span class="p">),</span>
            <span class="s2">&quot;w&quot;</span><span class="p">,</span>
            <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;train&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;test&#39;</span><span class="si">}</span><span class="s2">_data_cfg.yaml&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="s2">&quot;w&quot;</span><span class="p">,</span>
            <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;train_cfg.yaml&quot;</span><span class="p">),</span>
            <span class="s2">&quot;w&quot;</span><span class="p">,</span>
            <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">train_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># --- 7.1. Model Training Branch --- #</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
        <span class="c1"># initialize the Monitor for training and validation</span>
        <span class="n">monitor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="kc">None</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="n">TrainValidMonitor</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># loading the model from the existing checkpoint for resuming the training process</span>
        <span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">resume</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">)</span>

        <span class="c1"># DDP Wrapping of the model must be done after model checkpoint loading</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">enable_syncbatchnorm</span><span class="p">:</span>
                <span class="c1"># turn the batchnorm layers into the sync counterparts</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="c1"># Here the model buffers and parameters of the master process are broadcast to the other processes</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gpu</span>
            <span class="p">)</span>

        <span class="c1"># initialize the OptimSchedulers after DDP wrapping (including optimization resuming)</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="s2">&quot;optim_sches&quot;</span> <span class="ow">in</span> <span class="n">train_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Please fill in the &#39;optim_sches&#39; tag!&quot;</span>
        <span class="n">optim_sches</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">build_optim_sches</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_sche_cfg</span><span class="o">=</span><span class="n">train_cfg</span><span class="p">[</span><span class="s2">&quot;optim_sches&quot;</span><span class="p">],</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span>
        <span class="p">)</span>

        <span class="c1"># logging the information of the optimschedulers</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> OptimScheduler: </span><span class="si">{</span><span class="n">optim_sche</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># start the training process</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">data_cfg</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">,</span>
            <span class="n">iterators</span><span class="o">=</span><span class="n">iterators</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim_sches</span><span class="o">=</span><span class="n">optim_sches</span><span class="p">,</span>
            <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
            <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># --- 7.2. Model Testing Branch --- #</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">test_model</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">]</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>

        <span class="c1"># loop each model to be tested</span>
        <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">test_model</span><span class="p">:</span>
            <span class="c1"># get the path of the target model parameters</span>
            <span class="n">_models_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">)</span>
            <span class="c1"># for compatibility with the older version</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.mdl&quot;</span><span class="p">)):</span>
                <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.mdl&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">)):</span>
                <span class="n">model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_models_path</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.pth&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">model_name</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not found!&quot;</span>
                <span class="p">)</span>

            <span class="c1"># load the target model parameters</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

            <span class="c1"># start the testing process</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">test</span><span class="p">(</span>
                <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">test_model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">iterators</span><span class="o">=</span><span class="n">iterators</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span>
            <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;train and test in args cannot be False at the same time!&quot;</span>
        <span class="p">)</span>

    <span class="c1"># --- 8. Release Computational Resource --- #</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.parse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">parse</span><span class="p">()</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The static function that outputs all the default arguments for the runner.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>a Dict containing the key-value pairs of all arguments</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The static function that outputs all the default arguments for the runner.</span>

<span class="sd">    Returns:</span>
<span class="sd">        a Dict containing the key-value pairs of all arguments</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>

    <span class="c1"># All-in-one configuration setting</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--config&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="c1"># default=None,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;recipes/asr/librispeech/train-clean-100/exp_cfg/100-bpe5k_conformer-medium_lr2e-3.yaml&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the all-in-one experiment configuration file. You can write all the arguments in this &quot;</span>
        <span class="s2">&quot;all-in-one file instead of giving them to `runner.py` by command lines.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Experimental environment</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 1: Calculation and System Backend&quot;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--seed&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Initial random seed for the experiment. (default: 0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cudnn_enabled&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cudnn_benchmark&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn.benchmark. &quot;</span>
        <span class="s2">&quot;When True, the process of model training will be speed up and the model performance may improve &quot;</span>
        <span class="s2">&quot;somewhat. But your results will become less reproducible. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cudnn_deterministic&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate torch.backends.cudnn.deterministic. &quot;</span>
        <span class="s2">&quot;This will improve the reproducibility of your experiments. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--train_num_workers&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
        <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
        <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
        <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
        <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
        <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--valid_num_workers&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
        <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
        <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
        <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
        <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
        <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test_num_workers&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. &quot;</span>
        <span class="s2">&quot;If you have complicated logic of data loading and data augmentation in the memory before passing the &quot;</span>
        <span class="s2">&quot;data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this &quot;</span>
        <span class="s2">&quot;argument may improve the speed of data loading and pre-augmentation. But the choice of the argument &quot;</span>
        <span class="s2">&quot;value should be within your machine capability (i.e., the number of CPU cores). &quot;</span>
        <span class="s2">&quot;If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--pin_memory&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate `pin_memory` for the Dataloader of each epoch. &quot;</span>
        <span class="s2">&quot;If True, the pinned memory in the dataloaders will be activated and the data loading will be further &quot;</span>
        <span class="s2">&quot;speed up. &quot;</span>
        <span class="s2">&quot;pin_memory=True is often used together with non_blocking=True. Note that this combination requires a &quot;</span>
        <span class="s2">&quot;large amount of memory and CPU cores. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--non_blocking&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to activate `non_blocking` when transferring data from the memory to GPUs. &quot;</span>
        <span class="s2">&quot;If True, the process of model training will be speed up. &quot;</span>
        <span class="s2">&quot;non_blocking=True is often used together with pin_memory=True. Note that this combination requires a &quot;</span>
        <span class="s2">&quot;large amount of memory and CPU cores. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># gradient descent related</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
        <span class="s2">&quot;Group 2: Gradient Calculation and Back-Propagation&quot;</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--use_amp&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether activate AMP (Automatic Mixed Precision) during the back-propagation. &quot;</span>
        <span class="s2">&quot;If True, the GPU consumption of your model will be smaller so that you can include more data &quot;</span>
        <span class="s2">&quot;instances in a single batch. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--grad_clip&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Gradient clipping threshold during the back-propagation. (default: 5.0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--grad_norm_type&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Normalization type used when clipping the gradients. (default: 2.0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--accum_grad&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of gradient accumulation steps. &quot;</span>
        <span class="s2">&quot;To mimic the gradients calculated by large batches with only a small amount of GPUs, please raise &quot;</span>
        <span class="s2">&quot;this argument. &quot;</span>
        <span class="s2">&quot;The virtual batch size will become (accum_grad * the actual batch size). &quot;</span>
        <span class="s2">&quot;Note that the model trained by accum_grad is not identical to the one actually trained by large &quot;</span>
        <span class="s2">&quot;batches because of the different randomness in each training step and the existence of BatchNorm. &quot;</span>
        <span class="s2">&quot;(default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--ft_factor&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The finetuing factor used to scale down learning rates during the parameter optimization. &quot;</span>
        <span class="s2">&quot;If `ft_factor` is smaller than 1.0, the learning rates will be proportionally decreased without &quot;</span>
        <span class="s2">&quot;changing its scheduling strategy. Usually, ft_factor could be set from 0.1 to 0.5 depending on your &quot;</span>
        <span class="s2">&quot;finetuning scenarios. (default: 1.0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># multi-GPU distributed training</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 3: Multi-GPU Distribution&quot;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--dist_backend&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Communication backend for multi-GPU distribution. &quot;</span>
        <span class="s2">&quot;If you are using NVIDIA GPUs, we recommend you set this argument to &#39;nccl&#39;. (default: nccl)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--dist_url&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;tcp://127.0.0.1&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Communication URL for multi-GPU distribution. &quot;</span>
        <span class="s2">&quot;The default value is &#39;tcp://127.0.0.1&#39; for single-node distributed training and an idle port will be &quot;</span>
        <span class="s2">&quot;automatically selected. &quot;</span>
        <span class="s2">&quot;The port number cannot be set manually, which means that the argument &#39;tcp://127.0.0.1:xxxxx&#39; will &quot;</span>
        <span class="s2">&quot;have the same effect with &#39;tcp://127.0.0.1&#39;. &quot;</span>
        <span class="s2">&quot;If you want to train your model on multiple nodes, please set dist_url=&#39;env://&#39; &quot;</span>
        <span class="s2">&quot;(Note: multi-node model distribution is still in beta). &quot;</span>
        <span class="s2">&quot;In this case, env values of &#39;MASTER_PORT&#39;, &#39;MASTER_ADDR&#39;, &#39;WORLD_SIZE&#39;, and &#39;RANK&#39; are referred in &quot;</span>
        <span class="s2">&quot;the command line.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--world_size&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of nodes for model distribution. &quot;</span>
        <span class="s2">&quot;This argument is fixed to 1. Currently, we don&#39;t recommend you to modify its value.&quot;</span>
        <span class="s2">&quot;If you want to conduct multi-node model distribution, please give `world_size` by `WORLD_SIZE=XXX` &quot;</span>
        <span class="s2">&quot;in your terminal (Note: multi-node model distribution is still in beta).&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--rank&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The global rank of the current node for model distribution. &quot;</span>
        <span class="s2">&quot;This argument is fixed to 0. Currently, we don&#39;t recommend you to modify its value.&quot;</span>
        <span class="s2">&quot;If you want to conduct multi-node model distribution, please give `rank` by `RANK=XXX` in your &quot;</span>
        <span class="s2">&quot;terminal (Note: multi-node model distribution is still in beta).&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--ngpu&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of GPUs used to run your experiment. &quot;</span>
        <span class="s2">&quot;If ngpu is larger than 1, multi-GPU model distribution will be activated. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--gpus&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2none</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This argument specifies the GPUs used to run your experiment. &quot;</span>
        <span class="s2">&quot;If you want to specify multiple GPUs, please give this argument in the form of &#39;x,x,x&#39; &quot;</span>
        <span class="s2">&quot;where different GPUs are separated by a comma (please don&#39;t end this argument with &#39;,&#39;). &quot;</span>
        <span class="s2">&quot;Of course, you could also specify your target GPUs by `CUDA_VISIBLE_DEVICES` in the terminal.&quot;</span>
        <span class="s2">&quot;If this argument is not given, the framework will automatically select `ngpu` idle GPUs. &quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--same_proc_seed&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to set the same initial random seed for all the GPU processes in DDP mode. &quot;</span>
        <span class="s2">&quot;The different random seeds can prevent model distribution from the process homogeneity, &quot;</span>
        <span class="s2">&quot;e.g., different GPU processes may have the same on-the-fly data augmentation strategy &quot;</span>
        <span class="s2">&quot;(noise addition, SpecAugment, ...) if they have the same initial random seed. &quot;</span>
        <span class="s2">&quot;Note: please set this argument to True if you want to use random data selection for your dataloaders &quot;</span>
        <span class="s2">&quot;in the DDP mode. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--ignore_train_exception&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to ignore the exceptions happening during training and validation. &quot;</span>
        <span class="s2">&quot;If set to True, your training would not be interrupted by some nonfatal errors, such as occasional &quot;</span>
        <span class="s2">&quot;&#39;RuntimeError: CUDA Out of memory&#39;, and etc. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--ignore_test_exception&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to ignore the exceptions happening during testing. &quot;</span>
        <span class="s2">&quot;If set to True, your testing would not be interrupted by some nonfatal errors, such as occasional &quot;</span>
        <span class="s2">&quot;&#39;RuntimeError: CUDA Out of memory&#39;, and etc. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--enable_syncbatchnorm&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to process the model by &#39;torch.nn.SyncBatchNorm.convert_sync_batchnorm&#39; for multi-GPU &quot;</span>
        <span class="s2">&quot;distributed training. Sometimes your training may be stuck at some points or terminate without being &quot;</span>
        <span class="s2">&quot;notified of any errors in the multi-GPU distributed mode. If that happens, you can disable &quot;</span>
        <span class="s2">&quot;SyncBatchNorm and debug your codes. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Training monitoring</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 4: Model Training&quot;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--train_result_path&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Where to place all the experiment folder that contains all the result files. &quot;</span>
        <span class="s2">&quot;If not given, `train_result_path` wil be automatically initialized by your input `config`. &quot;</span>
        <span class="s2">&quot;For example, if your input `config` is &quot;</span>
        <span class="s2">&quot;</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp_cfg/XXXXX.yaml, your `train_result_path` &quot;</span>
        <span class="s2">&quot;will be automatically initialized to `</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp/`.&quot;</span>
        <span class="s2">&quot;(default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--attach_config_folder_to_path&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to attach an additional folder named by your input `--config` at the end of your input &quot;</span>
        <span class="s2">&quot;`train_result_path`. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--train&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to go through the model training branch. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--dry_run&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to turn on the dry-running mode. &quot;</span>
        <span class="s2">&quot;In this mode, only the data loading will be done to see its speed and robustness. &quot;</span>
        <span class="s2">&quot;Model calculation and parameter optimization will be skipped. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--no_optim&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to turn on the no-optimization mode. &quot;</span>
        <span class="s2">&quot;In this mode, only the data loading and model calculation will be done to see their speed, &quot;</span>
        <span class="s2">&quot;robustness, and memory consumption. (default: False) &quot;</span>
        <span class="s2">&quot;(Note: &#39;dry_run&#39; has the higher priority than &#39;no_optim&#39;. It means that the model calculation will &quot;</span>
        <span class="s2">&quot;be skipped if you give both &#39;--dry_run True&#39; and &#39;--no_optim True&#39;.) &quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--resume&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to resume your model training or testing experiment from the checkpoints. &quot;</span>
        <span class="s2">&quot;If True, there must be .pth checkpoint files of your existing experiment in `train_result_path` or &quot;</span>
        <span class="s2">&quot;`test_result_path`. This argument is shared by the training and testing branches. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--start_epoch&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The starting epoch of your experiments. This argument will be automatically initialized by your &quot;</span>
        <span class="s2">&quot;checkpoint files if `--resume` is given. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_epochs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of training epochs of your experiments. (default: 1000)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--valid_per_epochs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The interval of going through the validation phase during training. &quot;</span>
        <span class="s2">&quot;If not given, validation will be done right after parameter optimization in each epoch. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--report_per_steps&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The interval of reporting step information logs during model training or testing. &quot;</span>
        <span class="s2">&quot;Positive integers mean the absolute reporting intervals that a step report will be made after each &quot;</span>
        <span class="s2">&quot;&#39;report_per_steps&#39; steps; &quot;</span>
        <span class="s2">&quot;Negative integers mean the relative reporting intervals that there will be -&#39;report_per_steps&#39; &quot;</span>
        <span class="s2">&quot;reports in each epoch. &quot;</span>
        <span class="s2">&quot;If not given, there will be default 10 reports in each epoch. &quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--best_model_selection&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The ways of selecting the best models. This argument should be given as a list of quad-tuples, i.e., &quot;</span>
        <span class="s2">&quot;(&#39;metric_group&#39;, &#39;metric_name&#39;, &#39;metric_mode&#39;, &#39;model_number&#39;). &quot;</span>
        <span class="s2">&quot;&#39;metric_group&#39; can be either &#39;train&#39; or &#39;valid&#39; which indicates the group the metric belongs to; &quot;</span>
        <span class="s2">&quot;&#39;metric_name&#39; is the name of the metric you select; &quot;</span>
        <span class="s2">&quot;&#39;metric_mode&#39; can be either &#39;min&#39; or &#39;max&#39; which indicates how to select the models by this metric; &quot;</span>
        <span class="s2">&quot;&#39;model_number&#39; indicates how many best models will be saved by this metric. &quot;</span>
        <span class="s2">&quot;Note: the metric of the first tuple in the list will be used to do early-stopping for model training.&quot;</span>
        <span class="s2">&quot;(default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--early_stopping_patience&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The maximum number of epochs when the model doesn&#39;t improve its performance before stopping the &quot;</span>
        <span class="s2">&quot;model training. If not given, early-stopping will not be adapted. (default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--early_stopping_threshold&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The threshold to refresh the early-stopping status in the monitor during model training. &quot;</span>
        <span class="s2">&quot;Positive float numbers in (0.0, 1.0) mean the relative threshold over the current best performance. &quot;</span>
        <span class="s2">&quot;Negative float numbers main the absolute threshold over the current best performance. &quot;</span>
        <span class="s2">&quot;early_stopping_threshold=0 means no early-stopping threshold is applied to the current best &quot;</span>
        <span class="s2">&quot;performance when deciding whether to refresh the status. (default: 0.005)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--last_model_number&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of models saved for the last several epochs. &quot;</span>
        <span class="s2">&quot;This argument cannot be lower than 1 otherwise the training will not be able to resume. &quot;</span>
        <span class="s2">&quot;(default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Training Snapshotting</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
        <span class="s2">&quot;Group 5: Real-time Model Visualization Snapshotting&quot;</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--monitor_snapshot_conf&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The configuration given to `matploblib.plot()` in `{SPEECHAIN_ROOT/speechain/snapshooter.py}` to &quot;</span>
        <span class="s2">&quot;plot curve figures for real-time model visualization during model training. &quot;</span>
        <span class="s2">&quot;This argument should be given in the form of a Dict. (default: an empty Dict)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--visual_snapshot_number&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of the validation data instances used to make snapshots made during model visualization. &quot;</span>
        <span class="s2">&quot;This argument should be smaller than the number of your validation data instances. &quot;</span>
        <span class="s2">&quot;(default: 0)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--visual_snapshot_interval&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The snapshotting interval of model visualization during model training. &quot;</span>
        <span class="s2">&quot;This argument should be a positive integer which means that model visualization will be done once &quot;</span>
        <span class="s2">&quot;in every `visual_snapshot_interval` epochs. (default: 5)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Testing</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s2">&quot;Group 6: Model Testing&quot;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test_result_path&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Where to place all the result files generated during model testing. &quot;</span>
        <span class="s2">&quot;If not given, `test_result_path` wil be automatically initialized by your input `train_result_path` &quot;</span>
        <span class="s2">&quot;and `test_model`. For example, if your `train_result_path` is &quot;</span>
        <span class="s2">&quot;`</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp`, and `test_model` is `MMMMM`, &quot;</span>
        <span class="s2">&quot;then your `test_result_path` will be automatically initialized to &quot;</span>
        <span class="s2">&quot;`</span><span class="si">{SPEECHAIN_ROOT}</span><span class="s2">/recipes/asr/librispeech/train-960/exp/XXXXX/MMMMM/` where &#39;XXXXX&#39; is the name of &quot;</span>
        <span class="s2">&quot;your configuration file given by `--config`.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to go through the model testing branch. (default: False)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test_model&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The names of the model you want to evaluate during model testing. &quot;</span>
        <span class="s2">&quot;If given, `</span><span class="si">{train_result_path}</span><span class="s2">/XXXXX/model/</span><span class="si">{test_model}</span><span class="s2">.pth` will be used to initialize the parameters &quot;</span>
        <span class="s2">&quot;of the Model object. If you only want to evaluate multiple models in one job, please give the &quot;</span>
        <span class="s2">&quot;strings of their names in a List. (default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--attach_model_folder_when_test&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2bool</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to attach an additional sub-folder named by your input `--test_model` in the testing &quot;</span>
        <span class="s2">&quot;result folder. (default: True)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--bad_cases_selection&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2list</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The selection methods of the top-N bad cases during model testing. &quot;</span>
        <span class="s2">&quot;This argument should be given as a list of tri-tuples &quot;</span>
        <span class="s2">&quot;(&#39;selection_metric&#39;, &#39;selection_mode&#39;, &#39;case_number&#39;). &quot;</span>
        <span class="s2">&quot;For example, (&#39;wer&#39;, &#39;max&#39;, 50) means 50 testing waveforms with the largest WER will be selected. &quot;</span>
        <span class="s2">&quot;Multiple tuples can be given to present different sets of top-n bad cases. (default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--saving_proc_num&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of daemon processes used to save data generated during testing to the disk. (default: 1)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Experiment configuration</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
        <span class="s2">&quot;Group 7: Experiment .yaml Configuration File&quot;</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--data_cfg&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the configuration file for data loading and batching. &quot;</span>
        <span class="s2">&quot;This argument is required for both model training and testing.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--train_cfg&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The path of the configuration file for model construction and parameter optimization. &quot;</span>
        <span class="s2">&quot;This argument is required for both model training (both &#39;model&#39; and &#39;optim_sche&#39; need to be given) &quot;</span>
        <span class="s2">&quot;and testing (only &#39;model&#39; needs to be given).&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--infer_cfg&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">str2dict</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The configuration file for model inference during model testing. &quot;</span>
        <span class="s2">&quot;This argument is required for model testing.&quot;</span>
        <span class="s2">&quot;For more details about how to give infer_cfg, please refer to the handbook.md. (default: None)&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Add customized arguments if needed</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">add_parse</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.resume" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">resume</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">monitor</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>load the model parameters to the current process. This operation is necessary in our toolkit because we need to
make sure that the models in all the processes have the same buffer and parameter tensors.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The input arguments.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code><span title="speechain.model.abs.Model">Model</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Model
The model to be trained.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>monitor</code>
            </td>
            <td>
                  <code><span title="speechain.monitor.TrainValidMonitor">TrainValidMonitor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>TrainValidMonitor
The train-valid monitor used to monitor the training phase</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>epoch will be loaded from the checkpoint; otherwise, 1 will be returned.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">resume</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span> <span class="n">monitor</span><span class="p">:</span> <span class="n">TrainValidMonitor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    load the model parameters to the current process. This operation is necessary in our toolkit because we need to</span>
<span class="sd">    make sure that the models in all the processes have the same buffer and parameter tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The input arguments.</span>
<span class="sd">        model: Model</span>
<span class="sd">            The model to be trained.</span>
<span class="sd">        monitor: TrainValidMonitor</span>
<span class="sd">            The train-valid monitor used to monitor the training phase</span>

<span class="sd">    Returns:</span>
<span class="sd">        The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting</span>
<span class="sd">        epoch will be loaded from the checkpoint; otherwise, 1 will be returned.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># start the training from the existing checkpoint</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
        <span class="c1"># load the existing checkpoint</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
            <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># load the latest training epoch</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;start_epoch&quot;</span><span class="p">]</span>
        <span class="c1"># for compatibility with old versions</span>
        <span class="k">if</span> <span class="s2">&quot;latest_model&quot;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;latest_model&quot;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">,</span> <span class="s2">&quot;latest.pth&quot;</span><span class="p">),</span>
                    <span class="n">map_location</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># loading the monitor</span>
        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># for compatibility with old versions</span>
            <span class="k">if</span> <span class="s2">&quot;monitor&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span>
                    <span class="nb">dict</span><span class="p">(</span>
                        <span class="n">train_monitor</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;train_monitor&quot;</span><span class="p">],</span>
                        <span class="n">valid_monitor</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;valid_monitor&quot;</span><span class="p">],</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;monitor&quot;</span><span class="p">])</span>
            <span class="c1"># info logging</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The training process resumes from the epoch no.</span><span class="si">{</span><span class="n">start_epoch</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># start the training from scratch</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">start_epoch</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.run" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">run</span><span class="p">()</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>The preparation area of Runner where the configuration is parsed and converted into code-friendly format.</p>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The preparation area of Runner where the configuration is parsed and converted into code-friendly format.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- 0. Get the Command Line Arguments --- #</span>
    <span class="n">args</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>

    <span class="c1"># --- 1. Read the Non-Config Arguments from the Command Line --- #</span>
    <span class="c1"># Currently, &#39;world_size&#39; and &#39;rank&#39; are not provided to users to set</span>
    <span class="n">given_args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;world_size&quot;</span><span class="p">,</span> <span class="s2">&quot;rank&quot;</span><span class="p">]</span>
    <span class="c1"># The arguments that users give in the command line should not be refreshed by the argument &#39;--config&#39;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">):</span>
            <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>

    <span class="c1"># check the train and test flags</span>
    <span class="k">if</span> <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">given_args</span> <span class="ow">and</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">^</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;A running job can only conduct either training process or testing process, &quot;</span>
            <span class="s2">&quot;so args.train and args.test cannot be True at the same time. &quot;</span>
            <span class="s2">&quot;If you want to conduct training and testing sequentially, &quot;</span>
            <span class="s2">&quot;please make two running jobs where the first job has args.train=True and args.test=False and &quot;</span>
            <span class="s2">&quot;the second job has args.train=False and args.test=True.&quot;</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="s2">&quot;train&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
        <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span>
    <span class="k">elif</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
        <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span>

    <span class="c1"># the command &#39;CUDA_VISIBLE_DEVICES&#39; has the higher priority than the argument &#39;gpus&#39;</span>
    <span class="k">if</span> <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">given_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;gpus&quot;</span><span class="p">)</span>
        <span class="n">args</span><span class="o">.</span><span class="n">gpus</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># --- 2. Overwrite the Arguments by &#39;--config&#39; --- #</span>
    <span class="c1"># overwrite args from the args.config</span>
    <span class="c1"># Note: the ones given in the command line has the higher priority than args.config</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">given_args</span><span class="p">:</span>
                <span class="c1"># remove the port number in &#39;dist_url&#39; if given</span>
                <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;dist_url&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mi">3</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;:&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="c1"># skip the existing &#39;report_per_steps&#39; (either use default value or give it in the command line)</span>
                <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="s2">&quot;report_per_steps&quot;</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># set the argument from config to args</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

    <span class="c1"># make sure that all the paths are absolute paths</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_cfg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_cfg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="n">parse_path_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">parse_path_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">cfg</span>
                <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;infer_cfg should be either a string, a List, or a Dict, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but got type(args.infer_cfg)=</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="c1"># --- 3. Start the Experimental Pipeline --- #</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span> <span class="o">^</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">&quot;A running job can only conduct either training process or testing process, &quot;</span>
        <span class="s2">&quot;so args.train and args.test cannot be True at the same time. &quot;</span>
        <span class="s2">&quot;If you want to conduct training and testing sequentially, &quot;</span>
        <span class="s2">&quot;please make two running jobs where the first job has args.train=True and args.test=False and &quot;</span>
        <span class="s2">&quot;the second job has args.train=False and args.test=True.&quot;</span>
    <span class="p">)</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.set_random_seeds" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_random_seeds</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Set random seeds for python environment, numpy environment and torch environment</p>


<details class="note" open>
  <summary>Note</summary>
  <ol>
<li>torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),
    so it is not necessary to be included here.</li>
<li>torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on
    different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.</li>
</ol>
</details>
            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">set_random_seeds</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set random seeds for python environment, numpy environment and torch environment</span>

<span class="sd">    Note:</span>
<span class="sd">        1. torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),</span>
<span class="sd">            so it is not necessary to be included here.</span>
<span class="sd">        2. torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on</span>
<span class="sd">            different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONHASHSEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.test" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">test</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">test_model</span><span class="p">,</span> <span class="n">iterators</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The input arguments.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>iterators</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[str, <span title="speechain.iterator.abs.Iterator">Iterator</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
The dictionary that contains all the iterators for training and validation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>test_model</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Model
The model to be trained.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span>
    <span class="n">test_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">iterators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The input arguments.</span>
<span class="sd">        iterators: Dict</span>
<span class="sd">            The dictionary that contains all the iterators for training and validation.</span>
<span class="sd">        test_model: Model</span>
<span class="sd">            The model to be trained.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># parse infer_cfg depending on different situations</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span> <span class="n">load_yaml</span><span class="p">(</span>
                <span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">}</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
        <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">infer_cfg_dict</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">cfg</span><span class="p">))</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="n">cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="n">infer_cfg_dict</span><span class="p">[</span>
                    <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">cfg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;If infer_cfg is given in the form of a List, &quot;</span>
                    <span class="s2">&quot;it must be either a List[str] or a List[Dict]!&quot;</span>
                <span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="s2">&quot;shared_args&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="s2">&quot;exclu_args&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;exclu_args&quot;</span><span class="p">],</span> <span class="n">List</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;If infer_cfg is given by &#39;shared_args&#39; and &#39;exclu_args&#39;, &quot;</span>
                <span class="s2">&quot;infer_cfg[&#39;shared_args&#39;] must be a Dict and infer_cfg[&#39;exclu_args&#39;] must be a List.&quot;</span>
            <span class="p">)</span>
            <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;exclu_args&quot;</span><span class="p">]:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">Dict</span><span class="p">),</span> <span class="s2">&quot;&quot;</span>
                <span class="k">for</span> <span class="n">cfg_key</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">cfg_key</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Find a duplicate argument </span><span class="si">{</span><span class="n">cfg_key</span><span class="si">}</span><span class="s2"> in both &#39;shared_args&#39; and &#39;exclu_args&#39;!&quot;</span>
                        <span class="p">)</span>

                <span class="n">cfg</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">[</span><span class="s2">&quot;shared_args&quot;</span><span class="p">])</span>
                <span class="n">cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                <span class="n">infer_cfg_dict</span><span class="p">[</span>
                    <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">cfg</span>

        <span class="k">elif</span> <span class="p">(</span>
            <span class="s2">&quot;shared_args&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="ow">and</span> <span class="s2">&quot;exclu_args&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">default_inference</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                    <span class="nb">sorted</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="p">)</span>
                <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                        <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
                    <span class="p">):</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span>
                <span class="p">}</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;If infer_cfg is given in the form of a Dict, &quot;</span>
                <span class="s2">&quot;&#39;shared_args&#39; and &#39;exclu_args&#39; must be or not be in the key list at the same time!&quot;</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">infer_cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">infer_cfg_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">default_inference</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;infer_cfg must be given in the form of a string, a List, or a Dict!&quot;</span>
        <span class="p">)</span>

    <span class="c1"># loop each test configuration</span>
    <span class="k">for</span> <span class="n">infer_cfg_name</span><span class="p">,</span> <span class="n">infer_cfg</span> <span class="ow">in</span> <span class="n">infer_cfg_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># configuration-specific result path</span>
        <span class="n">test_result_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">test_result_path</span>
            <span class="p">),</span>
            <span class="n">infer_cfg_name</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># load the existing testing configuration for resuming</span>
        <span class="n">infer_cfg_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="s2">&quot;infer_cfg.yaml&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">):</span>
            <span class="n">infer_cfg</span> <span class="o">=</span> <span class="n">load_yaml</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">))</span>

        <span class="c1"># save the testing configuration file to infer_cfg_path</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">infer_cfg</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">infer_cfg_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">infer_cfg</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># unlike training and validation, the testing iterators are looped one by one</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># replace the slash with a percent symbol</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;%&quot;</span><span class="p">)</span>
            <span class="c1"># add the identity symbol to the path for multi-GPU testing</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">attach_model_folder_when_test</span><span class="p">:</span>
                <span class="n">test_dset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">test_model</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">test_dset_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_result_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
            <span class="n">test_rank_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dset_path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;rank</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">_tmp&quot;</span><span class="p">)</span>
            <span class="n">logger</span> <span class="o">=</span> <span class="n">logger_stdout_file</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>

            <span class="c1"># initialize top-n bad case presentation</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bad_cases_selection</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;There is no configuration of topN bad case selection in either your input &quot;</span>
                        <span class="s2">&quot;arguments or default values of your selected model. &quot;</span>
                        <span class="s2">&quot;So there will not be any reports about topN bad cases.&quot;</span>
                    <span class="p">)</span>
            <span class="c1"># the main testing process</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The configuration of topN bad case selection in the current testing process is </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">bad_cases_selection</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># initialize the testing monitor</span>
            <span class="n">monitor</span> <span class="o">=</span> <span class="n">TestMonitor</span><span class="p">(</span>
                <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">result_path</span><span class="o">=</span><span class="n">test_dset_path</span>
            <span class="p">)</span>

            <span class="c1"># check the resuming status</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
                <span class="c1"># loading the existed checkpoint</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">test_checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
                        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">test_checkpoint</span><span class="p">[</span><span class="s2">&quot;monitor&quot;</span><span class="p">])</span>
                    <span class="n">start_step</span> <span class="o">=</span> <span class="n">test_checkpoint</span><span class="p">[</span><span class="s2">&quot;start_step&quot;</span><span class="p">]</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The testing process resumes from the step no.</span><span class="si">{</span><span class="n">start_step</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="p">)</span>
                <span class="c1"># checkpoint does not exist</span>
                <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                    <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;No checkpoint is found in </span><span class="si">{</span><span class="n">test_rank_path</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;The testing process will start from scratch. &quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">start_step</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;The testing process will start from scratch. &quot;</span><span class="p">)</span>

            <span class="c1"># initialize the dataloaders from the given starting point</span>
            <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                <span class="n">iterator</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">start_step</span><span class="o">=</span><span class="n">start_step</span><span class="p">))</span>
            <span class="p">)</span>
            <span class="n">test_indices</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                <span class="n">iterator</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="c1"># if there are multiple dataloaders for the current testing set,</span>
            <span class="c1"># the sample indices of the first element will be used to make the reports</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">test_indices</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                <span class="n">test_indices</span> <span class="o">=</span> <span class="n">test_indices</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">test_indices</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="c1"># report the total number of testing steps needed to be done</span>
            <span class="n">total_step_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_indices</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Totally </span><span class="si">{</span><span class="n">total_step_num</span><span class="si">}</span><span class="s2"> testing steps.&quot;</span><span class="p">)</span>

            <span class="c1"># make sure that no gradient appears during testing</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">(</span><span class="n">total_step_num</span><span class="o">=</span><span class="n">total_step_num</span><span class="p">)</span>
                <span class="c1"># iterate the testing batches</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step_num</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">start_step</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="c1"># only fetch the testing data right before decoding and evaluation</span>
                    <span class="n">test_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                        <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                    <span class="p">)</span>
                    <span class="c1"># skip the empty testing batch</span>
                    <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">test_batch</span><span class="p">):</span>
                        <span class="k">continue</span>
                    <span class="c1"># evaluate the current testing batch and get the evaluation results</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">test_results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                            <span class="n">test_batch</span><span class="o">=</span><span class="n">test_batch</span><span class="p">,</span> <span class="n">infer_conf</span><span class="o">=</span><span class="n">infer_cfg</span>
                        <span class="p">)</span>
                    <span class="c1"># skip the current step if encounter an error (any kind)</span>
                    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_test_exception</span><span class="p">:</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;0&#39;</span><span class="si">}</span><span class="s2"> meets the error &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2"> at step no.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;Indices of the involved testing samples in this step is </span><span class="si">{</span><span class="n">test_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="p">)</span>
                            <span class="k">continue</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="n">e</span>
                    <span class="c1"># record evaluation results</span>
                    <span class="n">monitor</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
                        <span class="n">step_num</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="n">test_results</span><span class="o">=</span><span class="n">test_results</span><span class="p">,</span>
                        <span class="n">test_index</span><span class="o">=</span><span class="n">test_indices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="p">)</span>

                    <span class="c1"># reduce the number of IO operations to speed up the testing</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">)</span> <span class="o">%</span> <span class="n">monitor</span><span class="o">.</span><span class="n">report_per_steps</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">total_step_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="c1"># save the checkpoint of the current step for both resuming and multi-GPU evaluation</span>
                        <span class="c1"># the iteration conditions of the test dataloader will also be saved for resuming</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                            <span class="nb">dict</span><span class="p">(</span><span class="n">start_step</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()),</span>
                            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_rank_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                        <span class="p">)</span>

            <span class="c1"># waiting for the data saving daemon process to finish before calling finish_epoch()</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">wait_empty_queues</span><span class="p">()</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># obtain the group information of the current iterator</span>
                <span class="n">group_info</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">):</span>
                    <span class="c1"># Dict[str, Dict[str, str]]</span>
                    <span class="n">group_info</span> <span class="o">=</span> <span class="n">iterator</span><span class="o">.</span><span class="n">get_group_info</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                    <span class="c1"># List[Dict[str, Dict[str, str]]]</span>
                    <span class="n">group_info_list</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">value</span><span class="o">.</span><span class="n">get_group_info</span><span class="p">()</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">iterator</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">]</span>
                    <span class="k">for</span> <span class="n">group_dict</span> <span class="ow">in</span> <span class="n">group_info_list</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">group_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">group_info</span> <span class="o">=</span> <span class="n">group_dict</span>
                            <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span>

                <span class="c1"># finish the evaluation and store the results to the disk</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">finish_epoch</span><span class="p">(</span><span class="n">meta_info</span><span class="o">=</span><span class="n">group_info</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="runner.Runner.train" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">data_cfg</span><span class="p">,</span> <span class="n">iterators</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim_sches</span><span class="p">,</span> <span class="n">logger</span><span class="p">,</span> <span class="n">monitor</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>args</code>
            </td>
            <td>
                  <code><span title="argparse.Namespace">Namespace</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>argparse.Namespace
The input arguments.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>data_cfg</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
The data loading configuration. Used to initialize the iterator for model visualization.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>iterators</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[str, <span title="speechain.iterator.abs.Iterator">Iterator</span>]] or <span title="typing.Dict">Dict</span>[str, <span title="speechain.iterator.abs.Iterator">Iterator</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
The dictionary that contains all the iterators for training and validation.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code><span title="speechain.model.abs.Model">Model</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Model
The model to be trained.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>optim_sches</code>
            </td>
            <td>
                  <code><span title="typing.Dict">Dict</span>[str, <span title="speechain.optim_sche.abs.OptimScheduler">OptimScheduler</span>] or <span title="speechain.optim_sche.abs.OptimScheduler">OptimScheduler</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dict
The dictionary that contains all the OptimSchedulers used to update the model parameters.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>logger</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>monitor</code>
            </td>
            <td>
                  <code><span title="speechain.monitor.TrainValidMonitor">TrainValidMonitor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>TrainValidMonitor
The wrapper class for a training monitor and a validation monitor.
The training monitor controls the training process of the model and generates the real-time logging
information.
The validation monitor controls the validation process of the model and generates the real-time
logging information.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>speechain/runner.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span>
    <span class="n">data_cfg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">iterators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">]]</span> <span class="ow">or</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Model</span><span class="p">,</span>
    <span class="n">optim_sches</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OptimScheduler</span><span class="p">]</span> <span class="ow">or</span> <span class="n">OptimScheduler</span><span class="p">,</span>
    <span class="n">logger</span><span class="p">,</span>
    <span class="n">monitor</span><span class="p">:</span> <span class="n">TrainValidMonitor</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        args: argparse.Namespace</span>
<span class="sd">            The input arguments.</span>
<span class="sd">        data_cfg: Dict</span>
<span class="sd">            The data loading configuration. Used to initialize the iterator for model visualization.</span>
<span class="sd">        iterators: Dict</span>
<span class="sd">            The dictionary that contains all the iterators for training and validation.</span>
<span class="sd">        model: Model</span>
<span class="sd">            The model to be trained.</span>
<span class="sd">        optim_sches: Dict</span>
<span class="sd">            The dictionary that contains all the OptimSchedulers used to update the model parameters.</span>
<span class="sd">        logger:</span>

<span class="sd">        monitor: TrainValidMonitor</span>
<span class="sd">            The wrapper class for a training monitor and a validation monitor.</span>
<span class="sd">            The training monitor controls the training process of the model and generates the real-time logging</span>
<span class="sd">            information.</span>
<span class="sd">            The validation monitor controls the validation process of the model and generates the real-time</span>
<span class="sd">            logging information.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span> <span class="o">&lt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span>
    <span class="p">),</span> <span class="s2">&quot;Your given start_epoch is larger than your given num_epochs!&quot;</span>

    <span class="c1"># --- checking the data lengths of all training iterators --- #</span>
    <span class="c1"># multiple dataloaders scenario</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">train_batch_nums</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="p">)</span>
        <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">train_batch_nums</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_batch_nums</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Your training iterators have different batch numbers: </span><span class="si">{</span><span class="n">train_batch_nums</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The actual batch number during training is set to </span><span class="si">{</span><span class="n">min_train_batch_num</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
    <span class="c1"># single dataloader scenario</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">Iterator</span><span class="p">):</span>
        <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please don&#39;t nest data_cfg[&#39;train&#39;] more than twice!&quot;</span><span class="p">)</span>

    <span class="c1"># --- checking the data lengths of all validation iterators --- #</span>
    <span class="c1"># multiple dataloaders scenario</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">valid_batch_nums</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span> <span class="k">for</span> <span class="n">iterator</span> <span class="ow">in</span> <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="p">)</span>
        <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">valid_batch_nums</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_batch_nums</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Your validation iterators have different batch numbers: </span><span class="si">{</span><span class="n">valid_batch_nums</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;The actual batch number during validation is set to </span><span class="si">{</span><span class="n">min_valid_batch_num</span><span class="si">}</span><span class="s2">!&quot;</span>
            <span class="p">)</span>
    <span class="c1"># single dataloader scenario</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="n">Iterator</span><span class="p">):</span>
        <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Please don&#39;t nest data_cfg[&#39;valid&#39;] more than twice!&quot;</span><span class="p">)</span>

    <span class="c1"># synchronize the batch numbers across all the distributed processes</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
        <span class="n">_world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
        <span class="c1"># make sure that all processes have the same number of training steps</span>
        <span class="n">_all_batch_num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_world_size</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
            <span class="n">_all_batch_num</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">min_train_batch_num</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">min_train_batch_num</span> <span class="o">=</span> <span class="n">_all_batch_num</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># make sure that all processes have the same number of validation steps</span>
        <span class="n">_all_batch_num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_world_size</span><span class="p">)])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
            <span class="n">_all_batch_num</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">min_valid_batch_num</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">min_valid_batch_num</span> <span class="o">=</span> <span class="n">_all_batch_num</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># --- Initialize the iterator for model visualization --- #</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_valid_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">_valid_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">_valid_keys</span> <span class="ow">and</span> <span class="s2">&quot;conf&quot;</span> <span class="ow">in</span> <span class="n">_valid_keys</span>
            <span class="p">):</span>
                <span class="n">visual_iterator</span> <span class="o">=</span> <span class="n">Iterator</span><span class="p">(</span>
                    <span class="n">dataset_type</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span><span class="s2">&quot;dataset_type&quot;</span><span class="p">],</span>
                    <span class="n">dataset_conf</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span><span class="s2">&quot;dataset_conf&quot;</span><span class="p">],</span>
                    <span class="n">batches_per_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">,</span>
                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">ngpu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">distributed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">is_descending</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">visual_domain</span> <span class="o">=</span> <span class="n">_valid_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;There are multiple sub-Dict in your given data_cfg[&#39;valid&#39;]. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;The one named </span><span class="si">{</span><span class="n">visual_domain</span><span class="si">}</span><span class="s2"> is used to initialize the visualization iterator.&quot;</span>
                <span class="p">)</span>
                <span class="n">visual_iterator</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">visual_domain</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">(</span>
                        <span class="n">dataset_type</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="n">visual_domain</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span>
                            <span class="s2">&quot;dataset_type&quot;</span>
                        <span class="p">],</span>
                        <span class="n">dataset_conf</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">][</span><span class="n">visual_domain</span><span class="p">][</span><span class="s2">&quot;conf&quot;</span><span class="p">][</span>
                            <span class="s2">&quot;dataset_conf&quot;</span>
                        <span class="p">],</span>
                        <span class="n">batches_per_epoch</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">,</span>
                        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">ngpu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">distributed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">is_descending</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">visual_iterator</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">visual_iterator</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># loop each epoch until the end</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># update the random seeds for the current epoch to keep in line with the dataloaders</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">set_random_seeds</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span> <span class="o">+</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="c1"># start the current training epoch</span>
        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">start_train_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="c1"># initialize all the training dataloaders</span>
        <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
            <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="c1"># --- Training Stage --- #</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># loop all the training batches</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_train_batch_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">step_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">min_train_batch_num</span><span class="p">)</span>

            <span class="c1"># --- data loading part --- #</span>
            <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
            <span class="p">)(</span><span class="s2">&quot;data_load_time&quot;</span><span class="p">):</span>
                <span class="n">train_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                    <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                <span class="p">)</span>
                <span class="c1"># single-GPU case, directly skip the current step when meeting an empty batch</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                    <span class="c1"># skip the empty validation batch</span>
                    <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">):</span>
                        <span class="k">continue</span>

                <span class="c1"># multi-GPU case, scatter the skip flag to all nodes</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">skip_flag_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
                        <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">train_batch</span><span class="p">):</span>
                        <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="c1"># as long as one node meets an empty batch, all nodes will simultaneously skip the current step</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                        <span class="n">skip_flag_list</span><span class="p">,</span> <span class="n">skip_flag</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">skip_flag_list</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">continue</span>

            <span class="c1"># forward the batch to get the training criteria and optimize the model</span>
            <span class="n">train_metrics</span><span class="p">,</span> <span class="n">optim_lr</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
            <span class="c1"># whether to skip the model forward part and model optimization part</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span><span class="p">:</span>
                <span class="c1"># --- model forward part --- #</span>
                <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_amp</span><span class="p">):</span>
                    <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                        <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
                    <span class="p">)(</span><span class="s2">&quot;model_forward_time&quot;</span><span class="p">):</span>
                        <span class="k">try</span><span class="p">:</span>
                            <span class="n">losses</span><span class="p">,</span> <span class="n">train_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                                <span class="n">batch_data</span><span class="o">=</span><span class="n">train_batch</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span>
                            <span class="p">)</span>
                        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_train_exception</span><span class="p">:</span>
                                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                    <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                    <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> training step will be skipped!&quot;</span>
                                <span class="p">)</span>
                                <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                        <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                        <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> training step will be skipped!&quot;</span>
                                    <span class="p">)</span>
                                <span class="k">continue</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">raise</span> <span class="n">e</span>

                <span class="c1"># whether to skip the model optimization part</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">no_optim</span><span class="p">:</span>
                    <span class="c1"># --- loss backward and optimization part --- #</span>
                    <span class="n">optim_lr</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">optim_sche</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="n">optim_sche</span><span class="o">.</span><span class="n">step</span><span class="p">(</span>
                            <span class="n">losses</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
                            <span class="n">time_func</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                                <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">train_monitor</span>
                            <span class="p">),</span>
                            <span class="n">optim_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                            <span class="n">step_num</span><span class="o">=</span><span class="n">step_num</span><span class="p">,</span>
                            <span class="n">epoch_num</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
                            <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="n">optim_lr</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optim_sche</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span>

            <span class="c1"># log the information of the current training step</span>
            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">monitor</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span>
                    <span class="n">step_num</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">optim_lr</span><span class="o">=</span><span class="n">optim_lr</span><span class="p">,</span> <span class="n">train_metrics</span><span class="o">=</span><span class="n">train_metrics</span>
                <span class="p">)</span>

        <span class="c1"># finish the current training epoch</span>
        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">finish_train_epoch</span><span class="p">()</span>

        <span class="c1"># --- Validation Stage --- #</span>
        <span class="c1"># start the validation part of the current epoch</span>
        <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">monitor</span><span class="o">.</span><span class="n">start_valid_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">valid_flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">valid_per_epochs</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">valid_flag</span><span class="p">:</span>
            <span class="c1"># initialize all the validation dataloaders</span>
            <span class="n">data_loaders</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                <span class="n">iterators</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">build_loader</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
            <span class="p">)</span>

            <span class="c1"># make sure that no gradient appears during validation</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                <span class="c1"># loop all validation batches</span>
                <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_valid_batch_num</span><span class="p">):</span>
                    <span class="c1"># --- data loading part --- #</span>
                    <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                        <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">valid_monitor</span>
                    <span class="p">)(</span><span class="s2">&quot;data_load_time&quot;</span><span class="p">):</span>
                        <span class="n">valid_batch</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">dict_transform</span><span class="p">(</span>
                            <span class="n">src_dict</span><span class="o">=</span><span class="n">data_loaders</span><span class="p">,</span> <span class="n">transform_func</span><span class="o">=</span><span class="nb">next</span>
                        <span class="p">)</span>
                        <span class="c1"># single-GPU case, directly skip the current step when meeting an empty batch</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                            <span class="c1"># skip the empty validation batch</span>
                            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">valid_batch</span><span class="p">):</span>
                                <span class="k">continue</span>
                        <span class="c1"># multi-GPU case, scatter the skip flag to all nodes</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">skip_flag_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span>
                                <span class="p">[</span>
                                    <span class="kc">False</span>
                                    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
                                        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
                                    <span class="p">)</span>
                                <span class="p">]</span>
                            <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">valid_batch</span><span class="p">):</span>
                                <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                                    <span class="n">model</span><span class="o">.</span><span class="n">device</span>
                                <span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">skip_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
                                    <span class="n">model</span><span class="o">.</span><span class="n">device</span>
                                <span class="p">)</span>
                            <span class="c1"># as long as one node meets an empty batch,</span>
                            <span class="c1"># all nodes will skip the current step at the same time</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span>
                                <span class="n">skip_flag_list</span><span class="p">,</span> <span class="n">skip_flag</span>
                            <span class="p">)</span>
                            <span class="k">if</span> <span class="n">skip_flag_list</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                                <span class="k">continue</span>

                    <span class="c1"># forward the batch to get the validation criteria</span>
                    <span class="n">valid_metrics</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="c1"># whether to skip the model forward part</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span><span class="p">:</span>
                        <span class="c1"># --- model forward part --- #</span>
                        <span class="c1"># with autocast(enabled=args.use_amp) is not used here for accurate validation</span>
                        <span class="k">with</span> <span class="bp">cls</span><span class="o">.</span><span class="n">measure_time</span><span class="p">(</span>
                            <span class="kc">None</span> <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">monitor</span><span class="o">.</span><span class="n">valid_monitor</span>
                        <span class="p">)(</span><span class="s2">&quot;model_forward_time&quot;</span><span class="p">):</span>
                            <span class="k">try</span><span class="p">:</span>
                                <span class="n">valid_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch_data</span><span class="o">=</span><span class="n">valid_batch</span><span class="p">)</span>
                            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">ignore_train_exception</span><span class="p">:</span>
                                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                                        <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                        <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> validation step will be skipped!&quot;</span>
                                    <span class="p">)</span>
                                    <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                            <span class="sa">f</span><span class="s2">&quot;Rank no.</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> meets error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">! &quot;</span>
                                            <span class="sa">f</span><span class="s2">&quot;no.</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> validation step will be skipped!&quot;</span>
                                        <span class="p">)</span>
                                    <span class="k">continue</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="k">raise</span> <span class="n">e</span>

                    <span class="c1"># no step log for the validation step</span>
                    <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">monitor</span><span class="o">.</span><span class="n">valid_step</span><span class="p">(</span><span class="n">valid_metrics</span><span class="o">=</span><span class="n">valid_metrics</span><span class="p">)</span>

        <span class="c1"># --- Visualization Stage --- #</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_interval</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="c1"># make sure that all processes go through the validation phase smoothly</span>
            <span class="k">if</span> <span class="n">visual_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">visual_iterator</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
                    <span class="n">visual_domain</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="o">.</span><span class="n">build_loader</span><span class="p">()</span>
                    <span class="n">visual_indices</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">visual_domain</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">visual_iterator</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="p">[</span>
                        <span class="n">visual_domain</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">build_loader</span><span class="p">()</span>
                    <span class="n">visual_indices</span> <span class="o">=</span> <span class="n">visual_iterator</span><span class="p">[</span>
                        <span class="n">visual_domain</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">get_batch_indices</span><span class="p">()</span>

                <span class="c1"># make sure that no gradient appears during validation</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">visual_dataloader</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">visual_dataloader</span><span class="p">)</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
                    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">visual_snapshot_number</span><span class="p">):</span>
                        <span class="n">visual_sample</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">visual_dataloader</span><span class="p">)</span>
                        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">is_empty_batch</span><span class="p">(</span><span class="n">visual_sample</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;The visual sample </span><span class="si">{</span><span class="n">visual_indices</span><span class="p">[</span><span class="n">step</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> is empty, &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;so its visualization is skipped!&quot;</span>
                            <span class="p">)</span>
                            <span class="k">continue</span>
                        <span class="c1"># feed the current sample to the model</span>
                        <span class="n">monitor</span><span class="o">.</span><span class="n">valid_model_snapshot</span><span class="p">(</span>
                            <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
                            <span class="n">domain</span><span class="o">=</span><span class="n">visual_domain</span><span class="p">,</span>
                            <span class="n">sample_index</span><span class="o">=</span><span class="n">visual_indices</span><span class="p">[</span><span class="n">step</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">used_sample</span><span class="o">=</span><span class="n">visual_sample</span><span class="p">,</span>
                        <span class="p">)</span>
            <span class="c1"># synchronize all the GPU processes at the end of the visualization stage</span>
            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1"># finish_valid_epoch() should be called before checkpoint saving</span>
        <span class="n">finish_valid_flag</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">finish_valid_flag</span> <span class="o">=</span> <span class="n">monitor</span><span class="o">.</span><span class="n">finish_valid_epoch</span><span class="p">(</span>
                <span class="n">valid_flag</span><span class="o">=</span><span class="n">valid_flag</span><span class="p">,</span> <span class="n">valid_per_epochs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">valid_per_epochs</span>
            <span class="p">)</span>

        <span class="c1"># store the checkpoint of the current epoch for later resuming</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">no_optim</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;start_epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="s2">&quot;latest_model&quot;</span><span class="p">:</span> <span class="p">(</span>
                            <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                            <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span>
                            <span class="k">else</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                        <span class="p">),</span>
                        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="n">monitor</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                        <span class="s2">&quot;optim_sches&quot;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="n">name</span><span class="p">:</span> <span class="n">o</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">optim_sches</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="p">},</span>
                    <span class="p">},</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train_result_path</span><span class="p">,</span> <span class="s2">&quot;checkpoint.pth&quot;</span><span class="p">),</span>
                <span class="p">)</span>

        <span class="c1"># early-stopping checking for single-GPU</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">and</span> <span class="n">finish_valid_flag</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># early-stopping checking for multi-GPU</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
            <span class="n">stop_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">flag_list</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">finish_valid_flag</span><span class="p">:</span>
                    <span class="n">stop_flag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">([</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">flag_list</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">stop_flag</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
                <span class="p">]</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stop_flag</span><span class="p">,</span> <span class="n">flag_list</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">stop_flag</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>
                <span class="k">break</span>

    <span class="c1"># check whether all the monitor queues become empty in every minute</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">monitor</span><span class="o">.</span><span class="n">wait_empty_queues</span><span class="p">()</span>

    <span class="c1"># synchronize all the GPU processes at the end</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">distributed</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>