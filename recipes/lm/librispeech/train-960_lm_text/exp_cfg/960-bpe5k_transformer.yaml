###############################################################################
# Model: Transformer-LM
# Tokens: BPE 5k trained from train-960 of LibriSpeech
# losses: 0.0 Label Smooth
# Training: lm_text and train-960 of LibriSpeech
# Validation: dev of LibriSpeech
# Testing: test-clean & test-other of LibriSpeech
# Authors: Heli Qi
# Required GPUs: 2 Ã— NVIDIA RTX A6000 (49GB)
# Required Time: up to
# Expected Perplexity performance (test-clean / test-other):  /
# ############################################################################


###################################
# Experiment Parameters and setup #
###################################
# if your dumped dataset is outside the toolkit folder, please change dataset_path. There should be a folder named 'libritts' in dataset_path
dataset_path: datasets/

# data-related
vocab_set: train-960
valid_set: dev

# tokenizer-related
txt_format: no-punc
token_type: sentencepiece
token_num: bpe5k

# batch-related values, 6e4
batch_len: 6e3

# model-related
label_smoothing: 0.0
length_normalized: true
d_model: 768
num_heads: 12
fdfwd_dim: 3072
enc_layer_num: 12
dropout: 0.0

# optimizer-related
warmup_steps: 50000

# running-related
seed: 0
train_num_workers: 0
valid_num_workers: 0
pin_memory: False
non_blocking: False

# gradient-related
accum_grad: 8
ft_factor: 1.0
grad_clip: 5.0

# multi-GPU-related
ngpu: 2 # please change ngpu based on the situation of your machine
gpus: null # null means the GPUs with the largest free memory will be used
ignore_train_exception: True

# training-related
train: True
best_model_selection:
  - !tuple (valid, text_ppl, min, 5)
  - !tuple (valid, loss, min, 5)
early_stopping_patience: 5
num_epochs: 20

# snapshot-related
visual_snapshot_number: 3
visual_snapshot_interval: 5

# testing-related
test: False
test_model: valid_text_ppl_best

# These arguments are shared by data_cfg and train_cfg
data_root: !ref <dataset_path>/librispeech/data



##############################
# Data Loading Configuration #
##############################
data_cfg:
  train:
    type: block.BlockIterator
    conf:
      dataset_type: speech_text.SpeechTextDataset
      dataset_conf:
        main_data:
          text:
          - !ref <data_root>/lm_text/<txt_format>_lm_text
          - !ref <data_root>/wav/train-960/idx2<txt_format>_text

      shuffle: True
      is_descending: True
      batch_len: !ref <batch_len>

  valid:
    type: abs.Iterator
    conf:
      dataset_type: speech_text.SpeechTextDataset
      dataset_conf:
        main_data:
          text: !ref <data_root>/wav/<valid_set>/idx2<txt_format>_text
      shuffle: False

  test:
    test-clean:
      type: abs.Iterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            text: !ref <data_root>/wav/test-clean/idx2<txt_format>_text
        shuffle: False

    test-other:
      type: abs.Iterator
      conf:
        dataset_type: speech_text.SpeechTextDataset
        dataset_conf:
          main_data:
            text: !ref <data_root>/wav/test-other/idx2<txt_format>_text
        shuffle: False



####################################
# Model Construction Configuration #
####################################
train_cfg:
  model:
    model_type: lm.LM
    model_conf:
      customize_conf:
        token_type: !ref <token_type>
        token_path: !ref <data_root>/<token_type>/<vocab_set>/<token_num>/<txt_format>

    module_conf:
      emb:
        type: embed
        conf:
          embedding_dim: !ref <d_model>

      encoder:
        type: transformer
        conf:
          posenc_dropout: !ref <dropout>
          posenc_scale: false
          emb_layernorm: false
          emb_scale: false
          d_model: !ref <d_model>
          num_heads: !ref <num_heads>
          num_layers: !ref <enc_layer_num>
          att_dropout: !ref <dropout>
          fdfwd_dim: !ref <fdfwd_dim>
          fdfwd_activation: GELU
          fdfwd_dropout: !ref <dropout>
          res_dropout: !ref <dropout>
          layernorm_first: true

    criterion_conf:
      length_normalized: !ref <length_normalized>
      label_smoothing: !ref <label_smoothing>

  optim_sches:
    type: noam.Noamlr
    conf:
      optim_type: Adam
      optim_conf:
        betas:
          - 0.9
          - 0.98
        eps: 1.0e-9
      warmup_steps: !ref <warmup_steps>
