##############################################################################
# Model: Conformer-ASR small version (13.79M)
# Tokens: BPE 5k trained on train-clean-100 of LibriSpeech
# Batching: 241 steps per epoch (25 minutes speech per batch)
# Optimization: 16000 warming-up steps (~66 warming-up epochs) & 0.002 peak LR
# Augmentation: Speech Perturbation + SpecAugment
# losses: 0.2 Label Smooth + CTC loss
# Training: train-clean-5 of LibriSpeech
# Validation: dev-clean-2 of LibriSpeech
# Testing: test-clean LibriSpeech
# Authors: Bagus Tris Atmaja (@bagustris)
# Required GPUs: 1 Ã— NVIDIA RTX 3090 (24GB)
# Required Time: 4 hours
# Expected WER performance (test-clean): ASR-only (42%) and ASR-LM (N/A)
# ############################################################################


###################################
# Experiment Parameters and setup #
###################################
# dataset-related
# if your dumped dataset is outside the toolkit folder, please change dataset_path. There should be a folder named 'librispeech' in dataset_path
dataset_path: datasets/
dataset: librispeech
train_set: train-clean-5
valid_set: dev-clean-2

# waveform-related
wav_format: wav
sample_rate: 16000

# tokenizer-related
txt_format: no-punc
vocab_subset: train-clean-5
token_type: sentencepiece
token_num: bpe1k

# batch-related
batch_len: 2.4e6 #7

# model-related
ctc_weight: 0.3
label_smoothing: 0.2
d_model: 144
num_heads: 4
fdfwd_dim: 1024
dropout: 0.1

# optimizer-related
warmup_steps: 16000

# running-related
seed: 0
num_workers: 1
pin_memory: False
non_blocking: False

# gradient-related
accum_grad: 1
ft_factor: 1.0
grad_clip: 5.0

# multi-GPU-related
ngpu: 1 # please change ngpu based on the situation of your machine
gpus: null # null means the GPUs with the largest free memory will be used

# training-related
train: True
best_model_selection: !tuple (valid, accuracy, max, 10)
early_stopping_patience: 20
num_epochs: 200

# snapshot-related
visual_snapshot_number: 3
visual_snapshot_interval: 5

# testing-related
test: False

# This argument is shared by data_cfg and train_cfg
data_root: !ref <dataset_path>/<dataset>/data



#################################
# Model Inference Configuration #
#################################
infer_cfg:
  shared_args:
    beam_size: 16
  exclu_args:
    # ASR-only decoding
    - ctc_weight: 0.2
    # # ASR-LM decoding
    # - ctc_weight: 0.3
    #   lm_weight: 0.6



##############################
# Data Loading Configuration #
##############################
data_cfg:
  train:
    type: block.BlockIterator
    conf:
      dataset_type: speech_text.SpeechTextDataset
      dataset_conf:
        main_data:
          feat: !ref <data_root>/<wav_format>/<train_set>/idx2wav
          text: !ref <data_root>/<wav_format>/<train_set>/idx2<txt_format>_text
        use_speed_perturb: False

      data_len: !ref <data_root>/<wav_format>/<train_set>/idx2wav_len
      shuffle: True
      is_descending: True
      batch_len: !ref <batch_len>

  valid:
    type: abs.Iterator
    conf:
      dataset_type: speech_text.SpeechTextDataset
      dataset_conf:
        main_data:
          feat: !ref <data_root>/<wav_format>/<valid_set>/idx2wav
          text: !ref <data_root>/<wav_format>/<valid_set>/idx2<txt_format>_text

      shuffle: False
      data_len: !ref <data_root>/<wav_format>/<valid_set>/idx2wav_len

# Model Construction Configuration #
####################################
train_cfg:
  model:
    model_type: ar_asr.ARASR
    model_conf:
      customize_conf:
        ctc_weight: !ref <ctc_weight>
        token_type: !ref <token_type>
        token_path: !ref <data_root>/<token_type>/<vocab_subset>/<token_num>/<txt_format>
        # lm_model_cfg: recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer/train_cfg.yaml
        # lm_model_path: recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer/models/5_valid_text_ppl_average.pth

    module_conf:
      frontend:
        type: frontend.speech2mel.Speech2MelSpec
        conf:
          sr: !ref <sample_rate>
          preemphasis: 0.97
          hop_length: 0.010
          win_length: 0.025
          n_mels: 80

      normalize: True

      enc_prenet:
        type: prenet.conv2d.Conv2dPrenet
        conf:
          conv_dims:
            - !ref <d_model>
            - !ref <d_model>
          conv_kernel: 3
          conv_stride: 2
          conv_batchnorm: true
          conv_activation: LeakyReLU
          lnr_dims: !ref <d_model>

      encoder:
        type: conformer.encoder.ConformerEncoder
        conf:
          posenc_dropout: !ref <dropout>
          emb_scale: False
          d_model: !ref <d_model>
          num_heads: !ref <num_heads>
          num_layers: 12
          att_dropout: !ref <dropout>
          fdfwd_dim: !ref <fdfwd_dim>
          fdfwd_activation: GELU
          fdfwd_dropout: !ref <dropout>
          res_dropout: !ref <dropout>
          layernorm_first: true

      dec_emb:
        type: prenet.embed.EmbedPrenet
        conf:
          embedding_dim: !ref <d_model>

      decoder:
        type: transformer.decoder.TransformerDecoder
        conf:
          posenc_dropout: !ref <dropout>
          posenc_scale: false
          emb_layernorm: true
          emb_scale: false
          d_model: !ref <d_model>
          num_heads: !ref <num_heads>
          num_layers: 6
          att_dropout: !ref <dropout>
          fdfwd_dim: !ref <fdfwd_dim>
          fdfwd_activation: GELU
          fdfwd_dropout: !ref <dropout>
          res_dropout: !ref <dropout>
          layernorm_first: true

    criterion_conf:
      ce_loss:
        label_smoothing: !ref <label_smoothing>

  optim_sches:
    type: noam.Noamlr
    conf:
      optim_type: Adam
      optim_conf:
        lr: 0.002
        betas:
          - 0.9
          - 0.98
        eps: 1.0e-9
      warmup_steps: !ref <warmup_steps>