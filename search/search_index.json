{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>A Machine Speech Chain Toolkit for ASR, TTS, and Both</p> <p>SpeeChain is an open-source PyTorch-based speech and language processing toolkit produced by the AHC lab at Nara Institute of Science and Technology (NAIST).  This toolkit is designed to simplify the pipeline of the research on the machine speech chain,  i.e., the joint model of automatic speech recognition (ASR) and text-to-speech synthesis (TTS). </p> <p>SpeeChain is currently in beta. Contribution to this toolkit is warmly welcomed anywhere, anytime! </p> <p>If you find our toolkit helpful for your research, we sincerely hope that you can give us a star\u2b50!  Anytime you encounter problems when using our toolkit, please don't hesitate to leave us an issue!</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Machine Speech Chain</li> <li>Toolkit Characteristics</li> <li>Directory Structure</li> <li>Quick Start</li> <li>Work flow</li> </ol>"},{"location":"#machine-speech-chain","title":"Machine Speech Chain","text":"<ul> <li>Offline TTS\u2192ASR Chain</li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"#toolkit-characteristics","title":"Toolkit Characteristics","text":"<ul> <li>Data Processing:<ul> <li>On-the-fly Log-Mel Spectrogram Extraction  </li> <li>On-the-fly SpecAugment  </li> <li>On-the-fly Feature Normalization  </li> </ul> </li> <li>Model Training:<ul> <li>Multi-GPU Model Distribution based on torch.nn.parallel.DistributedDataParallel </li> <li>Real-time status reporting by online Tensorboard and offline Matplotlib </li> <li>Real-time learning dynamics visualization (attention visualization, spectrogram visualization)  </li> </ul> </li> <li>Data Loading:<ul> <li>On-the-fly mixture of multiple datasets in a single dataloader.  </li> <li>On-the-fly data selection for each dataloader to filter the undesired data samples.  </li> <li>Multi-dataloader batch generation is used to form training batches using multiple datasets.   </li> </ul> </li> <li>Optimization:<ul> <li>Model training can be done by multiple optimizers. Each optimizer is responsible for a specific part -  model parameters.  </li> <li>Gradient accumulation for mimicking the large-batch gradients by the ones on several small batches.  </li> <li>Easy-to-set finetuning factor to scale down the learning rates without any modification of the scheduler configuration.  </li> </ul> </li> <li>Model Evaluation:<ul> <li>Multi-level .md evaluation reports (overall-level, group-level model, and sample-level) without any - yout misplacement.  </li> <li>Histogram visualization for the distribution of evaluation metrics.  </li> <li>Top N bad case analysis for better model diagnosis.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"#directory-structure","title":"Directory Structure","text":"<pre><code>\u251c\u2500\u2500 config                # configuration for feature extraction\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 feat\n\u251c\u2500\u2500 CONTRIBUTING.md       # convention for contributor\n\u251c\u2500\u2500 create_env.sh         # bash shell to create environment\n\u251c\u2500\u2500 datasets              # dataset folder, put data here, make softlink, or set in config file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data_dumping.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 librispeech\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 libritts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ljspeech\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_generator.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_post_processor.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mfa_preparation.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vctk\n\u251c\u2500\u2500 docs                  # folder to build docs\n\u251c\u2500\u2500 environment.yaml    \n\u251c\u2500\u2500 LICENSE              \n\u251c\u2500\u2500 recipes               # folder for experiment, you will work here\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 asr\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lm\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 offline_tts2asr\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 run.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tts\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 run.sh\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gen_ref_pages.py\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 speechain             # directory for speechain toolkit code\n    \u251c\u2500\u2500 criterion\n    \u251c\u2500\u2500 dataset\n    \u251c\u2500\u2500 infer_func\n    \u251c\u2500\u2500 iterator\n    \u251c\u2500\u2500 model\n    \u251c\u2500\u2500 module\n    \u251c\u2500\u2500 monitor.py\n    \u251c\u2500\u2500 optim_sche\n    \u251c\u2500\u2500 pyscripts\n    \u251c\u2500\u2500 runner.py\n    \u251c\u2500\u2500 snapshooter.py\n    \u251c\u2500\u2500 tokenizer\n    \u2514\u2500\u2500 utilbox\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Try minilibrispeech recipe <code>train-clean-5</code> in ASR.</p>"},{"location":"#workflow","title":"Workflow","text":"<p>We recommend you first install Anaconda into your machine before using our toolkit.  After the installation of Anaconda, please follow the steps below to deploy our toolkit on your machine:</p> <ol> <li>Find a path with enough disk memory space. (e.g., at least 500GB if you want to use LibriSpeech or LibriTTS datasets).  </li> <li>Clone our toolkit by <code>git clone https://github.com/bagustris/SpeeChain.git</code>.  </li> <li>Go to the root path of our toolkit by <code>cd SpeeChain</code>.  </li> <li>Run <code>source envir_preparation.sh</code> to build the environment for SpeeChain toolkit. After execution, a virtual environment named <code>speechain</code> will be created and two environmental variables <code>SPEECHAIN_ROOT</code> and <code>SPEECHAIN_PYTHON</code> will be initialized in your <code>~/.bashrc</code>. Note: It must be executed in the root path <code>SpeeChain</code> and by the command <code>source</code> rather than <code>./envir_preparation.sh</code>.  </li> <li>Run <code>conda activate speechain</code> in your terminal to examine the installation of Conda environment.  If the environment <code>speechain</code> is not successfully activated, please run <code>conda env create -f environment.yaml</code>, <code>conda activate speechain</code> and <code>pip install -e ./</code> to manually install it.  </li> <li> <p>Run <code>echo ${SPEECHAIN_ROOT}</code> and <code>echo ${SPEECHAIN_PYTHON}</code> in your terminal to examine the environmental variables. If either one is empty, please manually add them into your <code>~/.bashrc</code> by <code>export SPEECHAIN_ROOT=xxx</code> or <code>export SPEECHAIN_PYTHON=xxx</code> and then activate them by <code>source ~/.bashrc</code>. </p> <ul> <li> <p><code>SPEECHAIN_ROOT</code> should be the absolute path of the <code>SpeeChain</code> folder you have just cloned (i.e. <code>/xxx/SpeeChain</code> where <code>/xxx/</code> is the parent directory); </p> </li> <li> <p><code>SPEECHAIN_PYTHON</code> should be the absolute path of the python compiler in the folder of <code>speechain</code> environment (i.e. <code>/xxx/anaconda3/envs/speechain/bin/python3.X</code> where <code>/xxx/</code> is where your <code>anaconda3</code> is placed and <code>X</code> depends on <code>environment.yaml</code>).  </p> </li> </ul> </li> <li> <p>Read the handbook and start your journey in SpeeChain!  </p> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/","title":"ASR","text":"<p>ASR (Automatic Speech Recognition) is a technology that converts spoken language into text. ASR is widely used in various applications such as voice assistants, dictation software, and transcription services. SpeeChain provides a collection of ASR recipes that allow users to train and evaluate ASR models on various datasets. The recipes include configurations for different backbones, such as transformer and conformer, and provide pretrained models for reproducibility. Users can also create their own ASR models by following the instructions in the recipes.</p> <p>\ud83d\udc46Back to the recipe README.md</p>"},{"location":"asr/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Available Backbones</li> <li>Pretrained Models for Reproducibility</li> <li>Training an ASR model</li> <li>Creating your own ASR model</li> </ol>"},{"location":"asr/#available-backbones-wer-withwithout-lm","title":"Available Backbones (WER with/without LM)","text":"Dataset Subset Configuration Test Clean Test Other librispeech train-clean-5 5-bpe5k_conformer-small_lr2e-3 15.2% / 35.8% 35.8% / 42.3% train-clean-100 100-bpe5k_transformer-wide_lr2e-3  8.40% / 21.92%   5.50% / 15.56%  100-bpe5k_conformer-small_lr2e-3  6.3% / 9.3%   19.14% / 25.2%  100-bpe5k_conformer-medium_lr2e-3  7.87% / 21.36%   5.30% / 15.57%  100-bpe5k_conformer-large_lr2e-3  7.30% / 20.24%   5.33% / 15.15%  train-clean-460 460-bpe5k_transformer-large  % / %   % / %  460-bpe5k_conformer-large  % / %   % / %  train-960 960-bpe5k_transformer-large  % / %   % / %  960-bpe5k_conformer-large  % / %   % / %  libritts_librispeech train-960 960-bpe5k_transformer-large  % / %   % / %  <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#pretrained-models-for-reproducibility","title":"Pretrained Models for Reproducibility","text":"<p>For reproducibility of our ASR model configuration files in <code>${SPEECHAIN_ROOT}/recipes/asr/</code>, we provide the following pretrained models to ensure consistent performance:  </p> <ol> <li> <p>SentencePiece tokenizer models  </p> <ul> <li> <p>Please download tokenizer model and vocabulary to where your dataset is dumped. The default path is <code>${SPEECHAIN_ROOT}/datasets</code>.  Note: If your dataset is dumped outside SpeeChain, please replace <code>${SPEECHAIN_ROOT}/datasets</code> in the following commands by your place.</p> </li> <li> <p>LibriSpeech: </p> <ol> <li> <p>train-clean-100: </p> <pre><code># Download BPE model\ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n\n# Download BPE vocabulary\ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n</code></pre> </li> <li> <p>train-clean-460: </p> <pre><code># Download BPE model\ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n\n# Download BPE vocabulary\ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n</code></pre> </li> <li> <p>train-960: </p> <pre><code># Download BPE model\ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n\n# Download BPE vocabulary by \ngdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc \n</code></pre> </li> </ol> </li> </ul> </li> <li> <p>Transformer-based language models  </p> <ul> <li>Please download both LM model and configuration file. The default path is <code>${SPEECHAIN_ROOT}/recipes/lm</code>.  Note: If you want to store model files outside SpeeChain, please replace <code>${SPEECHAIN_ROOT}/recipes/lm</code> in the following commands by your place. Also, change the <code>lm_cfg_path</code> and <code>lm_model_path</code> arguments in each ASR configuration file.</li> <li> <p>LibriSpeech: </p> <ol> <li> <p>train-clean-100: </p> <pre><code># Download LM model  \ngdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer_gelu/models   \n# Download LM configuration  \ngdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer_gelu  \n</code></pre> </li> <li> <p>train-960:</p> <pre><code># Download LM model\ngdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/train-960_lm_text/exp/960-bpe5k_transformer_gelu/models \n\n# Download LM configuration\ngdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/train-960_lm_text/exp/960-bpe5k_transformer_gelu  \n</code></pre> </li> </ol> </li> </ul> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#training-an-asr-model","title":"Training an ASR model","text":"<p>Before training an ASR model, ensure that your target datasets are dumped by the scripts in <code>${SPEECHAIN_ROOT}/datasets/{your-target-dataset}</code>. More details on how to dump a dataset can be found here.</p>"},{"location":"asr/#use-an-existing-dataset-with-a-pre-tuned-configuration","title":"Use an existing dataset with a pre-tuned configuration","text":"<ol> <li> <p>locate a .yaml configuration file in <code>${SPEECHAIN_ROOT}/recipes/asr</code>.     Suppose we want to train an ASR model by the configuration <code>${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml</code>.</p> </li> <li> <p>Train and evaluate the ASR model on your target training set</p> </li> </ol> <p><pre><code>cd ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960\nbash run.sh --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb (--ngpu x --gpus x,x)\n</code></pre> Note:        1. Review the comments on the top of the configuration file to ensure that your computational resources fit the configuration before training the model.       If your resources do not match the configuration, adjust it by <code>--ngpu</code> and <code>--gpus</code> to match your available GPU memory.       2. To save the experimental results outside the toolkit folder <code>${SPEECHAIN_ROOT}</code>,           specify your desired location by appending <code>--train_result_path {your-target-path}</code> to <code>bash run.sh</code>.          In this example, <code>bash run.sh --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --train_result_path /a/b/c</code>          will save results to <code>/a/b/c/960-bpe5k_transformer-wide_ctc_perturb</code>.</p>"},{"location":"asr/#creating-a-new-configuration-for-a-non-existing-dataset","title":"Creating a new configuration for a non-existing dataset","text":"<ol> <li> <p>Dump your target dataset from the Internet following these instructions.  </p> </li> <li> <p>Create a folder for your dumped dataset <code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}</code>:    <pre><code>mkdir ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}\ncp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/run.sh ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/run.sh\n</code></pre> Note: </p> <ul> <li>Update the arguments <code>dataset</code> and <code>subset</code> (line no.16 &amp; 17) in <code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/run.sh</code>:  </li> </ul> <pre><code>dataset=librispeech -&gt; 'your-new-dataset'\nsubset='train-960' -&gt; 'your-new-dataset'\n</code></pre> </li> <li> <p>Copy a pre-tuned configuration file into your newly created folder.     Suppose we want to use the configuration <code>${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml</code>: <pre><code>cd ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}\nmkdir ./data_cfg ./exp_cfg\ncp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml ./exp_cfg\n</code></pre> Note: </p> <ul> <li>Update the dataset arguments at the beginning of your selected configuration:   <pre><code># dataset-related\ndataset: librispeech -&gt; 'your-new-dataset'\ntrain_set: train-960 -&gt; 'your-target-subset'\nvalid_set: dev -&gt; 'valid-set-of-new-dataset'\n\n# tokenizer-related\ntxt_format: asr\nvocab_set: train-960 -&gt; 'your-target-subset'\ntoken_type: sentencepiece\ntoken_num: bpe5k\n</code></pre></li> </ul> </li> <li> <p>Train the ASR model on your target training set:    <pre><code>cd ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}\nbash run.sh --test false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb (--ngpu x --gpus x,x)\n</code></pre> Note: </p> <ol> <li><code>--test false</code> is used to skip the testing stage.</li> <li>Ensure your computational resources match the configuration before training the model.</li> <li>To save experimental results outside ${SPEECHAIN_ROOT}, specify your desired location by appending --train_result_path {your-target-path} to bash run.sh.</li> </ol> </li> <li> <p>Tune the inference hyperparameters on the corresponding validation set    <pre><code>cp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/data_cfg/test_dev-clean+other.yaml ./data_cfg\nmv ./data_cfg/test_dev-clean.yaml ./data_cfg/test_{your-valid-set-name}.yaml\nbash run.sh --train false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --data_cfg test_{your-valid-set-name}\n</code></pre> Note: </p> <ol> <li> <p>Update the dataset arguments in <code>./data_cfg/test_{your-valid-set-name}.yaml</code>:      <pre><code>dataset: librispeech -&gt; 'your-new-dataset'\nvalid_dset: &amp;valid_dset dev-clean -&gt; &amp;valid_dset 'valid-set-of-new-dataset'\n</code></pre></p> </li> <li> <p><code>--train false</code> is used to skip the training stage.</p> </li> <li><code>--data_cfg</code> switches the data loading configuration from the original one for training in exp_cfg to the one for validation tuning.</li> <li>To access experimental results saved outside <code>${SPEECHAIN_ROOT}</code>, append <code>--train_result_path {your-target-path}</code> to <code>bash run.sh</code>.</li> </ol> </li> <li> <p>Evaluate the trained ASR model on the official test sets    <pre><code>bash run.sh --train false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --infer_cfg \"{the-best-configuration-you-get-during-validation-tuning}\"\n</code></pre> Note: </p> <ol> <li><code>--train false</code> is used to skip the training stage.  </li> <li>There are two ways to specify the optimal <code>infer_cfg</code> tuned on the validation set:  <ol> <li>Update <code>infer_cfg</code> in <code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml</code>.  </li> <li>Provide a parsable string as the value for <code>--infer_cfg</code> in the terminal. For example, <code>beam_size:16,ctc_weight:0.2</code> can be converted into a dictionary with two key-value items (<code>beam_size=16</code> and <code>ctc_weight=0.2</code>).  For more details about this syntax, refer to here.  </li> </ol> </li> <li>To access experimental results saved outside <code>${SPEECHAIN_ROOT}</code>, append <code>--train_result_path {your-target-path}</code> to <code>bash run.sh</code>.</li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#how-to-create-your-own-asr-model","title":"How to create your own ASR model","text":"<p>The detailed instructions for creating your own ASR model using SpeeChain are coming soon.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"criterion/","title":"Criterion","text":"<p>Criterion  is a Callable object which is the base class for all criterion objects in this toolkit.  It serves the role of evaluating the model forward calculation results.  Its output can be either a loss function used for training or an evaluation metric used for validation.</p> <p>This base class has two abstract interface functions: <code>criterion_init()</code> for criterion initialization and <code>__call__()</code> for criterion forward calculation.  </p> <ol> <li><code>__call__()</code> must be overridden if you want to make your own Criterion implementation.  </li> <li><code>criterion_init()</code> is not mandatory to be overridden because some criteria can directly be applied to the input data without any initialization such as speechain.criterion.accuracy.Accuracy.</li> </ol> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"criterion/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Criterion Library </li> <li>API Document </li> </ol>"},{"location":"criterion/#criterion-library","title":"Criterion Library","text":"<pre><code>/speechain\n    /criterion          \n        /abs.py             # Abstract class of Criterion. Base of all Criterion implementations.\n        /accuracy.py        # Criterion implementation of classification accuracy. Mainly used for ASR teacher-forcing accuracy.\n        /bce_logits.py      # Criterion implementation of binary cross entropy. Mainly used for TTS stop flag.\n        /cross_entropy.py   # Criterion implementation of cross entropy. Mainly used for ASR seq2seq loss function.\n        /error_rate.py      # Criterion implementation of word and char error rate. Mainly used for ASR evaluation.\n        /fbeta_score.py     # Criterion implementation of F_beta score. Mainly used for evaluation of TTS stop flag.\n        /least_error.py     # Criterion implementation of mean square error and mean absolute error. Mainly used for TTS seq2seq loss function.\n</code></pre>"},{"location":"criterion/#api-document","title":"API Document","text":"<ol> <li>speechain.criterion.abs.Criterion.__init__</li> <li>speechain.criterion.abs.Criterion.criterion_init</li> <li>speechain.criterion.abs.Criterion.__call__</li> </ol>"},{"location":"criterion/#speechaincriterionabscriterion__init__self-criterion_conf","title":"speechain.criterion.abs.Criterion.__init__(self, **criterion_conf)","text":"<ul> <li>Description:     This initialization function is shared by all Criterion subclasses.     Currently, the shared logic only contains calling the initialization function of the parent class.</li> <li>Arguments:<ul> <li>**criterion_conf:   The arguments used by <code>criterion_init()</code> for your customized Criterion initialization.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"criterion/#speechaincriterionabscriterioncriterion_initself-criterion_conf","title":"speechain.criterion.abs.Criterion.criterion_init(self, **criterion_conf)","text":"<ul> <li>Description:     Abstract interface function for customized initialization of each Criterion subclass.     This interface function is not mandatory to be overridden by your implementation.</li> <li>Arguments:<ul> <li>**criterion_conf:   The arguments used for customized Criterion initialization.   For more details, please refer to the docstring of your target Criterion subclass.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"criterion/#speechaincriterionabscriterion__call__self-kwargs","title":"speechain.criterion.abs.Criterion.__call__(self, **kwargs)","text":"<ul> <li>Description:     This abstract interface function receives the model forward calculation results and ground-truth labels.     The output is a scalar which could be either trainable for parameter optimization or non-trainable for information recording.     This interface function is mandatory to be overridden by your implementation.</li> <li>Arguments:<ul> <li>**kwargs:   model forward calculation results and ground-truth labels.   For more details, please refer to the docstring of <code>__call__()</code> of your target Criterion subclass.</li> </ul> </li> <li>Return:   A trainable or non-trainable scalar.   For more details, please refer to the docstring of <code>__call__()</code> of your target Criterion subclass.</li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/","title":"Dataset (Class)","text":"<p>Dataset is the base class that takes charge of reading the data instances from the disk into the memory and packaging them into a batch for model training or testing.</p> <p>A Dataset object receives indices of the selected data instances from the Dataloader object created by the high-level Iterator object. The output batches of packaged data instances generated by the Dataset object may not be well-processed.  Some post-processing steps need to be done in the Model object later.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"dataset/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configuration File Format</li> <li>Dataset Library</li> <li>API Document</li> <li>How to Mix Multiple Data Sources in my Dataset Object</li> <li>How to Perform Data Selection in my Dataset Object</li> </ol>"},{"location":"dataset/#configuration-file-format","title":"Configuration File Format","text":"<pre><code>dataset_conf:\n    main_data:\n        {data_name1}: {data_file_path1}\n        {data_name2}: {data_file_path2}\n        ...\n    data_selection:\n        - ...\n    # Customized Arguments passed to the hook dataset_init_fn()\n    ...\n</code></pre> <ul> <li> <p>The first-level key should be <code>dataset_conf</code> to fit the setting of Iterator.  </p> </li> <li> <p>The second-level keys are made up of two parts:</p> <ol> <li> <p>main_data:     The main body dictionary of the data instances you want to load for training or testing your models.     Each key-value item corresponds to a data variable in a data instance where the key is the variable name and the value is the file path of the data to be extracted.     For more details about how to give <code>data_name</code>, please refer to the API document of your target Dataset subclass.</p> </li> <li> <p>data_selection:     This argument defines how to select the target data instances in the given dataset.     For more details about how to configure this argument, please refer to the AIP document below.</p> </li> <li> <p>Arguments that are passed to the hook <code>dataset_init_fn()</code> for customized Dataset initialization.     For more details about the customized arguments, please refer to the AIP document of your target Dataset subclass.</p> </li> </ol> </li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/#dataset-library","title":"Dataset Library","text":"<p><pre><code>/speechain\n    /dataset\n        /abs.py             # Abstract class of Dataset. Base of all Dataset implementations.\n        /speech_text.py     # Dataset implementation of speech-text datasets. Mainly used for ASR and TTS models.\n</code></pre> \ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/#api-document","title":"API Document","text":"<p>speechain.dataset.abs.Dataset </p> <p>Non-overridable backbone functions:    1. __init__    2. __getitem__    3. data_selection    4. get_data_index    5. remove_data_by_index    6. collate_fn </p> <p>Overridable interface functions:    1. dataset_init_fn    2. extract_main_data_fn    3. collate_main_data_fn </p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/#speechaindatasetabsdataset","title":"speechain.dataset.abs.Dataset","text":"<p>This base class inherits <code>torch.utils.data.Dataset</code> and provides three hook functions: <code>dataset_init_fn()</code>, <code>extract_main_data_fn()</code>, and <code>collate_main_data_fn()</code>.  If you want to make your own Dataset implementation, please follow the instructions to override those hooks.</p>"},{"location":"dataset/#__init__self-main_data-data_selection-dataset_conf","title":"__init__(self, main_data, data_selection, **dataset_conf)","text":"<ul> <li>Description:   This initialization function reads the main body of the data instances into its memory.    The main body is used to extract individual data instances from the disk to form a batch during model training or testing.   The hook <code>dataset_init_fn()</code> is executed here after reading the main body files.</li> <li> <p>Arguments:</p> <ul> <li>main_data: Dict[str, str or List[str]] The main body dictionary of the data instances used in this Dataset object.  In each key-value item, the key is the name of the data variable and the value is the absolute path of the target idx2data files. The value can be given as a single path string or a list of multiple path strings.  </li> <li> <p>data_selection: List[str or List[str]] = None   The strategies for data selection during the iterator initialization to shrink the used data instances.   Multiple strategies can be specified in a list. Each data selection strategy must be either a bi-list (non-meta strategy) or tri-list (meta strategy).  </p> <ol> <li> <p>non-meta strategy: The rule-based selection strategies that don't involve metadata.  These strategies should be given as a bi-list, i.e., ['selection mode', 'selection number']. 'selection mode' indicates the way to select data instances while 'selection number' indicates how many data instances to be selected. Currently, available non-meta selection modes include:</p> <ol> <li>'order': Select the data instances from the beginning of the dataset.  </li> <li>'rev_order': Select the data instances from the end of the dataset.  </li> <li>'random': Randomly select the data instances from the dataset. Note: You should keep the same random seeds for all the GPU processes in the DDP mode to ensure that the selected data instances are the same in each process. In this case, please set the 'same_proc_seed' argument to True in your configuration given to speechain.runner.py.  </li> </ol> </li> <li> <p>meta strategy: The selection strategies that involves metadata.  These strategies should be given as a tri-list, i.e., ['selection mode', 'selection threshold', 'metadata path'].  'selection mode' indicates the way to select data instances, 'selection threshold' indicates the metadata threshold to select data instances, and 'metadata path' indicates where is the metadata used for selection. Currently, available meta selection modes include:</p> <ol> <li>'min': Select the data instances who have smaller metadata.  </li> <li>'max': Select the data instances who have larger  metadata.  </li> <li>'middle': Remove the data instances whose metadata is the largest and smallest.  </li> </ol> </li> </ol> </li> <li> <p>**dataset_conf: The configuration arguments for customized Dataset initialization.</p> </li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#__getitem__self-index","title":"__getitem__(self, index)","text":"<ul> <li>Description:     This function is the implementation of the one in the parent class <code>torch.utils.data.Dataset</code>.      This function is activated by the Dataloader object one data instance a time.      In each time, this function receives an index and returns the selected data instance.     The hook <code>extract_main_data_fn()</code> is executed here to extract the main body of the selected data instance from the disk.   </li> <li>Arguments:<ul> <li>index: str   The index of the selected data instance given by the Dataloader object.</li> </ul> </li> <li>Return: Dict[str, Any]     A dictionary containing a data instance.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#data_selectionself-selection_mode-selection_num-meta_info","title":"data_selection(self, selection_mode, selection_num, meta_info)","text":"<ul> <li>Description:     This function executes the data selection by the input selection strategy arguments.      A new batching view of the selected data instances will be returned.</li> <li> <p>Arguments: </p> <ul> <li>selection_mode: str   The mode indicating how the data instances are selected.    Selection modes are grouped by different types of data selection strategies.</li> <li>non-meta strategy:     The rule-based selection strategies that don't involve metadata.      Currently, available non-meta selection modes include:<ol> <li>'order': Select the data instances from the beginning of the dataset.</li> <li>'rev_order': Select the data instances from the end of the dataset.</li> <li>'random': Randomly select the data instances from the dataset. Note: You should keep the same   random seeds for all the GPU processes in the DDP mode to ensure that the selected data instances   are the same in each process. In this case, please set the 'same_proc_seed' argument to True in   your configuration given to speechain.runner.py.</li> </ol> </li> <li>meta strategy:     The selection strategies that involves metadata.      Currently, available meta selection modes include:<ol> <li>'min': Select the data instances who have smaller metadata.</li> <li>'max': Select the data instances who have larger  metadata.</li> <li>'middle': Remove the data instances whose metadata is the largest and smallest.</li> </ol> </li> <li>selection_num: float or int or str   This argument has the different usage with different data types.  </li> <li>float type:   Float value represents the relative number of data instances to be selected.    If selection_num is given as a float number, it must be between 0 and 1.</li> <li>int type:   Integer value represents the absolute number of data instances to be selected.    If selection_num is given as an interger number, it must be negative (its absolute value will be taken).</li> <li>str type:   String value represents the metadata threshold used to select the data instances.    Only 'min' and 'max' modes support string selection_num. Note: You can use the !-suffixed representer <code>!str</code> to convert a float or integer number to a string in your .yaml file.</li> <li>meta_info: str = None   The path where the metadata information used for selection is placed.    Only the meta strategies 'min', 'max', and 'middle' need this argument.</li> </ul> </li> <li> <p>Return: List[str]     A list of indices of the selected data instances.</p> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#get_data_indexself","title":"get_data_index(self)","text":"<ul> <li>Description:     This function is designed to make users know the data indices of this Dataset object without accessing its members for the lower coupling principle.</li> <li>Return: List[str]     The list of the indices of all data instances in this dataset.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#remove_data_by_indexself","title":"remove_data_by_index(self)","text":"<ul> <li>Description:   This function removes the corresponding data instance from this Dataset object by the given index.    It's mainly used for solving the index mismatch of data instances with the high-level Iterator object.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#dataset_init_fnself-dataset_conf","title":"dataset_init_fn(self, **dataset_conf)","text":"<ul> <li>Description:  This hook function initializes the customized part of your Dataset implementations.  This hook is not mandatory to be overridden and the original one in the base class does nothing.  If your Dataset subclass has some customized part, please override this hook function and put your logic here.</li> <li>Arguments:<ul> <li>**dataset_conf:   The configuration arguments for customized Dataset initialization received from <code>__init__()</code>.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#extract_main_data_fnbatch_dict-main_data","title":"extract_main_data_fn(batch_dict, main_data)","text":"<ul> <li>Description:     This hook function extracts the selected data instance from the disk to the memory.      The original hook in the base class does nothing and directly return <code>main_data</code>.      If you want to implement your own data instance extraction, please override this hook function and give your logic here.</li> <li>Arguments:<ul> <li>main_data: Dict[str, str]   The dictionary containing necessary information for extracting the data instance from the disk to the memory.    For example, the audio file path for the waveform data and the feature file path for the speaker embedding.</li> </ul> </li> <li>Return: Dict[str, Any]     The dictionary containing the extracted data instance.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#collate_fnself-batch","title":"collate_fn(self, batch)","text":"<ul> <li>Description:     This hook function is used as the value of the argument <code>collate_fn</code> for initializing Dataloader object at the beginning of each epoch.     If you have your own batch collating strategy, we don't recommend you to override this hook but another hook named <code>collate_main_data_fn()</code>.     This function should return the processed batch data in the form of a dictionary.</li> <li>Arguments:<ul> <li>batch: List[Dict[str, Any]]   The tuple of data instance dictionaries extracted by <code>extract_main_data_fn()</code>.</li> </ul> </li> <li>Return: Dict[str, Any]     The batch dictionary that will be passed to the model.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"dataset/#collate_main_data_fnself-batch_dict","title":"collate_main_data_fn(self, batch_dict)","text":"<ul> <li>Description:     This hook function decides how to preprocess a dictionary of the extracted batch of data instances before giving them to the model.      The original hook in the base class packages all the elements other than strings of the batch into a <code>torch.Tensor</code>.      Therefore, the <code>torch.Tensor</code> elements must have the same shape. The string elements will remain a list.     If you have your own batch collating strategy, please override this hook function and give your logic here.</li> <li>Arguments:<ul> <li>batch_dict: Dict[str, List]   The reshaped dictionary of the extracted batch.    In each key-value item, the key is the name of the data variable that will be passed to the model and the value is the list of unorganized data from all the elements in the batch.</li> </ul> </li> <li>Return: Dict[str, torch.Tensor or List]   The dictionary containing the collated batch of data instances.</li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/#how-to-mix-multiple-data-sources-in-my-dataset-object","title":"How to Mix Multiple Data Sources in my Dataset Object","text":"<p>If you want to initialize your iterator with multiple datasets and want your  dataloader to pick up batches from the mixed dataset,  you can simply give a list of file paths to the src_data  and tgt_label arguments to initialize the built-in dataset  of your iterator like the example below. <pre><code>data_root: ./datasets/speech/librispeech/data/wav\ntrain:\n    type: block.BlockIterator\n    conf:\n        dataset_type: speech.speech_text.SpeechTextDataset\n        dataset_conf:\n            src_data:\n                - !ref &lt;data_root&gt;/train_clean_100/feat.scp\n                - !ref &lt;data_root&gt;/train_clean_360/feat.scp\n                - !ref &lt;data_root&gt;/train_other_500/feat.scp\n            tgt_label:\n                - !ref &lt;data_root&gt;/train_clean_100/text\n                - !ref &lt;data_root&gt;/train_clean_100/text\n                - !ref &lt;data_root&gt;/train_other_500/text\n        ...\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"dataset/#how-to-perform-data-selection-in-my-dataset-object","title":"How to Perform Data Selection in my Dataset Object","text":"<p>If you only need to load a part of the data samples from the built-in dataset,  you can use the arguments selection_mode and selection_num.  selection_mode specifies the selection method and selection_num specifies  the number of selected samples.  selection_num can be given as a positive float number  or a negative integer number.  The positive float number means the ratio of the dataset.  In the example below, the first 50% of LibriSpeech-train_clean_100  will be selected.  <pre><code>data_root: ./datasets/speech/librispeech/data/wav\ntrain:\n    type: block.BlockIterator\n    conf:\n        dataset_type: speech.speech_text.SpeechTextDataset\n        dataset_conf:\n            src_data: !ref &lt;data_root&gt;/train_clean_100/feat.scp\n            tgt_label: !ref &lt;data_root&gt;/train_clean_100/text\n\n        selection_mode: order\n        selection_num: 0.5\n        ...\n</code></pre></p> <p>The negative integer number means the absolute number of the selected samples.  In the example below, 1000 data samples of LibriSpeech-train_clean_100  will be randomly selected.  <pre><code>data_root: ./datasets/speech/librispeech/data/wav\ntrain:\n    type: block.BlockIterator\n    conf:\n        dataset_type: speech.speech_text.SpeechTextDataset\n        dataset_conf:\n            src_data: !ref &lt;data_root&gt;/train_clean_100/feat.scp\n            tgt_label: !ref &lt;data_root&gt;/train_clean_100/text\n)\n        selection_mode: random\n        selection_num: -1000\n        ...\n</code></pre> Moreover, data selection and datasets mixing can be used in a single iterator  but they will be done sequentially.  In the example below, train_clean_100, train_clean_360,  and train_other_500 datasets of the LibriSpeech corpus will be first mixed  into a large dataset, and then the last 50% of the large dataset  will be selected. <pre><code>data_root: ./datasets/speech/librispeech/data/wav\ntrain:\n    type: block.BlockIterator\n    conf:\n        dataset_type: speech.speech_text.SpeechTextDataset\n        dataset_conf:\n            src_data:\n                - !ref &lt;data_root&gt;/train_clean_100/feat.scp\n                - !ref &lt;data_root&gt;/train_clean_360/feat.scp\n                - !ref &lt;data_root&gt;/train_other_500/feat.scp\n            tgt_label:\n                - !ref &lt;data_root&gt;/train_clean_100/text\n                - !ref &lt;data_root&gt;/train_clean_360/text\n                - !ref &lt;data_root&gt;/train_other_500/text\n        )\n        selection_mode: rev_order\n        selection_num: 0.5\n        ...\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/","title":"Datasets (Directory)","text":"<p>The <code>dataset</code> folder contains all the available datasets in this toolkit.  Each dataset corresponds to a sub-folder and has a uniform file system.  You can easily dump your target dataset to your machine by following the instructions below.  If you want to contribute a new dataset, we would appreciate it if you could follow our file systems and metadata formats.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"datasets/#table-of-contents","title":"Table of Contents","text":"<ol> <li>File System</li> <li>Metadata Format<ol> <li>idx2wav</li> <li>idx2wav_len</li> <li>idx2feat</li> <li>idx2feat_len</li> <li>idx2text</li> <li>idx2spk</li> <li>idx2spk_feat</li> <li>spk_list</li> <li>idx2gen</li> </ol> </li> <li>How to Dump a Dataset on your Machine</li> <li>How to Extract Speaker Embedding by my own model</li> <li>How to Contribute a New Dataset</li> </ol>"},{"location":"datasets/#file-system","title":"File System","text":"<p><pre><code>/datasets\n    /pyscripts                  # fixed .py scripts provided by the toolkit\n    data_dumping.sh             # the shared .sh script across all the speech-text datasets. It contains the complete pipeline of data dumping.\n    meta_generator.py           # the abstract .py script used by each dataset to decide their own meta generation logic.\n    meta_post_processor.py      # the abstract .py script used by each dataset to decide their own meta post-processing logic.\n    /{dataset_name}             # root folder of each dataset\n        /data                       # main folder of each dataset (the folder name 'data' is shared across all the datasets)\n            /wav                        # waveform folder (the folder name 'wav' is shared across all the datasets)\n                /{subset_name}              # the name of a subset in this dataset \n                    /wav_{comp_file_ext}        # (optional) the folder that contains the compressed package files of all the waveform data\n                    idx2wav_{comp_file_ext}     # (optional) the file that contains the pairs of index and the absolute address of waveforms in the compressed package files\n                    idx2wav                     # the file that contains the pairs of index and the absolute address of waveform files\n                    idx2wav_len                 # the file that contains the pairs of index and waveform length\n                    idx2{txt_format}_text       # the file that contains the pairs of index and transcript text. (txt_format is the format of processed text which is used to distinguish different idx2text files)\n                    ...                         # (optional) some other metadata files available in the dataset (such as idx2spk, idx2spk_feat, idx2gen, ...)\n            /wav{sample_rate}           # (optional) downsampled waveform folder (sample_rate is the samping rate of waveforms after downsampling)\n                ...                         # same structure as '/wav'\n            /{feat_config}              # (optional) acoustic feature folder by a given configuration (feat_config is the name of the configuration file)\n                /{subset_name}              # the name of a subset in this dataset \n                    /feat_{comp_file_ext}       # (optional) the folder that contains the compressed chunk files of all the acoustic feature data\n                    idx2feat_{comp_file_ext}    # (optional) the file that contains the pairs of index and the absolute address of waveforms in the compressed package files\n                    idx2feat                    # the file that contains the pairs of index and the absolute address of acoustic feature files\n                    idx2feat_len                # the file that contains the pairs of index and feature length\n                    idx2{txt_format}_text       # the file that contains the pairs of index and transcript text. (txt_format is the format of processed text which is used to distinguish different idx2text files)\n                    ...                         # (optional) some metadata files available in the dataset (such as idx2spk, idx2gen, ...)\n            /{token_type}               # token folder of a specific type (token_type is the name of the specified token type)\n                /{src_subset}               # the source subset used for generating the token vocabulary (src_subset is the name of the used source subset)\n                    /{token_config}             # the configuration for generating the vocabulary list.\n                        /{txt_format}               # the format of text data used to generate the token vocabulary\n                            idx2text                    # the file that contains the pairs of index and transcript text after tokenization\n                            idx2text_len                # the file that contains the pairs of index and transcript text length after tokenization\n                            vocab                       # token vocabulary\n                            model                       # (optional) tokenizer model. Used for sentencepiece tokenizers.\n        run.sh                      # the dataset-specific .sh script that controls how the data dumping for the corresponding dataset is going\n        data_download.sh            # the dataset-specific .sh script that downloads the corresponding dataset from the internet\n        meta_generator.py           # the dataset-specific .py script that generates the meta data files of the dataset (idx2wav, idx2spk, ...)\n        meta_post_processor.py      # (optional) the dataset-specific .py script that performs post-processing for the meta data files (needed by some datasets like LibriSpeech)\n</code></pre> The names in the braces({}) mean the undefined names depending on the settings of datasets and configuration.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#metadata-format","title":"Metadata Format","text":"<p>Metadata files are suffix-free .txt files used to access data instances during training.  The data formats of metadata files are uniform for all datasets.  The format uniformity enables the automatic configuration for the data loading part of our toolkit.  The format for each metadata file is shown below.</p>"},{"location":"datasets/#idx2wav","title":"idx2wav","text":"<p>In <code>idx2wav</code>, each line corresponds to a pair of file index and the absolute address of raw waveforms.  Index and address are separated by a blank. </p> <p>For example,  <pre><code>103-1240-0000 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0000.flac\n103-1240-0001 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0001.flac\n103-1240-0002 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0002.flac\n103-1240-0003 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0003.flac\n103-1240-0004 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0004.flac\n</code></pre> Any audio files that can be processed by <code>soundfile.read()</code> (such as .flac, .wav, ...) are OK in idx2wav.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2wav_len","title":"idx2wav_len","text":"<p>In <code>idx2wav_len</code>, each line corresponds to a pair of file index and file length which are separated by a blank.  The file length means the number of sampling points of the waveform.</p> <p>For example,  <pre><code>103-1240-0000 225360\n103-1240-0001 255120\n103-1240-0002 223120\n103-1240-0003 235360\n103-1240-0004 200240\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2feat","title":"idx2feat","text":"<p>In <code>idx2feat</code>, each line corresponds to a pair of file index and absolute address of the acoustic feature.  The index and absolute address are separated by a blank. </p> <p>For example,  <pre><code>103-1240-0000 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0000.npz\n103-1240-0001 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0001.npz\n103-1240-0002 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0002.npz\n103-1240-0003 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0003.npz\n103-1240-0004 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0004.npz\n</code></pre> feat_config is the name of the used feature extraction configuration file. In our toolkit, acoustic feature of a waveform is saved as a .npy file by the NumPy package.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2feat_len","title":"idx2feat_len","text":"<p>In <code>idx2feat_len</code>, each line corresponds to a pair of file index and file length which are separated by a blank.  The file length means the number of time frames in the extracted acoustic features (e.g. log-mel spectrogram or MFCC).</p> <p>For example,  <pre><code>103-1240-0000 1408\n103-1240-0001 1594\n103-1240-0002 1394\n103-1240-0003 1471\n103-1240-0004 1251\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2text","title":"idx2text","text":"<p>In <code>idx2text</code>, each line corresponds to a pair of file index and transcript text string which are separated by a blank.  <code>idx2text</code> will be renamed as <code>idx2{txt_format}_text</code> to indicate the text processing format used to generate this file. In our toolkit, there are many available text processing format to generate transcript text strings with different styles.</p> <p>For example,  <pre><code>103-1240-0000 chapter one missus rachel lynde is surprised missus rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies eardrops and traversed by a brook\n103-1240-0001 that had its source away back in the woods of the old cuthbert place it was reputed to be an intricate headlong brook in its earlier course through those woods with dark secrets of pool and cascade but by the time it reached lynde's hollow it was a quiet well conducted little stream\n103-1240-0002 for not even a brook could run past missus rachel lynde's door without due regard for decency and decorum it probably was conscious that missus rachel was sitting at her window keeping a sharp eye on everything that passed from brooks and children up\n103-1240-0003 and that if she noticed anything odd or out of place she would never rest until she had ferreted out the whys and wherefores thereof there are plenty of people in avonlea and out of it who can attend closely to their neighbor's business by dint of neglecting their own\n103-1240-0004 but missus rachel lynde was one of those capable creatures who can manage their own concerns and those of other folks into the bargain she was a notable housewife her work was always done and well done she ran the sewing circle\n</code></pre> Note: you don't need to worry about the blanks inside each transcript text string.  Those additional blanks will be ignored in the subsequent processing.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2spk","title":"idx2spk","text":"<p>In <code>idx2spk</code>, each line corresponds to a pair of file index and speaker ID. </p> <p>For example,  <pre><code>103-1240-0000 103\n103-1240-0001 103\n103-1240-0002 103\n103-1240-0003 103\n103-1240-0004 103\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2spk_feat","title":"idx2spk_feat","text":"<p>In <code>idx2spk_feat</code>, each line corresponds to a pair of file index and the absolute address of a speaker embedding. </p> <p>For example,  <pre><code>1034_121119_000001_000001 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000001_000001.npy\n1034_121119_000002_000001 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000002_000001.npy\n1034_121119_000010_000004 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000010_000004.npy\n1034_121119_000010_000006 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000010_000006.npy\n1034_121119_000012_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000012_000000.npy\n1034_121119_000014_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000014_000000.npy\n1034_121119_000018_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000018_000000.npy\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#spk_list","title":"spk_list","text":"<p>In spk_list, each line corresponds the string ID of a speaker in the corresponding subset of the dataset.  This file is mainly used for training multi-speaker TTS models.</p> <p>For example,  <pre><code>116\n1255\n1272\n1462\n1585\n1630\n1650\n1651\n1673\n1686\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2gen","title":"idx2gen","text":"<p>In idx2gen, each line corresponds to a pair of file index and gender. </p> <p>For example,  <pre><code>103-1240-0000 F\n103-1240-0001 F\n103-1240-0002 F\n103-1240-0003 F\n103-1240-0004 F\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-dump-a-dataset-on-your-machine","title":"How to Dump a Dataset on your Machine","text":"<p>For dumping an existing dataset,    1. Go to the folder of your target dataset <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> (e.g. if you want to dump LibriTTS, please go to <code>${SPEECHAIN_ROOT}/datasets/libritts</code>)    2. Run <code>bash run.sh --help</code> to familiarize yourself with the involved arguments.    3. Run <code>bash run.sh</code> to dump your target dataset (add some arguments if needed).</p> <p>Note:     1. If you already have the decompressed dataset on your disk, please attach the argument <code>--src_path {the-path-of-your-existing-dataset}</code> to the command <code>bash run.sh</code> in the no.3 step above.    Please make sure that <code>src_path</code> is an absolute path starting with a slash '/' and the content of <code>src_path</code> should be exactly the same with the one downloaded from the internet (please see the help message of <code>--src_path</code> in each <code>run.sh</code>).    2. If you want to save the dumped data and metadata files outside the toolkit folder (<code>${SPEECHAIN_ROOT}</code>), please attach the argument <code>--tgt_path {the-path-you-want-to-save-files}</code> to the command <code>bash run.sh</code> in the no.3 step above.    Please make sure that <code>tgt_path</code> is an absolute path starting with a slash '/'.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-extract-speaker-embedding-by-my-own-model","title":"How to Extract Speaker Embedding by my own model","text":"<p>If you want to use the pretrained speaker embedding model on your machine, please: 1. Don't give the argument <code>--spk_emb_model</code> when running the command <code>bash run.sh</code> 2. Write your own extraction script. You can use the metadata files <code>idx2wav</code> and <code>idx2wav_len</code> to read and organize the audio files. Please save all the speaker embedding vectors to a specific folder in the same directory of <code>idx2wav</code> and give a metadata file named <code>idx2spk_feat</code> for data reference.  </p> <p>Note:    1. For the file format of <code>idx2spk_feat</code>, please click here for reference.    2. Please keep the same data index with <code>idx2wav</code> in your <code>idx2spk_feat</code>.    3. Each speaker embedding vector should be in the shape of <code>[1, spk_feat_dim]</code>.    4. Speaker embedding vectors could be saved in two ways:       1. save each vector to an individual <code>.npy</code> file       2. save all vectors to a <code>.npz</code> file where the index of each vector is exactly the one in <code>idx2spk_feat</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-contribute-a-new-dataset","title":"How to Contribute a New Dataset","text":"<p>If the dataset that you want to use for your experiments is not included here,  you could make the dumping pipeline of your target dataset by the following instructions:</p> <ol> <li>Go to <code>${SPEECHAIN_ROOT}/datasets/</code>.  </li> <li>Run <code>bash data_dumping.sh --help</code> to familiarize yourself with the involved arguments.  </li> <li>Make a new folder in <code>${SPEECHAIN_ROOT}/datasets/</code> with the name as your target dataset.  </li> <li>Make a new data_download.sh in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to download your target dataset from the internet. Please download the dataset into <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}/data/wav</code>.</li> <li>Make a new meta_generator.py in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to extract the metadata files of your target dataset. Please refer to <code>${SPEECHAIN_ROOT}/datasets/meta_generator.py</code> for instructions of how to override the pipeline of metadata generation.  </li> <li>If needed, make a new meta_post_processor.py in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to post-process the extracted metadata files of all the subsets.  (e.g. combine train-clean-100 and train-clean-360 of LibriSpeech into train-clean-460) Please refer to <code>${SPEECHAIN_ROOT}/datasets/meta_post_processor.py</code> for instructions of how to override the pipeline of metadata post-processing.  </li> <li>Make a new run.sh in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to manipulate the dumping pipeline of your target dataset.  You could refer to the ones in the existing dataset folders as a template.</li> </ol> <p>Note:  Please keep the same script names (i.e., <code>data_download.sh</code>, <code>meta_generator.py</code>, and <code>meta_post_processor.py</code>) for the compatibility with <code>data_dumping.sh</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/","title":"Handbook","text":"<p>Our documentation is organized by different roles in this toolkit. You can start exploring SpeeChain by reading the following sections or by jumping directly to the section that interests you. These sections also appear as README files in some subfolders of the toolkit repository.</p> <p>\ud83d\udc46Back to the home page</p>"},{"location":"handbook/#table-of-contents","title":"Table of Contents","text":"<ol> <li>For those who just discovered SpeeChain</li> <li>How to dump a dataset to your machine</li> <li>How to prepare a configuration file</li> <li>How to train and evaluate a model</li> <li>How to interpret the files generated in the exp folder</li> <li>For those who want to use SpeeChain for research</li> <li>SpeeChain file system</li> <li>How to customize my own data loading and batching strategy</li> <li>How to customize my own model</li> <li>How to customize my own learning rate scheduling strategy</li> <li>For those who want to contribute to SpeeChain</li> <li>Contribution specifications</li> </ol>"},{"location":"handbook/#for-those-who-just-discovered-speechain","title":"For those who just discovered SpeeChain","text":"<p>In SpeeChain toolkit, a basic research pipeline has 5 steps:  </p> <ol> <li>Dump a dataset from the Internet to your disk.  </li> <li>Prepare experimental configuration files.  </li> <li>Train a model.  </li> <li>Evaluate the trained model.  </li> <li>Analyse the evaluation results.  </li> </ol> <p>The following subsections will explain how to execute the steps above one by one.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-dump-a-dataset-to-your-machine","title":"How to dump a dataset to your machine","text":"<p>In our toolkit, the datasets are grouped by their data types.  Each available dataset corresponds a specific folder in <code>${SPEECHAIN_ROOT}/datasets</code>:</p> <p>SpeeChain follows the all-in-one dumping style by a bash script named <code>data_dumping.sh</code> where the procedure of dataset dumping is divided into individual steps and each step is executed by a specific script.</p> <p>We provide an executable script named <code>run.sh</code> in each dataset folder under <code>${SPEECHAIN_ROOT}/datasets</code>.  Please refer to here before starting the dumping pipeline.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-prepare-configuration-files","title":"How to prepare configuration files","text":"<p>Please refer to here for some configuration templates in those <code>exp_cfg</code> folders.</p> <p>In order to avoid messy and unreadable configuration setting in the terminal, SpeeChain provides some advanced services to simplify the configuration setting. PS: The following contents are not urgent for you to learn. If you don't want to jump into details too much right now, please go to the next step</p>"},{"location":"handbook/#flexible-path-parsing-services","title":"Flexible Path Parsing Services","text":"<p>In SpeeChain, the path arguments can be given in 3 ways:  </p> <ol> <li> <p>Absolute Path: You can indicate an absolute path by beginning the path with a slash '/', e.g., <code>/x/xx/xxx/speechain/runner.py</code>.</p> </li> <li> <p>General Relative Path: If your input path begins with <code>.</code> or <code>..</code>, it will be converted to the corresponding absolute path in our framework. Note: The relative path will be parsed by the directory where you execute the script rather than the directory where the executable script is placed!</p> </li> <li> <p>In-toolkit Relative Path:     The path arguments can be given as the relative location under the toolkit root, i.e., <code>${SPEECHAIN_ROOT}</code>.      The toolkit root <code>${SPEECHAIN_ROOT}</code> is created by the bash script <code>envir_preparation.sh</code>.     For example, <code>speechain/runn.py</code> will be parsed to to <code>${SPEECHAIN_ROOT}/speechain/runner.py</code>.      If you would like to specify a place outside the toolkit root, you can directly give its absolute path with a slash <code>/</code> at the beginning to notify the framework of an absolute path, e.g., <code>/x/xx/xxx/speechain/runner.py</code>.</p> </li> </ol>"},{"location":"handbook/#convertable-arguments-in-the-terminal","title":"Convertable Arguments in the Terminal","text":"<p>Conventionally, it's hard for us to assign the values of List and Dict arguments in the terminal.  In SpeeChain, our framework provides a convenient way to convert your entered strings in the specified format into the corresponding List or Dict variables.</p> <ol> <li> <p>For the List variables, your entered string should be surrounded by a pair of square brackets and each element inside the brackets should be split by a comma.    The structure can be nested to initialize sub-List in the return List variable.    For example, the string <code>[a,[1,2,[1.1,2.2,3.3],[h,i,j,k]],c,[d,e,[f,g,[h,i,j,k]]]]</code> will be parsed to     <pre><code>- 'a'\n- - 1\n  - 2\n  - - 1.1\n    - 2.2\n    - 3.3\n  - - 'h'\n    - 'i'\n    - 'j'\n    - 'k'\n- 'c'\n- - 'd'\n  - 'e'\n  - - 'f'\n    - 'g'\n    - - 'h'\n      - 'i'\n      - 'j'\n      - 'k'\n</code></pre></p> </li> <li> <p>For the Dict variables, the key and its value should be split by a colon.     The value should be surrounded by a pair of braces if it's a sub-Dict.     The structure can be nested to initialize sub-Dict in the return Dict variable.    For example, the string <code>a:{b:12.3,c:{d:123,e:{g:xyz}}},g:xyz</code> will be parsed to <pre><code>a:\n    b: 12.3\n    c:\n        d: 123\n        e:\n            g:xyz\ng: xyz\n</code></pre></p> </li> </ol> <p>Moreover, the List string can also be nested into the Dict string like <code>a:[1,2,3]</code> will be parsed as <pre><code>a:\n- 1\n- 2\n- 3\n</code></pre></p>"},{"location":"handbook/#concise-configuration-file","title":"Concise Configuration File","text":"<p>As the number of arguments increases, it would be hard for us to given all the arguments one by one in the terminal.  As a frequently-used file format for configuration, .yaml has been popular in many well-known toolkits. </p> <p>In SpeeChain, we wrap the conventional .yaml file and provides some advanced !-suffixed .yaml representers to further simplify its layout and improve the readability:  </p> <ol> <li> <p>!str allows you to cast a numerical value into a string by replacing <code>key_name: 10</code> with <code>key_name: !str 10</code>.       In this scenario, the value of <code>key_name</code> will be a string '10' instead of an integer 10.</p> </li> <li> <p>!list allows you to compress the configuration of a list into one line from <pre><code>key_name: \n- - a\n  - b\n  - c\n- - d\n  - e\n  - f\n</code></pre>       to       <pre><code>key_name:\n- !list [a,b,c]\n- !list [d,e,f]\n</code></pre> Note: </p> <ol> <li>The elements should be separated by commas ',' and surrounded by a pair of angle brackets '[]'.  </li> <li>Nested structures like <code>key_name: !list [!list [a,b,c],!list [d,e,f]]</code> are not supported yet.  </li> </ol> </li> <li> <p>!tuple allows you to create tuples in your configuration.        The statement       <pre><code>key_name: \n- a\n- b\n- c\n</code></pre>       can only give us a list, but sometimes we may need to create a tuple. Instead, we can use <code>key_name: !tuple (a,b,c)</code> to create a tuple. Note: The elements should be separated by commas ',' and surrounded by a pair of brackets '()'. </p> </li> <li> <p>!ref allows you to reuse the values you have already created by replacing</p> <pre><code>key_name1: abc/def/ghi/jkl\nkey_name2: abc/def/ghi/jkl/mno\nkey_name3: abc/def/ghi/jkl/mno/pqr\n</code></pre> <p>with</p> <pre><code>key_name1: abc/def/ghi/jkl\nkey_name2: !ref &lt;key_name1&gt;/mno\nkey_name3: !ref &lt;key_name2&gt;/pqr\n</code></pre> <p>In this scenario, the value of <code>key_name1</code> will be reused to create <code>key_name2</code> which will be further reused to create <code>key_name3</code>.  </p> <p>Note: </p> <ol> <li> <p>Nested structures like <pre><code>key_name1: abc/def/ghi/jkl\nkey_name2: !ref &lt;key_name1&gt;/mno\nkey_name3: !list [!ref &lt;key_name1&gt;,!ref &lt;key_name2&gt;]\n</code></pre>       are not supported yet.  </p> </li> <li> <p>Different !ref representers must be used in order. The following usage is invalid (<code>key_name3</code> is used before <code>key_name2</code>): <pre><code>key_name1: abc/def/ghi/jkl\nkey_name3: !ref &lt;key_name2&gt;/pqr\nkey_name2: !ref &lt;key_name1&gt;/mno\n</code></pre></p> </li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#inference-configuration-for-hyperparameter-adjustment","title":"Inference Configuration for hyperparameter adjustment","text":"<p>Model inference configuration is given by <code>infer_cfg</code> in the configuration file.  There could be either one inference configuration or multiple configurations in infer_cfg:  </p> <ol> <li> <p>If infer_cfg is not given, the default inference configuration will be used for model inference.</p> </li> <li> <p>If you only want to give one inference configuration, please give it by either a string or a Dict.</p> <ol> <li> <p>String: The string indicates where the inference configuration file is placed. For example,      <code>infer_cfg: config/infer/asr/greedy_decoding.yaml</code> means the configuration file <code>${SPEECHAIN_ROOT}/config/infer/asr/greedy_decoding.yaml</code> will be used for model inference.      In this example, the evaluation results will be saved to a folder named <code>greedy_decoding</code>.     If there are many arguments you need to give in the configuration, we recommend you to give them by a configuration file for concision. </p> </li> <li> <p>Dict: The Dict indicates the content of your inference configuration. For example.     <pre><code>infer_cfg:\n    beam_size: 1\n    temperature: 1.0\n</code></pre>     means that <code>beam_size=1</code> and <code>temperature=1.0</code> will be used for ASR decoding.      In this example, the evaluation results will be saved to a folder named <code>beam_size=1_temperature=1.0</code> which is decided by the keys and values in your given Dict.     If there are not so many arguments in your configuration, we recommend you to give them by a Dict to avoid messy configuration files on your disk.</p> </li> </ol> </li> <li> <p>If you want to give multiple inference configuration in infer_cfg, please give them by either a List or a Dict.  </p> <ol> <li>List: Each element in the List could be either a string or a Dict.  <ul> <li>The string indicates the file paths of a given inference configuration. For example,    <pre><code>infer_cfg:\n  - config/infer/asr/greedy_decoding.yaml\n  - config/infer/asr/beam_size=16.yaml\n</code></pre>    means that both <code>greedy_decoding.yaml</code> and <code>beam_size=16.yaml</code> in <code>${SPEECHAIN_ROOT}/config/infer/asr/</code> will be used for ASR decoding.  </li> <li>The Dict indicates the content of a given inference configuration. For example,    <pre><code>infer_cfg:\n  - beam_size: 1\n    temperature: 1.0\n  - beam_size: 16\n    temperature: 1.0\n</code></pre>    could be used and two folders <code>beam_size=1_temperature=1.0</code> and <code>beam_size=16_temperature=1.0</code> will be created to place their evaluation results.  </li> <li>Of course, strings and Dicts can be mixed in infer_cfg like    <pre><code>infer_cfg:\n  - config/infer/asr/greedy_decoding.yaml\n  - beam_size: 16\n    temperature: 1.0\n</code></pre></li> </ul> </li> <li>Dict: There must be two keys in the Dict: <code>shared_args</code> and <code>exclu_args</code>. <code>shared_args</code> (short of 'shared arguments') is a Dict which contains the arguments shared by all the configurations in the Dict. <code>exclu_args</code> (short of 'exclusive arguments') is a List[Dict] where each element contains the exclusive arguments for each configuration.      For example,      <pre><code>  infer_cfg:\n    shared_args:\n        beam_size: 16\n    exclu_args:\n        - temperature: 1.0\n        - temperature: 1.5\n</code></pre>      means that there will be two configurations used for model inference:      <pre><code>beam_size: 16\ntemperature: 1.0\n</code></pre>      and      <pre><code>beam_size: 16\ntemperature: 1.5\n</code></pre>      Their evaluation results will be saved to <code>beam_size=16_temperature=1.0</code> and <code>beam_size=16_temperature=1.5</code>.      If your configurations don't contain too many arguments and you only want to change one or two arguments for each of them, we recommend you to give your configurations in this way.</li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-train-and-evaluate-a-model","title":"How to train and evaluate a model","text":"<p>We provide two levels of executable bash scripts:</p> <ol> <li> <p>All-in-one executable <code>run.sh</code> in <code>${SPEECHAIN_ROOT}/recipes/</code>. This bash script is task-independent and can be called everywhere to run an experimental job.    For more details, please go to <code>${SPEECHAIN_ROOT}/recipes</code> and run <code>bash run.sh --help</code> for the message about involved arguments.</p> </li> <li> <p>Low-level <code>run.sh</code> designed for sub-folder in <code>${SPEECHAIN_ROOT}/recipes/</code>. Those scripts are used to run the experiments of the specific task.    For more details, please go to the target sub-folder and run <code>bash run.sh --help</code> for the message about involved arguments.</p> </li> </ol> <p>The execution hierarchy of the scripts is: <pre><code>${SPEECHAIN_ROOT}/recipes/{task_name}/{dataset_name}/{subset_name}/run.sh\n    ---&gt;${SPEECHAIN_ROOT}/recipes/run.sh\n        ---&gt;${SPEECHAIN_ROOT}/speechain/runner.py\n</code></pre></p> <p>For the detailed instructions about how to launch the jobs for each model, please refer to here and click your target model.</p> <p>By the way, you can also directly use the command <code>${SPEECHAIN_PYTHON} ${SPEECHAIN_ROOT}/speechain/runn.py</code> in your terminal or your own bash script to run your experimental jobs.  Before doing so, we recommend you to first use the command <code>${SPEECHAIN_PYTHON} ${SPEECHAIN_ROOT}/speechain/runn.py --help</code> to familiarize yourself with the involved arguments.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-interpret-the-files-generated-in-the-exp-folder","title":"How to interpret the files generated in the exp folder","text":"<p>Please refer to ${SPEECHAIN_ROOT}/recipes/README.md for more details.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#for-those-who-want-to-use-speechain-for-research","title":"For those who want to use SpeeChain for research","text":""},{"location":"handbook/#speechain-file-system","title":"SpeeChain file system","text":""},{"location":"handbook/#configuration-folder","title":"Configuration Folder","text":"<p>This folder contains off-the-shelf configuration files that can be shared across different tasks, models, or datasets.  Each type of configuration corresponds to a specific sub-folder where each category of configuration corresponds to a specific sub-sub-folder.</p> <p>Folder architecture is shown below: <pre><code>/config\n    /feat       # Configuration for acoustic feature extraction\n        /log_mel    # Configuration files for log-Mel spectrogram extraction\n            /...\n        /mfcc       # Configuration files for MFCC extraction\n            /...\n    /infer      # Configuration for model inference\n        /asr        # Configuration files for ASR inference\n            /...\n        /tts        # Configuration files for TTS inference\n            /...\n</code></pre> For more details about the configuration files in <code>${SPEECHAIN_ROOT}/config/feat/</code>, please refer to the docstring of ${SPEECHAIN_ROOT}/datasets/pyscripts/feat_extractor.py.</p> <p>For more details about the configuration files in <code>${SPEECHAIN_ROOT}/config/infer/</code>, please refer to the docstring of the corresponding inference function in ${SPEECHAIN_ROOT}/speechain/infer_func/.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#dataset-folder","title":"Dataset Folder","text":"<p>This folder contains off-the-shelf processing scripts to dump datasets into your machine.  Each type of datasets corresponds to a specific sub-folder where each dataset corresponds a specific sub-sub-folder.</p> <p>Folder architecture is shown below: <pre><code>/datasets\n    /speech_text        # Datasets that are made up of speech and text data\n        /librispeech        # Processing scripts for the LibriSpeech dataset\n            /...\n        /libritts           # Processing scripts for the LibriTTS dataset\n            /...\n        /ljspeech           # Processing scripts for the LJSpeech dataset\n            /...\n        /data_dumping.sh    # all-in-one speech-text dataset dumping script\n</code></pre> For more details, please refer to the README.md of each type of dataset in ${SPEECHAIN_ROOT}/datasets/.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#recipes-folder","title":"Recipes Folder","text":"<p>This folder contains our recipes for all tasks on the available datasets.  Each task corresponds to a specific sub-folder where each dataset corresponds a specific sub-sub-folder. In the dataset folder, there may be some sub-folders corresponding to different settings of model training where a sub-sub-folder <code>/data_cfg</code> contains all the configuration files of data loading that are shared by all the model sub-sub-folders.</p> <p>Folder architecture is shown below: <pre><code>/recipes\n    /asr                    # Recipes for the ASR task\n        /librispeech            # Recipes for ASR models on the LibriSpeech dataset\n            ...                     # different ASR settings for LibriSpeech\n        /libritts               # Recipes for ASR models on the LibriTTS dataset\n            ...                     # different ASR settings for LibriTTS\n        /libritts+librispeech   # Recipes for ASR models on the 16khz-downsampled LibriTTS and LibriSpeech datasets\n            ...                     # different ASR settings for 16khz-downsampled LibriTTS and LibriSpeech\n    /tts                    # Recipes for the TTS task\n        /libritts               # Recipes for TTS models on the LibriTTS dataset\n            ...                     # different TTS settings for LibriTTS\n        /ljspeech               # Recipes for TTS models on the LJSpeech dataset\n            ...\n    /offline_tts2asr        # Recipes for the offline TTS-to-ASR chain\n        /libritts_librispeech   # Recipes for TTS trained on LibriTTS and ASR trained on LibriSpeech\n            ...                     # different TTS-to-ASR settings for LibriSpeech and LibriTTS\n    /offline_asr2tts        # Recipes for the offline ASR-to-TTS chain\n        /libritts                # Recipes for ASR and TTS trained on LibriTTS \n            ...                     # different ASR-to-TTS settings for LibriTTS\n</code></pre> For more details, please refer to ${SPEECHAIN_ROOT}/recipes/README.md.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#main-folder","title":"Main Folder","text":"<p>The folder <code>/speechain</code> is the core part of our toolkit where each sub-folder corresponds to a specific part of an experimental pipeline.  In each sub-folder, there is a .py file named <code>abs.py</code> that declares the abstract class of the corresponding pipeline part.  Based on the abstract class, many implementation classes are included in the same sub-folder with the name like <code>xxx.py</code>. <pre><code>/speechain\n    # Sub-folders for all specific parts of an experimental pipeline\n    /criterion\n        ...\n    /dataset\n        ...\n    /infer_func\n        /beam_search.py     # Inference function of the beam searching. Mainly used for ASR models.\n        /tts_decoding.py    # Inference function of the autoregressive TTS decoding.\n        ...\n    /iterator\n        ...\n    /model\n        ...\n    /module\n        ...\n    /optim_sche\n        ...\n    /tokenizer\n        ...\n    # General part of the pipeline\n    /run.py             # The entrance of SpeeChain toolkit for both model training and testing.\n    /monitor.py         # The training and testing monitors. Used to record and regulate the training and testing process.\n    /snapshooter.py     # The figure snapshooter. Used to transform the input snapshotting materials into the visible figures.\n</code></pre></p> <p>Read more about <code>/speechain/criterion</code> Read more about <code>/speechain/dataset</code> Read more about <code>/speechain/iterator</code> Read more about <code>/speechain/model</code> Read more about <code>/speechain/module</code> Read more about <code>/speechain/optim_sche</code> Read more about <code>/speechain/tokenizer</code> </p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-customize-my-own-data-loading-and-batching-strategy","title":"How to customize my own data loading and batching strategy","text":"<p>For how to customize your own data loading strategy, please refer to the API document of <code>/speechain/dataset</code>.  </p> <p>For how to customize your own data batching, please refer to the API document of <code>/speechain/iterator</code>.  </p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-customize-my-own-model","title":"How to customize my own model","text":"<p>For how to customize your own model, please refer to the API document of <code>/speechain/model</code>.</p> <p>If the existing Module implementations in <code>/speechain/module</code>, you can refer to the API document of <code>/speechain/module</code> for the instructions about how to customize your own modules.</p> <p>For the model involving text tokenization like ASR and TTS, if the existing Tokenizer implementations cannot satisfy your needs, you can refer to the API document of <code>/speechain/tokenizer</code> for the instructions about how to customize your own tokenizers.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#how-to-customize-my-own-learning-rate-scheduling-strategy","title":"How to customize my own learning rate scheduling strategy","text":"<p>For how to customize your own optimization strategy, please refer to the API document of <code>/speechain/optim_sche</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"handbook/#for-those-who-want-to-contribute-to-speechain","title":"For those who want to contribute to SpeeChain","text":""},{"location":"handbook/#contribution-specifications","title":"Contribution specifications","text":"<p>We have some specifications for you to standardize your contribution:</p> <ol> <li> <p>Documentation: We will appreciate it a lot if you could provide enough documents for your contribution.</p> <ul> <li> <p>We recommend you to use the Google-style function docstring.  If you are using PyCharm, you can set the docstring style in File\u2192Setting\u2192Tools\u2192Python Integrated Tools\u2192Docstrings\u2192Docstring format.</p> <p>As for argument explanation in the docstring, we recommend you to write the argument type after the colon and give its description below with a tab retract as follows. <pre><code>    Args:\n        d_model: int\n            The dimension of the input feature sequences.\n</code></pre> If the argument type is <code>torch.Tensor</code> or <code>numpy.array</code>, please replace the type with its shape as follows. <pre><code>    Args:\n        emb_feat: (batch_size, seq_len, d_model)\n            Embedded input feature sequences\n</code></pre></p> </li> <li> <p>For in-line comments, we recommend you start a new line every time you want to comment (it's better not to type a long comment after the code).  The codes are better to be divided into several code blocks by their roles with an in-line comment right above the block as follows.   <pre><code>  # member registration\n  self.d_model = d_model\n  self.num_layers = num_layers\n  self.num_heads = num_heads\n  self.layernorm_first = layernorm_first\n</code></pre></p> </li> </ul> <p>Note you can format the docstring using docformatter with the following command at the root directory of SpeeChain. <pre><code>docformatter --in-place -s google -r speechain --black\n</code></pre></p> </li> <li> <p>Naming: We have several recommendations for class names and variable names.</p> <ul> <li> <p>For class names, we recommend you to name your class in the CamelCase style. The names are better to be in the form of \"What is it made up of\" + \"What is it\". </p> <p>For example, <code>SpeechTextDataset</code> means a dataset class that returns speech-text paired data during training.   <code>Conv2dPrenet</code> means a prenet module that is made up of Conv2d layers.</p> </li> <li> <p>For long variable names, please make some abbreviations. For the abbreviations, we recommend the following 2 frequently-used strategies:</p> <ul> <li>Tail-Truncating: delete the letters from the tail and only retain the part before the second vowel.   For example, 'convolution' -&gt; 'conv', 'previous' -&gt; 'prev'.</li> <li>Vowel-Omitting: directly delete all vowels and some trivial consonants behind each vowel.   For example, 'transformer' -&gt; 'trfm', 'source' -&gt; 'src', 'target' -&gt; 'tgt'.</li> </ul> </li> <li> <p>For the temporary variables only used to register data for a short period, please add an underline at the beginning of the name to notify other users.  For example, <code>_tmp_feat_dim</code> means the temporary variable used to register the intermediate value of the feature dimension. </p> </li> </ul> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"iterator/","title":"Iterator","text":"<p>Iterator is the base class that takes charge of grouping data instances into batches for training or testing models. Each iterator object has a built-in speechain.dataset.Dataset object as a member variable.  Actually, an Iterator object cannot directly access the data instances in the built-in Dataset object but maintains a batching view of the indices of the data instances used for model training or testing.</p> <p>The iterators are divided into 3 groups: train, valid, and test.  In each group, 2 or more iterator objects can be constructed so that there could be multiple data-label pairs in a single batch.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"iterator/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configuration File Format</li> <li>Iterator Library</li> <li>API Document</li> <li>How to Construct Multiple Dataloaders for a Batch</li> </ol>"},{"location":"iterator/#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration of Iterator is given in data_cfg.  The configuration format is shown below. <pre><code>train:\n    {iterator_name}:\n        type: {file_name}.{class_name}\n        conf:\n            # Built-in Dataset Configuration\n            dataset_type: {file_name}.{class_name}\n            dataset_conf:\n                ...\n            # General Iterator Configuration\n            batches_per_epoch:\n            data_selection:\n            is_descending:\n            shuffle:\n            data_len:\n            group_info:\n            # Customized Iterator Configuration\n            ...\n    ...\nvalid:\n    {iterator_name}:\n        type: {file_name}.{class_name}\n        conf:\n            dataset_type: {file_name}.{class_name}\n            dataset_conf:\n                ...\n            ...\n    ...\ntest:\n    {test_set_name}:\n        {iterator_name}:\n            type: {file_name}.{class_name}\n            conf:\n                dataset_type: {file_name}.{class_name}\n                dataset_conf:\n                    ...\n                ...\n        ...\n</code></pre> * The first-level keys must be one of train, valid, and test. </p> <p>The combination of your first-level keys must be one of train &amp; valid &amp; test (for training and testing), train &amp; valid (for training only), or test (for testing only).</p> <ul> <li> <p>The second-level keys are iterator names used for distinguishing the loaded data of each iterator.     There is no restriction on the iterator names, so you can name them in your own preference.    Under the name of each iterator, there are two third-level keys whose names are fixed:</p> <ol> <li> <p>type:  The value of this key acts as the query string to pick up your target Iterator subclass in <code>SPEECHAIN_ROOT/speechain/iterator/</code>.   Your given query should be in the form of <code>{file_name}.{class_name}</code> where <code>file_name</code> specifies your target .py file in <code>SPEECHAIN_ROOT/speechain/iterator/</code> and <code>class_name</code> indicates your target Iterator subclass in <code>SPEECHAIN_ROOT/speechain/iterator/{file_name}.py</code>.   For example, <code>block.BlockIterator</code> means the subclass <code>BlockIterator</code> in <code>SPEECHAIN_ROOT/speechain/iterator/block.py</code>.  </p> </li> <li> <p>conf:  The value of this key indicates the configuration of your iterator.   The configuration is made up of the following 4 fourth-level keys:</p> <ol> <li> <p>dataset_type:  The value of this key acts as the query string to pick up your target built-in Dataset subclass in <code>SPEECHAIN_ROOT/speechain/dataset/</code>.  Your given query should be in the form of <code>{file_name}.{class_name}</code> where <code>file_name</code> specifies your target .py file in <code>SPEECHAIN_ROOT/speechain/dataset/</code>, and <code>class_name</code> indicates your target Dataset subclass in <code>SPEECHAIN_ROOT/speechain/dataset/{file_name}.py</code>.   For example, <code>speech_text.SpeechTextDataset</code> means the subclass <code>SpeechTextDataset</code> in <code>./speechain/dataset/speech_text.py</code>.</p> </li> <li> <p>dataset_conf:  The value of this key contains all the configuration used to initialize the built-in Dataset object.   Please refer to Dataset API Document for more details.</p> </li> <li> <p>General Iterator Configuration:  These configurations are used to initialize the general part shared by all iterator subclasses.   There are 6 general arguments that can be set manually in data_cfg: (please refer to speechain.iterator.abs.Iterator.__init__ for more details)  </p> <ol> <li>batches_per_epoch</li> <li>is_descending</li> <li>shuffle</li> <li>data_len</li> <li>group_info</li> </ol> </li> <li>Customized Iterator Configuration:  The arguments of the customized configuration are used by each Iterator subclass to generate the batching view.   Please refer to your target Iterator subclass for more details.</li> </ol> </li> </ol> </li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"iterator/#iterator-library","title":"Iterator Library","text":"<pre><code>/speechain\n    /iterator\n        /abs.py         # Abstract class of Iterator. Base of all Iterator implementations.\n        /block.py       # Iterator implementation of the block strategy (variable utterances per batch). Mainly used for ASR and TTS training.\n        /piece.py       # Iterator implementation of the piece strategy (fixed utterances per batch). Mainly used for ASR and TTS evaluation.\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"iterator/#api-document","title":"API Document","text":"<p>speechain.iterator.abs.Iterator </p> <p>Non-overridable backbone functions:    1. __init__    2. __len__    3. get_batch_indices    4. get_group_info    5. build_loader Overridable interface functions:    1. batches_generate_fn</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"iterator/#speechainiteratorabsiterator","title":"speechain.iterator.abs.Iterator","text":"<p>The initialization of the built-in Dataset object is done automatically during the initialization of the iterator. At the beginning of each epoch, the iterator generates a <code>torch.utils.data.DataLoader</code> object to fetch the batches of data instances from the disk. Each iterator subclass should override a static hook function <code>batches_generate_fn()</code> to generate the batching view of data instances in the built-in Dataset object based on their own data batching strategy.</p>"},{"location":"iterator/#__init__self-dataset_type-dataset_conf-batches_per_epoch-data_len-group_info-data_selection-is_descending-shuffle-seed-ngpu-num_workers-pin_memory-distributed-iter_conf","title":"__init__(self, dataset_type, dataset_conf, batches_per_epoch, data_len, group_info, data_selection, is_descending, shuffle, seed, ngpu, num_workers, pin_memory, distributed, **iter_conf)","text":"<ul> <li>Description:     The general initialization function shared by all the Iterator classes.      Dataset initialization is automatically done here by the given dataset_type and dataset_conf.</li> <li>Arguments:<ul> <li>dataset_type: str   Query string to pick up the target Dataset subclass in <code>SPEECHAIN_ROOT/speechain/dataset/</code></li> <li>dataset_conf:  Dict   Dataset configuration for the automatic initialization of the built-in Dataset object.</li> <li>batches_per_epoch: int = None   The number of batches in each epoch. This number can be either smaller or larger than the real batch number.    If not given (None), all batches will be used in each epoch.</li> <li>data_len: str or List[str] = None   The path of the data length file. Multiple data length files can be given in a list, but they must contain non-overlapping data instances.</li> <li>group_info: Dict[str, str or List[str]]   The dictionary of paths for the idx2data files used for group-wise evaluation results visualization.</li> <li>is_descending: bool = True   Whether the batches are sorted in the descending order by the length (True) or in the ascending order (False).    This argument is effective only when data_len is given.</li> <li>shuffle: bool = True   Whether the batches are shuffled at the beginning of each epoch.</li> <li>seed: int = 0   Random seed for iterator initialization. This argument is automatically given by the experiment environment configuration.   The seed will be used to<ol> <li>shuffle batches before giving to the Dataloader of each epoch.</li> <li>initialize all the workers of the Dataloader for the reproducibility.</li> </ol> </li> <li>ngpu: int = 1   The number of GPUs used to train or test models. This argument is automatically given by the experiment environment configuration.   The GPU number is used to ensure that each GPU process in the DDP mode has the batches with the same number of data instances.</li> <li>num_workers: int = 1   Number of workers for the Dataloader. This argument is automatically given by the experiment environment configuration.  </li> <li>pin_memory: bool = False   Whether pin_memory is activated in the Dataloader. This argument is automatically given by the experiment environment configuration.  </li> <li>distributed: bool = False   Whether DDP is used to distribute the model. This argument is automatically given by the experiment environment configuration.  </li> <li>iter_conf: Dict   Iterator configuration for customized batch generation</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"iterator/#__len__self","title":"__len__(self)","text":"<ul> <li>Description:     Get the number of batches the iterator will load in each epoch. </li> <li>Return:     If batches_per_epoch is given, its value will be returned; otherwise, the total number of all the batches in the built-in Dataset object will be returned.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"iterator/#get_batch_indicesself","title":"get_batch_indices(self)","text":"<ul> <li>Description:     This function return the current batching view of the iterator object.</li> <li>Return: List[List[str]]     The batching view generated by the customized hook interface <code>batches_generate_fn()</code>.      Each element of the returned batching view list is a sub-list of data indices where each index corresponds to a data instance in the built-in Dataset object.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"iterator/#get_group_infoself","title":"get_group_info(self)","text":"<ul> <li>Description:     This function returns the group information of the data instances in the built-in Dataset object.     The returned metadata is mainly used for group-wise testing results visualization.</li> <li>Return: Dict     If metadata information is not initialized in the built-in Dataset object, None will be returned.     Otherwise, the meta_info member of the built-in Dataset object will be returned which is a dictionary.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"iterator/#build_loaderself-epoch-start_step","title":"build_loader(self, epoch, start_step)","text":"<ul> <li>Description:     This function generate a <code>torch.util.data.DataLoader</code> object to load the batches of data instances for the current epoch.     If <code>batches_per_epoch</code> is not given, all the batches in <code>self.batches</code> will be used to generate the Dataloader;     If <code>batches_per_epoch</code> is given, a batch clip containing <code>batches_per_epoch</code> batches will be used to generate the Dataloader. <code>batches_per_epoch</code> can be either larger or smaller than the total number of batches.      For a smaller <code>batches_per_epoch</code>, a part of <code>self.batches</code> will be used as the batch clip;      For a larger <code>batches_per_epoch</code>, <code>self.batches</code> will be supplemented by a part of itself to form the batch clip.</li> <li>Arguments: <ul> <li>epoch: int = 1   The number of the current epoch. Used as part of the random seed to shuffle the batches.</li> <li>start_step: int = 0   The start point for the dataloader of the current epoch.    Mainly used for resuming a model testing job from a checkpoint.</li> </ul> </li> <li>Return: torch.util.data.DataLoader     A DataLoader built on the batch clip of the current epoch.      If <code>batches_per_epoch</code> is not given, the batch clip is <code>self.batches</code>.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"iterator/#batches_generate_fnself-data_index-data_len-batch_size","title":"batches_generate_fn(self, data_index, data_len, batch_size)","text":"<ul> <li>Description:     This hook function generates the batching view based on your customized batch generation strategy.     Your overridden function should return the batches of instance indices as a List[List[str]] where each sub-list corresponds to a batch of data instances.      Each element in the sub-list is the index of a data instance.     In this original hook implementation, all the data instances in the built-in Dataset object will be grouped into batches with exactly the same amount of instances.      <code>data_len</code> is not used in this hook function but used for sorting all the instances in the general initialization function of the iterator.      The sorted data instances make sure that the instances in a single batch have similar lengths.</li> <li>Arguments:<ul> <li>data_index: List[str]   The list of indices of all the data instances available to generate the batching view.</li> <li>data_len: Dict[str, int]   The dictionary that indicates the data length of each available data instance in data_index.</li> <li>batch_size: int = None   How many data instances does a batch should have.    If not given, it will be the number of GPUs (ngpu) to ensure that the model validation or testing is done one data instance at each step on a single GPU process. Note: <code>batch_size</code> is implicitly given by <code>**iter_conf</code> in <code>__init__()</code> to this static hook function, so your implementation don't need to keep this argument, and you can declare your own argument.</li> </ul> </li> <li>Return: List[List[str]]   A list of batches generated by your batching strategy. This List[List[str]] is called the batching view of the iterator object.   Each batch in the returned list is a sub-list whose elements are the indices of data instances in the corresponding batch.</li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"iterator/#how-to-construct-multiple-dataloaders","title":"How to Construct Multiple Dataloaders","text":"<p>Multiple Dataloaders can be easily constructed by giving the configuration of multiple iterators. Each iterator creates an independent Dataloader that contributes a data-label pair in the batch. </p> <p>An example of semi-supervised ASR training is shown below. There are two iterators in the train group: sup and unsup (the iterator names are given by users based on their preferences) .</p> <p>These two iterators are in the same type and have built-in datasets with the same type. <pre><code>train:\n    sup:\n        type: block.BlockIterator\n        conf:\n            dataset_type: speech_text.SpeechTextDataset\n            dataset_conf:\n                ...\n            ...\n    unsup:\n        type: block.BlockIterator\n        conf:\n            dataset_type: speech_text.SpeechTextDataset\n            dataset_conf:\n                ...\n            ...\n</code></pre></p> <p>If there are multiple Dataloaders used to load data, each Dataloader will contribute a sub-Dict in the batch Dict train_batch as shown below. </p> <p>The name of each sub-Dict is the one users give as the name of the corresponding iterator. <pre><code>train_batch:\n    sup:\n        feat: torch.Tensor\n        feat_len: torch.Tensor\n        text: torch.Tensor\n        text_len: torch.Tensor\n    unsup:\n        feat: torch.Tensor\n        feat_len: torch.Tensor\n        text: torch.Tensor\n        text_len: torch.Tensor\n</code></pre></p> <p>If you have only one iterator like the configuration below, your train_batch will not have any sub-Dict but only the data-label pair from that iterator. </p> <p>In this case, you don't need to give the name tag for the iterator. <pre><code>train:\n    type: block.BlockIterator\n    conf:\n        dataset_type: speech.speech_text.SpeechTextDataset\n        dataset_conf:\n            ...\n        ...\n\ntrain_batch:\n    feat: torch.Tensor\n    feat_len: torch.Tensor\n    text: torch.Tensor\n    text_len: torch.Tensor\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/","title":"Model","text":"<p>Model is the hub of this part where different Module and Criterion objects can be freely assembled to create a model. Model encapsulates the general model-related services and provides sufficient interface functions for you to override to customize your own models. </p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"model/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configuration File Format</li> <li>Model Library</li> <li>API Document</li> <li>Supported Models</li> <li>How to Freeze a Specific Part of your Model</li> <li>How to Initialize your Model by the Pretrained Models</li> </ol>"},{"location":"model/#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration of your model is given in train_cfg.  The configuration format is shown below. <pre><code>model:\n    model_type: {file_name}.{class_name}\n    model_conf:\n        init: {init_function}\n        frozen_modules:\n            - {frozen_module1}\n            - {frozen_module2}\n            - ...\n        pretrained_model:\n            - path: {model_path1}\n              mapping: \n                {src_name1}: {tgt_name1}\n                {src_name2}: {tgt_name2}\n                ...\n            - path: {model_path2}\n            - ...\n        visual_infer_conf:\n            ...\n        customize_conf: \n            {customize_arg1}: {arg_value1}\n            {customize_arg2}: {arg_value2}\n            ...\n    module_conf:\n        ...\n    criterion_conf:\n        ...    \n</code></pre> The first-level key must be model to notify the framework of the model configuration.  </p> <ol> <li> <p>model_type is used as the query to pick up your target Model subclass in <code>{SPEECHAIN_ROOT}/speechain/model/</code> for model initialization.   Your given query should be in the form of <code>{file_name}.{class_name}</code>, e.g., <code>asr.ASR</code> means the subclass <code>ASR</code> in <code>{SPEECHAIN_ROOT}/speechain/model/asr.py</code>.</p> </li> <li> <p>model_conf contains the general configuration of your model. It is made up of the following 5 parts:</p> <ol> <li> <p>init indicates the function used to initialize the parameters of your model before training.    The available initialization functions are shown in the keys of the built-in dictionary <code>init_class_dict</code>.   For more details about the available initialization functions, please refer to the built-in dictionary <code>init_class_dict</code>.</p> </li> <li> <p>frozen_modules contains the names of the modules that don't need to be updated during training.    If a list of module names is given, all those modules will be frozen. </p> </li> <li> <p>pretrained_model contains the pretrained models you would like to load into your model as the initial parameters.    If a list of pretrained models is given, all those pretrained models will be used to initialize your model.  </p> <ol> <li>path indicates where the pretrained model file is placed.</li> <li>mapping is a dictionary used to solve the mismatch between the parameter names of the pretrained model and the model you want to train.   Each key-value item solves a name mismatch where the key is the name in the pretrained model and the value is the name in the model to be trained.</li> </ol> </li> <li> <p>visual_infer_conf contains the inference configuration you want to use for model visualization during training.       This argument is default to be an empty dictionary which means the default inference configuration of each model will be used.      For more details, please refer to the docstring of <code>inference()</code> of each Model subclass.</p> </li> <li> <p>customize_conf will be used to initialize the main body of the model in the interface function module_init().      For more details about the argument setting, please refer to the README.md of each Model subclass.</p> </li> </ol> </li> <li> <p>module_conf contains all the configuration about the module initialization.    These configuration arguments will be used to initialize the network structure of the model in the interface function module_init().   For more details about the argument setting, please refer to the README.md of each Model subclass.</p> </li> <li> <p>criterion_conf contains all the information about the criterion initialization.    These configuration arguments will be used to initialize all the criteria of the model in the interfance function criterion_init().   For more details about the argument setting, please refer to the README.md of each Model subclass.</p> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#model-library","title":"Model Library","text":"<pre><code>/speechain\n    /model\n        /abs.py     # Abstract Model class. Base of all Model implementations.\n        /asr.py     # All the model implementations of ASR.\n        /tts.py     # All the model implementations of TTS.\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#api-document","title":"API Document","text":"<p>speechain.model.abs.Model </p> <ol> <li> <p>Non-overridable backbone functions: </p> <ol> <li>__init__ </li> <li>batch_to_cuda </li> <li>forward </li> <li>aver_metrics_across_procs </li> <li>evaluate </li> </ol> </li> <li> <p>Overridable interface functions: </p> <ol> <li>bad_cases_selection_init_fn </li> <li>module_init</li> <li>criterion_init </li> <li>batch_preprocess_fn </li> <li>module_forward </li> <li>criterion_forward </li> <li>visualize </li> <li>inference</li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#speechainmodelabsmodel","title":"speechain.model.abs.Model","text":"<p>speechain.model.abs.Model is the base class for all models in this toolkit.  The main job of a model includes:   </p> <ol> <li>(optional) preprocess the input batch data to the trainable format  </li> <li>calculate the model prediction results by the Module members  </li> <li>evaluate the prediction results by the Criterion members  </li> </ol> <p>Each model has several built-in Module members that make up the neural network structure of the model.  These Module members will be initialized by the <code>module_conf</code> given in your configuration.</p> <p>There are a built-in dictionary named <code>init_class_dict</code> and a built-in list named <code>default_init_modules</code> in the base class. <code>init_class_dict</code> contains all the available initialization functions of the model parameters while <code>default_init_modules</code> includes the network layers that have their own initialization functions.</p>"},{"location":"model/#__init__self-args-device-model_conf-module_conf-criterion_conf","title":"__init__(self, args, device, model_conf, module_conf, criterion_conf)","text":"<ul> <li> <p>Description:     In this initialization function, there are two parts of initialization: model-specific customized initialization and model-independent general initialization.</p> <ol> <li> <p>Model-specific customized initialization is done by two interface functions: <code>module_init()</code> and <code>criterion_init()</code>.  <code>module_init()</code> initializes the neural network structure of the model while <code>criterion_init()</code> initializes the criteria used to optimize (loss functions) and evaluate (validation metrics) the model.</p> </li> <li> <p>After the customized initialization, there are 3 steps for general initialization shared by all Model subclasses:</p> </li> <li> <p>Pretrained parameters will be loaded into your model if the key <code>pretrained_model</code> is given.     Multiple pretrained models can be specified and each of them can be loaded into different parts of your model.     The mismatch between the names of pretrained parameters and the parameters of your model is handled by the key <code>mapping</code>.     The value of the key <code>mapping</code> is a dictionary where each key-value item corresponds to a mapping of parameter names.     The key is the parameter name in the pretrained parameters while the value is the parameter name of your model.</p> </li> <li> <p>If <code>pretrained_model</code> is not given, the parameters of your model will be initialized by the function that matches your input query <code>init</code>.     For more details about the available initialization functions, please refer to the built-in dictionary <code>init_class_dict</code>.     If <code>init</code> is not given, the default initialization function <code>torch.nn.init.xavier_normal_</code> will be used to initialize your model.</p> </li> <li> <p>Finally, the specified parts of your model will be frozen if <code>frozen_modules</code> is given.     If there is only one frozen module, you can directly give the string of its name to <code>frozen_modules</code> like <code>frozen_modules: {module_name}</code>.     If there are multiple modules you want to freeze, you can give their names in a list as</p> <pre><code>frozen_modules:\n - {module_name1}\n - {module_name2}\n - ...\n</code></pre> </li> </ol> <p>Moreover, the frozen granularity depends on your input <code>frozen_modules</code>. For example,     1. If you give <code>frozen_modules: encoder_prenet</code>, all parameters of the prenet of your encoder will be frozen     2. If you give <code>frozen_modules: encoder_prenet.conv</code>, only the convolution layers of the prenet of your encoder will be frozen     3. If you give <code>frozen_modules: encoder_prenet.conv.0</code>, only the first convolution layer of the prenet of your encoder will be frozen     4. If you give <code>frozen_modules: encoder_prenet.conv.0.bias</code>, only the bias vector of the first convolution layer of the prenet of your encoder will be frozen</p> </li> <li> <p>Arguments: </p> <ul> <li>args: argparse.Namespace   Experiment pipeline arguments received from the <code>Runner</code> object in <code>runner.py</code>.</li> <li>device: torch.device    The computational device used for model calculation in the current GPU process.</li> <li>model_conf: Dict   The model configuration used for general model initialization.</li> <li>module_conf: Dict   The module configuration used for network structure initialization.</li> <li>criterion_conf: Dict = None   The criterion configuration used for criterion (loss functions and evaluation metrics) initialization.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#batch_to_cudaself-data","title":"batch_to_cuda(self, data)","text":"<ul> <li>Description:     The recursive function that transfers the batch data to the specified device in the current process.</li> <li>Arguments:<ul> <li>data: Dict or torch.Tensor   The input batch data. It should be either a Tensor or a Dict of Tensors.    For the Dict input, the function itself will be called once by each Tensor element.</li> </ul> </li> <li>Return: Dict or torch.Tensor       If the input is a Dict, the returned output will also be a Dict of Tensors transferred to the target device;       If the input is a Tensor, the returned output will be its copy on the target device.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#forwardself-batch_data-epoch-kwargs","title":"forward(self, batch_data, epoch, **kwargs)","text":"<ul> <li> <p>Description:     The general model forward function shared by all the Model subclasses. This forward function has 3 steps:</p> <ol> <li>preprocess and transfer the batch data to GPUs</li> <li>obtain the model prediction results</li> <li>calculate the loss function and evaluate the prediction results</li> </ol> <p>For each step above, we provide interface functions for you to override and make your own implementation.</p> </li> <li> <p>Arguments: </p> <ul> <li>batch_data: Dict   The input batch data received from the <code>train</code> or <code>valid</code> dataloader object in the experimental pipeline.   The batch is in the form of a Dict where the key is the data name and the value is the data content.   </li> <li>epoch: int = None    The number of the current epoch. Used for real-time model visualization and model prediction.  </li> <li>**kwargs:   The additional arguments for real-time model visualization. If given, the code will go through the model visualization branch.  </li> </ul> </li> <li>Return:     In the training branch, the loss functions and evaluation metrics will be returned each of which is in the form of a Dict.     In the validation branch, only the evaluation metrics will be returned.     In the visualization branch, the model snapshots on the given validation instance will be returned.  </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#aver_metrics_across_procsself-metrics-batch_data","title":"aver_metrics_across_procs(self, metrics, batch_data)","text":"<ul> <li>Description:     This function averages the evaluation metrics across all GPU processes in the DDP mode for model distribution.</li> <li>Arguments:<ul> <li>metrics: Dict[str, torch.Tensor]   The evaluation metrics to be averaged across all GPU processes. </li> <li>batch_data: Dict   The input batch data used to calculate the batch size for averaging evaluation metrics.</li> </ul> </li> <li>Return: Dict[str, torch.Tensor]     The evaluation metrics Dict after averaging. The key names remain the same.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#evaluateself-test_batch-infer_conf","title":"evaluate(self, test_batch, infer_conf)","text":"<ul> <li> <p>Description:     The shared evaluation function by all Model subclasses. This evaluation function has 2 steps:</p> <ol> <li>preprocess and transfer the batch data to GPUs</li> <li>calculate the inference results</li> </ol> <p>For each step above, we provide interface functions for you to override and make your own implementation.</p> </li> <li> <p>Arguments:</p> <ul> <li>test_batch: Dict   The input batch data received from the <code>test</code> dataloader object in the experimental pipeline.</li> <li>infer_conf: Dict   The configuration used for model inference.</li> </ul> </li> <li>Return: Dict     A Dict of the inference results where each key-value item corresponds to one evaluation metric you want to save to the disk.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#bad_cases_selection_init_fn","title":"bad_cases_selection_init_fn()","text":"<ul> <li>Description:     This hook function returns the default bad case selection method of each Model object.      This default value will be referred by the Runner to present the top-N bad cases.      The original hook implementation in the base Model class returns None which means no default value.</li> <li>Return: List[List[str or int]]     The returned default value should be a list of tri-list where each tri-list is in the form of [<code>selection_metric</code>, <code>selection_mode</code>, <code>case_number</code>].     For example, ['wer', 'max', 50] means 50 testing waveforms with the largest WER will be selected.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#module_initself-kwargs","title":"module_init(self, **kwargs)","text":"<ul> <li>Description:     The interface function that initializes the Module members of the model. These Module members make up the neural network structure of the model.      Some models have their customized part that also needs to be initialization in this function, e.g. the tokenizer of ASR and TTS models. Note: This interface function must be overridden for each Model subclass.</li> <li>Arguments:<ul> <li>**kwargs:   The combination of the arguments in your <code>module_conf</code> and <code>model_conf['customize_conf']</code>.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#criterion_initself-criterion_conf","title":"criterion_init(self, **criterion_conf)","text":"<ul> <li>Description:     The interface function that initializes the Criterion members of the model.      These Criterion members can be divided into two parts: the loss functions used for training and the evaluation metrics used for validation. Note: This interface function must be overridden for each Model subclass.</li> <li>Arguments:<ul> <li>**criterion_conf:   The arguments in your given <code>criterion_conf</code>.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#batch_preprocess_fnself-batch_data","title":"batch_preprocess_fn(self, batch_data)","text":"<ul> <li>Description:   This hook function does the preprocessing for the input batch data before using them in <code>self.model_forward()</code>.    This function is not mandatory to be overridden and the original implementation in the base Model class does nothing but return the input <code>batch_data</code>. Note: the key names in the returned Dict should match the argument names in <code>self.model_forward()</code>.</li> <li>Arguments:<ul> <li>batch_data: Dict   Raw data of the input batch to be preprocessed in this hook function.</li> </ul> </li> <li>Return: Dict     Processed data of the input batch that is ready to be used in <code>self.model_forward()</code>.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#module_forwardself-batch_data","title":"module_forward(self, **batch_data)","text":"<ul> <li>Description:     This interface function forwards the input batch data by all Module members. Note: <ol> <li>This interface function must be overridden for each Model subclass.</li> <li>The argument names should match the key names in the returned Dict of <code>self.batch_preprocess_fn()</code>.</li> <li>The key names in the returned Dict should match the argument names of <code>self.loss_calculation()</code> and <code>self.metrics_calculation()</code>.</li> </ol> </li> <li>Arguments:<ul> <li>**batch_data:   Processed data of the input batch received from <code>self.batch_preprocess_fn()</code>.</li> </ul> </li> <li>Return: Dict     Prediction results (logits) of the model on the input batch data.      Some intermediate results (e.g., attention matrices) can also be returned for later use.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#criterion_forwardself-kwargs","title":"criterion_forward(self, **kwargs)","text":"<ul> <li>Description:     This interface function is activated after <code>self.model_forward()</code>.      It receives the model prediction results from <code>self.model_forward()</code> and input batch data from <code>self.batch_preprocess_fn()</code>. Note: This interface function must be overridden for each Model subclass.</li> <li>Arguments:<ul> <li>**kwargs:    The combination of the returned arguments from <code>self.batch_preprocess_fn()</code> and <code>self.model_forward()</code>.</li> </ul> </li> <li>Return: (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]     The returned values should be different for the training and validation branches.<ol> <li>For training, two Dict[str, torch.Tensor] should be returned where the first one contains all the trainable training losses for optimization and the second one contains all the non-trainable evaluation metrics used to record the training status.</li> <li>For validation, only one Dict[str, torch.Tensor] should be returned which contains all the non-trainable evaluation metrics used to record the validation status.</li> </ol> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#visualizeself-epoch-sample_index-valid_sample","title":"visualize(self, epoch, sample_index, **valid_sample)","text":"<ul> <li>Description:</li> <li>Arguments:</li> <li>Return:</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"model/#inferenceself-infer_conf-kwargs","title":"inference(self, infer_conf, **kwargs)","text":"<ul> <li> <p>Description:     This function receives the test data and test configuration.      The inference results will be packaged into a Dict[str, Dict] which is passed to the TestMonitor object for disk storage.      The returned Dict should be in the form of     <pre><code>dict(\n    {file_name}=dict(\n        format={file_format},\n        content={file_content}\n    )\n)\n</code></pre></p> <p>The first-level key is used to decide the name of the meta file as <code>idx2{file_name}</code>.  Its value is also a Dict and there must be two keys in this sub-Dict: <code>format</code> and <code>content</code>.  The configuration of the sub-Dict is different for different file formats: 1. For pure text metadata files, the value of <code>format</code> must be <code>txt</code> and the value of <code>content</code> must be a List of Python built-in data type (i.e.,. int, float, str, bool, ...). Each line of the file <code>idx2{file_name}</code> will be made up of the index of a test data instance and its metadata value in the <code>content</code> List which are separated by a blank. For example, <code>dict(cer=dict(format='txt', content=[0.1, 0.2, 0.3]))</code> will create a pure text file named <code>idx2cer</code> which looks like <pre><code>{test_index1} 0.1\n{test_index2} 0.2\n{test_index3} 0.3\n</code></pre> Note: if the first-level key ends with .md, there will not be 'idx2' attached at the beginning of the file name.</p> <ol> <li> <p>For audio files, the value of <code>format</code> must be either <code>wav</code> or <code>flac</code> and the value of <code>content</code> must be a List of array-like data type (e.g. numpy.ndarry, torch.Tensor, ...). Moreover, there must be an additional key named <code>sample_rate</code> to indicate the sampling rate of the waveforms to be saved in audio files. There will be a folder named <code>{file_name}</code> that contains all the audio files and a pure text file named <code>idx2{file_name}</code> that contains the absolute paths of all the saved audio files. For example, <code>dict(wav=dict(format='flac', content=[np_arr1, np_arr2, np_arr3]))</code> will create a folder named <code>wav</code> and a pure text file named <code>idx2wav</code> in the same directory.  The file <code>idx2wav</code> looks like:      <pre><code>{test_index1} /x/xx/wav/{test_index1}.flac\n{test_index2} /x/xx/wav/{test_index2}.flac\n{test_index3} /x/xx/wav/{test_index3}.flac\n</code></pre>      where <code>/x/xx/</code> is your result path given in your <code>exp_cfg</code>.</p> </li> <li> <p>For binary files, the value of <code>format</code> in the sub-Dict must be <code>npy</code> and the value of <code>content</code> must be a List of numpy.ndarry (torch.Tensor is not supported). There will be a folder named <code>{file_name}</code> that contains all the .npy files and a pure text file named <code>idx2{file_name}</code> that contains the absolute paths of all the saved binary files. For example, <code>dict(feat=dict(format='npy', content=[np_arr1, np_arr2, np_arr3]))</code> will create a folder named <code>feat</code> and a pure text file named <code>idx2feat</code>.  The <code>idx2feat</code> file is like:    <pre><code>{test_index1} /x/xx/feat/{test_index1}.npy\n{test_index2} /x/xx/feat/{test_index2}.npy\n{test_index3} /x/xx/feat/{test_index3}.npy\n</code></pre>    where <code>/x/xx/</code> is your result path given in your <code>exp_cfg</code>.</p> </li> <li>Arguments:</li> <li>infer_conf: Dict   The configuration Dict used for model inference.</li> <li>**kwargs:   The testing data loaded from <code>test</code> dataloader object in the experimental pipeline. </li> <li>Return: Dict[str, Dict[str, str or List]] The model inference results to be saved on the disk.</li> </ol> </li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#supported-models","title":"Supported Models","text":"<ol> <li>ASR Recipes <ol> <li>asr.ARASR <ul> <li>Structure: Auto-Regressive CTC-Attention ASR model.  </li> <li>Input: One tuple of speech-text paired data (feat, feat_len, text, text_len) in model_forward().  </li> <li>Output: One ASR loss calculated on the input data tuple in criterion_calculation().</li> </ul> </li> <li>asr.SemiARASR <ul> <li>Structure: Semi-supervised Auto-Regressive CTC-Attention ASR model.  </li> <li>Input: Multiple tuples of speech-text paired data  (feat, feat_len, text, text_len) in model_forward().  Each of them is generated by a specific <code>torch.utils.data.Dataloader</code>.</li> <li>Output: Multiple ASR losses calculated on all the input data tuples in criterion_calculation().  A loss named loss is also returned which is the trainable overall loss summed by all ASR losses. </li> </ul> </li> </ol> </li> <li>TTS Recipes <ol> <li>tts.ARTTS<ul> <li>Structure: Auto-Regressive Attention TTS model.  </li> <li>Input: One tuple of speech-text paired data (feat, feat_len, text, text_len) in model_forward().  </li> <li>Output: One TTS loss calculated on the input data tuple in criterion_calculation().</li> </ul> </li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#how-to-freeze-a-specific-part-of-your-model","title":"How to Freeze a Specific Part of your Model","text":"<p>Parameter freezing can be done simply by giving the name of the module you want to freeze in frozen_modules.  In the example below, the encoder of the ASR model will be frozen while other modules are still trainable. <pre><code>model:\n    model_type: asr.ARASR\n    model_conf:\n        frozen_modules: encoder\n</code></pre> If you want to freeze multiple modules, you can give their names as a list in frozen_modules.  In the example below, the prenets of both the encoder and decoder will be frozen. <pre><code>model:\n    model_type: asr.ARASR\n    model_conf:\n        frozen_modules:\n          - encoder.prenet\n          - decoder.prenet\n</code></pre> The parameter freezing granularity can be very fine if you specify the module name by a series of dots.  In the example below, the convolution layers of the prenet of the encoder will be frozen. <pre><code>model:\n    model_type: asr.ARASR\n    model_conf:\n        frozen_modules: \n            - encoder.prenet.conv\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"model/#how-to-initialize-your-model-by-the-pretrained-model","title":"How to Initialize your Model by the Pretrained Model","text":"<p>Pretrained model loading can be easily done by giving the model path in pretrained_model.  In the example below, the entire ASR model will be initialized by the given best_accuracy.pth model. <pre><code>mdl_root: recipe/asr/librispeech/train-clean-100/exp/{exp_name}/models\nmodel:\n    model_type: asr.ARASR\n    model_conf:\n        pretrained_model:\n            path: !ref &lt;mdl_root&gt;/accuracy_best.pth\n</code></pre> If you only want to initialize a part of your model, you can use the mapping argument in pretrained_model.  The parameter name mismatch can also be solved by the mapping argument.  In the example below, only the encoder of the ASR model will be initialized by the given pretrained model.  Even though the pretrained model is constructed by unit modules, it can still be loaded into the ASR model constructed by template modules by aligning their module names. <pre><code>model_root: recipe/asr/librispeech/train-clean-100/exp/{exp_name}/models\nmodel:\n    model_type: asr.ARASR\n    model_conf:\n        pretrained_model:\n            path: !ref &lt;model_root&gt;/accuracy_best.pth\n            mapping: \n              encoder_prenet: encoder.prenet\n              encoder: encoder.encoder\n</code></pre> There could be multiple pretrained models in pretrained_model that are used to initialize your model.  In the example below, the encoder and decoder of the ASR model are initialized by different pretrained models.</p> <p>Note that if there are overlapping modules between the mapping arguments of different pretrained models,  the module will be initialized by the pretrained models at the back of the list. <pre><code>model_root: recipe/asr/librispeech/train-clean-100/exp/{exp_name}/models\nmodel:\n    model_type: asr.ARASR\n    model_conf:\n        pretrained_model:\n            - path: !ref &lt;model_root&gt;/accuracy_best.pth\n              mapping:\n                encoder_prenet: encoder.prenet\n                encoder: encoder.encoder\n            - path: !ref &lt;model_root&gt;/10_accuracy_average.pth\n              mapping:\n                decoder_prenet: decoder.prenet\n                decoder: decoder.decoder\n                decoder_postnet: decoder.postnet\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"module/","title":"Module","text":"<p>Module inherits <code>torch.nn.Module</code> and it is the base class for all Module objects in this toolkit.  The neural network parts of all the Model objects in this toolkit are constructed by multiple Module objects in a nested structure. Below is the nested Module tree of an encoder-decoder ASR model: <pre><code>ASR (Model)\n    ---&gt; ASREncoder (Module)\n        ---&gt; Speech2MelSpec (Module)\n            ---&gt; Speech2LinearSpec (Module)\n            ---&gt; LinearSpec2MelSpec (Module)\n        ---&gt; Conv2dPrenet (Module)\n            ---&gt; LinearPrenet (Module)\n        ---&gt; TransformerEncoder (Module)\n            ---&gt; PositionalEncoding (Module)\n            ---&gt; MultiHeadedAttention (Module)\n            ---&gt; PositionwiseFeedForward (Module)\n    ---&gt; ASRDecoder (Module)\n        ---&gt; EmbedPrenet (Module)\n        ---&gt; TransformerDecoder (Module)\n            ---&gt; PositionalEncoding (Module)\n            ---&gt; MultiHeadedAttention (Module)\n            ---&gt; PositionwiseFeedForward (Module)\n        ---&gt; TokenPostnet (Module)\n</code></pre> This base class has two required abstract interface functions that must be overriden by all Module subclasses: <code>module_init()</code> for module initialization and <code>forward()</code> for output calculation.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"module/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Module Library</li> <li>API Document</li> </ol>"},{"location":"module/#module-library","title":"Module Library","text":"<pre><code>/speechain\n    /module\n        /abs.py             # Abstract class of Module. Base of all Module implementations.\n        /frontend           # Acoustic feature extraction frontend modules\n            /speech2linear.py   # Module implementation of speech-to-linear frontend. Used to transform the input speech waveforms into linear spectrogram.\n            /linear2mel.py      # Module implementation of linear-to-mel frontend. Used to transform the input linear spectrogram into log-mel spectrogram.\n            /speech2mel.py      # Module implementation of speech-to-mel frontend. Used to transform the input speech waveforms into log-mel spectrogram.\n            /delta_feat.py      # Module implementation of delta frontend. Mainly used for ASR training when we want to take the first and second derivatives of log-mel spectrogram.\n        /norm               # Normalization modules\n            /feat_norm.py       # Module implementation of per-channel feature normalization.\n        /augment            # Data augmentation modules\n            /specaug.py         # Module implementation of SpecAugment. Mainly used for ASR training.\n        /encoder            # Model encoder modules\n            /asr.py             # Module implementation of ASR encoders. Used for ASR model construction.\n            /tts.py             # Module implementation of TTS encoders. Used for TTS model construction.\n        /decoder            # Model decoder modules\n            /asr.py             # Module implementation of ASR autoregressive decoders. Used for autoregressive ASR model construction.\n            /tts.py             # Module implementation of TTS autoregressive decoders. Used for autoregressive TTS model construction.\n        /prenet             # Model prenet modules in front of encoders and decoders\n            /conv1d.py          # Module implementation of 1D Convolutional prenet.\n            /conv2d.py          # Module implementation of 2D Convolutional prenet.\n            /embed.py           # Module implementation of token embedding prenet.\n            /linear.py          # Module implementation of stacked linear prenet.\n            /spk_embed.py       # Module implementation of speaker embedding prenet.\n        /postnet            # Model postnet modules behind encoders and decoders\n            /conv1d.py          # Module implementation of 1D Convolutional postnet.\n            /token.py           # Module implementation of token prediction postnet.\n        /transformer        # Transformer-related modules\n            /encoder.py         # Module implementation of Transformer encoder layers. Used for decoder construction of ASR and TTS models.\n            /decoder.py         # Module implementation of Transformer autoregressive decoder layers. Used for decoder construction of autoregressive ASR and TTS models.\n            /pos_enc.py         # Module implementation of positional encoding layers.\n            /attention.py       # Module implementation of multi-head attention layers.\n            /feed_forward.py    # Module implementation of point-wise feed-forward layers.\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"module/#api-document","title":"API Document","text":"<p>Non-overridable backbone functions:    1. speechain.module.abs.Module.__init__ </p> <p>Overridable interface functions:    1. speechain.module.abs.Module.module_init    2. speechain.module.abs.Module.forward    3. speechain.module.abs.Module.recover    4. speechain.module.abs.Module.reset_parameters    5. speechain.module.abs.Module.get_recordable_para</p>"},{"location":"module/#speechainmoduleabsmodule__init__self-input_size-distributed-module_conf","title":"speechain.module.abs.Module.__init__(self, input_size, distributed, **module_conf)","text":"<ul> <li>Description:   This initialization function is shared by all Module subclasses.    There are two built-in variable members: <code>input_size</code> and <code>output_size</code>.    <code>input_size</code> is the last dimension of the input tensor while <code>output_size</code> is the last dimension of the output tensor.   These two member variables serve as the socket and plug that are used to communicate with the front and back Module objects in a Model object.   You could utilize <code>self.input_size</code> in your <code>module_init()</code> implement to initialize your module and give the output data dimension to <code>self.output_size</code>. Note: The usage of these two member variables is not mandatory, but it would be a convenient way for you to initialize your module.</li> <li>Arguments:<ul> <li>input_size: int = None   The last dimension of the tensor from the front Module object. If not given, this argument would be None.</li> <li>distributed: bool = False   Whether the Model object this Module object is belong to is distributed to multiple GPUs.</li> <li>**module_conf:   The arguments used by <code>module_init()</code> for your customized Module initialization.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"module/#speechainmoduleabsmodulemodule_initself-module_conf","title":"speechain.module.abs.Module.module_init(self, **module_conf)","text":"<ul> <li>Description:   Abstract interface function for customized initialization of each Module subclass.    This interface function is mandatory to be overridden by your implementation.</li> <li>Arguments:<ul> <li>**module_conf:   The arguments used for customized Module initialization.   For more details, please refer to the docstring of your target Module subclass.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"module/#speechainmoduleabsmoduleforwardself-kwargs","title":"speechain.module.abs.Module.forward(self, **kwargs)","text":"<ul> <li>Description:     This abstract interface function is the customized implementation of <code>torch.nn.Module.forward()</code> used during model forward calculation.      This interface function is mandatory to be overridden by your implementation.</li> <li>Arguments:<ul> <li>**kwargs:   The input arguments for module forward calculation.   For more details, please refer to the docstring of <code>forward()</code> of your target Module subclass.</li> </ul> </li> <li>Return:   Module forward calculation results.   For more details, please refer to the docstring of <code>forward()</code> of your target Module subclass.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"module/#speechainmoduleabsmodulerecoverself-kwargs","title":"speechain.module.abs.Module.recover(self, **kwargs)","text":"<ul> <li>Description:   This abstract interface function is used to recover the module forward calculation results back to the input data.    It can be considered as the reverse process of <code>forward()</code>.   This interface function is not mandatory to be overridden.</li> <li>Arguments:<ul> <li>**kwargs:   The input forward calculation results to be recovered.    For more details, please refer to the docstring of <code>recover()</code> of your target Module subclass.</li> </ul> </li> <li>Return:   The recovered data or closely-recovered data (sometimes <code>forward()</code> may not be totally recoverable).   For more details, please refer to the docstring of <code>recover()</code> of your target Module subclass.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"module/#speechainmoduleabsmodulereset_parametersself","title":"speechain.module.abs.Module.reset_parameters(self)","text":"<ul> <li>Description:   This abstract interface function is used to initialize the customized parameters in the Module subclass if had.   Some Module subclasses have their customized parameters with specific initialization functions.   If your Module implementation has some customized parameters and you want to initialize them by yourself,   please give the initialization logic in this interface function.   This interface function is not mandatory to be overridden. Note: Don't forget to add <code>self.default_init_modules.append(YourModule)</code> in <code>model_init()</code> of your Model.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"module/#speechainmoduleabsmoduleget_recordable_paraself","title":"speechain.module.abs.Module.get_recordable_para(self)","text":"<ul> <li>Description:   This function returns the parameters of the module that you want to record as part of step information.   If you want to record the value of the customized parameters of your module:<ol> <li>when it is a leaf (no Module members) in the nested Module tree of the model,    please override this function and return the parameter values in a Dict.   For an example, you can refer to ${SPEECHAIN_ROOT}/speechain/module/transformer/pos_enc.py.</li> <li>when it is a non-leaf (with Module members) in the nested Module tree of the model,    please follow the pseudocode below: <pre><code>class YourModule(Module):\n    def get_recordable_para(self) -&gt; Dict or None:\n      output = dict()\n      # add the value of your target parameter into the output as key-value items\n      output.update(super(YourModule, self).get_recordable_para())\n      return output\n</code></pre></li> </ol> </li> <li>Return: Dict or None   For the leaf module, the default implementation returns None;   For the non-leaf module, the default implementation returns a Dict containing names and recordable parameters of its member modules.</li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/","title":"OptimScheduler","text":"<p>OptimScheduler is the base class of all OptimScheduler objects that combine the roles of traditional optimizers and schedulers together.  Its main job is optimizing the target model parameters and scheduling the learning rate during training. In this toolkit, we combine traditional optimizers and schedulers into a single class: OptimScheduler.  Each OptimScheduler object has one built-in member optimizer (<code>torch.optim.Optimizer</code>) which is initialized automatically by the <code>optim_type</code> and <code>optim_conf</code> given in your configuration.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"optim_sche/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configuration File Format</li> <li>OptimScheduler Library</li> <li>API Document</li> <li>How to Construct Multiple Optimizers on Multiple Losses</li> <li>How to Simulate Large Batch Training with Limited GPUs</li> <li>How to Perform Fine-tuning</li> </ol>"},{"location":"optim_sche/#configuration-file-format","title":"Configuration File Format","text":"<p>The configuration of OptimScheduler is given in the <code>optim_sches</code> tag of train_cfg.  The configuration format is shown below. <pre><code>optim_sches:\n    type: {file_name}.{class_name}\n    conf:\n        optim_type: {class_name}\n        optim_conf:\n            ...\n        # general optimscheduler configuration\n        optim_loss:\n        updated_modules:\n        step_per_update:\n        # customized optimscheduler configuration\n        ...\n</code></pre></p> <ul> <li> <p>The first-level key must be optim_sches to notify the framework of the optimscheduler configuration.  </p> <ol> <li> <p>type is a second-level key that indicates your optimscheduler type.  The value of this key is used as the query to pick up your target OptimScheduler subclass for initialization.  Your given query should be in the form of <code>file_name.class_name</code> to indicate the place of your target subclass. For example, <code>noam.NoamLr</code> means the class <code>NoamLr</code> in <code>./speechain/optim_sche/noam.py</code>.  </p> </li> <li> <p>conf is a second-level key that indicates your optimscheduler configuration.  The value of this key is a Dict whose configuration is as following:</p> <ol> <li> <p>optim_type is a query that indicates the type of the built-in <code>torch.optim.Optimizer</code> in this optimscheduler.   Your given query should be in the form of <code>class_name</code> to indicate your target subclass in <code>torch.optim</code>.   For example, <code>Adam</code> means the class <code>torch.optim.Adam</code>.</p> </li> <li> <p>optim_conf contains all the configuration used to initialize the built-in optimizer.     For more details, please refer to the PyTorch document of your target <code>torch.optim.Optimizer</code> subclass.  </p> </li> <li> <p>optimscheduler general configuration is shared by all OptimScheduler subclasses.         1. optim_loss         2. updated_modules         3. step_per_update  </p> </li> <li> <p>optimscheduler customized configuration is used to initialize the customized part of each optimscheduler subclass.    This part defines the scheduling strategy to adjust the learning rates during training.   Please refer to the docstrings of your target OptimScheduler subclass for more details.</p> </li> </ol> </li> </ol> </li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#optimscheduler-library","title":"OptimScheduler Library","text":"<pre><code>/speechain\n    /optim_sche\n        /abs.py     # Abstract class of OptimScheduler. Base of all OptimScheduler implementations.\n        /noam.py    # OptimScheduler implementation of the Noam scheduler. Mainly used for Transformer training.\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#api-document","title":"API Document","text":"<p>Non-overridable backbone functions:    1. speechain_optim_sche.abs.OptimScheduler.__init__    2. speechain.optim_sche.abs.OptimScheduler.step    3. speechain.optim_sche.abs.OptimScheduler.get_lr    4. speechain.optim_sche.abs.OptimScheduler.state_dict    5. speechain.optim_sche.abs.OptimScheduler.load_state_dict    6. speechain.optim_sche.abs.OptimScheduler.__repr__ </p> <p>Overridable interface functions:    1. speechain.optim_sche.abs.OptimScheduler.sche_init    2. speechain.optim_sche.abs.OptimScheduler.update_lr    3. speechain.optim_sche.abs.OptimScheduler.extra_repr_fn </p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimscheduler__init__self-optim_type-optim_conf-model-distributed-optim_loss-updated_modules-step_per_update-use_amp-accum_grad-ft_factor-grad_clip-grad_norm_type-sche_conf","title":"speechain_optim_sche.abs.OptimScheduler.__init__(self, optim_type, optim_conf, model, distributed, optim_loss, updated_modules, step_per_update, use_amp, accum_grad, ft_factor, grad_clip, grad_norm_type, **sche_conf)","text":"<ul> <li>Description:     This initialization function initializes the general part shared by all OptimScheduler subclasses.     At the end of this function, an interface function <code>sche_init()</code> is called to initialize the customized part of each OptimScheduler subclass.</li> <li> <p>Arguments: </p> <p>Arguments received from <code>exp_cfg</code>:  </p> <ul> <li>model: speechain.model.abs.Model    The pointer to the model whose parameters will be optimized by the built-in <code>torch.optim.Optimizer</code>.  </li> <li>distributed: bool = False    Whether the model to be optimized is distributed to multiple GPUs.     If True, gradient accumulation will be done asynchronously in the DDP mode to speed up training.  </li> <li>use_amp: bool = True    Whether the Automatic Mixed Precision (AMP) technique is used during back-propagation.    If True, a built-in <code>torch.cuda.amp.GradScaler</code> will be initialized to calculate the gradients and optimize the parameters.  </li> <li>accum_grad: int = 1    The number of steps to accumulate gradients before optimization.     The larger this argument is, the larger your virtual batches will be.  </li> <li>ft_factor: float = 1.0    The finetuning factor used to scale down the learning rates during training.</li> </ul> </li> </ul> <p>Arguments received from <code>train_cfg</code>:  </p> <ul> <li>optim_type: str      The optimizer query used to pick up the target Optimizer subclass from <code>torch.optim</code>.  </li> <li>optim_conf: Dict      The configuration used to initialize the built-in <code>torch.optim.Optimizer</code>.  </li> <li>optim_loss: str = None      The name of the target loss used in this OptimScheduler object to calculate the gradients.       If not given, the loss named <code>loss</code> will be used for optimization.  </li> <li>updated_modules: str or List[str]      This argument allows you to update only a part of parameters of the built-in model pointer.       <code>updated_modules</code> indicate the names of your target modules (first-level module in the nested module tree) in the built-in model pointer.      Its value can be either a string (only one target module) or a list (multiple target modules).      If not given, the entire model will be updated.  </li> <li>step_per_update: int = 1      The optimization interval for the built-in optimizer.      It means that the parameter optimization will be done once every <code>step_per_update</code> steps.  </li> <li>**sche_conf:      The arguments used to initialize the customized part of this OptimScheduler.       Mainly used to decide the learning rate scheduling strategy.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulersteplosses-time_func-optim_name-step_num","title":"speechain_optim_sche.abs.OptimScheduler.step(losses, time_func, optim_name, step_num)","text":"<ul> <li>Description:     This function optimizes the target parameters of the built-in model pointer with the input training losses.</li> <li>Arguments:<ul> <li>losses: Dict[str, torch.Tensor]   The training loss Dict received from the <code>criterion_forward()</code> of the bulit-in model pointer.</li> <li>time_func:   The context function used to record the consumed time during gradient back-propagation and parameter optimization.</li> <li>optim_name: str   The name of the OptimScheduler object.    This argument is used to identify the recorded consumed time information.</li> <li>step_num: int   The number of the current training step.    This argument is used to update the learning rate for the current step by <code>self.update_lr()</code>.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulerget_lrself","title":"speechain_optim_sche.abs.OptimScheduler.get_lr(self)","text":"<ul> <li>Description:     This function returns the current learning rate of the built-in <code>torch.optim.Optimizer</code> member.</li> <li>Return: float   The value of the learning rates obtained from <code>self.optimizer.param_groups</code>.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulerstate_dictself","title":"speechain_optim_sche.abs.OptimScheduler.state_dict(self)","text":"<ul> <li>Description:     This function returns the current status of the OptimScheduler object for checkpoint storage.</li> <li>Return: Dict   The status Dict containing the current status of the built-in <code>torch.optim.Optimizer</code> and the built-in <code>torch.cuda.amp.GradScaler</code> (if had).</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulerload_state_dictself-state_dict","title":"speechain_optim_sche.abs.OptimScheduler.load_state_dict(self, state_dict)","text":"<ul> <li>Description:   This function loads the existing checkpoint information into the OptimScheduler object as the starting status.</li> <li>Arguments:<ul> <li>state_dict: Dict   The status information loaded from the existing checkpoint.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimscheduler__repr__self","title":"speechain_optim_sche.abs.OptimScheduler.__repr__(self)","text":"<ul> <li>Description:     This function returns the description string of the OptimScheduler object.      There is a general description part shared by all the OptimScheduler subclasses.     In this function, an interface hook function <code>extra_repr_fn()</code> will be called to generate the specific description part of each OptimScheduler subclass.</li> <li>Return: str     The description string for the OptimScheduler object.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulersche_initsche_conf","title":"speechain_optim_sche.abs.OptimScheduler.sche_init(**sche_conf)","text":"<ul> <li>Description:     This abstract interface function is the customized initialization function which decides how the learning rate is scheduled as the training goes.     This interface is mandatory to be overridden.  </li> <li>Arguments:<ul> <li>**sche_conf:   The arguments used to initialize the customized part of this OptimScheduler.   For more details about the learning rate scheduling strategy, please refer to the docstring of <code>sche_init()</code> of your target OptimScheduler subclass.  </li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulerupdate_lrself-real_step","title":"speechain_optim_sche.abs.OptimScheduler.update_lr(self, real_step)","text":"<ul> <li>Description:     This abstract interface function generates the learning rate by the input step number.  </li> <li>Arguments:<ul> <li>real_step: int   The number of the real step for parameter optimization.    Due to the existence of <code>self.accum_grad</code>, parameter optimization may not be done at each training step.    The real step number here means the training steps where parameter optimization is done.</li> </ul> </li> <li>Return: float     The learning rate used for parameter optimization in the current training step.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"optim_sche/#speechain_optim_scheabsoptimschedulerextra_repr_fnself","title":"speechain_optim_sche.abs.OptimScheduler.extra_repr_fn(self)","text":"<ul> <li>Description:     This interface hook function returns the specific part of the description string of the OptimScheduler object.      The original implementation in the base class returns an empty string.     In principle, this interface hook function must be overridden by each OptimScheduler subclass.      But there won't be any errors if you don't override it in your implementation.</li> <li>Return: str     The specific part of the description string of the OptimScheduler object. </li> </ul> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#how-to-construct-multiple-optimizers-on-multiple-losses","title":"How to Construct Multiple Optimizers on Multiple Losses","text":"<p>The cooperation of multiple optimizers is handled by 3 arguments: optim_losses, updated_modules, and step_per_update.  </p> <ol> <li> <p>optim_losses means the training loss used to calculate the gradients for the optimizer.   </p> </li> <li> <p>update_modules means the target module in your where that you would like the optimizer to update the parameters.  </p> </li> <li> <p>step_per_update means the updating frequency of the optimizer (i.e. the parameter optimization can be done once per step_per_update steps).</p> </li> </ol> <p>In the example below, there are two optimschedulers for optimizing the parameters of an Encoder-Decoder model.  encoder_optim optimizes the encoder part using the training loss called encoder_loss while decoder_optim optimizes the decoder part using the training loss called decoder_loss.  The encoder optimization is done once every 2 steps while the decoder optimization is done once every step. <pre><code>optim_sches:\n    encoder_optim:\n        type: noam.NoamLr\n        conf:\n            optim_type: Adam\n            optim_conf:\n                ...\n            optim_losses: encoder_loss\n            updated_modules: encoder\n            step_per_update: 2\n\n    decoder_optim:\n        type: noam.NoamLr\n        conf:\n            optim_type: Adam\n            optim_conf:\n                ...\n            optim_losses: decoder_loss\n            updated_modules: decoder\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#how-to-simulate-large-batch-training-with-limited-gpus","title":"How to Simulate Large Batch Training with Limited GPUs","text":"<p>We provide a method called gradient accumulation (implemented by the argument <code>accum_grad</code> in exp_cfg) to train your model with large batches that are beyond the memory of your GPUs.  The basic idea is to accumulate the gradients calculated in several small batches and update the model with the accumulated gradients to mimic a large batch.  So, the actual batch size becomes <code>accum_grad * batch_size</code>.</p> <p>The pseudo-code of gradient accumulation is like this: <pre><code>for step in range(max_step):\n    loss /= accum_grad\n    loss.backward()\n    if step % accum_grad == 0:\n        # real_step = (step - 1) // accum_grad + 1\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre> Let me show you an intuitive example.  Suppose we want to calculate the mean value of 1, 2, ..., 9, 10 but we cannot directly divide the sum by 10 because our calculator is not powerful enough.  Instead, we can calculate the mean value of two sub-groups: 1, 2, .., 5 and 6, 7, ..., 10.  We get two sub-mean values: 3 and 8.  The overall mean value can be calculated by taking the mean value of these two sub-mean values: (3 + 8) / 2 = 5.5.</p> <p>Unfortunately, gradient accumulation is not identical to large batch training.  Since small batches are used to calculate the gradients of each step, some calculations of large batch training cannot be simulated (e.g. BatchNorm and FeatureNormalization).  Therefore, the performance of the model trained by gradient accumulation may be slightly different from the one trained by the actual large batches. </p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"optim_sche/#how-to-perform-fine-tuning","title":"How to Perform Fine-tuning","text":"<p>In the normal setting, we need to scale down the learning rates by a factor of 10 to 100 for fine-tuning a pretrained model.  In this toolkit, the learning rates can be easily scaled down by the input argument <code>ft_factor</code> in exp_cfg without changing the scheduling configuration of your optimscheduler.  It's no longer necessary for you to redesign the scheduler configuration for fine-tuning!</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"readme/","title":"Readme","text":""},{"location":"readme/#how-to-build-speechain-documentation","title":"How to build speechain documentation","text":"<pre><code># install necessary packages\npip install mkdocs-material mkdocstrings[python] mkdocs-gen-files mkdos-literate-nav mkdocs-section-index\n\n# build documentation\nmkdocs build -v\n\n# serve documentation\nmkdocs serve -v\n\n# deploy to github pages, not mandatory auto build when push\nmkdocs gh-deploy\n</code></pre>"},{"location":"recipes/","title":"Recipes","text":""},{"location":"recipes/#directory-for-reripes-of-the-speechain-toolkit","title":"Directory for reripes of the SpeeChain toolkit","text":"<p>The SpeeChain toolkit organizes its recipes by task, each located within a dedicated sub-folder in <code>/speechain/recipes/</code>.  Every task sub-folder hosts second-level sub-folders that pertain to individual datasets.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"recipes/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Available Datasets</li> <li>ASR (Automatic Speech Recognition)</li> <li>TTS (Text-To-Speech Synthesis)</li> <li>Offline TTS-to-ASR Chain</li> <li>Experimental File System</li> </ol>"},{"location":"recipes/#available-datasets","title":"Available Datasets","text":"<p>Each task comes with a dedicated README.md file found within its respective folder, providing further details and instructions.  Follow the hyperlinks below to navigate to the README.md file for your target task.</p>"},{"location":"recipes/#automatic-speech-recognition-asr","title":"Automatic Speech Recognition (ASR)","text":"<p>Refer to this sample structure for an overview of how the ASR folder is organized: <pre><code>/asr\n    /librispeech            # ASR Recipes for the LibriSpeech dataset\n        /train-clean-100        # Labeled data: train-clean-100\n            /data_cfg               # Data loading configuration files\n            /exp_cfg                # Experimental configuration files\n        /train-clean-460        # Labeled data: train-clean-460 (train-clean-100 + train-clean-360)\n            ...\n        /train-960              # Labeled data: train-960 (train-clean-460 + train-other-500)\n            ...\n    /libritts_librispeech   # ASR Recipes for the joint dataset of LibriSpeech and 16khz-downsampled LibriTTS\n        /train-960              # Labeled data: LibriSpeech_train-960 &amp; 16khz-LibriTTS_train-960\n            ...\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"recipes/#text-to-speech-synthesis-tts","title":"Text-To-Speech Synthesis (TTS)","text":"<p>Refer to this sample structure for an overview of how the TTS folder is organized: <pre><code>/tts\n    /libritts               # TTS Recipes for the LibriTTS dataset\n        /train-clean-100        # Labeled data: train-clean-100\n            /data_cfg               # Data loading configuration files that are shared by different models\n            /exp_cfg                # Experimental configuration files\n        /train-clean-460        # Labeled data: train-clean-460 (train-clean-100 + train-clean-360)\n            ...\n        /train-960              # Labeled data: train-960 (train-clean-460 + train-other-500)\n            ...\n    /ljspeech               # TTS Recipes for the LJSpeech dataset, LJSpeech doesn't have the official subset division.\n        ...\n    /vctk                   # TTS Recipes for the VCTK dataset, VCTK doesn't have the official subset division.\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"recipes/#offline-tts-to-asr-chain","title":"Offline TTS-to-ASR Chain","text":"<p>Refer to this sample structure for an overview of how the offline TTS-to-ASR Chain folder is organized: <pre><code>/offline_tts2asr\n    /librispeech   # LibriSpeech is used as labeled data to train ASR models\n        /train-clean-100    # ASR Labeled data: LibriSpeech_train-clean-100\n            /data_cfg               # Data loading configuration files\n            /exp_cfg                # Experimental configuration files\n        /train-960          # ASR Labeled data: LibriSpeech_train-960\n            ...\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"recipes/#experimental-file-system","title":"Experimental File System","text":"<p>The file structure of an experiment folder is as follows: <pre><code>/data_cfg\n/train_cfg\n/exp_cfg\n/exp                # 'exp' folder of each model\n    /{exp_name}         # Name of a specific experiment\n        /{test_cfg_name}    # Name of a testing configuration file\n            /{test_model_name}  # Name of the model you want to test the performance\n                /{test_set_name}    # Name of a test set\n                    /figures                # Folder that contains all the distribution figures of each metric on the test set\n                        {test_metric_name}.png  # Histogram distribution figure of the {test_metric_name} values of all the testing instances\n                        ...\n                    test.log                # Log file that contains the testing process of a specific test set\n                    overall_results.md      # .md file that contains the model overall performance on the test set\n                    instance_reports.md     # .md file that contains the detailed performance reports of each testing instance\n                    topn_(max/min)_{xxx}.md # .md file that contains the top-n bad cases selected by the metric 'xxx'\n                    idx2{xxx}               # suffix-free .txt files that contain individual metrics of each testing instance, a file corresponds to a metric 'xxx'\n                ...                 # other test sets\n            ...                 # other test models\n        ...                 # other test configurations\n\n        models/             # This sub-folder contains all the model files\n            N_{xxx}_average.pth # Average model obtained by the metric 'xxx' on 'N' best models\n            {xxx}_best.pth      # File soft link to the best model obtained by the metric 'xxx'\n            {xxx}_best_2.pth    # File soft link to the second best model obtained by the metric 'xxx'\n            ...                \n            {xxx}_best_n.pth    # File soft link to the n-th best model obtained by the metric 'xxx'  \n            epoch_{X}.pth       # Actual model file of the X-th epoch\n            epoch_{Y}.pth       # Actual model file of the Y-th epoch\n            ...          \n\n        figures/            # This sub-folder contains all the snapshotting figures. The .txt files in this folder contain the data used to plot summary.png which can be used to plot your own figures.\n            train/              # Snapshotting figures made during training\n                consumed_memory/    # Folder containing the GPU memory consumption records\n                     Rank0.txt                           # Numerical records of the memory consumption for the GPU rank.0 through epochs\n                     ...                                 # Other GPUs if have\n                     RankN.txt                           # Numerical records of the memory consumption for the GPU rank.N through epochs\n                     summary.png                         # Curve graph of all the records above through epochs\n                consumed_time/      # Folder containing the time consumption records\n                     data_load_time.txt                  # Numerical records of the data loading time through epochs\n                     model_forward_time.txt              # Numerical records of the model forward time through epochs\n                     loss_backward_time_{optim_name}.txt # Numerical records of the loss backward time for the optimizer named {optim_name} through epochs\n                     optim_time_{optim_name}.txt         # Numerical records of the parameter optimization time for the optimizer named {optim_name} through epochs\n                     summary.png                         # Curve graph of all the records above through epochs\n                criteria/           # Folder containing the training criterion value records\n                     {train_criterion_name}              # Numerical records of the training criterion named {train_criterion_name} through epochs\n                     ...                                 # Other training criteria if have\n                     summary.png                         # Curve graph of all the records above through epochs\n                optim_lr/           # Folder containing the learning rate records of each optimizer\n                     {optim_name}.txt                    # Numerical recores of the learning rates of the optimizer named {optim_name} through epochs\n                     ...                                 # Other optimizers if have\n                     summary.png                         # Curve graph of all the records above through epochs\n            valid/              # Snapshotting figures made during validation\n                consumed_memory/    \n                     Rank0.txt                           \n                     ...\n                     RankN.txt                           \n                     summary.png                         \n                consumed_time/      \n                     data_load_time.txt                  \n                     model_forward_time.txt              \n                     summary.png                         \n                criteria/           # Folder containing the validation criterion value records\n                     {valid_criterion_name}              # Numerical records of the validation criterion named {valid_criterion_name} through epochs\n                     ...                                 # Other validation criteria if have\n                     summary.png                         # Curve graph of all the records above through epochs\n\n        tensorboard/        # This sub-folder contains the writer events for tensorboard visualization \n        checkpoint.pth      # Checkpoint of the training process so far, used for resuming the training process\n        train_data_cfg.yaml # Data loading configuration for the training-validation part of the experiment, used for resuming the training process\n        test_data_cfg.yaml  # Data loading configuration for the testing part of the experiment, used for resuming the testing process\n        exp_cfg.yaml        # Experiment environment configuration for the experiment, used for resuming the training process\n        train_cfg.yaml      # Model and optimizer configuration for the experiment, used for resuming the training process\n        train.log           # Log file that contains the training process of the given training sets and validation sets\n</code></pre></p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. See the speechain repo for more details.</p>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset","title":"<code>speechain.dataset.speech_text.SpeechTextDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This Dataset subclass is mainly used by ASR and TTS models.</p> <p>In this subclass, each data instance is made up of an utterance and a sentence as well as the speaker information (speaker ID + speaker embedding feature).</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>class SpeechTextDataset(Dataset):\n    \"\"\"This Dataset subclass is mainly used by ASR and TTS models.\n\n    In this subclass, each data instance is made up of an utterance and a sentence as\n    well as the speaker information (speaker ID + speaker embedding feature).\n    \"\"\"\n\n    def dataset_init_fn(\n        self,\n        use_g2p: bool = False,\n        unk_mask_prob: float = 0.0,\n        use_speed_perturb: bool = False,\n        sample_rate: int = 16000,\n        perturb_range: List[float] = [0.9, 1.0, 1.1],\n        pitch_conf: Dict = None,\n    ):\n        \"\"\"Dataset initialization function.\n\n        Args:\n            use_g2p (bool, optional): Whether to process the raw string by G2P. We don't\n                recommend you to turn it on because on-the-fly transformer from string to\n                phoneme list consumes a lot of CPU resources. Defaults to False.\n\n            unk_mask_prob (float, optional): Probability of masking tokens as unknown.\n                Defaults to 0.0.\n            use_speed_perturb (bool, optional): Whether to perturb the speed of the\n                waveforms. Defaults to False.\n            sample_rate (int, optional): Audio sampling rate in Hz. Defaults to 16000.\n            perturb_range (List[float], optional): Range of speed perturbation factors.\n                Defaults to [0.9, 1.0, 1.1].\n\n            pitch_conf (Dict, optional): The configuration given to convert_wav_to_pitch()\n                for pitch extraction. If not given, pitch extraction will not be done\n                on-the-fly. Defaults to None.\n\n        Note:\n            Phoneme related: use_g2p\n            Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range\n            Pitch related: pitch_conf\n        \"\"\"\n        # register sampling rate for later check\n        self.sample_rate = sample_rate\n        warnings.warn(\n            f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n            f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n            f\"Please make sure that {sample_rate} is the same with your model! \"\n            f\"If this is not your target sampling rate, \"\n            f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n            f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n        )\n\n        assert (\n            0 &lt;= unk_mask_prob &lt;= 1\n        ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n        self.unk_mask_prob = unk_mask_prob\n\n        # phoneme extraction\n        if use_g2p:\n            self.g2p = G2p()\n\n        if use_speed_perturb:\n            self.perturb_range = perturb_range\n            self.speed_resampler_list = [\n                torchaudio.transforms.Resample(\n                    orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n                )\n                for factor in perturb_range\n            ]\n\n        # pitch extraction\n        if pitch_conf is not None:\n            if \"sr\" in pitch_conf.keys():\n                assert pitch_conf[\"sr\"] == self.sample_rate, (\n                    f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                    f\"given sample_rate ({self.sample_rate})!\"\n                )\n            pitch_conf[\"sr\"] = self.sample_rate\n            self.pitch_extract_fn = partial(\n                convert_wav_to_pitch, return_tensor=True, **pitch_conf\n            )\n\n    @staticmethod\n    def data_len_register_fn(\n        main_data: Dict[str, Dict[str, str]],\n    ) -&gt; Dict[str, int or float] or None:\n        \"\"\"\n\n        Returns:\n            If 'text' is given in main_data, return the number of characters in each sentence.\n            Otherwise, return None\n\n        \"\"\"\n        if \"text\" in main_data.keys():\n            return {key: len(value) for key, value in main_data[\"text\"].items()}\n        else:\n            return None\n\n    def collate_main_data_fn(\n        self, batch_dict: Dict[str, List]\n    ) -&gt; Dict[str, torch.Tensor or List]:\n        \"\"\"The utterances used for training ASR and TTS models may have different\n        lengths, so we need to do the padding operations to make them equal in length.\n\n        The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n        vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n        Args:\n            batch_dict (Dict[str, List]): The keys of the input `batch_dict` dictionary should be one of the following:\n                1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n                2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n                3. `text`: a List of text strings.\n                4. `spk_ids`: a List of speaker ID strings.\n                5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n        Returns:\n            A dictionary mapping strings to either torch.Tensor or List, where:\n                - feat and spk_feat are three-dimensional torch.Tensor\n                - text and spk_ids are lists of raw strings whose discretization is done in the Model object\n        \"\"\"\n\n        # --- 1. Pad Speech Data and Stack them together --- #\n        if \"feat\" in batch_dict.keys():\n            # para init\n            feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n            batch_size, feat_maxlen, feat_dim = (\n                len(batch_dict[\"feat\"]),\n                feat_len.max().item(),\n                batch_dict[\"feat\"][0].shape[-1],\n            )\n\n            # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n            # overwrite the padding matrix with each feat vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                    feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n                elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                    feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            # update 'feat' and attach 'feat_len' for later model forward\n            batch_dict[\"feat\"] = feat\n            batch_dict[\"feat_len\"] = feat_len\n\n        # --- 2. Pad Pitch Data and Stack them together --- #\n        if \"pitch\" in batch_dict.keys():\n            # para init\n            pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n            batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n            # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n            pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n            # overwrite the padding matrix with each pitch vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                    pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n                elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                    pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            batch_dict[\"pitch\"] = pitch\n            batch_dict[\"pitch_len\"] = pitch_len\n\n        # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n        if \"duration\" in batch_dict.keys():\n            # para init\n            batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n                [len(ele) for ele in batch_dict[\"duration\"]]\n            )\n\n            # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            duration = torch.zeros(\n                (batch_size, duration_len.max().item()), dtype=torch.float32\n            )\n            # overwrite the padding matrix with each duration vector\n            for i in range(batch_size):\n                # process duration data based on data type\n                if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                    duration[i][: duration_len[i]] = torch.tensor(\n                        batch_dict[\"duration\"][i]\n                    )\n                elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                    duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n                else:\n                    raise TypeError(\n                        f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                    )\n\n            # attach 'duration' and 'duration_len' for model forward\n            batch_dict[\"duration\"] = duration\n            batch_dict[\"duration_len\"] = duration_len\n\n        # --- 4. Stack Speaker Embedding Feature together --- #\n        if \"spk_feat\" in batch_dict.keys():\n            batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n        return batch_dict\n\n    def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n        \"\"\"The function that loads speech-text data from the disk. If the speech is in\n        the form of raw waveforms, the last dimension should be expanded to 1 of raw\n        speech for compatibility with acoustic feature.\n\n        Args:\n            main_data: Dict[str, str]\n                The keys of the input main_data dictionary should be one of the following:\n                    1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                    2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                    TTS models.\n                    3. 'duration': phoneme durations. used for training fastspeech2 model.\n                    4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                    ASR and TTS models.\n                    5. 'spk_feat': speaker embedding features.\n                `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n                in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n                However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n                CPU burden.\n\n        Returns:\n            `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n            `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n        \"\"\"\n        assert (\n            \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n        ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n        for key in main_data.keys():\n            if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n                raise RuntimeError(\n                    f\"Unknown data name {key}! \"\n                    f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                    \"'feat' (for paths of raw waveforms or acoustic features), \"\n                    \"'text' (for transcript text data), \"\n                    \"'duration' (for phoneme duration data), \"\n                    \"'spk_ids' (for speaker IDs), \"\n                    \"'spk_feat' (for speaker embedding features).\"\n                )\n\n        # --- 1. Speech Data Extraction --- #\n        if \"feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"feat\"], sample_rate = read_data_by_path(\n                main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n            )\n            # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n            if main_data[\"feat\"].size(0) == 0:\n                return None\n\n            # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n            if sample_rate &gt; self.sample_rate:\n                if not hasattr(self, \"wav_resampler_dict\"):\n                    self.wav_resampler_dict = {\n                        sample_rate: torchaudio.transforms.Resample(\n                            orig_freq=sample_rate, new_freq=self.sample_rate\n                        )\n                    }\n                main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n            # extracted waveforms could not have lower sampling rate than the built-in one\n            elif sample_rate &lt; self.sample_rate:\n                raise RuntimeError(\n                    f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n                )\n\n            # perturb the speed of the extracted speech if specified\n            if hasattr(self, \"speed_resampler_list\"):\n                assert sample_rate == self.sample_rate, (\n                    f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                    f\"waveform ({sample_rate})!\"\n                )\n                resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n                main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n\n            # extract the pitch from the speech on-the-fly\n            if hasattr(self, \"pitch_extract_fn\"):\n                try:\n                    main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n                # IndexError means all the pitch values are unvoiced (=0.0)\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 2. Transcript Text Extraction --- #\n        if \"text\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"text\"], str\n            ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n            # for the text data in the format of a list\n            if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n                main_data[\"text\"] = main_data[\"text\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"text\"] = main_data[\"text\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"text\"] = [\n                    (\n                        token[1:-1]\n                        if token.startswith(\"'\") and token.endswith(\"'\")\n                        else token\n                    )\n                    for token in main_data[\"text\"]\n                ]\n            # process the raw string by G2P if specified\n            elif hasattr(self, \"g2p\"):\n                phn_list = self.g2p(main_data[\"text\"])\n                main_data[\"text\"] = [\n                    phn if phn != \" \" else \"&lt;space&gt;\"\n                    for phn in phn_list\n                    if phn not in abnormal_phns\n                ]\n\n        # --- 3. Phoneme Duration Extraction --- #\n        if \"duration\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"duration\"], str\n            ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n            # for the text data in the format of a list\n            if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n                \"]\"\n            ):\n                main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"duration\"] = [\n                    (\n                        float(duration[1:-1])\n                        if duration.startswith(\"'\") and duration.endswith(\"'\")\n                        else float(duration)\n                    )\n                    for duration in main_data[\"duration\"]\n                ]\n            else:\n                raise RuntimeError(\n                    \"The 'duration' string should be surrounded by a pair of square brackets!\"\n                )\n\n        # --- 4. Silence Trimming at the two ends --- #\n        # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n        if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n            main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n        ):\n            # trim both feat and text\n            if \"feat\" in main_data.keys():\n                assert \"duration\" in main_data.keys(), (\n                    \"If you want to trim the silence at two ends of speech, \"\n                    \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n                )\n                front_trim_len, tail_trim_len, total_duration = (\n                    0,\n                    0,\n                    sum(main_data[\"duration\"]),\n                )\n                try:\n                    # sum up all the silence tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        front_trim_len += main_data[\"duration\"][0]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][1:],\n                            main_data[\"duration\"][1:],\n                        )\n                    # sum up all the silence tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        tail_trim_len += main_data[\"duration\"][-1]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][:-1],\n                            main_data[\"duration\"][:-1],\n                        )\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n                # normalize the trimming lengths by the total duration length\n                front_trim_len, tail_trim_len = (\n                    front_trim_len / total_duration,\n                    tail_trim_len / total_duration,\n                )\n                # trim the extra silence in feat (waveforms or acoustic features)\n                feat_start, feat_end = int(\n                    front_trim_len * len(main_data[\"feat\"])\n                ), int(tail_trim_len * len(main_data[\"feat\"]))\n                main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n                if feat_end &gt; 0:\n                    main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n                # also trim the two ends of pitch values if extracted\n                if \"pitch\" in main_data.keys():\n                    pitch_start, pitch_end = int(\n                        front_trim_len * len(main_data[\"pitch\"])\n                    ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                    main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                    if pitch_end &gt; 0:\n                        main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n            # only trim text if feat is not given\n            else:\n                try:\n                    # sum up all the &lt;space&gt; tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][1:]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][1:]\n                    # sum up all the &lt;space&gt; tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][:-1]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][:-1]\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n        if self.unk_mask_prob &gt; 0:\n            assert \"text\" in main_data.keys() and isinstance(\n                main_data[\"text\"], List\n            ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n            # Get the start and end indices of words based on the positions of space tokens\n            space_indices = [\n                i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n            ]\n            word_start_indices, word_end_indices = [0] + [\n                s_i + 1 for s_i in space_indices\n            ], space_indices + [len(main_data[\"text\"])]\n\n            # Determine which words to mask\n            word_mask_flags = (\n                np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n            )\n\n            _tmp_text, _tmp_duration = [], []\n            for i in range(len(word_mask_flags)):\n                # If the word should be masked, add an '&lt;unk&gt;' token\n                if word_mask_flags[i]:\n                    _tmp_text.append(\"&lt;unk&gt;\")\n                    if \"duration\" in main_data.keys():\n                        _sum_duration = sum(\n                            main_data[\"duration\"][\n                                word_start_indices[i] : word_end_indices[i]\n                            ]\n                        )\n                        _tmp_duration.append(round(_sum_duration, 2))\n\n                # If the word shouldn't be masked, add the original tokens of the word\n                else:\n                    _tmp_text += main_data[\"text\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration += main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n\n                # Add space tokens and their durations between words, except for the last word\n                if i != len(word_mask_flags) - 1:\n                    _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n            # Update main_data with the new text and duration information\n            main_data[\"text\"] = _tmp_text\n            if \"duration\" in main_data.keys():\n                main_data[\"duration\"] = _tmp_duration\n\n        # --- 6. Speaker ID Extraction --- #\n        if \"spk_ids\" in main_data.keys():\n            # the speaker ID here is just a raw string\n            assert isinstance(\n                main_data[\"spk_ids\"], str\n            ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n        # --- 7. Speaker Embedding Feature --- #\n        if \"spk_feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"spk_feat\"] = read_data_by_path(\n                main_data[\"spk_feat\"], return_tensor=True\n            )\n\n        return main_data\n\n    def __repr__(self):\n        outputs = f\"{self.__class__.__name__}(sample_rate={self.sample_rate}\"\n        if hasattr(self, \"g2p\"):\n            outputs += \", use_g2p=True\"\n        if hasattr(self, \"speed_resampler_list\"):\n            outputs += f\", speed_perturb_range={self.perturb_range}\"\n        if hasattr(self, \"pitch_extract_fn\"):\n            outputs += \", pitch_extract=True\"\n        if self.unk_mask_prob &gt; 0:\n            outputs += f\", unk_mask_prob={self.unk_mask_prob}\"\n        return outputs + \")\"\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.collate_main_data_fn","title":"<code>collate_main_data_fn(batch_dict)</code>","text":"<p>The utterances used for training ASR and TTS models may have different lengths, so we need to do the padding operations to make them equal in length.</p> <p>The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dict</code> <code>Dict[str, List]</code> <p>The keys of the input <code>batch_dict</code> dictionary should be one of the following: 1. <code>feat</code>: a List of 2d <code>torch.Tensor</code> with different lengths. 2. <code>pitch</code>: a List of 1d <code>torch.Tensor</code> with different lengths. 3. <code>text</code>: a List of text strings. 4. <code>spk_ids</code>: a List of speaker ID strings. 5. <code>spk_feat</code>: a List of 2d <code>torch.Tensor</code> with equal lengths.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor or List]</code> <p>A dictionary mapping strings to either torch.Tensor or List, where: - feat and spk_feat are three-dimensional torch.Tensor - text and spk_ids are lists of raw strings whose discretization is done in the Model object</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def collate_main_data_fn(\n    self, batch_dict: Dict[str, List]\n) -&gt; Dict[str, torch.Tensor or List]:\n    \"\"\"The utterances used for training ASR and TTS models may have different\n    lengths, so we need to do the padding operations to make them equal in length.\n\n    The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n    vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n    Args:\n        batch_dict (Dict[str, List]): The keys of the input `batch_dict` dictionary should be one of the following:\n            1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n            2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n            3. `text`: a List of text strings.\n            4. `spk_ids`: a List of speaker ID strings.\n            5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n    Returns:\n        A dictionary mapping strings to either torch.Tensor or List, where:\n            - feat and spk_feat are three-dimensional torch.Tensor\n            - text and spk_ids are lists of raw strings whose discretization is done in the Model object\n    \"\"\"\n\n    # --- 1. Pad Speech Data and Stack them together --- #\n    if \"feat\" in batch_dict.keys():\n        # para init\n        feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n        batch_size, feat_maxlen, feat_dim = (\n            len(batch_dict[\"feat\"]),\n            feat_len.max().item(),\n            batch_dict[\"feat\"][0].shape[-1],\n        )\n\n        # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n        # overwrite the padding matrix with each feat vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n            elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        # update 'feat' and attach 'feat_len' for later model forward\n        batch_dict[\"feat\"] = feat\n        batch_dict[\"feat_len\"] = feat_len\n\n    # --- 2. Pad Pitch Data and Stack them together --- #\n    if \"pitch\" in batch_dict.keys():\n        # para init\n        pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n        batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n        # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n        pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n        # overwrite the padding matrix with each pitch vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n            elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        batch_dict[\"pitch\"] = pitch\n        batch_dict[\"pitch_len\"] = pitch_len\n\n    # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n    if \"duration\" in batch_dict.keys():\n        # para init\n        batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n            [len(ele) for ele in batch_dict[\"duration\"]]\n        )\n\n        # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        duration = torch.zeros(\n            (batch_size, duration_len.max().item()), dtype=torch.float32\n        )\n        # overwrite the padding matrix with each duration vector\n        for i in range(batch_size):\n            # process duration data based on data type\n            if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                duration[i][: duration_len[i]] = torch.tensor(\n                    batch_dict[\"duration\"][i]\n                )\n            elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n            else:\n                raise TypeError(\n                    f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                )\n\n        # attach 'duration' and 'duration_len' for model forward\n        batch_dict[\"duration\"] = duration\n        batch_dict[\"duration_len\"] = duration_len\n\n    # --- 4. Stack Speaker Embedding Feature together --- #\n    if \"spk_feat\" in batch_dict.keys():\n        batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n    return batch_dict\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.data_len_register_fn","title":"<code>data_len_register_fn(main_data)</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, int or float] or None</code> <p>If 'text' is given in main_data, return the number of characters in each sentence.</p> <code>Dict[str, int or float] or None</code> <p>Otherwise, return None</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>@staticmethod\ndef data_len_register_fn(\n    main_data: Dict[str, Dict[str, str]],\n) -&gt; Dict[str, int or float] or None:\n    \"\"\"\n\n    Returns:\n        If 'text' is given in main_data, return the number of characters in each sentence.\n        Otherwise, return None\n\n    \"\"\"\n    if \"text\" in main_data.keys():\n        return {key: len(value) for key, value in main_data[\"text\"].items()}\n    else:\n        return None\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.dataset_init_fn","title":"<code>dataset_init_fn(use_g2p=False, unk_mask_prob=0.0, use_speed_perturb=False, sample_rate=16000, perturb_range=[0.9, 1.0, 1.1], pitch_conf=None)</code>","text":"<p>Dataset initialization function.</p> <p>Parameters:</p> Name Type Description Default <code>use_g2p</code> <code>bool</code> <p>Whether to process the raw string by G2P. We don't recommend you to turn it on because on-the-fly transformer from string to phoneme list consumes a lot of CPU resources. Defaults to False.</p> <code>False</code> <code>unk_mask_prob</code> <code>float</code> <p>Probability of masking tokens as unknown. Defaults to 0.0.</p> <code>0.0</code> <code>use_speed_perturb</code> <code>bool</code> <p>Whether to perturb the speed of the waveforms. Defaults to False.</p> <code>False</code> <code>sample_rate</code> <code>int</code> <p>Audio sampling rate in Hz. Defaults to 16000.</p> <code>16000</code> <code>perturb_range</code> <code>List[float]</code> <p>Range of speed perturbation factors. Defaults to [0.9, 1.0, 1.1].</p> <code>[0.9, 1.0, 1.1]</code> <code>pitch_conf</code> <code>Dict</code> <p>The configuration given to convert_wav_to_pitch() for pitch extraction. If not given, pitch extraction will not be done on-the-fly. Defaults to None.</p> <code>None</code> Note <p>Phoneme related: use_g2p Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range Pitch related: pitch_conf</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def dataset_init_fn(\n    self,\n    use_g2p: bool = False,\n    unk_mask_prob: float = 0.0,\n    use_speed_perturb: bool = False,\n    sample_rate: int = 16000,\n    perturb_range: List[float] = [0.9, 1.0, 1.1],\n    pitch_conf: Dict = None,\n):\n    \"\"\"Dataset initialization function.\n\n    Args:\n        use_g2p (bool, optional): Whether to process the raw string by G2P. We don't\n            recommend you to turn it on because on-the-fly transformer from string to\n            phoneme list consumes a lot of CPU resources. Defaults to False.\n\n        unk_mask_prob (float, optional): Probability of masking tokens as unknown.\n            Defaults to 0.0.\n        use_speed_perturb (bool, optional): Whether to perturb the speed of the\n            waveforms. Defaults to False.\n        sample_rate (int, optional): Audio sampling rate in Hz. Defaults to 16000.\n        perturb_range (List[float], optional): Range of speed perturbation factors.\n            Defaults to [0.9, 1.0, 1.1].\n\n        pitch_conf (Dict, optional): The configuration given to convert_wav_to_pitch()\n            for pitch extraction. If not given, pitch extraction will not be done\n            on-the-fly. Defaults to None.\n\n    Note:\n        Phoneme related: use_g2p\n        Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range\n        Pitch related: pitch_conf\n    \"\"\"\n    # register sampling rate for later check\n    self.sample_rate = sample_rate\n    warnings.warn(\n        f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n        f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n        f\"Please make sure that {sample_rate} is the same with your model! \"\n        f\"If this is not your target sampling rate, \"\n        f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n        f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n    )\n\n    assert (\n        0 &lt;= unk_mask_prob &lt;= 1\n    ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n    self.unk_mask_prob = unk_mask_prob\n\n    # phoneme extraction\n    if use_g2p:\n        self.g2p = G2p()\n\n    if use_speed_perturb:\n        self.perturb_range = perturb_range\n        self.speed_resampler_list = [\n            torchaudio.transforms.Resample(\n                orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n            )\n            for factor in perturb_range\n        ]\n\n    # pitch extraction\n    if pitch_conf is not None:\n        if \"sr\" in pitch_conf.keys():\n            assert pitch_conf[\"sr\"] == self.sample_rate, (\n                f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                f\"given sample_rate ({self.sample_rate})!\"\n            )\n        pitch_conf[\"sr\"] = self.sample_rate\n        self.pitch_extract_fn = partial(\n            convert_wav_to_pitch, return_tensor=True, **pitch_conf\n        )\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.extract_main_data_fn","title":"<code>extract_main_data_fn(main_data)</code>","text":"<p>The function that loads speech-text data from the disk. If the speech is in the form of raw waveforms, the last dimension should be expanded to 1 of raw speech for compatibility with acoustic feature.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict</code> <p>Dict[str, str] The keys of the input main_data dictionary should be one of the following:     1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.     2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and     TTS models.     3. 'duration': phoneme durations. used for training fastspeech2 model.     4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the     ASR and TTS models.     5. 'spk_feat': speaker embedding features. <code>spk_ids</code> and <code>spk_feat</code> are designed for multi-speaker TTS model and are not mandatory to be included in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training. However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the CPU burden.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] or None</code> <p><code>feat</code> and <code>spk_feat</code> are in the form of two-dimensional <code>torch.Tensor</code>;</p> <code>Dict[str, Any] or None</code> <p><code>text</code> and <code>spk_ids</code> are in the form of raw strings whose discretization is done in the Model object.</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n    \"\"\"The function that loads speech-text data from the disk. If the speech is in\n    the form of raw waveforms, the last dimension should be expanded to 1 of raw\n    speech for compatibility with acoustic feature.\n\n    Args:\n        main_data: Dict[str, str]\n            The keys of the input main_data dictionary should be one of the following:\n                1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                TTS models.\n                3. 'duration': phoneme durations. used for training fastspeech2 model.\n                4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                ASR and TTS models.\n                5. 'spk_feat': speaker embedding features.\n            `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n            in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n            However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n            CPU burden.\n\n    Returns:\n        `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n        `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n    \"\"\"\n    assert (\n        \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n    ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n    for key in main_data.keys():\n        if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n            raise RuntimeError(\n                f\"Unknown data name {key}! \"\n                f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                \"'feat' (for paths of raw waveforms or acoustic features), \"\n                \"'text' (for transcript text data), \"\n                \"'duration' (for phoneme duration data), \"\n                \"'spk_ids' (for speaker IDs), \"\n                \"'spk_feat' (for speaker embedding features).\"\n            )\n\n    # --- 1. Speech Data Extraction --- #\n    if \"feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"feat\"], sample_rate = read_data_by_path(\n            main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n        )\n        # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n        if main_data[\"feat\"].size(0) == 0:\n            return None\n\n        # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n        if sample_rate &gt; self.sample_rate:\n            if not hasattr(self, \"wav_resampler_dict\"):\n                self.wav_resampler_dict = {\n                    sample_rate: torchaudio.transforms.Resample(\n                        orig_freq=sample_rate, new_freq=self.sample_rate\n                    )\n                }\n            main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n        # extracted waveforms could not have lower sampling rate than the built-in one\n        elif sample_rate &lt; self.sample_rate:\n            raise RuntimeError(\n                f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n            )\n\n        # perturb the speed of the extracted speech if specified\n        if hasattr(self, \"speed_resampler_list\"):\n            assert sample_rate == self.sample_rate, (\n                f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                f\"waveform ({sample_rate})!\"\n            )\n            resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n            main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n\n        # extract the pitch from the speech on-the-fly\n        if hasattr(self, \"pitch_extract_fn\"):\n            try:\n                main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n            # IndexError means all the pitch values are unvoiced (=0.0)\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 2. Transcript Text Extraction --- #\n    if \"text\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"text\"], str\n        ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n        # for the text data in the format of a list\n        if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n            main_data[\"text\"] = main_data[\"text\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"text\"] = main_data[\"text\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"text\"] = [\n                (\n                    token[1:-1]\n                    if token.startswith(\"'\") and token.endswith(\"'\")\n                    else token\n                )\n                for token in main_data[\"text\"]\n            ]\n        # process the raw string by G2P if specified\n        elif hasattr(self, \"g2p\"):\n            phn_list = self.g2p(main_data[\"text\"])\n            main_data[\"text\"] = [\n                phn if phn != \" \" else \"&lt;space&gt;\"\n                for phn in phn_list\n                if phn not in abnormal_phns\n            ]\n\n    # --- 3. Phoneme Duration Extraction --- #\n    if \"duration\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"duration\"], str\n        ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n        # for the text data in the format of a list\n        if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n            \"]\"\n        ):\n            main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"duration\"] = [\n                (\n                    float(duration[1:-1])\n                    if duration.startswith(\"'\") and duration.endswith(\"'\")\n                    else float(duration)\n                )\n                for duration in main_data[\"duration\"]\n            ]\n        else:\n            raise RuntimeError(\n                \"The 'duration' string should be surrounded by a pair of square brackets!\"\n            )\n\n    # --- 4. Silence Trimming at the two ends --- #\n    # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n    if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n        main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n    ):\n        # trim both feat and text\n        if \"feat\" in main_data.keys():\n            assert \"duration\" in main_data.keys(), (\n                \"If you want to trim the silence at two ends of speech, \"\n                \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n            )\n            front_trim_len, tail_trim_len, total_duration = (\n                0,\n                0,\n                sum(main_data[\"duration\"]),\n            )\n            try:\n                # sum up all the silence tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    front_trim_len += main_data[\"duration\"][0]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][1:],\n                        main_data[\"duration\"][1:],\n                    )\n                # sum up all the silence tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    tail_trim_len += main_data[\"duration\"][-1]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][:-1],\n                        main_data[\"duration\"][:-1],\n                    )\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n            # normalize the trimming lengths by the total duration length\n            front_trim_len, tail_trim_len = (\n                front_trim_len / total_duration,\n                tail_trim_len / total_duration,\n            )\n            # trim the extra silence in feat (waveforms or acoustic features)\n            feat_start, feat_end = int(\n                front_trim_len * len(main_data[\"feat\"])\n            ), int(tail_trim_len * len(main_data[\"feat\"]))\n            main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n            if feat_end &gt; 0:\n                main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n            # also trim the two ends of pitch values if extracted\n            if \"pitch\" in main_data.keys():\n                pitch_start, pitch_end = int(\n                    front_trim_len * len(main_data[\"pitch\"])\n                ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                if pitch_end &gt; 0:\n                    main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n        # only trim text if feat is not given\n        else:\n            try:\n                # sum up all the &lt;space&gt; tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][1:]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][1:]\n                # sum up all the &lt;space&gt; tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][:-1]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][:-1]\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n    if self.unk_mask_prob &gt; 0:\n        assert \"text\" in main_data.keys() and isinstance(\n            main_data[\"text\"], List\n        ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n        # Get the start and end indices of words based on the positions of space tokens\n        space_indices = [\n            i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n        ]\n        word_start_indices, word_end_indices = [0] + [\n            s_i + 1 for s_i in space_indices\n        ], space_indices + [len(main_data[\"text\"])]\n\n        # Determine which words to mask\n        word_mask_flags = (\n            np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n        )\n\n        _tmp_text, _tmp_duration = [], []\n        for i in range(len(word_mask_flags)):\n            # If the word should be masked, add an '&lt;unk&gt;' token\n            if word_mask_flags[i]:\n                _tmp_text.append(\"&lt;unk&gt;\")\n                if \"duration\" in main_data.keys():\n                    _sum_duration = sum(\n                        main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n                    )\n                    _tmp_duration.append(round(_sum_duration, 2))\n\n            # If the word shouldn't be masked, add the original tokens of the word\n            else:\n                _tmp_text += main_data[\"text\"][\n                    word_start_indices[i] : word_end_indices[i]\n                ]\n                if \"duration\" in main_data.keys():\n                    _tmp_duration += main_data[\"duration\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n\n            # Add space tokens and their durations between words, except for the last word\n            if i != len(word_mask_flags) - 1:\n                _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                if \"duration\" in main_data.keys():\n                    _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n        # Update main_data with the new text and duration information\n        main_data[\"text\"] = _tmp_text\n        if \"duration\" in main_data.keys():\n            main_data[\"duration\"] = _tmp_duration\n\n    # --- 6. Speaker ID Extraction --- #\n    if \"spk_ids\" in main_data.keys():\n        # the speaker ID here is just a raw string\n        assert isinstance(\n            main_data[\"spk_ids\"], str\n        ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n    # --- 7. Speaker Embedding Feature --- #\n    if \"spk_feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"spk_feat\"] = read_data_by_path(\n            main_data[\"spk_feat\"], return_tensor=True\n        )\n\n    return main_data\n</code></pre>"},{"location":"tokenizer/","title":"Tokenizer","text":"<p>Tokenizer is the base class of all the Tokenizer objects in this toolkit.  It on-the-fly transforms text data between strings and tensors.  </p> <p>For data storage and visualization, the text data should be in the form of strings which is not friendly for model forward calculation.  For model forward calculation, the text data is better to be in the form of vectors (<code>torch.tensor</code> or <code>numpy.ndarray</code>).</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"tokenizer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Tokenizer Library </li> <li>API Documents </li> </ol>"},{"location":"tokenizer/#tokenizer-library","title":"Tokenizer Library","text":"<pre><code>/speechain\n    /tokenizer\n        /abs.py         # Abstract class of Tokenizer. Base of all Tokenizer implementations.\n        /char.py        # Tokenizer implementation of the character tokenizer.\n        /sp.py          # Tokenizer implementation of the subword tokenizer by SentencePiece package.\n        /g2p.py         # Tokenizer implementation of the phoneme tokenizer by G2P package.\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tokenizer/#api-document","title":"API Document","text":"<p>Non-overridable backbone functions: 1. speechain.tokenizer.abs.Tokenizer.__init__</p> <p>Overridable interface functions: 1. speechain.tokenizer.abs.Tokenizer.tokenizer_init_fn 2. speechain.tokenizer.abs.Tokenizer.tensor2text 3. speechain.tokenizer.abs.Tokenizer.text2tensor </p>"},{"location":"tokenizer/#speechaintokenizerabstokenizer__init__self-token_vocab-tokenizer_conf","title":"speechain.tokenizer.abs.Tokenizer.__init__(self, token_vocab, **tokenizer_conf)","text":"<ul> <li>Description:     This function registers some shared member variables for all Tokenizer subclasses: <ol> <li><code>self.idx2token</code>: the mapping Dict from the token index to token string.</li> <li><code>self.token2idx</code>: the mapping Dict from the token string to token index.</li> <li><code>self.vocab_size</code>: the number of tokens in the given vocabulary.</li> <li><code>self.sos_eos_idx</code>: the index of the joint  token used as the beginning and end of a sentence. <li><code>self.ignore_idx</code>: the index of the blank token used for either CTC blank modeling or ignored token for encoder-decoder ASR&amp;TTS models.</li> <li><code>self.unk_idx</code>: the index of the unknown token.</li> <li>Arguments:<ul> <li>token_vocab: str   The path where the token vocabulary is placed.</li> <li>**tokenizer_conf:   The arguments used by <code>tokenizer_init_fn()</code> for your customized Tokenizer initialization.</li> </ul> </li> <p>\ud83d\udc46Back to the API list</p>"},{"location":"tokenizer/#speechaintokenizerabstokenizertokenizer_init_fnself-tokenizer_conf","title":"speechain.tokenizer.abs.Tokenizer.tokenizer_init_fn(self, **tokenizer_conf)","text":"<ul> <li>Description:     This hook interface function initializes the customized part of a Tokenizer subclass if had.     This interface is not mandatory to be overridden.</li> <li>Arguments:<ul> <li>**tokenizer_conf:    The arguments used by <code>tokenizer_init_fn()</code> for your customized Tokenizer initialization.    For more details, please refer to the docstring of your target Tokenizer subclass.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"tokenizer/#speechaintokenizerabstokenizertensor2textself-tensor","title":"speechain.tokenizer.abs.Tokenizer.tensor2text(self, tensor)","text":"<ul> <li>Description:     This functions decodes a text tensor into a human-friendly string.     The default implementation transforms each token index in the input tensor to the token string by <code>self.idx2token</code>.      If the token index is <code>self.unk_idx</code>, an asterisk (*) will be used to represent an unknown token in the string.     This interface is not mandatory to be overridden. If your Tokenizer subclass uses some third-party packages to decode the input tensor rather than the built-in <code>self.idx2token</code>,      please override this function.</li> <li>Arguments:<ul> <li>tensor: torch.LongTensor   1D integer torch.Tensor that contains the token indices of the sentence to be decoded.</li> </ul> </li> <li>Return:     The string of the decoded sentence.</li> </ul> <p>\ud83d\udc46Back to the API list</p>"},{"location":"tokenizer/#speechaintokenizerabstokenizertext2tensorself-text","title":"speechain.tokenizer.abs.Tokenizer.text2tensor(self, text)","text":"<ul> <li>Description:     This functions encodes a text string into a model-friendly tensor.     This interface is mandatory to be overridden.     By default, this function will attach two  at the beginning and end of the returned token id sequence. <li>Arguments: <ul> <li>text: str    The input text string to be encoded</li> <li>no_sos: bool = False    Whether to remove the  at the beginning of the token id sequence. <li>no_eos: bool = False    Whether to remove the  at the end of the token id sequence. <li>Return: torch.LongTensor     The tensor of the encoded sentence</li> <p>\ud83d\udc46Back to the API list</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts/","title":"TTS","text":"<p>This folder contains recipes for training a Text-To-Speech Synthesis (TTS) model.</p> <p>\ud83d\udc46Back to the recipe README.md</p>"},{"location":"tts/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Available Backbones</li> <li>Preparing Durations for FastSpeech2</li> <li>Training a TTS model</li> </ol>"},{"location":"tts/#available-backbones","title":"Available Backbones","text":"<p>Below is a table of available backbones:</p> Dataset Subset Configuration Audio Samples Link libritts train-clean-100 train-clean-460 train-960 ljspeech train 22.05khz_mfa_fastspeech2 train 22.05khz_mfa_fastspeech2_nopunc vctk <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts/#preparing-durations-for-fastspeech2","title":"Preparing Durations for FastSpeech2","text":"<p>For training a FastSpeech2 model, you need to acquire additional duration data for your target dataset.  Follow these steps:  1. Create a virtual environment for MFA: <code>conda create -n speechain_mfa -c conda-forge montreal-forced-aligner gdown</code>.  2. Activate the <code>speechain_mfa</code> environment: <code>conda activate speechain_mfa</code>.  3. Downsample your target TTS dataset to 16khz. For details, please see how to dump a dataset on your machine.  4. By default, MFA package will store all the temporary files to your user directory. If you lack sufficient space, add <code>export MFA_ROOT_DIR={your-target-directory}</code> to <code>~/.bashrc</code> and run <code>source ~/.bashrc</code>.  5. Navigate to <code>${SPEECHAIN_ROOT}/datasets</code> and run <code>bash mfa_preparation.sh -h</code> for help. Then, add appropriate arguments to <code>bash mfa_preparation.sh</code> to acquire duration data.  </p> <p>Note: MFA cannot process duration calculations for multiple datasets concurrently on a single machine (or a single node on a cluster).  Please process each dataset one at a time.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts/#training-an-tts-model","title":"Training an TTS model","text":"<p>To train a TTS model, follow the ASR model training instructions located in <code>recipes/asr</code>.  Make sure to replace the folder names and configuration file names from <code>recipes/asr</code> with their corresponding names in <code>recipes/tts</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts_ljspeech_tutorial/","title":"TTS LJSpeech Tutorial: FastSpeech2 Experiments","text":"<p>This tutorial guides you through running Text-to-Speech (TTS) experiments on the LJSpeech dataset using FastSpeech2 with two configurations: no-punctuation and punctuation.</p>"},{"location":"tts_ljspeech_tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>GPU: NVIDIA GPU with at least 8GB VRAM (tested on RTX 3090/5090)</li> <li>OS: Linux (Ubuntu 20.04+ recommended)</li> <li>Storage: ~20GB free space for dataset and models</li> </ul>"},{"location":"tts_ljspeech_tutorial/#environment-setup","title":"Environment Setup","text":""},{"location":"tts_ljspeech_tutorial/#option-a-using-uv-venv-recommended","title":"Option A: Using uv + venv (Recommended)","text":"<p>This option uses <code>uv</code> for fast Python environment management without conda.</p> <pre><code># Install uv if not already installed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Navigate to the speechain directory\ncd /path/to/speechain\n\n# Create virtual environment with Python 3.10\nuv venv .venv --python 3.10\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install PyTorch (adjust CUDA version as needed)\n# For CUDA 12.1:\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# For newer GPUs (e.g., RTX 5090 with sm_120), use PyTorch nightly:\nuv pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n\n# Install speechain requirements\nuv pip install -r requirements.txt\n\n# Install speechain in development mode\nuv pip install -e .\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#option-b-using-conda","title":"Option B: Using Conda","text":"<pre><code># Create conda environment\nconda create -n speechain python=3.10 -y\nconda activate speechain\n\n# Install PyTorch with CUDA\nconda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n\n# Install requirements\npip install -r requirements.txt\n\n# Install speechain\npip install -e .\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-1-download-ljspeech-dataset","title":"Step 1: Download LJSpeech Dataset","text":"<pre><code># Set environment variable\nexport SPEECHAIN_ROOT=/path/to/speechain\n\n# Navigate to dataset directory\ncd $SPEECHAIN_ROOT/datasets/ljspeech\n\n# Download LJSpeech dataset\nbash data_download.sh\n</code></pre> <p>This downloads and extracts the LJSpeech dataset (~2.6GB) to <code>datasets/ljspeech/data/</code>.</p>"},{"location":"tts_ljspeech_tutorial/#step-2-install-montreal-forced-aligner-mfa","title":"Step 2: Install Montreal Forced Aligner (MFA)","text":"<p>MFA is required for phoneme alignment. This step requires conda even if you're using venv for the main environment.</p> <pre><code># Create a separate conda environment for MFA\nconda create -n aligner -c conda-forge montreal-forced-aligner -y\n\n# Activate MFA environment\nconda activate aligner\n\n# Download MFA models\nmfa model download acoustic english_us_arpa\nmfa model download dictionary english_us_arpa\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-3-generate-metadata","title":"Step 3: Generate Metadata","text":"<pre><code># Activate your main environment (venv or conda)\nsource $SPEECHAIN_ROOT/.venv/bin/activate  # or: conda activate speechain\n\n# Set environment\nexport SPEECHAIN_ROOT=/path/to/speechain\ncd $SPEECHAIN_ROOT/datasets/ljspeech\n\n# Generate metadata for train/valid/test splits\npython meta_generator.py\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-4-run-mfa-alignment","title":"Step 4: Run MFA Alignment","text":"<p>Important: This step must be run in the conda MFA environment.</p> <pre><code># Activate MFA environment\nconda activate aligner\n\n# Navigate to dataset preparation scripts\ncd $SPEECHAIN_ROOT/datasets\n\n# Prepare data for MFA alignment\nbash mfa_preparation.sh ljspeech\n\n# Run MFA alignment (this may take 30-60 minutes)\n# The script aligns phonemes to audio using the english_us_arpa model\nmfa align \\\n    $SPEECHAIN_ROOT/datasets/ljspeech/data/mfa_input \\\n    english_us_arpa \\\n    english_us_arpa \\\n    $SPEECHAIN_ROOT/datasets/ljspeech/data/mfa/acoustic=english_us_arpa_lexicon=english_us_arpa \\\n    --clean\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-5-post-process-mfa-output","title":"Step 5: Post-process MFA Output","text":"<pre><code># Switch back to main environment\nsource $SPEECHAIN_ROOT/.venv/bin/activate  # or: conda activate speechain\n\ncd $SPEECHAIN_ROOT/datasets/ljspeech\n\n# Generate duration files from MFA alignment\npython meta_post_processor.py\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-6-generate-duration-data","title":"Step 6: Generate Duration Data","text":"<pre><code>cd $SPEECHAIN_ROOT/datasets/pyscripts\n\n# Generate duration files for training\npython duration_calculator.py \\\n    --data_root $SPEECHAIN_ROOT/datasets/ljspeech/data \\\n    --mfa_model acoustic=english_us_arpa_lexicon=english_us_arpa\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#step-7-training","title":"Step 7: Training","text":""},{"location":"tts_ljspeech_tutorial/#train-no-punctuation-model","title":"Train No-Punctuation Model","text":"<pre><code># Activate main environment\nsource $SPEECHAIN_ROOT/.venv/bin/activate\nexport SPEECHAIN_ROOT=/path/to/speechain\n\ncd $SPEECHAIN_ROOT\n\n# Run training (adjust num_epochs in config for full training)\npython speechain/runner.py \\\n    --config recipes/tts/ljspeech/exp_cfg/22.05khz_mfa_fastspeech2.yaml \\\n    --train true \\\n    --test false\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#train-punctuation-model","title":"Train Punctuation Model","text":"<pre><code>python speechain/runner.py \\\n    --config recipes/tts/ljspeech/exp_cfg/22.05khz_mfa_fastspeech2_punc.yaml \\\n    --train true \\\n    --test false\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#training-configuration","title":"Training Configuration","text":"<p>Key parameters in the config files (<code>recipes/tts/ljspeech/exp_cfg/</code>):</p> Parameter Description Default <code>num_epochs</code> Number of training epochs 500 <code>batch_len</code> Batch length for training 1.5e7 <code>ngpu</code> Number of GPUs 1 <code>early_stopping_patience</code> Epochs before early stopping 20 <code>valid_per_epochs</code> Validation frequency 10 <p>For quick experiments, you can reduce <code>num_epochs</code> to 5-10.</p>"},{"location":"tts_ljspeech_tutorial/#step-8-inference-generate-speech","title":"Step 8: Inference (Generate Speech)","text":""},{"location":"tts_ljspeech_tutorial/#generate-wav-files-from-no-punctuation-model","title":"Generate WAV Files from No-Punctuation Model","text":"<pre><code>python speechain/runner.py \\\n    --config recipes/tts/ljspeech/exp_cfg/22.05khz_mfa_fastspeech2.yaml \\\n    --train false \\\n    --test true \\\n    --test_model latest\n</code></pre> <p>Output WAV files will be saved to: <pre><code>recipes/tts/ljspeech/exp/22.05khz_mfa_fastspeech2/default_inference/latest/test/wav/\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#generate-wav-files-from-punctuation-model","title":"Generate WAV Files from Punctuation Model","text":"<pre><code>python speechain/runner.py \\\n    --config recipes/tts/ljspeech/exp_cfg/22.05khz_mfa_fastspeech2_punc.yaml \\\n    --train false \\\n    --test true \\\n    --test_model latest\n</code></pre> <p>Output WAV files will be saved to: <pre><code>recipes/tts/ljspeech/exp/22.05khz_mfa_fastspeech2_punc/default_inference/latest/test/wav/\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#hifi-gan-vocoder","title":"HiFi-GAN Vocoder","text":"<p>The HiFi-GAN vocoder converts mel spectrograms to audio waveforms. It is automatically downloaded from Hugging Face Hub on first use:</p> <ul> <li>Model: <code>speechbrain/tts-hifigan-ljspeech</code></li> <li>Cache location: <code>recipes/tts/speechbrain_vocoder/hifigan-ljspeech/</code></li> </ul> <p>No manual download is required.</p>"},{"location":"tts_ljspeech_tutorial/#output-structure","title":"Output Structure","text":"<p>After training and inference, the experiment folder structure looks like:</p> <pre><code>recipes/tts/ljspeech/exp/\n\u251c\u2500\u2500 22.05khz_mfa_fastspeech2/          # No-punctuation experiment\n\u2502   \u251c\u2500\u2500 models/                         # Saved model checkpoints\n\u2502   \u2502   \u251c\u2500\u2500 epoch_1.pth\n\u2502   \u2502   \u251c\u2500\u2500 epoch_2.pth\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 latest.pth -&gt; epoch_N.pth\n\u2502   \u251c\u2500\u2500 tensorboard/                    # Training logs\n\u2502   \u251c\u2500\u2500 figures/                        # Visualization\n\u2502   \u251c\u2500\u2500 default_inference/\n\u2502   \u2502   \u2514\u2500\u2500 latest/\n\u2502   \u2502       \u2514\u2500\u2500 test/\n\u2502   \u2502           \u2514\u2500\u2500 wav/                # Generated WAV files\n\u2502   \u2514\u2500\u2500 train.log\n\u2502\n\u2514\u2500\u2500 22.05khz_mfa_fastspeech2_punc/      # Punctuation experiment\n    \u251c\u2500\u2500 models/\n    \u251c\u2500\u2500 tensorboard/\n    \u251c\u2500\u2500 figures/\n    \u251c\u2500\u2500 default_inference/\n    \u2502   \u2514\u2500\u2500 latest/\n    \u2502       \u2514\u2500\u2500 test/\n    \u2502           \u2514\u2500\u2500 wav/                # Generated WAV files\n    \u2514\u2500\u2500 train.log\n</code></pre>"},{"location":"tts_ljspeech_tutorial/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tts_ljspeech_tutorial/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Reduce <code>batch_len</code> in the config file: <pre><code>batch_len: 1.0e7  # Reduce from 1.5e7\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#mfa-alignment-errors","title":"MFA Alignment Errors","text":"<p>Ensure you're using the conda <code>aligner</code> environment: <pre><code>conda activate aligner\nmfa model download acoustic english_us_arpa\nmfa model download dictionary english_us_arpa\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#missing-speechain_root","title":"Missing SPEECHAIN_ROOT","text":"<p>Always set the environment variable before running: <pre><code>export SPEECHAIN_ROOT=/path/to/speechain\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#pytorch-cuda-compatibility","title":"PyTorch CUDA Compatibility","text":"<p>For newer GPUs (RTX 40xx, 50xx), you may need PyTorch nightly: <pre><code>uv pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n</code></pre></p>"},{"location":"tts_ljspeech_tutorial/#quick-reference","title":"Quick Reference","text":"Step Environment Command 1. Setup venv/conda <code>uv venv .venv --python 3.10</code> 2. Download data venv/conda <code>bash data_download.sh</code> 3. Install MFA conda only <code>conda create -n aligner -c conda-forge montreal-forced-aligner</code> 4. Generate meta venv/conda <code>python meta_generator.py</code> 5. MFA alignment conda (aligner) <code>mfa align ...</code> 6. Post-process venv/conda <code>python meta_post_processor.py</code> 7. Training venv/conda <code>python speechain/runner.py --train true</code> 8. Inference venv/conda <code>python speechain/runner.py --test true</code>"},{"location":"tts_ljspeech_tutorial/#expected-results","title":"Expected Results","text":"<p>After training for 5 epochs (quick experiment): - Training loss: ~2.5-3.0 - Generated WAV files: 523 samples per experiment - Audio quality: Intelligible but may have artifacts (more epochs improve quality)</p> <p>For production-quality speech, train for 200+ epochs or until early stopping.</p>"},{"location":"tts_ljspeech_tutorial/#references","title":"References","text":"<ul> <li>LJSpeech Dataset</li> <li>FastSpeech2 Paper</li> <li>Montreal Forced Aligner</li> <li>HiFi-GAN Paper</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>criterion<ul> <li>abs</li> <li>accuracy</li> <li>att_guid</li> <li>bce_logits</li> <li>cross_entropy</li> <li>ctc</li> <li>error_rate</li> <li>fbeta_score</li> <li>least_error</li> <li>perplexity</li> </ul> </li> <li>dataset<ul> <li>abs</li> <li>speech_text</li> </ul> </li> <li>infer_func<ul> <li>beam_search</li> <li>ctc_decoding</li> <li>tts_decoding</li> </ul> </li> <li>iterator<ul> <li>abs</li> <li>block</li> </ul> </li> <li>model<ul> <li>abs</li> <li>ar_asr</li> <li>ar_tts</li> <li>lm</li> <li>nar_tts</li> </ul> </li> <li>module<ul> <li>abs</li> <li>augment<ul> <li>specaug</li> </ul> </li> <li>conformer<ul> <li>attention</li> <li>encoder</li> <li>pos_enc</li> </ul> </li> <li>decoder<ul> <li>ar_asr</li> <li>ar_tts</li> <li>nar_tts</li> </ul> </li> <li>encoder<ul> <li>asr</li> <li>speaker</li> <li>test_speaker</li> <li>tts</li> </ul> </li> <li>frontend<ul> <li>delta_feat</li> <li>linear2mel</li> <li>speech2linear</li> <li>speech2mel</li> </ul> </li> <li>norm<ul> <li>feat_norm</li> </ul> </li> <li>postnet<ul> <li>conv1d</li> <li>token</li> </ul> </li> <li>prenet<ul> <li>conv1d</li> <li>conv2d</li> <li>embed</li> <li>linear</li> <li>spk_embed</li> <li>var_pred</li> </ul> </li> <li>standalone<ul> <li>lm</li> </ul> </li> <li>transformer<ul> <li>attention</li> <li>decoder</li> <li>encoder</li> <li>feed_forward</li> <li>pos_enc</li> </ul> </li> <li>vocoder<ul> <li>hifigan</li> <li>test_hifigan</li> </ul> </li> </ul> </li> <li>monitor</li> <li>optim_sche<ul> <li>abs</li> <li>exp</li> <li>noam</li> </ul> </li> <li>pyscripts<ul> <li>empty_file_checker</li> <li>folder_summarizer</li> <li>model_para_renamer</li> <li>phn_duaration_visualizer</li> <li>text_dist_visualizer</li> <li>wavlen_dist_visualizer</li> </ul> </li> <li>runner</li> <li>snapshooter</li> <li>tokenizer<ul> <li>abs</li> <li>char</li> <li>g2p</li> <li>sp</li> </ul> </li> <li>utilbox<ul> <li>data_loading_util</li> <li>data_saving_util</li> <li>dump_util</li> <li>eval_util</li> <li>feat_util</li> <li>humanfriendly</li> <li>import_util</li> <li>log_util</li> <li>md_util</li> <li>regex_util</li> <li>sb_util</li> <li>spk_util</li> <li>tensor_util</li> <li>test_humanfriendly</li> <li>text_util</li> <li>train_util</li> <li>type_util</li> <li>yaml_util</li> </ul> </li> </ul>"},{"location":"reference/monitor/","title":"monitor","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/monitor/#monitor.Monitor","title":"<code>Monitor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class (ABC) for a Monitor that keeps track of various aspects of training and evaluatio.</p> Source code in <code>speechain/monitor.py</code> <pre><code>class Monitor(ABC):\n    \"\"\"Abstract Base Class (ABC) for a Monitor that keeps track of various aspects of\n    training and evaluatio.\"\"\"\n\n    def __init__(\n        self, logger, args: argparse.Namespace, result_path: str = None, **kwargs\n    ) -&gt; None:\n        \"\"\"Initializes the monitor class.\n\n        Args:\n            logger: A logger object for logging messages.\n            args: Command line arguments passed as a Namespace object.\n            result_path: Path to the directory where results should be saved.\n            **kwargs: Additional keyword arguments.\n        \"\"\"  # Initialize shared members\n        self.logger = logger\n        self.result_path = (\n            args.train_result_path\n            if result_path is None\n            else parse_path_args(result_path)\n        )\n        self.gpus = args.gpus if isinstance(args.gpus, List) else [args.gpus]\n\n        # Initialize shared record information\n        self.epoch_records = dict(\n            consumed_time=dict(data_load_time=[], model_forward_time=[]),\n            consumed_memory=dict(),\n            criteria=dict(),\n        )\n        # Add GPU ranks to consumed_memory if they don't exist\n        for rank, gpu in enumerate(self.gpus):\n            if f\"Rank{rank}\" not in self.epoch_records[\"consumed_memory\"].keys():\n                self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"] = []\n\n        # Initialize step-level records\n        self.step_records = dict(\n            consumed_time=dict(data_load_time=[], model_forward_time=[]),\n            criteria=dict(),\n        )\n        self.mode = None\n        self.monitor_init(args, **kwargs)\n\n        # Initialize snapshooter for log snapshotting\n        self.logs_queue = Queue()\n        snapshot_conf = dict(\n            result_path=self.result_path,\n            snap_mode=self.mode,\n            **args.monitor_snapshot_conf,\n        )\n        # Initialize multiprocessing event to enable communication with the snapshooter process\n        self.event = Event()\n        self.event.clear()\n        Process(\n            target=snapshot_logs,\n            args=(self.logs_queue, self.event, snapshot_conf),\n            daemon=True,\n        ).start()\n\n    def enqueue(self, logs: Dict or List[Dict]) -&gt; None:\n        \"\"\"Enqueues logs into the logs_queue.\n\n        Args:\n            logs:\n                A dictionary or list of dictionaries containing logs.\n        \"\"\"\n        if isinstance(logs, Dict):\n            self.logs_queue.put(logs)\n        elif isinstance(logs, List):\n            for log in logs:\n                self.logs_queue.put(log)\n        else:\n            raise RuntimeError(\"Expected logs to be a Dict or a List[Dict].\")\n\n    def empty_queue(self) -&gt; None:\n        \"\"\"Checks if the logs queue is empty.\n\n        Returns:\n            Boolean value indicating whether the logs queue is empty or not.\n        \"\"\"\n        self.logs_queue.qsize()\n        return self.logs_queue.empty()\n\n    @contextmanager\n    def measure_time(self, names: str or List[str]) -&gt; None:\n        \"\"\"Context manager for measuring time of execution.\n\n        Args:\n            names (str or List[str]):\n                Name or list of names for the operations to be timed.\n        \"\"\"\n        start = time.perf_counter()\n        yield\n        t = time.perf_counter() - start\n\n        names = (\n            [\"consumed_time\", names]\n            if isinstance(names, str)\n            else [\"consumed_time\"] + names\n        )\n        dict_pointer = self.step_records\n        for i, name in enumerate(names):\n            if name not in dict_pointer.keys():\n                dict_pointer[name] = [] if i == len(names) - 1 else dict()\n            dict_pointer = dict_pointer[name]\n        dict_pointer.append(t)\n\n    def refresh_step_records(self, records: Dict = None) -&gt; None:\n        \"\"\"Refreshes the step records by resetting the values.\n\n        Args:\n            records (Dict, optional):\n                A dictionary containing the records to be refreshed.\n        \"\"\"\n        if records is None:\n            records = self.step_records\n        if isinstance(records, Dict):\n            for key in records.keys():\n                if isinstance(records[key], Dict):\n                    self.refresh_step_records(records[key])\n                elif isinstance(records[key], List):\n                    records[key] = []\n                else:\n                    raise RuntimeError(\n                        f\"Unexpected type in records: {type(records[key])}.\"\n                    )\n        else:\n            raise RuntimeError(\"Expected records to be of type Dict.\")\n\n    def record_step_info(self, key: str, step_info: Dict) -&gt; None:\n        \"\"\"Records information at each step during training or evaluation.\n\n        Args:\n            key (str):\n                Key under which information should be recorded.\n            step_info (Dict):\n                Dictionary containing the information to be recorded.\n        \"\"\"\n        for name, info in step_info.items():\n            if name not in self.step_records[key].keys():\n                self.step_records[key][name] = []\n            # result is in the form of torch.Tensor, so it needs to be transformed by .item()\n            if isinstance(info, (torch.Tensor, np.ndarray)):\n                if len(info.shape) == 1:\n                    info = info[0]\n                info = info.item()\n            elif isinstance(info, List):\n                info = info[0]\n            self.step_records[key][name].append(info)\n\n    def record_consumed_time(self, epoch_message: str) -&gt; str:\n        \"\"\"Records time consumed in each epoch during training or evaluation.\n\n        Args:\n            epoch_message (str):\n                String to be included in the epoch message.\n\n        Returns:\n            The updated epoch message.\n        \"\"\"\n        epoch_message += \" -- Consumed Time -- \\n\"\n\n        # Record data loading time\n        _total_time = sum(self.step_records[\"consumed_time\"][\"data_load_time\"])\n        epoch_message += f\"Total data load time: {_total_time:.2f}s -- \"\n        self.epoch_records[\"consumed_time\"][\"data_load_time\"].append(_total_time)\n\n        # Record model forward time\n        _total_time = sum(self.step_records[\"consumed_time\"][\"model_forward_time\"])\n        epoch_message += f\"Total model forward time: {_total_time:.2f}s -- \"\n        self.epoch_records[\"consumed_time\"][\"model_forward_time\"].append(_total_time)\n        epoch_message += \"\\n\"\n\n        return epoch_message\n\n    def record_consumed_memory(self, epoch_message: str) -&gt; str:\n        \"\"\"Records memory consumed in each epoch during training or evaluation.\n\n        Args:\n            epoch_message (str):\n                String to be included in the epoch message.\n\n        Returns:\n            The updated epoch message.\n        \"\"\"\n        epoch_message += \" -- Consumed Memory -- \\n\"\n        gpus = GPUtil.getGPUs()\n        if len(gpus) == 0:\n            self.logger.warn(\n                f\"GPUtil.getGPUs() returns nothing at the {self.mode} part of epoch no.{self.epoch}. \"\n            )\n\n        # Record consumed memory for each GPU\n        for rank, gpu in enumerate(self.gpus):\n            # recover the GPU number from 'CUDA_VISIBLE_DEVICES'\n            if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n                gpu = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")[gpu]\n                # if gpu is in the form of the local ID, it will be converted into an integer for List indexing\n                if gpu.isdigit():\n                    gpu = int(gpu)\n                # if gpu is in the form of the UUID, it will be kept as a string\n\n            # --- torch.cuda is only able to report the GPU used in the current rank --- #\n            # --- but torch.cuda can report the precise allocated and reserved memory information of the model --- #\n            # turn bytes into MB\uff0c\n            # memory_allocated = int(torch.cuda.memory_allocated(gpu) * (10 ** (-6)))\n            # memory_reserved = int(torch.cuda.memory_reserved(gpu) * (10 ** (-6)))\n            # epoch_message += f\"GPU rank no.{rank} (cuda:{gpu}): \" \\\n            #                  f\"allocated memory {memory_allocated} MB, \" \\\n            #                  f\"reserved memory {memory_reserved} MB -- \"\n            # self.epoch_records['consumed_memory'][f'Rank{rank}']['memory_allocated'].append(memory_allocated)\n            # self.epoch_records['consumed_memory'][f'Rank{rank}']['memory_reserved'].append(memory_reserved)\n\n            # --- GPUtil can load the information of all the GPUs no matter the rank of the current process --- #\n            # --- but it sometimes fails to report anything because of some IO errors --- #\n            # --- and the returned used memory is the overall memory consumption of the GPU, --- #\n            # --- which means that if there are more than one jobs running on the same GPU, --- #\n            # --- the used memory is not precise for the current job. --- #\n            memory_used = 0\n            # for local GPU ID\n            if isinstance(gpu, int) and gpu &lt; len(gpus):\n                memory_used = gpus[gpu].memoryUsed\n                gpu_name = f\"cuda:{gpu}\"\n            # for GPU UUID\n            elif isinstance(gpu, str):\n                curr_gpu = None\n                for g in gpus:\n                    if g.uuid == gpu:\n                        curr_gpu = g\n                if curr_gpu is not None:\n                    memory_used = curr_gpu.memoryUsed\n                gpu_name = gpu\n            else:\n                raise RuntimeError(\n                    f\"Unexpected errors happen when retrieving GPU IDs. (got {gpu})\"\n                )\n            # if some unexpected errors happen above, memory_used will remain 0\n            epoch_message += f\"GPU rank no.{rank} ({gpu_name}): {memory_used} MB -- \"\n\n            if f\"Rank{rank}\" not in self.epoch_records[\"consumed_memory\"].keys():\n                self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"] = []\n            self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"].append(memory_used)\n        epoch_message += \"\\n\"\n\n        return epoch_message\n\n    def record_criteria(self, epoch_message: str) -&gt; str:\n        \"\"\"Records criteria in each epoch during training or evaluation.\n\n        Args:\n            epoch_message (str):\n                String to be included in the epoch message.\n\n        Returns:\n            The updated epoch message.\n        \"\"\"\n        epoch_message += \" -- Criteria information -- \\n\"\n\n        # Loop through all training criteria and record average and standard deviation\n        for name, results in self.step_records[\"criteria\"].items():\n            if name not in self.epoch_records[\"criteria\"].keys():\n                self.epoch_records[\"criteria\"][name] = []\n\n            # calculate the average criterion value\n            aver_result = np.mean(results).item()\n            std_result = np.std(results).item()\n            epoch_message += f\"Average {name}: {aver_result:.2e} \u00b1 {std_result:.2f}\\n\"\n            # record the average criterion value\n            self.epoch_records[\"criteria\"][name].append(aver_result)\n        epoch_message += \"\\n\"\n\n        return epoch_message\n\n    @abstractmethod\n    def monitor_init(self, args: argparse.Namespace, **kwargs) -&gt; None:\n        \"\"\"Abstract method for initializing the monitor.\n\n        Args:\n            args (str):\n                Command line arguments passed as a Namespace object.\n            kwargs:\n                Additional keyword arguments.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def start_epoch(self, **kwargs) -&gt; None:\n        \"\"\"Abstract method to be called at the start of each epoch.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def step(self, **kwargs) -&gt; None:\n        \"\"\"Abstract method to be called at each step in an epoch.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def finish_epoch(self, **kwargs):\n        \"\"\"Abstract method to be called at the end of each epoch.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def state_dict(self) -&gt; Dict:\n        \"\"\"This method currently performs no operation and always returns None.\n\n        The intention is for subclasses to override this method to return a dictionary\n        containing the state of the Monitor. However, in the base Monitor class,\n        it does not have any state to save, so it returns None.\n\n        Returns:\n            None\n        \"\"\"\n        return None\n\n    def load_state_dict(self, state_dict: Dict = None) -&gt; None:\n        \"\"\"Loads the Monitor state.\n\n        Args:\n            state_dict (Dict):\n                Dictionary containing a whole state of the Monitor.\n        \"\"\"\n        if state_dict is not None:\n            for key, value in state_dict.items():\n                self.__setattr__(key, value)\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.__init__","title":"<code>__init__(logger, args, result_path=None, **kwargs)</code>","text":"<p>Initializes the monitor class.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>A logger object for logging messages.</p> required <code>args</code> <code>Namespace</code> <p>Command line arguments passed as a Namespace object.</p> required <code>result_path</code> <code>str</code> <p>Path to the directory where results should be saved.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>speechain/monitor.py</code> <pre><code>def __init__(\n    self, logger, args: argparse.Namespace, result_path: str = None, **kwargs\n) -&gt; None:\n    \"\"\"Initializes the monitor class.\n\n    Args:\n        logger: A logger object for logging messages.\n        args: Command line arguments passed as a Namespace object.\n        result_path: Path to the directory where results should be saved.\n        **kwargs: Additional keyword arguments.\n    \"\"\"  # Initialize shared members\n    self.logger = logger\n    self.result_path = (\n        args.train_result_path\n        if result_path is None\n        else parse_path_args(result_path)\n    )\n    self.gpus = args.gpus if isinstance(args.gpus, List) else [args.gpus]\n\n    # Initialize shared record information\n    self.epoch_records = dict(\n        consumed_time=dict(data_load_time=[], model_forward_time=[]),\n        consumed_memory=dict(),\n        criteria=dict(),\n    )\n    # Add GPU ranks to consumed_memory if they don't exist\n    for rank, gpu in enumerate(self.gpus):\n        if f\"Rank{rank}\" not in self.epoch_records[\"consumed_memory\"].keys():\n            self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"] = []\n\n    # Initialize step-level records\n    self.step_records = dict(\n        consumed_time=dict(data_load_time=[], model_forward_time=[]),\n        criteria=dict(),\n    )\n    self.mode = None\n    self.monitor_init(args, **kwargs)\n\n    # Initialize snapshooter for log snapshotting\n    self.logs_queue = Queue()\n    snapshot_conf = dict(\n        result_path=self.result_path,\n        snap_mode=self.mode,\n        **args.monitor_snapshot_conf,\n    )\n    # Initialize multiprocessing event to enable communication with the snapshooter process\n    self.event = Event()\n    self.event.clear()\n    Process(\n        target=snapshot_logs,\n        args=(self.logs_queue, self.event, snapshot_conf),\n        daemon=True,\n    ).start()\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.empty_queue","title":"<code>empty_queue()</code>","text":"<p>Checks if the logs queue is empty.</p> <p>Returns:</p> Type Description <code>None</code> <p>Boolean value indicating whether the logs queue is empty or not.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def empty_queue(self) -&gt; None:\n    \"\"\"Checks if the logs queue is empty.\n\n    Returns:\n        Boolean value indicating whether the logs queue is empty or not.\n    \"\"\"\n    self.logs_queue.qsize()\n    return self.logs_queue.empty()\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.enqueue","title":"<code>enqueue(logs)</code>","text":"<p>Enqueues logs into the logs_queue.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>Dict or List[Dict]</code> <p>A dictionary or list of dictionaries containing logs.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def enqueue(self, logs: Dict or List[Dict]) -&gt; None:\n    \"\"\"Enqueues logs into the logs_queue.\n\n    Args:\n        logs:\n            A dictionary or list of dictionaries containing logs.\n    \"\"\"\n    if isinstance(logs, Dict):\n        self.logs_queue.put(logs)\n    elif isinstance(logs, List):\n        for log in logs:\n            self.logs_queue.put(log)\n    else:\n        raise RuntimeError(\"Expected logs to be a Dict or a List[Dict].\")\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.finish_epoch","title":"<code>finish_epoch(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be called at the end of each epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>@abstractmethod\ndef finish_epoch(self, **kwargs):\n    \"\"\"Abstract method to be called at the end of each epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.load_state_dict","title":"<code>load_state_dict(state_dict=None)</code>","text":"<p>Loads the Monitor state.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict</code> <p>Dictionary containing a whole state of the Monitor.</p> <code>None</code> Source code in <code>speechain/monitor.py</code> <pre><code>def load_state_dict(self, state_dict: Dict = None) -&gt; None:\n    \"\"\"Loads the Monitor state.\n\n    Args:\n        state_dict (Dict):\n            Dictionary containing a whole state of the Monitor.\n    \"\"\"\n    if state_dict is not None:\n        for key, value in state_dict.items():\n            self.__setattr__(key, value)\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.measure_time","title":"<code>measure_time(names)</code>","text":"<p>Context manager for measuring time of execution.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>Name or list of names for the operations to be timed.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>@contextmanager\ndef measure_time(self, names: str or List[str]) -&gt; None:\n    \"\"\"Context manager for measuring time of execution.\n\n    Args:\n        names (str or List[str]):\n            Name or list of names for the operations to be timed.\n    \"\"\"\n    start = time.perf_counter()\n    yield\n    t = time.perf_counter() - start\n\n    names = (\n        [\"consumed_time\", names]\n        if isinstance(names, str)\n        else [\"consumed_time\"] + names\n    )\n    dict_pointer = self.step_records\n    for i, name in enumerate(names):\n        if name not in dict_pointer.keys():\n            dict_pointer[name] = [] if i == len(names) - 1 else dict()\n        dict_pointer = dict_pointer[name]\n    dict_pointer.append(t)\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.monitor_init","title":"<code>monitor_init(args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for initializing the monitor.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>str</code> <p>Command line arguments passed as a Namespace object.</p> required <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>speechain/monitor.py</code> <pre><code>@abstractmethod\ndef monitor_init(self, args: argparse.Namespace, **kwargs) -&gt; None:\n    \"\"\"Abstract method for initializing the monitor.\n\n    Args:\n        args (str):\n            Command line arguments passed as a Namespace object.\n        kwargs:\n            Additional keyword arguments.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.record_consumed_memory","title":"<code>record_consumed_memory(epoch_message)</code>","text":"<p>Records memory consumed in each epoch during training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_message</code> <code>str</code> <p>String to be included in the epoch message.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The updated epoch message.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def record_consumed_memory(self, epoch_message: str) -&gt; str:\n    \"\"\"Records memory consumed in each epoch during training or evaluation.\n\n    Args:\n        epoch_message (str):\n            String to be included in the epoch message.\n\n    Returns:\n        The updated epoch message.\n    \"\"\"\n    epoch_message += \" -- Consumed Memory -- \\n\"\n    gpus = GPUtil.getGPUs()\n    if len(gpus) == 0:\n        self.logger.warn(\n            f\"GPUtil.getGPUs() returns nothing at the {self.mode} part of epoch no.{self.epoch}. \"\n        )\n\n    # Record consumed memory for each GPU\n    for rank, gpu in enumerate(self.gpus):\n        # recover the GPU number from 'CUDA_VISIBLE_DEVICES'\n        if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n            gpu = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")[gpu]\n            # if gpu is in the form of the local ID, it will be converted into an integer for List indexing\n            if gpu.isdigit():\n                gpu = int(gpu)\n            # if gpu is in the form of the UUID, it will be kept as a string\n\n        # --- torch.cuda is only able to report the GPU used in the current rank --- #\n        # --- but torch.cuda can report the precise allocated and reserved memory information of the model --- #\n        # turn bytes into MB\uff0c\n        # memory_allocated = int(torch.cuda.memory_allocated(gpu) * (10 ** (-6)))\n        # memory_reserved = int(torch.cuda.memory_reserved(gpu) * (10 ** (-6)))\n        # epoch_message += f\"GPU rank no.{rank} (cuda:{gpu}): \" \\\n        #                  f\"allocated memory {memory_allocated} MB, \" \\\n        #                  f\"reserved memory {memory_reserved} MB -- \"\n        # self.epoch_records['consumed_memory'][f'Rank{rank}']['memory_allocated'].append(memory_allocated)\n        # self.epoch_records['consumed_memory'][f'Rank{rank}']['memory_reserved'].append(memory_reserved)\n\n        # --- GPUtil can load the information of all the GPUs no matter the rank of the current process --- #\n        # --- but it sometimes fails to report anything because of some IO errors --- #\n        # --- and the returned used memory is the overall memory consumption of the GPU, --- #\n        # --- which means that if there are more than one jobs running on the same GPU, --- #\n        # --- the used memory is not precise for the current job. --- #\n        memory_used = 0\n        # for local GPU ID\n        if isinstance(gpu, int) and gpu &lt; len(gpus):\n            memory_used = gpus[gpu].memoryUsed\n            gpu_name = f\"cuda:{gpu}\"\n        # for GPU UUID\n        elif isinstance(gpu, str):\n            curr_gpu = None\n            for g in gpus:\n                if g.uuid == gpu:\n                    curr_gpu = g\n            if curr_gpu is not None:\n                memory_used = curr_gpu.memoryUsed\n            gpu_name = gpu\n        else:\n            raise RuntimeError(\n                f\"Unexpected errors happen when retrieving GPU IDs. (got {gpu})\"\n            )\n        # if some unexpected errors happen above, memory_used will remain 0\n        epoch_message += f\"GPU rank no.{rank} ({gpu_name}): {memory_used} MB -- \"\n\n        if f\"Rank{rank}\" not in self.epoch_records[\"consumed_memory\"].keys():\n            self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"] = []\n        self.epoch_records[\"consumed_memory\"][f\"Rank{rank}\"].append(memory_used)\n    epoch_message += \"\\n\"\n\n    return epoch_message\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.record_consumed_time","title":"<code>record_consumed_time(epoch_message)</code>","text":"<p>Records time consumed in each epoch during training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_message</code> <code>str</code> <p>String to be included in the epoch message.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The updated epoch message.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def record_consumed_time(self, epoch_message: str) -&gt; str:\n    \"\"\"Records time consumed in each epoch during training or evaluation.\n\n    Args:\n        epoch_message (str):\n            String to be included in the epoch message.\n\n    Returns:\n        The updated epoch message.\n    \"\"\"\n    epoch_message += \" -- Consumed Time -- \\n\"\n\n    # Record data loading time\n    _total_time = sum(self.step_records[\"consumed_time\"][\"data_load_time\"])\n    epoch_message += f\"Total data load time: {_total_time:.2f}s -- \"\n    self.epoch_records[\"consumed_time\"][\"data_load_time\"].append(_total_time)\n\n    # Record model forward time\n    _total_time = sum(self.step_records[\"consumed_time\"][\"model_forward_time\"])\n    epoch_message += f\"Total model forward time: {_total_time:.2f}s -- \"\n    self.epoch_records[\"consumed_time\"][\"model_forward_time\"].append(_total_time)\n    epoch_message += \"\\n\"\n\n    return epoch_message\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.record_criteria","title":"<code>record_criteria(epoch_message)</code>","text":"<p>Records criteria in each epoch during training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_message</code> <code>str</code> <p>String to be included in the epoch message.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The updated epoch message.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def record_criteria(self, epoch_message: str) -&gt; str:\n    \"\"\"Records criteria in each epoch during training or evaluation.\n\n    Args:\n        epoch_message (str):\n            String to be included in the epoch message.\n\n    Returns:\n        The updated epoch message.\n    \"\"\"\n    epoch_message += \" -- Criteria information -- \\n\"\n\n    # Loop through all training criteria and record average and standard deviation\n    for name, results in self.step_records[\"criteria\"].items():\n        if name not in self.epoch_records[\"criteria\"].keys():\n            self.epoch_records[\"criteria\"][name] = []\n\n        # calculate the average criterion value\n        aver_result = np.mean(results).item()\n        std_result = np.std(results).item()\n        epoch_message += f\"Average {name}: {aver_result:.2e} \u00b1 {std_result:.2f}\\n\"\n        # record the average criterion value\n        self.epoch_records[\"criteria\"][name].append(aver_result)\n    epoch_message += \"\\n\"\n\n    return epoch_message\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.record_step_info","title":"<code>record_step_info(key, step_info)</code>","text":"<p>Records information at each step during training or evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key under which information should be recorded.</p> required <code>step_info</code> <code>Dict</code> <p>Dictionary containing the information to be recorded.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def record_step_info(self, key: str, step_info: Dict) -&gt; None:\n    \"\"\"Records information at each step during training or evaluation.\n\n    Args:\n        key (str):\n            Key under which information should be recorded.\n        step_info (Dict):\n            Dictionary containing the information to be recorded.\n    \"\"\"\n    for name, info in step_info.items():\n        if name not in self.step_records[key].keys():\n            self.step_records[key][name] = []\n        # result is in the form of torch.Tensor, so it needs to be transformed by .item()\n        if isinstance(info, (torch.Tensor, np.ndarray)):\n            if len(info.shape) == 1:\n                info = info[0]\n            info = info.item()\n        elif isinstance(info, List):\n            info = info[0]\n        self.step_records[key][name].append(info)\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.refresh_step_records","title":"<code>refresh_step_records(records=None)</code>","text":"<p>Refreshes the step records by resetting the values.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>Dict</code> <p>A dictionary containing the records to be refreshed.</p> <code>None</code> Source code in <code>speechain/monitor.py</code> <pre><code>def refresh_step_records(self, records: Dict = None) -&gt; None:\n    \"\"\"Refreshes the step records by resetting the values.\n\n    Args:\n        records (Dict, optional):\n            A dictionary containing the records to be refreshed.\n    \"\"\"\n    if records is None:\n        records = self.step_records\n    if isinstance(records, Dict):\n        for key in records.keys():\n            if isinstance(records[key], Dict):\n                self.refresh_step_records(records[key])\n            elif isinstance(records[key], List):\n                records[key] = []\n            else:\n                raise RuntimeError(\n                    f\"Unexpected type in records: {type(records[key])}.\"\n                )\n    else:\n        raise RuntimeError(\"Expected records to be of type Dict.\")\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.start_epoch","title":"<code>start_epoch(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be called at the start of each epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>@abstractmethod\ndef start_epoch(self, **kwargs) -&gt; None:\n    \"\"\"Abstract method to be called at the start of each epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.state_dict","title":"<code>state_dict()</code>  <code>abstractmethod</code>","text":"<p>This method currently performs no operation and always returns None.</p> <p>The intention is for subclasses to override this method to return a dictionary containing the state of the Monitor. However, in the base Monitor class, it does not have any state to save, so it returns None.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>None</p> Source code in <code>speechain/monitor.py</code> <pre><code>@abstractmethod\ndef state_dict(self) -&gt; Dict:\n    \"\"\"This method currently performs no operation and always returns None.\n\n    The intention is for subclasses to override this method to return a dictionary\n    containing the state of the Monitor. However, in the base Monitor class,\n    it does not have any state to save, so it returns None.\n\n    Returns:\n        None\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/monitor/#monitor.Monitor.step","title":"<code>step(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be called at each step in an epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>@abstractmethod\ndef step(self, **kwargs) -&gt; None:\n    \"\"\"Abstract method to be called at each step in an epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor","title":"<code>TestMonitor</code>","text":"<p>               Bases: <code>Monitor</code></p> <p>Class responsible for monitoring the testing process and logging real- time information. It extends the base Monitor class.</p> <p>Attributes:</p> Name Type Description <code>distributed</code> <code>bool</code> <p>If True, indicates distributed training.</p> <code>report_per_steps</code> <code>int</code> <p>Frequency of reporting in steps.</p> <code>bad_cases_selection</code> <code>List[List]</code> <p>Criteria for selecting bad cases, if any.</p> <code>data_saving_logs_queue</code> <code>Queue</code> <p>Queue object for storing logs to be saved.</p> <code>prev_test_time</code> <code>float</code> <p>Time of the previous test, used to calculate elapsed time.</p> <code>total_step_num</code> <code>int</code> <p>Total number of steps in the test.</p> <code>step_info</code> <code>dict</code> <p>Information about the current test step.</p> <code>finished_group_num</code> <code>int</code> <p>Number of groups that have finished testing.</p> Source code in <code>speechain/monitor.py</code> <pre><code>class TestMonitor(Monitor):\n    \"\"\"Class responsible for monitoring the testing process and logging real- time\n    information. It extends the base Monitor class.\n\n    Attributes:\n        distributed (bool):\n            If True, indicates distributed training.\n        report_per_steps (int):\n            Frequency of reporting in steps.\n        bad_cases_selection (List[List]):\n            Criteria for selecting bad cases, if any.\n        data_saving_logs_queue (Queue):\n            Queue object for storing logs to be saved.\n        prev_test_time (float):\n            Time of the previous test, used to calculate elapsed time.\n        total_step_num (int):\n            Total number of steps in the test.\n        step_info (dict):\n            Information about the current test step.\n        finished_group_num (int):\n            Number of groups that have finished testing.\n    \"\"\"\n\n    def monitor_init(self, args: argparse.Namespace, **kwargs):\n        \"\"\"Initializes the TestMonitor with the given arguments.\n\n        Args:\n            args (argparse.Namespace):\n                Parsed command line arguments.\n        \"\"\"\n        self.distributed = args.distributed\n        self.report_per_steps = args.report_per_steps\n        self.bad_cases_selection = args.bad_cases_selection\n        if self.bad_cases_selection is None:\n            self.bad_cases_selection = []\n        elif not isinstance(self.bad_cases_selection[0], List):\n            self.bad_cases_selection = [self.bad_cases_selection]\n\n        # initialize the snapshooter of the monitor\n        self.data_saving_logs_queue = Queue()\n        # create daemon processes for data saving\n        assert (\n            args.saving_proc_num &gt;= 1\n        ), \"'saving_proc_num' should be an integer larger than 1!\"\n        self.saving_proc_num = args.saving_proc_num\n        for proc_id in range(self.saving_proc_num):\n            Process(\n                target=data_saving_logs,\n                args=(proc_id, self.data_saving_logs_queue),\n                daemon=True,\n            ).start()\n\n    def start_epoch(self, total_step_num: int):\n        \"\"\"Starts a new testing epoch.\n\n        Args:\n            total_step_num (int): Total number of steps in the epoch.\n        \"\"\"\n        # para init\n        self.prev_test_time = time.time()\n        self.total_step_num = total_step_num\n\n        if not hasattr(self, \"step_info\"):\n            self.step_info = dict(group_time=[], total_time=0)\n        if not hasattr(self, \"finished_group_num\"):\n            self.finished_group_num = 0\n\n    def step(self, step_num: int, test_results: Dict[str, Dict], test_index: List[str]):\n        \"\"\"Executes a single test step and logs the results.\n\n        Args:\n            step_num (int): Current step number.\n            test_results (Dict[str, Dict]): Results of the test step.\n            test_index (List[str]): Indexes of the tests.\n        \"\"\"\n        # --- Write the testing results of the current step to the testing files --- #\n        # loop each result list in the returned Dict\n        for name, result in test_results.items():\n            # for .txt file, register the result contents into self.step_info. text data doesn't occupy too much memory\n            if result[\"format\"].lower() == \"txt\":\n                if name not in self.step_info.keys():\n                    self.step_info[name] = dict()\n\n                if not isinstance(result[\"content\"], List):\n                    result[\"content\"] = [result[\"content\"]]\n\n                for index, content in zip(test_index, result[\"content\"]):\n                    # for the List-type element, turn it into its string format\n                    if isinstance(content, List):\n                        content = str(content)\n                    self.step_info[name][index] = content\n\n            # for other files, data needs to be saved to the disk in real time to reduce the memory burden\n            else:\n                # we enqueue a whole batch of data to be saved in order to reduce the number of query operations of each daemon process\n                self.data_saving_logs_queue.put(\n                    dict(\n                        file_format=result[\"format\"].lower(),\n                        save_path=os.path.join(self.result_path, name),\n                        file_name_list=test_index,\n                        file_content_list=result[\"content\"],\n                        group_ids=(\n                            result[\"group_ids\"]\n                            if \"group_ids\" in result.keys()\n                            else None\n                        ),\n                        sample_rate=(\n                            result[\"sample_rate\"]\n                            if \"sample_rate\" in result.keys()\n                            else None\n                        ),\n                    )\n                )\n\n        # monitor the approximate size of the queue to be more memory-friendly\n        while self.data_saving_logs_queue.qsize() &gt; 5 * self.saving_proc_num:\n            self.logger.warning(\n                f\"There has been more than {3 * self.saving_proc_num} batches in data_saving_logs_queue, \"\n                f\"so the data generation is paused for 30 seconds.\"\n            )\n            time.sleep(30)\n\n        # --- Report the testing midway information to users --- #\n        test_step_message = None\n        # record the tesing time of the current step\n        curr_test_time = time.time()\n        self.step_info[\"group_time\"].append(curr_test_time - self.prev_test_time)\n        self.prev_test_time = curr_test_time\n\n        # meet the reporting interval\n        if step_num % self.report_per_steps == 0:\n            curr_group_time = sum(self.step_info[\"group_time\"])\n\n            # the first testing step\n            if self.finished_group_num == 0:\n                prev_group_time = curr_group_time\n            # other testing steps\n            else:\n                # calculate the average time of all the previous groups\n                prev_group_time = self.step_info[\"total_time\"] / self.finished_group_num\n\n            # calculate the number of remaining steps\n            self.finished_group_num += 1\n            finish_step_num = int(self.finished_group_num * self.report_per_steps)\n            remaining_step_num = self.total_step_num - finish_step_num\n            # take the weighted average consuming time of each group\n            aver_group_time = (prev_group_time + curr_group_time) / 2\n            remaining_time = aver_group_time * (\n                remaining_step_num / self.report_per_steps\n            )\n\n            # update the time records\n            self.step_info[\"total_time\"] += curr_group_time\n            self.step_info[\"group_time\"] = []\n\n            test_step_message = (\n                f\"Testing Midway Report -- \"\n                f\"testing time for the recent {self.report_per_steps} steps: {curr_group_time:.2f}s -- \"\n                f\"finished step number: {finish_step_num} -- \"\n                f\"remaining step number: {remaining_step_num} -- \"\n                f\"expected remaining time: \"\n            )\n\n            remaining_days, remaining_time = int(\n                remaining_time // (3600 * 24)\n            ), remaining_time % (3600 * 24)\n            if remaining_days &gt; 0:\n                test_step_message += f\"{remaining_days:d}d \"\n\n            remaining_hours, remaining_time = (\n                int(remaining_time // 3600),\n                remaining_time % 3600,\n            )\n            if remaining_hours &gt; 0:\n                test_step_message += f\"{remaining_hours:d}h \"\n\n            remaining_minutes, remaining_time = (\n                int(remaining_time // 60),\n                remaining_time % 60,\n            )\n            if remaining_minutes &gt; 0:\n                test_step_message += f\"{remaining_minutes:d}m \"\n\n            remaining_seconds = remaining_time\n            test_step_message += f\"{remaining_seconds:.2f}s\"\n\n        if test_step_message is not None:\n            self.logger.info(test_step_message)\n\n    def wait_empty_queues(self, sleep_time: int = 60):\n        \"\"\"Waits until the log queue is empty before continuing.\n\n        Args:\n            sleep_time (int, optional): Time to wait in seconds when the queue is not empty. Defaults to 60.\n        \"\"\"\n        while True:\n            if self.data_saving_logs_queue.empty():\n                # wait for one more time when the queue becomes empty to left enough time for data saving\n                time.sleep(sleep_time)\n                break\n            else:\n                self.logger.info(\n                    f\"The data saving process is still working. Waiting for {sleep_time} seconds.\"\n                )\n                time.sleep(sleep_time)\n\n        # If using distributed training, synchronize all processes before continuing\n        if self.distributed:\n            torch.distributed.barrier()\n\n    def finish_epoch(self, meta_info: Dict = None):\n        \"\"\"This method completes various tasks at the end of each epoch, such\n        as:\n\n        - Gathering checkpoint information from all GPU processes\n        - Generating data path files\n        - Producing evaluation reports for overall and group-level performance\n        - Presenting top-N bad cases (if 'instance_reports.md' is in self.step_info)\n        - Plotting histograms for numerical metrics in step_info\n\n        Arguments:\n            meta_info (Dict, optional):\n                Meta information about testing samples. Used for group-level evaluation.\n\n        Note:\n            The method is designed for a distributed setting, where results from different processes need to be gathered.\n            If the program is not running in a distributed setting, some steps (like gathering checkpoint information) will be skipped.\n        \"\"\"\n        # group all the testing samples by their meta info\n        group_meta_info = None\n        if meta_info is not None:\n            group_meta_info = dict()\n            # loop each type of meta data\n            for meta_type, meta_dict in meta_info.items():\n                if meta_type not in group_meta_info.keys():\n                    group_meta_info[meta_type] = dict()\n\n                # loop each group of samples\n                for index, group in meta_info[meta_type].items():\n                    if group not in group_meta_info[meta_type].keys():\n                        group_meta_info[meta_type][group] = []\n                    group_meta_info[meta_type][group].append(index)\n\n        # --- Gather the checkpoint information of all the processes --- #\n        # load the checkpoint of rank0 and delete testing time information\n        self.load_state_dict(\n            torch.load(os.path.join(self.result_path, \"rank0_tmp\", \"checkpoint.pth\"))[\n                \"monitor\"\n            ]\n        )\n        self.step_info.pop(\"group_time\")\n        self.step_info.pop(\"total_time\")\n\n        if self.distributed:\n            for rank in range(1, torch.distributed.get_world_size()):\n                _tmp_dict = torch.load(\n                    os.path.join(self.result_path, f\"rank{rank}_tmp\", \"checkpoint.pth\")\n                )[\"monitor\"][\"step_info\"]\n                for key in self.step_info.keys():\n                    self.step_info[key].update(_tmp_dict[key])\n\n        # make sure that all the samples are sorted by their indices\n        for key in self.step_info.keys():\n            self.step_info[key] = dict(\n                sorted(self.step_info[key].items(), key=lambda x: x[0])\n            )\n            # .md files remain their original names\n            if key.endswith(\".md\"):\n                np.savetxt(\n                    os.path.join(self.result_path, key),\n                    list(self.step_info[key].items()),\n                    fmt=\"%s\",\n                )\n            # normal .txt files have the prefix 'idx2' attached at the beginning of their names\n            else:\n                np.savetxt(\n                    os.path.join(self.result_path, f\"idx2{key}\"),\n                    list(self.step_info[key].items()),\n                    fmt=\"%s\",\n                )\n\n        # --- Gather all the save-during-testing files &amp; Generate their path files --- #\n        # generate the data path files\n        for file_name in os.listdir(self.result_path):\n            # only consider folders\n            if not os.path.isdir(os.path.join(self.result_path, file_name)):\n                continue\n            # only consider the folders not named as 'figures' and 'rank_tmp'\n            if (\n                file_name.startswith(\"rank\")\n                or file_name == \"figures\"\n                or \"=\" in file_name\n            ):\n                continue\n            idx2path = {}\n            for data_file in search_file_in_subfolder(\n                os.path.join(self.result_path, file_name),\n                tgt_match_fn=lambda x: len(x.split(\".\")) &gt; 1,\n            ):\n                data_index = \".\".join(os.path.basename(data_file).split(\".\")[:-1])\n\n                # add new file into the Dict\n                if data_index not in idx2path.keys():\n                    idx2path[data_index] = data_file\n                # for multiple files with the same index (probably because of the non-reproducible resuming issue)\n                else:\n                    # same the latest file\n                    if get_file_birthtime(data_file) &gt; get_file_birthtime(\n                        idx2path[data_index]\n                    ):\n                        while os.path.exists(idx2path[data_index]):\n                            os.remove(idx2path[data_index])\n                        idx2path[data_index] = data_file\n\n            idx2path = list(sorted(idx2path.items(), key=lambda x: x[0]))\n            np.savetxt(\n                os.path.join(self.result_path, f\"idx2{file_name}\"), idx2path, fmt=\"%s\"\n            )\n\n        # --- Group-level Evaluation Report Production --- #\n        result_path = os.path.join(self.result_path, \"overall_results.md\")\n        result_string = \"\"\n\n        # The overall evaluation performance\n        result_string += \"# Overall Evaluation (mean \u00b1 std):\\n\"\n        content_dict = dict()\n        # loop each metric and record the overall model performance\n        for metric, result_dict in self.step_info.items():\n            result_list = list(result_dict.values())\n            # only average the numerical results\n            if not isinstance(result_list[0], (int, float)):\n                continue\n\n            content_dict[metric] = (\n                f\"{np.mean(result_list):.4f} \u00b1 {np.std(result_list):.4f}\"\n            )\n        result_string += get_list_strings(content_dict=content_dict)\n\n        # record the group-level model performance\n        if group_meta_info is not None:\n            for meta_name, group_dict in group_meta_info.items():\n                result_string += (\n                    f\"# {meta_name}-wise Evaluation:\\n\"\n                    f\"(***bold&amp;italic*** numbers represent the maximal ones in all groups while \"\n                    f\"**bold** numbers represent the minimal ones.)\\n\\n\"\n                )\n                table_headers, table_contents = [meta_name], dict()\n                # loop each group and calculate the group-specific performance\n                for group_name, group_list in group_dict.items():\n                    # loop each metric\n                    for metric, result_dict in self.step_info.items():\n                        result_list = [\n                            result_dict[index]\n                            for index in group_list\n                            if index in result_dict.keys()\n                        ]\n                        # skip the non-numerical results\n                        if len(result_list) &gt; 0 and not isinstance(\n                            result_list[0], (int, float)\n                        ):\n                            continue\n                        # average the numerical results and record them\n                        if len(result_list) != 0:\n                            # create group item lazily\n                            if group_name not in table_contents.keys():\n                                table_contents[group_name] = []\n\n                            if metric not in table_headers:\n                                table_headers.append(metric)\n                            table_contents[group_name].append(np.mean(result_list))\n\n                # get the max and min group value for each numerical metric\n                for i in range(len(table_headers) - 1):\n                    metric_value_list = [value[i] for value in table_contents.values()]\n                    max_value, min_value = max(metric_value_list), min(\n                        metric_value_list\n                    )\n                    # loop each group\n                    for group in table_contents.keys():\n                        # turn the max number into a bold&amp;italic string\n                        if table_contents[group][i] == max_value:\n                            table_contents[group][\n                                i\n                            ] = f\"***{table_contents[group][i]:.4f}***\"\n                        # turn the min number into a bold string\n                        elif table_contents[group][i] == min_value:\n                            table_contents[group][\n                                i\n                            ] = f\"**{table_contents[group][i]:.4f}**\"\n                        # turn other numbers into pure strings\n                        else:\n                            table_contents[group][i] = f\"{table_contents[group][i]:.4f}\"\n\n                # attach the list of the current group into the result string\n                result_string += get_table_strings(\n                    contents=list(table_contents.values()),\n                    first_col=list(table_contents.keys()),\n                    headers=table_headers,\n                )\n        np.savetxt(result_path, [result_string], fmt=\"%s\")\n\n        # --- Top-N Bad Cases Presentation --- #\n        # only present topn bad cases if instance_reports.md is given\n        if \"instance_reports.md\" in self.step_info.keys():\n            # loop each tri-tuple\n            for metric, mode, num in self.bad_cases_selection:\n                result_path = os.path.join(\n                    self.result_path, f\"top{num}_{mode}_{metric}.md\"\n                )\n\n                if metric in self.step_info.keys():\n                    # get the indices of the topn samples\n                    selected_samples = sorted(\n                        self.step_info[metric].items(),\n                        key=lambda x: x[1],\n                        reverse=True if mode.lower() == \"max\" else False,\n                    )[:num]\n                    selected_samples = [s[0] for s in selected_samples]\n\n                    # make the .md string for all the top-n bad samples\n                    sample_reports = \"\"\n                    for s_index in selected_samples:\n                        sample_reports += (\n                            f\"**{s_index}**\"\n                            + self.step_info[\"instance_reports.md\"][s_index]\n                        )\n                    np.savetxt(result_path, [sample_reports], fmt=\"%s\")\n\n        # --- Histograms Plotting --- #\n        # remove the old figures if have\n        if os.path.exists(os.path.join(self.result_path, \"figures\")):\n            shutil.rmtree(os.path.join(self.result_path, \"figures\"))\n\n        # loop each metric and plot the histogram figure\n        for metric, result_dict in self.step_info.items():\n            result_list = list(result_dict.values())\n            # only consider the numerical results\n            if not isinstance(result_list[0], (int, float)):\n                continue\n\n            self.enqueue(\n                dict(materials=copy.deepcopy({metric: result_list}), plot_type=\"hist\")\n            )\n\n        if not self.empty_queue():\n            for i in range(60):\n                if not self.empty_queue():\n                    self.logger.info(\n                        \"The snapshooter is still snapshotting. Waiting for 10 seconds.\"\n                    )\n                    time.sleep(10)\n\n    def state_dict(self):\n        \"\"\"Returns the state of the TestMonitor instance. This is useful for\n        checkpointing or logging purposes.\n\n        Returns:\n        - A dictionary that includes:\n            * step_info: A dictionary storing information about each step.\n            * finished_group_num: The number of groups that have finished processing.\n        \"\"\"\n        return dict(\n            step_info=self.step_info, finished_group_num=self.finished_group_num\n        )\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.finish_epoch","title":"<code>finish_epoch(meta_info=None)</code>","text":"<p>This method completes various tasks at the end of each epoch, such as:</p> <ul> <li>Gathering checkpoint information from all GPU processes</li> <li>Generating data path files</li> <li>Producing evaluation reports for overall and group-level performance</li> <li>Presenting top-N bad cases (if 'instance_reports.md' is in self.step_info)</li> <li>Plotting histograms for numerical metrics in step_info</li> </ul> <p>Parameters:</p> Name Type Description Default <code>meta_info</code> <code>Dict</code> <p>Meta information about testing samples. Used for group-level evaluation.</p> <code>None</code> Note <p>The method is designed for a distributed setting, where results from different processes need to be gathered. If the program is not running in a distributed setting, some steps (like gathering checkpoint information) will be skipped.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def finish_epoch(self, meta_info: Dict = None):\n    \"\"\"This method completes various tasks at the end of each epoch, such\n    as:\n\n    - Gathering checkpoint information from all GPU processes\n    - Generating data path files\n    - Producing evaluation reports for overall and group-level performance\n    - Presenting top-N bad cases (if 'instance_reports.md' is in self.step_info)\n    - Plotting histograms for numerical metrics in step_info\n\n    Arguments:\n        meta_info (Dict, optional):\n            Meta information about testing samples. Used for group-level evaluation.\n\n    Note:\n        The method is designed for a distributed setting, where results from different processes need to be gathered.\n        If the program is not running in a distributed setting, some steps (like gathering checkpoint information) will be skipped.\n    \"\"\"\n    # group all the testing samples by their meta info\n    group_meta_info = None\n    if meta_info is not None:\n        group_meta_info = dict()\n        # loop each type of meta data\n        for meta_type, meta_dict in meta_info.items():\n            if meta_type not in group_meta_info.keys():\n                group_meta_info[meta_type] = dict()\n\n            # loop each group of samples\n            for index, group in meta_info[meta_type].items():\n                if group not in group_meta_info[meta_type].keys():\n                    group_meta_info[meta_type][group] = []\n                group_meta_info[meta_type][group].append(index)\n\n    # --- Gather the checkpoint information of all the processes --- #\n    # load the checkpoint of rank0 and delete testing time information\n    self.load_state_dict(\n        torch.load(os.path.join(self.result_path, \"rank0_tmp\", \"checkpoint.pth\"))[\n            \"monitor\"\n        ]\n    )\n    self.step_info.pop(\"group_time\")\n    self.step_info.pop(\"total_time\")\n\n    if self.distributed:\n        for rank in range(1, torch.distributed.get_world_size()):\n            _tmp_dict = torch.load(\n                os.path.join(self.result_path, f\"rank{rank}_tmp\", \"checkpoint.pth\")\n            )[\"monitor\"][\"step_info\"]\n            for key in self.step_info.keys():\n                self.step_info[key].update(_tmp_dict[key])\n\n    # make sure that all the samples are sorted by their indices\n    for key in self.step_info.keys():\n        self.step_info[key] = dict(\n            sorted(self.step_info[key].items(), key=lambda x: x[0])\n        )\n        # .md files remain their original names\n        if key.endswith(\".md\"):\n            np.savetxt(\n                os.path.join(self.result_path, key),\n                list(self.step_info[key].items()),\n                fmt=\"%s\",\n            )\n        # normal .txt files have the prefix 'idx2' attached at the beginning of their names\n        else:\n            np.savetxt(\n                os.path.join(self.result_path, f\"idx2{key}\"),\n                list(self.step_info[key].items()),\n                fmt=\"%s\",\n            )\n\n    # --- Gather all the save-during-testing files &amp; Generate their path files --- #\n    # generate the data path files\n    for file_name in os.listdir(self.result_path):\n        # only consider folders\n        if not os.path.isdir(os.path.join(self.result_path, file_name)):\n            continue\n        # only consider the folders not named as 'figures' and 'rank_tmp'\n        if (\n            file_name.startswith(\"rank\")\n            or file_name == \"figures\"\n            or \"=\" in file_name\n        ):\n            continue\n        idx2path = {}\n        for data_file in search_file_in_subfolder(\n            os.path.join(self.result_path, file_name),\n            tgt_match_fn=lambda x: len(x.split(\".\")) &gt; 1,\n        ):\n            data_index = \".\".join(os.path.basename(data_file).split(\".\")[:-1])\n\n            # add new file into the Dict\n            if data_index not in idx2path.keys():\n                idx2path[data_index] = data_file\n            # for multiple files with the same index (probably because of the non-reproducible resuming issue)\n            else:\n                # same the latest file\n                if get_file_birthtime(data_file) &gt; get_file_birthtime(\n                    idx2path[data_index]\n                ):\n                    while os.path.exists(idx2path[data_index]):\n                        os.remove(idx2path[data_index])\n                    idx2path[data_index] = data_file\n\n        idx2path = list(sorted(idx2path.items(), key=lambda x: x[0]))\n        np.savetxt(\n            os.path.join(self.result_path, f\"idx2{file_name}\"), idx2path, fmt=\"%s\"\n        )\n\n    # --- Group-level Evaluation Report Production --- #\n    result_path = os.path.join(self.result_path, \"overall_results.md\")\n    result_string = \"\"\n\n    # The overall evaluation performance\n    result_string += \"# Overall Evaluation (mean \u00b1 std):\\n\"\n    content_dict = dict()\n    # loop each metric and record the overall model performance\n    for metric, result_dict in self.step_info.items():\n        result_list = list(result_dict.values())\n        # only average the numerical results\n        if not isinstance(result_list[0], (int, float)):\n            continue\n\n        content_dict[metric] = (\n            f\"{np.mean(result_list):.4f} \u00b1 {np.std(result_list):.4f}\"\n        )\n    result_string += get_list_strings(content_dict=content_dict)\n\n    # record the group-level model performance\n    if group_meta_info is not None:\n        for meta_name, group_dict in group_meta_info.items():\n            result_string += (\n                f\"# {meta_name}-wise Evaluation:\\n\"\n                f\"(***bold&amp;italic*** numbers represent the maximal ones in all groups while \"\n                f\"**bold** numbers represent the minimal ones.)\\n\\n\"\n            )\n            table_headers, table_contents = [meta_name], dict()\n            # loop each group and calculate the group-specific performance\n            for group_name, group_list in group_dict.items():\n                # loop each metric\n                for metric, result_dict in self.step_info.items():\n                    result_list = [\n                        result_dict[index]\n                        for index in group_list\n                        if index in result_dict.keys()\n                    ]\n                    # skip the non-numerical results\n                    if len(result_list) &gt; 0 and not isinstance(\n                        result_list[0], (int, float)\n                    ):\n                        continue\n                    # average the numerical results and record them\n                    if len(result_list) != 0:\n                        # create group item lazily\n                        if group_name not in table_contents.keys():\n                            table_contents[group_name] = []\n\n                        if metric not in table_headers:\n                            table_headers.append(metric)\n                        table_contents[group_name].append(np.mean(result_list))\n\n            # get the max and min group value for each numerical metric\n            for i in range(len(table_headers) - 1):\n                metric_value_list = [value[i] for value in table_contents.values()]\n                max_value, min_value = max(metric_value_list), min(\n                    metric_value_list\n                )\n                # loop each group\n                for group in table_contents.keys():\n                    # turn the max number into a bold&amp;italic string\n                    if table_contents[group][i] == max_value:\n                        table_contents[group][\n                            i\n                        ] = f\"***{table_contents[group][i]:.4f}***\"\n                    # turn the min number into a bold string\n                    elif table_contents[group][i] == min_value:\n                        table_contents[group][\n                            i\n                        ] = f\"**{table_contents[group][i]:.4f}**\"\n                    # turn other numbers into pure strings\n                    else:\n                        table_contents[group][i] = f\"{table_contents[group][i]:.4f}\"\n\n            # attach the list of the current group into the result string\n            result_string += get_table_strings(\n                contents=list(table_contents.values()),\n                first_col=list(table_contents.keys()),\n                headers=table_headers,\n            )\n    np.savetxt(result_path, [result_string], fmt=\"%s\")\n\n    # --- Top-N Bad Cases Presentation --- #\n    # only present topn bad cases if instance_reports.md is given\n    if \"instance_reports.md\" in self.step_info.keys():\n        # loop each tri-tuple\n        for metric, mode, num in self.bad_cases_selection:\n            result_path = os.path.join(\n                self.result_path, f\"top{num}_{mode}_{metric}.md\"\n            )\n\n            if metric in self.step_info.keys():\n                # get the indices of the topn samples\n                selected_samples = sorted(\n                    self.step_info[metric].items(),\n                    key=lambda x: x[1],\n                    reverse=True if mode.lower() == \"max\" else False,\n                )[:num]\n                selected_samples = [s[0] for s in selected_samples]\n\n                # make the .md string for all the top-n bad samples\n                sample_reports = \"\"\n                for s_index in selected_samples:\n                    sample_reports += (\n                        f\"**{s_index}**\"\n                        + self.step_info[\"instance_reports.md\"][s_index]\n                    )\n                np.savetxt(result_path, [sample_reports], fmt=\"%s\")\n\n    # --- Histograms Plotting --- #\n    # remove the old figures if have\n    if os.path.exists(os.path.join(self.result_path, \"figures\")):\n        shutil.rmtree(os.path.join(self.result_path, \"figures\"))\n\n    # loop each metric and plot the histogram figure\n    for metric, result_dict in self.step_info.items():\n        result_list = list(result_dict.values())\n        # only consider the numerical results\n        if not isinstance(result_list[0], (int, float)):\n            continue\n\n        self.enqueue(\n            dict(materials=copy.deepcopy({metric: result_list}), plot_type=\"hist\")\n        )\n\n    if not self.empty_queue():\n        for i in range(60):\n            if not self.empty_queue():\n                self.logger.info(\n                    \"The snapshooter is still snapshotting. Waiting for 10 seconds.\"\n                )\n                time.sleep(10)\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.monitor_init","title":"<code>monitor_init(args, **kwargs)</code>","text":"<p>Initializes the TestMonitor with the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Parsed command line arguments.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def monitor_init(self, args: argparse.Namespace, **kwargs):\n    \"\"\"Initializes the TestMonitor with the given arguments.\n\n    Args:\n        args (argparse.Namespace):\n            Parsed command line arguments.\n    \"\"\"\n    self.distributed = args.distributed\n    self.report_per_steps = args.report_per_steps\n    self.bad_cases_selection = args.bad_cases_selection\n    if self.bad_cases_selection is None:\n        self.bad_cases_selection = []\n    elif not isinstance(self.bad_cases_selection[0], List):\n        self.bad_cases_selection = [self.bad_cases_selection]\n\n    # initialize the snapshooter of the monitor\n    self.data_saving_logs_queue = Queue()\n    # create daemon processes for data saving\n    assert (\n        args.saving_proc_num &gt;= 1\n    ), \"'saving_proc_num' should be an integer larger than 1!\"\n    self.saving_proc_num = args.saving_proc_num\n    for proc_id in range(self.saving_proc_num):\n        Process(\n            target=data_saving_logs,\n            args=(proc_id, self.data_saving_logs_queue),\n            daemon=True,\n        ).start()\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.start_epoch","title":"<code>start_epoch(total_step_num)</code>","text":"<p>Starts a new testing epoch.</p> <p>Parameters:</p> Name Type Description Default <code>total_step_num</code> <code>int</code> <p>Total number of steps in the epoch.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def start_epoch(self, total_step_num: int):\n    \"\"\"Starts a new testing epoch.\n\n    Args:\n        total_step_num (int): Total number of steps in the epoch.\n    \"\"\"\n    # para init\n    self.prev_test_time = time.time()\n    self.total_step_num = total_step_num\n\n    if not hasattr(self, \"step_info\"):\n        self.step_info = dict(group_time=[], total_time=0)\n    if not hasattr(self, \"finished_group_num\"):\n        self.finished_group_num = 0\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the state of the TestMonitor instance. This is useful for checkpointing or logging purposes.</p> <ul> <li>A dictionary that includes:<ul> <li>step_info: A dictionary storing information about each step.</li> <li>finished_group_num: The number of groups that have finished processing.</li> </ul> </li> </ul> Source code in <code>speechain/monitor.py</code> <pre><code>def state_dict(self):\n    \"\"\"Returns the state of the TestMonitor instance. This is useful for\n    checkpointing or logging purposes.\n\n    Returns:\n    - A dictionary that includes:\n        * step_info: A dictionary storing information about each step.\n        * finished_group_num: The number of groups that have finished processing.\n    \"\"\"\n    return dict(\n        step_info=self.step_info, finished_group_num=self.finished_group_num\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.step","title":"<code>step(step_num, test_results, test_index)</code>","text":"<p>Executes a single test step and logs the results.</p> <p>Parameters:</p> Name Type Description Default <code>step_num</code> <code>int</code> <p>Current step number.</p> required <code>test_results</code> <code>Dict[str, Dict]</code> <p>Results of the test step.</p> required <code>test_index</code> <code>List[str]</code> <p>Indexes of the tests.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def step(self, step_num: int, test_results: Dict[str, Dict], test_index: List[str]):\n    \"\"\"Executes a single test step and logs the results.\n\n    Args:\n        step_num (int): Current step number.\n        test_results (Dict[str, Dict]): Results of the test step.\n        test_index (List[str]): Indexes of the tests.\n    \"\"\"\n    # --- Write the testing results of the current step to the testing files --- #\n    # loop each result list in the returned Dict\n    for name, result in test_results.items():\n        # for .txt file, register the result contents into self.step_info. text data doesn't occupy too much memory\n        if result[\"format\"].lower() == \"txt\":\n            if name not in self.step_info.keys():\n                self.step_info[name] = dict()\n\n            if not isinstance(result[\"content\"], List):\n                result[\"content\"] = [result[\"content\"]]\n\n            for index, content in zip(test_index, result[\"content\"]):\n                # for the List-type element, turn it into its string format\n                if isinstance(content, List):\n                    content = str(content)\n                self.step_info[name][index] = content\n\n        # for other files, data needs to be saved to the disk in real time to reduce the memory burden\n        else:\n            # we enqueue a whole batch of data to be saved in order to reduce the number of query operations of each daemon process\n            self.data_saving_logs_queue.put(\n                dict(\n                    file_format=result[\"format\"].lower(),\n                    save_path=os.path.join(self.result_path, name),\n                    file_name_list=test_index,\n                    file_content_list=result[\"content\"],\n                    group_ids=(\n                        result[\"group_ids\"]\n                        if \"group_ids\" in result.keys()\n                        else None\n                    ),\n                    sample_rate=(\n                        result[\"sample_rate\"]\n                        if \"sample_rate\" in result.keys()\n                        else None\n                    ),\n                )\n            )\n\n    # monitor the approximate size of the queue to be more memory-friendly\n    while self.data_saving_logs_queue.qsize() &gt; 5 * self.saving_proc_num:\n        self.logger.warning(\n            f\"There has been more than {3 * self.saving_proc_num} batches in data_saving_logs_queue, \"\n            f\"so the data generation is paused for 30 seconds.\"\n        )\n        time.sleep(30)\n\n    # --- Report the testing midway information to users --- #\n    test_step_message = None\n    # record the tesing time of the current step\n    curr_test_time = time.time()\n    self.step_info[\"group_time\"].append(curr_test_time - self.prev_test_time)\n    self.prev_test_time = curr_test_time\n\n    # meet the reporting interval\n    if step_num % self.report_per_steps == 0:\n        curr_group_time = sum(self.step_info[\"group_time\"])\n\n        # the first testing step\n        if self.finished_group_num == 0:\n            prev_group_time = curr_group_time\n        # other testing steps\n        else:\n            # calculate the average time of all the previous groups\n            prev_group_time = self.step_info[\"total_time\"] / self.finished_group_num\n\n        # calculate the number of remaining steps\n        self.finished_group_num += 1\n        finish_step_num = int(self.finished_group_num * self.report_per_steps)\n        remaining_step_num = self.total_step_num - finish_step_num\n        # take the weighted average consuming time of each group\n        aver_group_time = (prev_group_time + curr_group_time) / 2\n        remaining_time = aver_group_time * (\n            remaining_step_num / self.report_per_steps\n        )\n\n        # update the time records\n        self.step_info[\"total_time\"] += curr_group_time\n        self.step_info[\"group_time\"] = []\n\n        test_step_message = (\n            f\"Testing Midway Report -- \"\n            f\"testing time for the recent {self.report_per_steps} steps: {curr_group_time:.2f}s -- \"\n            f\"finished step number: {finish_step_num} -- \"\n            f\"remaining step number: {remaining_step_num} -- \"\n            f\"expected remaining time: \"\n        )\n\n        remaining_days, remaining_time = int(\n            remaining_time // (3600 * 24)\n        ), remaining_time % (3600 * 24)\n        if remaining_days &gt; 0:\n            test_step_message += f\"{remaining_days:d}d \"\n\n        remaining_hours, remaining_time = (\n            int(remaining_time // 3600),\n            remaining_time % 3600,\n        )\n        if remaining_hours &gt; 0:\n            test_step_message += f\"{remaining_hours:d}h \"\n\n        remaining_minutes, remaining_time = (\n            int(remaining_time // 60),\n            remaining_time % 60,\n        )\n        if remaining_minutes &gt; 0:\n            test_step_message += f\"{remaining_minutes:d}m \"\n\n        remaining_seconds = remaining_time\n        test_step_message += f\"{remaining_seconds:.2f}s\"\n\n    if test_step_message is not None:\n        self.logger.info(test_step_message)\n</code></pre>"},{"location":"reference/monitor/#monitor.TestMonitor.wait_empty_queues","title":"<code>wait_empty_queues(sleep_time=60)</code>","text":"<p>Waits until the log queue is empty before continuing.</p> <p>Parameters:</p> Name Type Description Default <code>sleep_time</code> <code>int</code> <p>Time to wait in seconds when the queue is not empty. Defaults to 60.</p> <code>60</code> Source code in <code>speechain/monitor.py</code> <pre><code>def wait_empty_queues(self, sleep_time: int = 60):\n    \"\"\"Waits until the log queue is empty before continuing.\n\n    Args:\n        sleep_time (int, optional): Time to wait in seconds when the queue is not empty. Defaults to 60.\n    \"\"\"\n    while True:\n        if self.data_saving_logs_queue.empty():\n            # wait for one more time when the queue becomes empty to left enough time for data saving\n            time.sleep(sleep_time)\n            break\n        else:\n            self.logger.info(\n                f\"The data saving process is still working. Waiting for {sleep_time} seconds.\"\n            )\n            time.sleep(sleep_time)\n\n    # If using distributed training, synchronize all processes before continuing\n    if self.distributed:\n        torch.distributed.barrier()\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor","title":"<code>TrainMonitor</code>","text":"<p>               Bases: <code>Monitor</code></p> <p>The TrainMonitor class extends the Monitor class by adding functionality to track training progress and performance.</p> <p>It provides methods to initialize monitoring, record metrics at each step and epoch, and store the state of the training process.</p> Source code in <code>speechain/monitor.py</code> <pre><code>class TrainMonitor(Monitor):\n    \"\"\"The TrainMonitor class extends the Monitor class by adding functionality to track\n    training progress and performance.\n\n    It provides methods to initialize monitoring, record metrics at each step and epoch,\n    and store the state of the training process.\n    \"\"\"\n\n    def monitor_init(self, args: argparse.Namespace, **kwargs):\n        \"\"\"Initializes the training monitor with the given arguments. This method is\n        responsible for setting up the general members and tracking optimizer\n        information.\n\n        Args:\n            args (argparse.Namespace):\n                Arguments provided for monitoring.\n        \"\"\"\n        # general members\n        self.report_per_steps = args.report_per_steps\n        self.dry_run = args.dry_run\n        self.no_optim = args.no_optim\n        self.mode = \"train\"\n\n        # training monitor needs to additionally track optimizer information\n        # update epoch-level records\n        self.epoch_records[\"consumed_time\"].update(\n            loss_backward_time=dict(), optim_time=dict()\n        )\n        self.epoch_records.update(optim_lr=dict())\n        # update step-level records\n        self.step_records[\"consumed_time\"].update(\n            loss_backward_time=dict(), optim_time=dict()\n        )\n        self.step_records.update(optim_lr=dict())\n\n    def start_epoch(self, epoch: int):\n        \"\"\"Prepares the monitor for a new epoch of training.\n\n        Args:\n            epoch (int):\n                The current epoch number.\n        \"\"\"\n        # epoch-level information\n        self.epoch = epoch\n        self.epoch_start_time = time.time()\n\n        # refresh the step-level records at the beginning of each epoch\n        self.refresh_step_records()\n\n        # logging the beginning information\n        self.logger.info(f\"The training part of epoch no.{epoch} starts.\")\n\n    def step(\n        self,\n        step_num: int,\n        optim_lr: Dict[str, float],\n        train_metrics: Dict[str, torch.Tensor],\n    ):\n        \"\"\"Records information for a single training step.\n\n        Args:\n            step_num (int):\n                The current step number.\n            optim_lr (Dict[str, float]):\n                The learning rates for each optimizer.\n            train_metrics (Dict[str, torch.Tensor]):\n                The training metrics for the current step.\n        \"\"\"\n        # accumulate the values of training criteria\n        if train_metrics is not None:\n            self.record_step_info(\"criteria\", train_metrics)\n\n        # accumulate the optimization times and learning rates of each OptimScheduler\n        if optim_lr is not None:\n            self.record_step_info(\"optim_lr\", optim_lr)\n\n        # report all the information for every 'report_per_steps' training steps\n        if step_num % self.report_per_steps == 0:\n            # calculate the accumulated time for reporting\n            _data_load_time = sum(\n                self.step_records[\"consumed_time\"][\"data_load_time\"][\n                    -self.report_per_steps :\n                ]\n            )\n            _model_forward_time = sum(\n                self.step_records[\"consumed_time\"][\"model_forward_time\"][\n                    -self.report_per_steps :\n                ]\n            )\n            # initialize the returned message of the current step\n            step_message = (\n                f\"Training step no.{step_num - self.report_per_steps + 1:d}-{step_num:d} -- \"\n                f\"data loading time: {_data_load_time:.2f}s -- \"\n                f\"model forward time: {_model_forward_time:.2f}s -- \"\n            )\n\n            if not self.dry_run:\n                # report the values of criteria in each training step\n                step_message += \"Training Criteria: \"\n                for name, result in self.step_records[\"criteria\"].items():\n                    _tmp_criteria = result[-self.report_per_steps :]\n                    step_message += f\"{name}: {np.mean(_tmp_criteria):.2e} -- \"\n\n                if not self.no_optim:\n                    # report the information of optimizers after each training step\n                    step_message += \"OptimSchedulers: \"\n                    for optim_name in self.step_records[\"optim_lr\"].keys():\n                        # accumulate the backward and optimization times\n                        _loss_backward_time = sum(\n                            self.step_records[\"consumed_time\"][\"loss_backward_time\"][\n                                optim_name\n                            ][-self.report_per_steps :]\n                        )\n                        _optim_time = sum(\n                            self.step_records[\"consumed_time\"][\"optim_time\"][\n                                optim_name\n                            ][-self.report_per_steps :]\n                        )\n                        # average the learning rate\n                        _lr = (\n                            sum(\n                                self.step_records[\"optim_lr\"][optim_name][\n                                    -self.report_per_steps :\n                                ]\n                            )\n                            / self.report_per_steps\n                        )\n\n                        # accumulated optimization time and averaged learning_rates are reported\n                        step_message += (\n                            f\"{optim_name}: \"\n                            f\"loss backward time= {_loss_backward_time:.2f}s, \"\n                            f\"optimization time={_optim_time:.2f}s, \"\n                            f\"learning rate={_lr:.2e} -- \"\n                        )\n\n            # logging the information of the current step\n            self.logger.info(step_message)\n\n    def finish_epoch(self):\n        \"\"\"Finishes monitoring for the current epoch, logging information and preparing\n        for the next epoch.\"\"\"\n        # ---- The Information Logging Part ---- #\n        # report the overall consuming time of the current epoch\n        epoch_message = (\n            f\"The training part of epoch no.{self.epoch} is finished in {time.time() - self.epoch_start_time:.2f}s.\\n\"\n            f\"Summary of all training steps:\\n\"\n        )\n\n        # report the information of the consumed calculation time\n        epoch_message = self.record_consumed_time(epoch_message)\n\n        # report the information of the consumed GPU memory\n        epoch_message = self.record_consumed_memory(epoch_message)\n\n        # data loading only\n        if not self.dry_run:\n            # report the information of all the training criteria\n            epoch_message = self.record_criteria(epoch_message)\n\n            # no optimization\n            if not self.no_optim:\n                # report the information of all the OptimSchedulers\n                epoch_message += \" -- OptimScheduler information -- \\n\"\n                # record the optimization information of the current epoch\n                for optim_name in self.step_records[\"optim_lr\"].keys():\n                    if optim_name not in self.epoch_records[\"optim_lr\"].keys():\n                        self.epoch_records[\"optim_lr\"][optim_name] = []\n                    if (\n                        optim_name\n                        not in self.epoch_records[\"consumed_time\"][\n                            \"loss_backward_time\"\n                        ].keys()\n                    ):\n                        self.epoch_records[\"consumed_time\"][\"loss_backward_time\"][\n                            optim_name\n                        ] = []\n                    if (\n                        optim_name\n                        not in self.epoch_records[\"consumed_time\"][\"optim_time\"].keys()\n                    ):\n                        self.epoch_records[\"consumed_time\"][\"optim_time\"][\n                            optim_name\n                        ] = []\n\n                    epoch_message += f\"{optim_name} -- \"\n                    # accumulate the loss backward time\n                    _total_time = sum(\n                        self.step_records[\"consumed_time\"][\"loss_backward_time\"][\n                            optim_name\n                        ]\n                    )\n                    self.epoch_records[\"consumed_time\"][\"loss_backward_time\"][\n                        optim_name\n                    ].append(_total_time)\n                    epoch_message += f\"Total loss backward time: {_total_time:.2f}s, \"\n\n                    # accumulate the optimization time\n                    _total_time = sum(\n                        self.step_records[\"consumed_time\"][\"optim_time\"][optim_name]\n                    )\n                    self.epoch_records[\"consumed_time\"][\"optim_time\"][\n                        optim_name\n                    ].append(_total_time)\n                    epoch_message += f\"Total optimization time: {_total_time:.2f}s, \"\n\n                    # average the learning rate\n                    aver_lr = np.mean(self.step_records[\"optim_lr\"][optim_name]).item()\n                    self.epoch_records[\"optim_lr\"][optim_name].append(aver_lr)\n                    epoch_message += f\"Average learning rate: {aver_lr:.2e}\\n\"\n                epoch_message += \"\\n\"\n\n        # logging the information for the current epoch\n        self.logger.info(epoch_message)\n\n        # ---- The SnapShotting Part ---- #\n        for key in self.epoch_records.keys():\n            # only snapshot the time records in the dry running mode\n            if self.dry_run and key != \"consumed_time\":\n                continue\n            # skip the learning rate records in the no optimization mode\n            if self.no_optim and key == \"optim_lr\":\n                continue\n            # snapshot the epoch records so for to a curve figure\n            self.enqueue(\n                dict(\n                    materials=copy.deepcopy(self.epoch_records[key]),\n                    plot_type=\"curve\",\n                    epoch=self.epoch,\n                    xlabel=\"epoch\",\n                    sep_save=False,\n                    subfolder_names=key,\n                )\n            )\n\n        # notify the snapshooter process of the new queue elements\n        self.event.set()\n\n    def state_dict(self):\n        \"\"\"Returns the current state of the monitor as a dictionary.\n\n        Returns:\n            dict: The current state of the monitor.\n        \"\"\"\n        return dict(epoch_records=self.epoch_records)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor.finish_epoch","title":"<code>finish_epoch()</code>","text":"<p>Finishes monitoring for the current epoch, logging information and preparing for the next epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def finish_epoch(self):\n    \"\"\"Finishes monitoring for the current epoch, logging information and preparing\n    for the next epoch.\"\"\"\n    # ---- The Information Logging Part ---- #\n    # report the overall consuming time of the current epoch\n    epoch_message = (\n        f\"The training part of epoch no.{self.epoch} is finished in {time.time() - self.epoch_start_time:.2f}s.\\n\"\n        f\"Summary of all training steps:\\n\"\n    )\n\n    # report the information of the consumed calculation time\n    epoch_message = self.record_consumed_time(epoch_message)\n\n    # report the information of the consumed GPU memory\n    epoch_message = self.record_consumed_memory(epoch_message)\n\n    # data loading only\n    if not self.dry_run:\n        # report the information of all the training criteria\n        epoch_message = self.record_criteria(epoch_message)\n\n        # no optimization\n        if not self.no_optim:\n            # report the information of all the OptimSchedulers\n            epoch_message += \" -- OptimScheduler information -- \\n\"\n            # record the optimization information of the current epoch\n            for optim_name in self.step_records[\"optim_lr\"].keys():\n                if optim_name not in self.epoch_records[\"optim_lr\"].keys():\n                    self.epoch_records[\"optim_lr\"][optim_name] = []\n                if (\n                    optim_name\n                    not in self.epoch_records[\"consumed_time\"][\n                        \"loss_backward_time\"\n                    ].keys()\n                ):\n                    self.epoch_records[\"consumed_time\"][\"loss_backward_time\"][\n                        optim_name\n                    ] = []\n                if (\n                    optim_name\n                    not in self.epoch_records[\"consumed_time\"][\"optim_time\"].keys()\n                ):\n                    self.epoch_records[\"consumed_time\"][\"optim_time\"][\n                        optim_name\n                    ] = []\n\n                epoch_message += f\"{optim_name} -- \"\n                # accumulate the loss backward time\n                _total_time = sum(\n                    self.step_records[\"consumed_time\"][\"loss_backward_time\"][\n                        optim_name\n                    ]\n                )\n                self.epoch_records[\"consumed_time\"][\"loss_backward_time\"][\n                    optim_name\n                ].append(_total_time)\n                epoch_message += f\"Total loss backward time: {_total_time:.2f}s, \"\n\n                # accumulate the optimization time\n                _total_time = sum(\n                    self.step_records[\"consumed_time\"][\"optim_time\"][optim_name]\n                )\n                self.epoch_records[\"consumed_time\"][\"optim_time\"][\n                    optim_name\n                ].append(_total_time)\n                epoch_message += f\"Total optimization time: {_total_time:.2f}s, \"\n\n                # average the learning rate\n                aver_lr = np.mean(self.step_records[\"optim_lr\"][optim_name]).item()\n                self.epoch_records[\"optim_lr\"][optim_name].append(aver_lr)\n                epoch_message += f\"Average learning rate: {aver_lr:.2e}\\n\"\n            epoch_message += \"\\n\"\n\n    # logging the information for the current epoch\n    self.logger.info(epoch_message)\n\n    # ---- The SnapShotting Part ---- #\n    for key in self.epoch_records.keys():\n        # only snapshot the time records in the dry running mode\n        if self.dry_run and key != \"consumed_time\":\n            continue\n        # skip the learning rate records in the no optimization mode\n        if self.no_optim and key == \"optim_lr\":\n            continue\n        # snapshot the epoch records so for to a curve figure\n        self.enqueue(\n            dict(\n                materials=copy.deepcopy(self.epoch_records[key]),\n                plot_type=\"curve\",\n                epoch=self.epoch,\n                xlabel=\"epoch\",\n                sep_save=False,\n                subfolder_names=key,\n            )\n        )\n\n    # notify the snapshooter process of the new queue elements\n    self.event.set()\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor.monitor_init","title":"<code>monitor_init(args, **kwargs)</code>","text":"<p>Initializes the training monitor with the given arguments. This method is responsible for setting up the general members and tracking optimizer information.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Arguments provided for monitoring.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def monitor_init(self, args: argparse.Namespace, **kwargs):\n    \"\"\"Initializes the training monitor with the given arguments. This method is\n    responsible for setting up the general members and tracking optimizer\n    information.\n\n    Args:\n        args (argparse.Namespace):\n            Arguments provided for monitoring.\n    \"\"\"\n    # general members\n    self.report_per_steps = args.report_per_steps\n    self.dry_run = args.dry_run\n    self.no_optim = args.no_optim\n    self.mode = \"train\"\n\n    # training monitor needs to additionally track optimizer information\n    # update epoch-level records\n    self.epoch_records[\"consumed_time\"].update(\n        loss_backward_time=dict(), optim_time=dict()\n    )\n    self.epoch_records.update(optim_lr=dict())\n    # update step-level records\n    self.step_records[\"consumed_time\"].update(\n        loss_backward_time=dict(), optim_time=dict()\n    )\n    self.step_records.update(optim_lr=dict())\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor.start_epoch","title":"<code>start_epoch(epoch)</code>","text":"<p>Prepares the monitor for a new epoch of training.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def start_epoch(self, epoch: int):\n    \"\"\"Prepares the monitor for a new epoch of training.\n\n    Args:\n        epoch (int):\n            The current epoch number.\n    \"\"\"\n    # epoch-level information\n    self.epoch = epoch\n    self.epoch_start_time = time.time()\n\n    # refresh the step-level records at the beginning of each epoch\n    self.refresh_step_records()\n\n    # logging the beginning information\n    self.logger.info(f\"The training part of epoch no.{epoch} starts.\")\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the current state of the monitor as a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>The current state of the monitor.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def state_dict(self):\n    \"\"\"Returns the current state of the monitor as a dictionary.\n\n    Returns:\n        dict: The current state of the monitor.\n    \"\"\"\n    return dict(epoch_records=self.epoch_records)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainMonitor.step","title":"<code>step(step_num, optim_lr, train_metrics)</code>","text":"<p>Records information for a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>step_num</code> <code>int</code> <p>The current step number.</p> required <code>optim_lr</code> <code>Dict[str, float]</code> <p>The learning rates for each optimizer.</p> required <code>train_metrics</code> <code>Dict[str, Tensor]</code> <p>The training metrics for the current step.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def step(\n    self,\n    step_num: int,\n    optim_lr: Dict[str, float],\n    train_metrics: Dict[str, torch.Tensor],\n):\n    \"\"\"Records information for a single training step.\n\n    Args:\n        step_num (int):\n            The current step number.\n        optim_lr (Dict[str, float]):\n            The learning rates for each optimizer.\n        train_metrics (Dict[str, torch.Tensor]):\n            The training metrics for the current step.\n    \"\"\"\n    # accumulate the values of training criteria\n    if train_metrics is not None:\n        self.record_step_info(\"criteria\", train_metrics)\n\n    # accumulate the optimization times and learning rates of each OptimScheduler\n    if optim_lr is not None:\n        self.record_step_info(\"optim_lr\", optim_lr)\n\n    # report all the information for every 'report_per_steps' training steps\n    if step_num % self.report_per_steps == 0:\n        # calculate the accumulated time for reporting\n        _data_load_time = sum(\n            self.step_records[\"consumed_time\"][\"data_load_time\"][\n                -self.report_per_steps :\n            ]\n        )\n        _model_forward_time = sum(\n            self.step_records[\"consumed_time\"][\"model_forward_time\"][\n                -self.report_per_steps :\n            ]\n        )\n        # initialize the returned message of the current step\n        step_message = (\n            f\"Training step no.{step_num - self.report_per_steps + 1:d}-{step_num:d} -- \"\n            f\"data loading time: {_data_load_time:.2f}s -- \"\n            f\"model forward time: {_model_forward_time:.2f}s -- \"\n        )\n\n        if not self.dry_run:\n            # report the values of criteria in each training step\n            step_message += \"Training Criteria: \"\n            for name, result in self.step_records[\"criteria\"].items():\n                _tmp_criteria = result[-self.report_per_steps :]\n                step_message += f\"{name}: {np.mean(_tmp_criteria):.2e} -- \"\n\n            if not self.no_optim:\n                # report the information of optimizers after each training step\n                step_message += \"OptimSchedulers: \"\n                for optim_name in self.step_records[\"optim_lr\"].keys():\n                    # accumulate the backward and optimization times\n                    _loss_backward_time = sum(\n                        self.step_records[\"consumed_time\"][\"loss_backward_time\"][\n                            optim_name\n                        ][-self.report_per_steps :]\n                    )\n                    _optim_time = sum(\n                        self.step_records[\"consumed_time\"][\"optim_time\"][\n                            optim_name\n                        ][-self.report_per_steps :]\n                    )\n                    # average the learning rate\n                    _lr = (\n                        sum(\n                            self.step_records[\"optim_lr\"][optim_name][\n                                -self.report_per_steps :\n                            ]\n                        )\n                        / self.report_per_steps\n                    )\n\n                    # accumulated optimization time and averaged learning_rates are reported\n                    step_message += (\n                        f\"{optim_name}: \"\n                        f\"loss backward time= {_loss_backward_time:.2f}s, \"\n                        f\"optimization time={_optim_time:.2f}s, \"\n                        f\"learning rate={_lr:.2e} -- \"\n                    )\n\n        # logging the information of the current step\n        self.logger.info(step_message)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor","title":"<code>TrainValidMonitor</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for TrainMonitor and ValidMonitor. The motivations of wrapping TrainMonitor and ValidMonitor together are two-folds:     1. enable multi-metric best model recording among training and validation metrics.     2. decouple TrainMonitor and ValidMonitor from Runner to improve cohesion and code readability.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <p>Logger object for logging purposes.</p> <code>train_monitor</code> <p>An instance of the TrainMonitor class.</p> <code>valid_monitor</code> <p>An instance of the ValidMonitor class.</p> Source code in <code>speechain/monitor.py</code> <pre><code>class TrainValidMonitor(object):\n    \"\"\"\n    A wrapper class for TrainMonitor and ValidMonitor.\n    The motivations of wrapping TrainMonitor and ValidMonitor together are two-folds:\n        1. enable multi-metric best model recording among training and validation metrics.\n        2. decouple TrainMonitor and ValidMonitor from Runner to improve cohesion and code readability.\n\n    Attributes:\n        logger: Logger object for logging purposes.\n        train_monitor: An instance of the TrainMonitor class.\n        valid_monitor: An instance of the ValidMonitor class.\n    \"\"\"\n\n    def __init__(self, logger, args: argparse.Namespace, model: Model):\n        \"\"\"Initializes the TrainValidMonitor with a logger, arguments, and a model.\n\n        Args:\n            logger:\n                Logger object for logging purposes.\n            args:\n                argparse.Namespace object, contains command line arguments.\n            model:\n                Model object, the model to be trained and validated.\n        \"\"\"\n        self.logger = logger\n\n        self.train_monitor = TrainMonitor(logger=logger, args=args)\n        self.valid_monitor = ValidMonitor(logger=logger, args=args, model=model)\n\n    def start_train_epoch(self, epoch: int):\n        \"\"\"Starts a new training epoch.\n\n        Args:\n            epoch: The epoch number.\n        \"\"\"\n        self.train_monitor.start_epoch(epoch)\n\n    def train_step(\n        self,\n        step_num: int,\n        optim_lr: Dict[str, float],\n        train_metrics: Dict[str, torch.Tensor],\n    ):\n        \"\"\"Executes a training step.\n\n        Args:\n            step_num: The step number.\n            optim_lr: The learning rate(s) of the optimizer(s).\n            train_metrics: The training metrics.\n        \"\"\"\n        self.train_monitor.step(\n            step_num=step_num, optim_lr=optim_lr, train_metrics=train_metrics\n        )\n\n    def finish_train_epoch(self):\n        \"\"\"Finishes a training epoch.\"\"\"\n        self.train_monitor.finish_epoch()\n\n    def start_valid_epoch(self, epoch: int):\n        \"\"\"Starts a new validation epoch.\n\n        Args:\n            epoch: The epoch number.\n        \"\"\"\n        self.valid_monitor.start_epoch(epoch)\n\n    def valid_step(self, valid_metrics: Dict[str, torch.Tensor]):\n        \"\"\"Executes a validation step.\n\n        Args:\n            valid_metrics: The validation metrics.\n        \"\"\"\n        self.valid_monitor.step(valid_metrics=valid_metrics)\n\n    def valid_model_snapshot(\n        self, epoch: int, domain: str, sample_index: str, used_sample: Dict\n    ):\n        \"\"\"Creates a snapshot of the validation model.\n\n        Args:\n            epoch: The epoch number.\n            domain: The domain of validation.\n            sample_index: The sample index.\n            used_sample: The sample used for validation.\n        \"\"\"\n        self.valid_monitor.model_snapshot(\n            epoch=epoch,\n            domain=domain,\n            sample_index=sample_index,\n            used_sample=used_sample,\n        )\n\n    def finish_valid_epoch(self, valid_flag: bool, valid_per_epochs: int):\n        \"\"\"Finishes a validation epoch.\n\n        Args:\n            valid_flag: The validation flag.\n            valid_per_epochs: The number of epochs per validation.\n\n        Returns:\n            The result from the finish_epoch method of the ValidMonitor object.\n        \"\"\"\n        return self.valid_monitor.finish_epoch(\n            train_records=self.train_monitor.epoch_records,\n            valid_flag=valid_flag,\n            valid_per_epochs=valid_per_epochs,\n        )\n\n    def wait_empty_queues(self, sleep_time: int = 10, max_wait_round: int = 60):\n        \"\"\"Check whether the snapshot creator processes of train_monitor and\n        valid_monitor are still working. Wait until the material queues of the snapshot\n        creators become empty.\n\n        Args:\n            sleep_time: The sleep time in seconds between each check. Default is 10 seconds.\n            max_wait_round: The maximum number of waiting rounds. Default is 60 rounds.\n        \"\"\"\n        if not self.train_monitor.empty_queue() or not self.valid_monitor.empty_queue():\n            for _ in range(max_wait_round):\n                message = \"\"\n                if not self.train_monitor.empty_queue():\n                    message += \"The training snapshooter is still snapshotting. \"\n                if not self.valid_monitor.empty_queue():\n                    message += \"The validation snapshooter is still snapshotting. \"\n                self.logger.info(message + f\"Waiting for {sleep_time} seconds......\")\n                time.sleep(sleep_time)\n            self.logger.info(\n                f\"The maximal waiting time {max_wait_round * sleep_time} seconds is reached, \"\n                f\"so the snapshooters will be shut down......\"\n            )\n\n    def state_dict(self):\n        \"\"\"Retrieves the state of the TrainMonitor and ValidMonitor in dictionary form.\n\n        Returns:\n            Dict:\n                The current state of the TrainMonitor and ValidMonitor, typically including model parameters and\n                other related information.\n        \"\"\"\n        return dict(\n            train_monitor=self.train_monitor.state_dict(),\n            valid_monitor=self.valid_monitor.state_dict(),\n        )\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the state of the TrainMonitor and ValidMonitor from a dictionary.\n\n        Args:\n            state_dict:\n                The dictionary containing the state of the TrainMonitor and ValidMonitor.\n        \"\"\"\n        self.train_monitor.load_state_dict(state_dict[\"train_monitor\"])\n        self.valid_monitor.load_state_dict(state_dict[\"valid_monitor\"])\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.__init__","title":"<code>__init__(logger, args, model)</code>","text":"<p>Initializes the TrainValidMonitor with a logger, arguments, and a model.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>Logger object for logging purposes.</p> required <code>args</code> <code>Namespace</code> <p>argparse.Namespace object, contains command line arguments.</p> required <code>model</code> <code>Model</code> <p>Model object, the model to be trained and validated.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def __init__(self, logger, args: argparse.Namespace, model: Model):\n    \"\"\"Initializes the TrainValidMonitor with a logger, arguments, and a model.\n\n    Args:\n        logger:\n            Logger object for logging purposes.\n        args:\n            argparse.Namespace object, contains command line arguments.\n        model:\n            Model object, the model to be trained and validated.\n    \"\"\"\n    self.logger = logger\n\n    self.train_monitor = TrainMonitor(logger=logger, args=args)\n    self.valid_monitor = ValidMonitor(logger=logger, args=args, model=model)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.finish_train_epoch","title":"<code>finish_train_epoch()</code>","text":"<p>Finishes a training epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def finish_train_epoch(self):\n    \"\"\"Finishes a training epoch.\"\"\"\n    self.train_monitor.finish_epoch()\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.finish_valid_epoch","title":"<code>finish_valid_epoch(valid_flag, valid_per_epochs)</code>","text":"<p>Finishes a validation epoch.</p> <p>Parameters:</p> Name Type Description Default <code>valid_flag</code> <code>bool</code> <p>The validation flag.</p> required <code>valid_per_epochs</code> <code>int</code> <p>The number of epochs per validation.</p> required <p>Returns:</p> Type Description <p>The result from the finish_epoch method of the ValidMonitor object.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def finish_valid_epoch(self, valid_flag: bool, valid_per_epochs: int):\n    \"\"\"Finishes a validation epoch.\n\n    Args:\n        valid_flag: The validation flag.\n        valid_per_epochs: The number of epochs per validation.\n\n    Returns:\n        The result from the finish_epoch method of the ValidMonitor object.\n    \"\"\"\n    return self.valid_monitor.finish_epoch(\n        train_records=self.train_monitor.epoch_records,\n        valid_flag=valid_flag,\n        valid_per_epochs=valid_per_epochs,\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Loads the state of the TrainMonitor and ValidMonitor from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <p>The dictionary containing the state of the TrainMonitor and ValidMonitor.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"Loads the state of the TrainMonitor and ValidMonitor from a dictionary.\n\n    Args:\n        state_dict:\n            The dictionary containing the state of the TrainMonitor and ValidMonitor.\n    \"\"\"\n    self.train_monitor.load_state_dict(state_dict[\"train_monitor\"])\n    self.valid_monitor.load_state_dict(state_dict[\"valid_monitor\"])\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.start_train_epoch","title":"<code>start_train_epoch(epoch)</code>","text":"<p>Starts a new training epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def start_train_epoch(self, epoch: int):\n    \"\"\"Starts a new training epoch.\n\n    Args:\n        epoch: The epoch number.\n    \"\"\"\n    self.train_monitor.start_epoch(epoch)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.start_valid_epoch","title":"<code>start_valid_epoch(epoch)</code>","text":"<p>Starts a new validation epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def start_valid_epoch(self, epoch: int):\n    \"\"\"Starts a new validation epoch.\n\n    Args:\n        epoch: The epoch number.\n    \"\"\"\n    self.valid_monitor.start_epoch(epoch)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.state_dict","title":"<code>state_dict()</code>","text":"<p>Retrieves the state of the TrainMonitor and ValidMonitor in dictionary form.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>The current state of the TrainMonitor and ValidMonitor, typically including model parameters and other related information.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def state_dict(self):\n    \"\"\"Retrieves the state of the TrainMonitor and ValidMonitor in dictionary form.\n\n    Returns:\n        Dict:\n            The current state of the TrainMonitor and ValidMonitor, typically including model parameters and\n            other related information.\n    \"\"\"\n    return dict(\n        train_monitor=self.train_monitor.state_dict(),\n        valid_monitor=self.valid_monitor.state_dict(),\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.train_step","title":"<code>train_step(step_num, optim_lr, train_metrics)</code>","text":"<p>Executes a training step.</p> <p>Parameters:</p> Name Type Description Default <code>step_num</code> <code>int</code> <p>The step number.</p> required <code>optim_lr</code> <code>Dict[str, float]</code> <p>The learning rate(s) of the optimizer(s).</p> required <code>train_metrics</code> <code>Dict[str, Tensor]</code> <p>The training metrics.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def train_step(\n    self,\n    step_num: int,\n    optim_lr: Dict[str, float],\n    train_metrics: Dict[str, torch.Tensor],\n):\n    \"\"\"Executes a training step.\n\n    Args:\n        step_num: The step number.\n        optim_lr: The learning rate(s) of the optimizer(s).\n        train_metrics: The training metrics.\n    \"\"\"\n    self.train_monitor.step(\n        step_num=step_num, optim_lr=optim_lr, train_metrics=train_metrics\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.valid_model_snapshot","title":"<code>valid_model_snapshot(epoch, domain, sample_index, used_sample)</code>","text":"<p>Creates a snapshot of the validation model.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number.</p> required <code>domain</code> <code>str</code> <p>The domain of validation.</p> required <code>sample_index</code> <code>str</code> <p>The sample index.</p> required <code>used_sample</code> <code>Dict</code> <p>The sample used for validation.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def valid_model_snapshot(\n    self, epoch: int, domain: str, sample_index: str, used_sample: Dict\n):\n    \"\"\"Creates a snapshot of the validation model.\n\n    Args:\n        epoch: The epoch number.\n        domain: The domain of validation.\n        sample_index: The sample index.\n        used_sample: The sample used for validation.\n    \"\"\"\n    self.valid_monitor.model_snapshot(\n        epoch=epoch,\n        domain=domain,\n        sample_index=sample_index,\n        used_sample=used_sample,\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.valid_step","title":"<code>valid_step(valid_metrics)</code>","text":"<p>Executes a validation step.</p> <p>Parameters:</p> Name Type Description Default <code>valid_metrics</code> <code>Dict[str, Tensor]</code> <p>The validation metrics.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def valid_step(self, valid_metrics: Dict[str, torch.Tensor]):\n    \"\"\"Executes a validation step.\n\n    Args:\n        valid_metrics: The validation metrics.\n    \"\"\"\n    self.valid_monitor.step(valid_metrics=valid_metrics)\n</code></pre>"},{"location":"reference/monitor/#monitor.TrainValidMonitor.wait_empty_queues","title":"<code>wait_empty_queues(sleep_time=10, max_wait_round=60)</code>","text":"<p>Check whether the snapshot creator processes of train_monitor and valid_monitor are still working. Wait until the material queues of the snapshot creators become empty.</p> <p>Parameters:</p> Name Type Description Default <code>sleep_time</code> <code>int</code> <p>The sleep time in seconds between each check. Default is 10 seconds.</p> <code>10</code> <code>max_wait_round</code> <code>int</code> <p>The maximum number of waiting rounds. Default is 60 rounds.</p> <code>60</code> Source code in <code>speechain/monitor.py</code> <pre><code>def wait_empty_queues(self, sleep_time: int = 10, max_wait_round: int = 60):\n    \"\"\"Check whether the snapshot creator processes of train_monitor and\n    valid_monitor are still working. Wait until the material queues of the snapshot\n    creators become empty.\n\n    Args:\n        sleep_time: The sleep time in seconds between each check. Default is 10 seconds.\n        max_wait_round: The maximum number of waiting rounds. Default is 60 rounds.\n    \"\"\"\n    if not self.train_monitor.empty_queue() or not self.valid_monitor.empty_queue():\n        for _ in range(max_wait_round):\n            message = \"\"\n            if not self.train_monitor.empty_queue():\n                message += \"The training snapshooter is still snapshotting. \"\n            if not self.valid_monitor.empty_queue():\n                message += \"The validation snapshooter is still snapshotting. \"\n            self.logger.info(message + f\"Waiting for {sleep_time} seconds......\")\n            time.sleep(sleep_time)\n        self.logger.info(\n            f\"The maximal waiting time {max_wait_round * sleep_time} seconds is reached, \"\n            f\"so the snapshooters will be shut down......\"\n        )\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor","title":"<code>ValidMonitor</code>","text":"<p>               Bases: <code>Monitor</code></p> <p>The ValidMonitor class extends the Monitor class by adding functionality to track validation progress and performance.</p> <p>It provides methods to initialize monitoring, record metrics at each validation step, and snapshot the model at each epoch.</p> Source code in <code>speechain/monitor.py</code> <pre><code>class ValidMonitor(Monitor):\n    \"\"\"The ValidMonitor class extends the Monitor class by adding functionality to track\n    validation progress and performance.\n\n    It provides methods to initialize monitoring, record metrics at each validation\n    step, and snapshot the model at each epoch.\n    \"\"\"\n\n    def monitor_init(self, args: argparse.Namespace, model: Model = None):\n        \"\"\"Initializes the validation monitor with the given arguments and the model.\n        This method is responsible for setting up the general members, tracking best\n        models, early stopping, and last models.\n\n        Args:\n            args (argparse.Namespace):\n                Arguments provided for monitoring.\n            model (Model, optional):\n                The model being validated. This parameter must not be None.\n\n        Raises:\n            AssertionError: If the provided model is None.\n        \"\"\"\n        assert model is not None, \"Model must be provided and not None.\"\n        # register a pointer of the model\n        self.model = model\n\n        # running mode\n        self.dry_run = args.dry_run\n        self.no_optim = args.no_optim\n        self.mode = \"valid\"\n\n        # best models-related members\n        self.best_model_selection = args.best_model_selection\n        # receive a single metric as a standalone list or tuple\n        if isinstance(self.best_model_selection, (List, tuple)) and isinstance(\n            self.best_model_selection[0], str\n        ):\n            self.best_model_selection = [self.best_model_selection]\n        else:\n            assert isinstance(self.best_model_selection, List), (\n                f\"best_model_selection must be given as a list, \"\n                f\"but got type(best_model_selection)={self.best_model_selection}.\"\n            )\n\n        for i in range(len(self.best_model_selection)):\n            # checking the argument types\n            assert isinstance(self.best_model_selection[i], (List, tuple)), (\n                \"Each element of best_model_selection must be either a list or a tuple, \"\n                f\"but got type={type(self.best_model_selection[i])}.\"\n            )\n            assert len(self.best_model_selection[i]) == 4, (\n                f\"Each element of best_model_selection must be a quad-tuple or qual-list, \"\n                f\"but got length={len(self.best_model_selection[i])}.\"\n            )\n\n            if isinstance(self.best_model_selection[i], tuple):\n                self.best_model_selection[i] = list(self.best_model_selection[i])\n            self.best_model_selection[i][2] = self.best_model_selection[i][2].lower()\n            assert self.best_model_selection[i][2] in [\n                \"max\",\n                \"min\",\n            ], f\"The best_model_mode must be either 'max' or 'min', but got {self.best_model_selection[i][2]}.\"\n\n        # model saving-related members\n        self.best_model_performance = dict()\n        for metric in self.best_model_selection:\n            self.best_model_performance[\"_\".join(metric[:2])] = dict()\n        self.saved_model_epoch = []\n        self.model_save_path = os.path.join(self.result_path, \"models\")\n        if not os.path.exists(self.model_save_path):\n            os.makedirs(self.model_save_path, exist_ok=True)\n\n        # early stopping-related members, the first metric in self.best_model_selection is used\n        self.early_stopping_metric = \"_\".join(self.best_model_selection[0][:2])\n        self.early_stopping_mode = self.best_model_selection[0][2]\n        self.early_stopping_patience = args.early_stopping_patience\n        self.early_stopping_threshold = args.early_stopping_threshold\n        self.early_stopping_epochs = 0\n        self.last_best_performance = (\n            0.0 if self.early_stopping_mode == \"max\" else torch.inf\n        )\n\n        # last models-related members\n        self.last_model_number = args.last_model_number\n        if self.last_model_number &lt; 1:\n            raise ValueError(\n                \"last_model_number cannot be lower than 1, \"\n                \"otherwise the training will not be able to resume.\"\n                f\"Got last_model_number={self.last_model_number}!\"\n            )\n\n        # initialize the snapshooter of this validation monitor\n        self.visual_snapshot_interval = args.visual_snapshot_interval\n\n    def start_epoch(self, epoch: int):\n        \"\"\"Prepares the monitor for a new epoch of validation.\n\n        Args:\n            epoch (int): The current epoch number.\n        \"\"\"\n        # epoch-level information\n        if epoch in self.best_model_performance.keys():\n            self.logger.warning(\n                f\"The record of epoch no.{epoch} has already existed in the monitor! \"\n                f\"It will be overwritten by the new record obtained shortly thereafter.\"\n            )\n        self.epoch = epoch\n        self.epoch_start_time = time.time()\n\n        # refresh the step-level records at the beginning of each epoch\n        self.refresh_step_records()\n\n        # logging the beginning information\n        self.logger.info(f\"The validation part of epoch no.{epoch} starts.\")\n\n    def step(self, valid_metrics: Dict[str, torch.Tensor]):\n        \"\"\"Records information for a single validation step.\n\n        Args:\n            valid_metrics (Dict[str, torch.Tensor]):\n                The validation metrics for the current step.\n        \"\"\"\n        # accumulate the values of validation criteria\n        if valid_metrics is not None:\n            self.record_step_info(\"criteria\", valid_metrics)\n\n    def model_snapshot(\n        self, epoch: int, domain: str, sample_index: str, used_sample: Dict\n    ):\n        \"\"\"Takes a snapshot of the model at the given epoch for a given sample.\n\n        Args:\n            epoch (int):\n                The current epoch number.\n            domain (str):\n                The domain of the current sample.\n            sample_index (str):\n                The index of the current sample.\n            used_sample (Dict):\n                The current sample being used for validation.\n        \"\"\"\n        # initialize the sub-dict for each sample\n        if sample_index not in self.epoch_records.keys():\n            self.epoch_records[sample_index] = dict()\n\n        # get the visualization logs for model snapshotting\n        vis_logs = self.model(\n            batch_data=used_sample,\n            epoch=epoch,\n            domain=domain,\n            epoch_records=self.epoch_records,\n            sample_index=sample_index,\n            snapshot_interval=self.visual_snapshot_interval,\n        )\n\n        # put all the visualization logs into the queue\n        self.enqueue(vis_logs)\n\n    @staticmethod\n    def is_better(\n        query: int or float, target: int or float, mode: str, threshold: float = 0.0\n    ):\n        \"\"\"Compares a query value with a target value under a specified mode, optionally\n        considering a threshold.\n\n        Parameters:\n            query (int, float):\n                The value to be compared against the target.\n            target (int, float):\n                The reference value for the comparison.\n            mode (str):\n                The comparison mode - 'max' implies the query is considered better if it's larger,\n                'min' implies the query is considered better if it's smaller.\n            threshold (float, optional):\n                A value that adjusts the target value before comparison. Default is 0.0.\n\n        Returns:\n            bool: True if the query value is considered better than the target, False otherwise.\n        \"\"\"\n        _target = target\n        # relative threshold if the argument value is positive\n        if threshold &gt; 0:\n            _target *= 1 + threshold if mode == \"max\" else 1 - threshold\n        # absolute threshold if the argument value is negative\n        elif threshold &lt; 0:\n            _target += -threshold if mode == \"max\" else threshold\n\n        # the threshold is applied to the better comparison\n        return query &gt; _target if mode == \"max\" else query &lt; _target\n\n    def model_insert(self, train_records: Dict, valid_flag: bool):\n        \"\"\"Inserts the model into the ensemble if it's better than the existing models.\n        The model is evaluated using the training records and the validation flag.\n\n        Parameters:\n            train_records (Dict):\n                A dictionary containing the training records, presumably including model performance metrics.\n            valid_flag (bool):\n                A flag indicating whether the model has passed validation.\n\n        Returns:\n            None\n        \"\"\"\n        # loop each metric for best model selection\n        for metric in self.best_model_selection:\n            if metric[0] == \"valid\" and not valid_flag:\n                continue\n\n            _metric_name, _metric_mode, _model_num = (\n                \"_\".join(metric[:2]),\n                metric[2],\n                metric[3],\n            )\n            _criteria_dict = (\n                train_records[\"criteria\"]\n                if metric[0] == \"train\"\n                else self.epoch_records[\"criteria\"]\n            )\n            curr_performance = _criteria_dict[metric[1]][-1]\n\n            # controls whether to insert the current model into self.best_model_performance or not\n            model_insert_flag = False\n\n            # if there is no empty positions for the model of the current epoch\n            if len(self.best_model_performance[_metric_name]) == _model_num:\n                # as long as the current performance is better than one existing record, it should be inserted.\n                for performance in self.best_model_performance[_metric_name].values():\n                    if self.is_better(\n                        query=curr_performance, target=performance, mode=_metric_mode\n                    ):\n                        model_insert_flag = True\n                        break\n            # True if there are some empty positions for the best models\n            else:\n                model_insert_flag = True\n\n            # record the current model performance\n            if model_insert_flag:\n                # record the performance of the current epoch\n                self.best_model_performance[_metric_name][self.epoch] = curr_performance\n\n        # save the model of the latest epoch onto the disk\n        torch.save(\n            self.model.state_dict(),\n            os.path.join(self.model_save_path, f\"epoch_{self.epoch}.pth\"),\n        )\n        self.saved_model_epoch.append(self.epoch)\n\n    def update_best_and_pop_worst(self, epoch_message: str):\n        \"\"\"Updates the best model in the ensemble and removes the worst model. The best\n        and worst are determined based on the performance metrics. The function also\n        handles logging related to model performance.\n\n        Parameters:\n            epoch_message (str):\n                A string message related to the current epoch.\n\n        Returns:\n            Tuple[str, bool, Dict[str, bool]]:\n                Returns a tuple containing the updated epoch message, a flag indicating whether early stopping\n                conditions are met, and flags related to model performance metrics.\n        \"\"\"\n\n        def whether_remove(remove_epoch: int):\n            # retain the last several models within self.last_model_number\n            if self.epoch - remove_epoch &lt; self.last_model_number:\n                return False\n            else:\n                remove_flag = True\n                # access metric_epoch_records from the outer scope\n                for _epoch_record in metric_epoch_records.values():\n                    if remove_epoch in _epoch_record[\"sorted_epochs\"]:\n                        remove_flag = False\n                        break\n                return remove_flag\n\n        # --- Gather the epoch record information for each metric --- #\n        metric_epoch_records = dict()\n        # loop each metric for best model selection\n        for metric in self.best_model_selection:\n            _metric_name, _metric_mode, _model_num = (\n                \"_\".join(metric[:2]),\n                metric[2],\n                metric[3],\n            )\n\n            # find the best epoch and worst epoch in self.best_model_performance\n            sorted_epochs = dict(\n                sorted(\n                    self.best_model_performance[_metric_name].items(),\n                    key=lambda x: x[1],\n                    reverse=True if _metric_mode == \"max\" else False,\n                )\n            )\n            metric_epoch_records[_metric_name] = dict(\n                sorted_epochs=list(sorted_epochs.keys()),\n                metric_mode=_metric_mode,\n                model_num=_model_num,\n            )\n\n        # --- Pop out the worst model and Update the model symbol links --- #\n        metric_pop_flags = dict()\n        for metric_name, epoch_record in metric_epoch_records.items():\n            # controls whether the worst model has been pooped out or not\n            metric_pop_flags[metric_name] = False\n            # pop out the worst model if there is a redundant one in self.best_model_performance\n            if len(epoch_record[\"sorted_epochs\"]) &gt; epoch_record[\"model_num\"]:\n                # pick up the epoch number of the worst model\n                worst_epoch = epoch_record[\"sorted_epochs\"][-1]\n                self.best_model_performance[metric_name].pop(worst_epoch)\n                epoch_record[\"sorted_epochs\"].remove(worst_epoch)\n                metric_pop_flags[metric_name] = True\n\n            # update the symbol links of all the best models so far\n            for i, epoch in enumerate(epoch_record[\"sorted_epochs\"]):\n                _best_model_pointer = (\n                    f\"{metric_name}_best.pth\"\n                    if i == 0\n                    else f\"{metric_name}_best_{i + 1}.pth\"\n                )\n                # create a soft link from the best model pointer to the model fi le of the current epoch\n                symlink_dst = os.path.join(self.model_save_path, _best_model_pointer)\n                if os.path.islink(symlink_dst) or os.path.exists(symlink_dst):\n                    os.unlink(symlink_dst)\n                os.symlink(\n                    os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\"),\n                    symlink_dst,\n                )\n\n        # update the symbol links of the last several models\n        for epoch in range(self.epoch, max(0, self.epoch - self.last_model_number), -1):\n            _last_model_pointer = (\n                \"latest.pth\"\n                if epoch == self.epoch\n                else f\"last_{self.epoch - epoch + 1}.pth\"\n            )\n            # create a soft link from the best model pointer to the model file of the current epoch\n            symlink_dst = os.path.join(self.model_save_path, _last_model_pointer)\n            if os.path.islink(symlink_dst) or os.path.exists(symlink_dst):\n                os.unlink(symlink_dst)\n            os.symlink(\n                os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\"), symlink_dst\n            )\n\n        # remove the redundant model files\n        saved_epochs = self.saved_model_epoch.copy()\n        for epoch in saved_epochs:\n            epoch_model_path = os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\")\n            if whether_remove(epoch):\n                # remove the record of epoch in the memory\n                self.saved_model_epoch.remove(epoch)\n\n                # remove the model file if it exists\n                if os.path.exists(epoch_model_path):\n                    # ensure that the model to be removed is successfully removed\n                    while os.path.exists(epoch_model_path):\n                        os.remove(epoch_model_path)\n\n        # --- Early-Stopping epoch number checking for the early-stopping metric --- #\n        if len(metric_epoch_records[self.early_stopping_metric][\"sorted_epochs\"]) != 0:\n            best_epoch = metric_epoch_records[self.early_stopping_metric][\n                \"sorted_epochs\"\n            ][0]\n            # refresh to 0 or add 1 depending on the comparison the best one and the second best one\n            if best_epoch == self.epoch:\n                epoch_message += f\"{self.early_stopping_metric} of the current epoch no.{self.epoch} is the best so far.\\n\"\n\n                # compare the current performance and the last best performance\n                best_performance = self.best_model_performance[\n                    self.early_stopping_metric\n                ][best_epoch]\n                if self.is_better(\n                    best_performance,\n                    self.last_best_performance,\n                    mode=self.early_stopping_mode,\n                    threshold=self.early_stopping_threshold,\n                ):\n                    epoch_message += (\n                        f\"The early-stopping threshold {self.early_stopping_threshold} is reached, \"\n                        \"so the early-stopping epoch number is refreshed.\\n\"\n                    )\n                    self.early_stopping_epochs = 0\n                    self.last_best_performance = best_performance\n                else:\n                    epoch_message += (\n                        f\"The early-stopping threshold {self.early_stopping_threshold} is not reached, \"\n                        \"so the early-stopping epoch number keeps increasing.\\n\"\n                    )\n                    self.early_stopping_epochs += 1\n            # directly add 1 if the current epoch is not the best\n            else:\n                epoch_message += f\"No improvement of {self.early_stopping_metric} in the current epoch no.{self.epoch}.\\n\"\n                self.early_stopping_epochs += 1\n\n            # report the updated early-stopping epoch number\n            epoch_message += f\"The early-stopping epoch number has been updated to {self.early_stopping_epochs}.\\n\"\n\n        # early-stopping check by the patience\n        early_stopping_flag = False\n        if (\n            self.early_stopping_patience is not None\n            and self.early_stopping_epochs &gt; self.early_stopping_patience\n        ):\n            epoch_message += (\n                f\"The early-stopping patience {self.early_stopping_patience} is reached, \"\n                f\"so the training process stops here.\\n\"\n            )\n            early_stopping_flag = True\n        elif self.early_stopping_patience is None:\n            epoch_message += (\n                \"The early-stopping patience is not set by your exp_cfg, \"\n                \"so the training will continue until num_epochs is reached.\\n\"\n            )\n\n        return epoch_message, early_stopping_flag, metric_pop_flags\n\n    def save_aver_model(self, epoch_message: str, metric_pop_flags: Dict[str, bool]):\n        \"\"\"Stores the average model in the ensemble and updates the epoch message based\n        on the model performance metrics.\n\n        Parameters:\n            epoch_message (str):\n                A string message related to the current epoch.\n            metric_pop_flags (Dict[str, bool]):\n                A dictionary containing flags related to model performance metrics.\n\n        Returns:\n            str: The updated epoch message.\n        \"\"\"\n\n        def save_aver_models(\n            aver_epoch_list: List, aver_num: int, aver_model_name: str\n        ):\n            # no average model is saved if there is only one candidate model\n            if len(aver_epoch_list) == 1:\n                return \"\"\n\n            # sum up the parameters of all best models\n            avg_model = None\n            for epoch in aver_epoch_list:\n                _tgt_model_path = os.path.join(\n                    self.model_save_path, f\"epoch_{epoch}.pth\"\n                )\n                # skip if the model doesn't exist\n                if not os.path.exists(_tgt_model_path):\n                    continue\n\n                _avg = None\n                # access self.model_save_path from the outer scope\n                if avg_model is not None:\n                    _avg = torch.load(_tgt_model_path, map_location=\"cpu\")\n                else:\n                    avg_model = torch.load(_tgt_model_path, map_location=\"cpu\")\n\n                if _avg is not None:\n                    for key in avg_model.keys():\n                        avg_model[key] += _avg[key]\n\n            # if no average model, skip this function and return an empty string\n            if avg_model is None:\n                return \"\"\n            else:\n                # for the parameters whose dtype is int, averaging is not performed\n                # reference: https://github.com/espnet/espnet/blob/5fa6dcc4e649dc66397c629d0030d09ecef36b80/espnet2/main_funcs/average_nbest_models.py#L90\n                for key in avg_model.keys():\n                    if not str(avg_model[key].dtype).startswith(\"torch.int\"):\n                        avg_model[key] /= aver_num\n\n                # save the average model\n                _aver_model_path = os.path.join(self.model_save_path, aver_model_name)\n                torch.save(avg_model, _aver_model_path)\n\n                return f\"{aver_model_name} has been updated to the average of epochs {aver_epoch_list}.\\n\"\n\n        # --- Save the average model for the best models of each metric --- #\n        # loop each metric for best model selection\n        for metric in self.best_model_selection:\n            _metric_name, _metric_mode, _model_num = (\n                \"_\".join(metric[:2]),\n                metric[2],\n                metric[3],\n            )\n\n            # average the recorded best models so far\n            if (\n                len(self.best_model_performance[_metric_name]) == _model_num\n                and metric_pop_flags[_metric_name]\n            ):\n                epoch_message += save_aver_models(\n                    aver_epoch_list=list(\n                        self.best_model_performance[_metric_name].keys()\n                    ),\n                    aver_num=len(self.best_model_performance[_metric_name]),\n                    aver_model_name=f\"{_model_num}_{_metric_name}_average.pth\",\n                )\n\n        # --- Save the average model of the last models --- #\n        if self.epoch &gt;= self.last_model_number:\n            epoch_message += save_aver_models(\n                aver_epoch_list=list(\n                    range(self.epoch, self.epoch - self.last_model_number, -1)\n                )[::-1],\n                aver_num=self.last_model_number,\n                aver_model_name=f\"{self.last_model_number}_last_average.pth\",\n            )\n\n        return epoch_message\n\n    def finish_epoch(\n        self, train_records: Dict, valid_flag: bool, valid_per_epochs: int\n    ):\n        \"\"\"Completes the current epoch by evaluating the model on the training records\n        and potentially performing validation based on the validation flag and number of\n        validation epochs.\n\n        Parameters:\n            train_records (Dict):\n                A dictionary containing the training records, presumably including model performance metrics.\n            valid_flag (bool):\n                A flag indicating whether the model has passed validation.\n            valid_per_epochs (int):\n                The number of epochs between validations.\n\n        Returns:\n            bool: A flag indicating whether early stopping conditions are met.\n        \"\"\"\n        # ---- The Information Logging Part ---- #\n        if valid_flag:\n            # report the overall consuming time of the current validation epoch\n            epoch_message = (\n                f\"The validation part of epoch no.{self.epoch} is finished in \"\n                f\"{time.time() - self.epoch_start_time:.2f}s.\\n\"\n                f\"Summary of all validation steps:\\n\"\n            )\n\n            # report the information of the consumed calculation time\n            epoch_message = self.record_consumed_time(epoch_message)\n\n            # report the information of the consumed GPU memory\n            epoch_message = self.record_consumed_memory(epoch_message)\n\n            if not self.dry_run:\n                # report the information of all the validation criteria\n                epoch_message = self.record_criteria(epoch_message)\n\n            # ---- The SnapShotting Part ---- #\n            for key in self.epoch_records.keys():\n                # only snapshot the time and memory info in the dry running mode\n                if self.dry_run and key != \"consumed_time\":\n                    continue\n                # skip the model visualization records\n                elif key in [\"consumed_time\", \"consumed_memory\", \"criteria\"]:\n                    # snapshot the epoch records so for to a curve figure\n                    self.enqueue(\n                        dict(\n                            materials=copy.deepcopy(self.epoch_records[key]),\n                            plot_type=\"curve\",\n                            epoch=self.epoch,\n                            xlabel=\"epoch\",\n                            sep_save=False,\n                            subfolder_names=key,\n                            x_stride=valid_per_epochs,\n                        )\n                    )\n\n            # notify the snapshooter process of the new queue elements\n            self.event.set()\n\n        else:\n            epoch_message = (\n                f\"The validation part of epoch no.{self.epoch} is skipped.\\n\"\n            )\n\n        # ---- The Model Saving and Early Stopping Part ---- #\n        early_stopping_flag = False\n        if not self.dry_run:\n            # insert the current model into self.best_model_performance if needed\n            self.model_insert(train_records, valid_flag)\n\n            # After inserting, deal with the worst model performance so far and check the early-stopping\n            (\n                epoch_message,\n                early_stopping_flag,\n                metric_pop_flags,\n            ) = self.update_best_and_pop_worst(epoch_message)\n\n            # save the average models of the best models so far if needed\n            epoch_message = self.save_aver_model(epoch_message, metric_pop_flags)\n\n        # log the information of the current validation epoch\n        self.logger.info(epoch_message)\n        return early_stopping_flag\n\n    def state_dict(self):\n        \"\"\"Retrieves the state of the ensemble in dictionary form.\n\n        Returns:\n            Dict: The current state of the ensemble, typically including model parameters and other related information.\n        \"\"\"\n        return dict(\n            epoch_records=self.epoch_records,\n            saved_model_epoch=self.saved_model_epoch,\n            best_model_performance=self.best_model_performance,\n            early_stopping_epochs=self.early_stopping_epochs,\n            last_best_performance=self.last_best_performance,\n        )\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.finish_epoch","title":"<code>finish_epoch(train_records, valid_flag, valid_per_epochs)</code>","text":"<p>Completes the current epoch by evaluating the model on the training records and potentially performing validation based on the validation flag and number of validation epochs.</p> <p>Parameters:</p> Name Type Description Default <code>train_records</code> <code>Dict</code> <p>A dictionary containing the training records, presumably including model performance metrics.</p> required <code>valid_flag</code> <code>bool</code> <p>A flag indicating whether the model has passed validation.</p> required <code>valid_per_epochs</code> <code>int</code> <p>The number of epochs between validations.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>A flag indicating whether early stopping conditions are met.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def finish_epoch(\n    self, train_records: Dict, valid_flag: bool, valid_per_epochs: int\n):\n    \"\"\"Completes the current epoch by evaluating the model on the training records\n    and potentially performing validation based on the validation flag and number of\n    validation epochs.\n\n    Parameters:\n        train_records (Dict):\n            A dictionary containing the training records, presumably including model performance metrics.\n        valid_flag (bool):\n            A flag indicating whether the model has passed validation.\n        valid_per_epochs (int):\n            The number of epochs between validations.\n\n    Returns:\n        bool: A flag indicating whether early stopping conditions are met.\n    \"\"\"\n    # ---- The Information Logging Part ---- #\n    if valid_flag:\n        # report the overall consuming time of the current validation epoch\n        epoch_message = (\n            f\"The validation part of epoch no.{self.epoch} is finished in \"\n            f\"{time.time() - self.epoch_start_time:.2f}s.\\n\"\n            f\"Summary of all validation steps:\\n\"\n        )\n\n        # report the information of the consumed calculation time\n        epoch_message = self.record_consumed_time(epoch_message)\n\n        # report the information of the consumed GPU memory\n        epoch_message = self.record_consumed_memory(epoch_message)\n\n        if not self.dry_run:\n            # report the information of all the validation criteria\n            epoch_message = self.record_criteria(epoch_message)\n\n        # ---- The SnapShotting Part ---- #\n        for key in self.epoch_records.keys():\n            # only snapshot the time and memory info in the dry running mode\n            if self.dry_run and key != \"consumed_time\":\n                continue\n            # skip the model visualization records\n            elif key in [\"consumed_time\", \"consumed_memory\", \"criteria\"]:\n                # snapshot the epoch records so for to a curve figure\n                self.enqueue(\n                    dict(\n                        materials=copy.deepcopy(self.epoch_records[key]),\n                        plot_type=\"curve\",\n                        epoch=self.epoch,\n                        xlabel=\"epoch\",\n                        sep_save=False,\n                        subfolder_names=key,\n                        x_stride=valid_per_epochs,\n                    )\n                )\n\n        # notify the snapshooter process of the new queue elements\n        self.event.set()\n\n    else:\n        epoch_message = (\n            f\"The validation part of epoch no.{self.epoch} is skipped.\\n\"\n        )\n\n    # ---- The Model Saving and Early Stopping Part ---- #\n    early_stopping_flag = False\n    if not self.dry_run:\n        # insert the current model into self.best_model_performance if needed\n        self.model_insert(train_records, valid_flag)\n\n        # After inserting, deal with the worst model performance so far and check the early-stopping\n        (\n            epoch_message,\n            early_stopping_flag,\n            metric_pop_flags,\n        ) = self.update_best_and_pop_worst(epoch_message)\n\n        # save the average models of the best models so far if needed\n        epoch_message = self.save_aver_model(epoch_message, metric_pop_flags)\n\n    # log the information of the current validation epoch\n    self.logger.info(epoch_message)\n    return early_stopping_flag\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.is_better","title":"<code>is_better(query, target, mode, threshold=0.0)</code>  <code>staticmethod</code>","text":"<p>Compares a query value with a target value under a specified mode, optionally considering a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(int, float)</code> <p>The value to be compared against the target.</p> required <code>target</code> <code>(int, float)</code> <p>The reference value for the comparison.</p> required <code>mode</code> <code>str</code> <p>The comparison mode - 'max' implies the query is considered better if it's larger, 'min' implies the query is considered better if it's smaller.</p> required <code>threshold</code> <code>float</code> <p>A value that adjusts the target value before comparison. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the query value is considered better than the target, False otherwise.</p> Source code in <code>speechain/monitor.py</code> <pre><code>@staticmethod\ndef is_better(\n    query: int or float, target: int or float, mode: str, threshold: float = 0.0\n):\n    \"\"\"Compares a query value with a target value under a specified mode, optionally\n    considering a threshold.\n\n    Parameters:\n        query (int, float):\n            The value to be compared against the target.\n        target (int, float):\n            The reference value for the comparison.\n        mode (str):\n            The comparison mode - 'max' implies the query is considered better if it's larger,\n            'min' implies the query is considered better if it's smaller.\n        threshold (float, optional):\n            A value that adjusts the target value before comparison. Default is 0.0.\n\n    Returns:\n        bool: True if the query value is considered better than the target, False otherwise.\n    \"\"\"\n    _target = target\n    # relative threshold if the argument value is positive\n    if threshold &gt; 0:\n        _target *= 1 + threshold if mode == \"max\" else 1 - threshold\n    # absolute threshold if the argument value is negative\n    elif threshold &lt; 0:\n        _target += -threshold if mode == \"max\" else threshold\n\n    # the threshold is applied to the better comparison\n    return query &gt; _target if mode == \"max\" else query &lt; _target\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.model_insert","title":"<code>model_insert(train_records, valid_flag)</code>","text":"<p>Inserts the model into the ensemble if it's better than the existing models. The model is evaluated using the training records and the validation flag.</p> <p>Parameters:</p> Name Type Description Default <code>train_records</code> <code>Dict</code> <p>A dictionary containing the training records, presumably including model performance metrics.</p> required <code>valid_flag</code> <code>bool</code> <p>A flag indicating whether the model has passed validation.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/monitor.py</code> <pre><code>def model_insert(self, train_records: Dict, valid_flag: bool):\n    \"\"\"Inserts the model into the ensemble if it's better than the existing models.\n    The model is evaluated using the training records and the validation flag.\n\n    Parameters:\n        train_records (Dict):\n            A dictionary containing the training records, presumably including model performance metrics.\n        valid_flag (bool):\n            A flag indicating whether the model has passed validation.\n\n    Returns:\n        None\n    \"\"\"\n    # loop each metric for best model selection\n    for metric in self.best_model_selection:\n        if metric[0] == \"valid\" and not valid_flag:\n            continue\n\n        _metric_name, _metric_mode, _model_num = (\n            \"_\".join(metric[:2]),\n            metric[2],\n            metric[3],\n        )\n        _criteria_dict = (\n            train_records[\"criteria\"]\n            if metric[0] == \"train\"\n            else self.epoch_records[\"criteria\"]\n        )\n        curr_performance = _criteria_dict[metric[1]][-1]\n\n        # controls whether to insert the current model into self.best_model_performance or not\n        model_insert_flag = False\n\n        # if there is no empty positions for the model of the current epoch\n        if len(self.best_model_performance[_metric_name]) == _model_num:\n            # as long as the current performance is better than one existing record, it should be inserted.\n            for performance in self.best_model_performance[_metric_name].values():\n                if self.is_better(\n                    query=curr_performance, target=performance, mode=_metric_mode\n                ):\n                    model_insert_flag = True\n                    break\n        # True if there are some empty positions for the best models\n        else:\n            model_insert_flag = True\n\n        # record the current model performance\n        if model_insert_flag:\n            # record the performance of the current epoch\n            self.best_model_performance[_metric_name][self.epoch] = curr_performance\n\n    # save the model of the latest epoch onto the disk\n    torch.save(\n        self.model.state_dict(),\n        os.path.join(self.model_save_path, f\"epoch_{self.epoch}.pth\"),\n    )\n    self.saved_model_epoch.append(self.epoch)\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.model_snapshot","title":"<code>model_snapshot(epoch, domain, sample_index, used_sample)</code>","text":"<p>Takes a snapshot of the model at the given epoch for a given sample.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>domain</code> <code>str</code> <p>The domain of the current sample.</p> required <code>sample_index</code> <code>str</code> <p>The index of the current sample.</p> required <code>used_sample</code> <code>Dict</code> <p>The current sample being used for validation.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def model_snapshot(\n    self, epoch: int, domain: str, sample_index: str, used_sample: Dict\n):\n    \"\"\"Takes a snapshot of the model at the given epoch for a given sample.\n\n    Args:\n        epoch (int):\n            The current epoch number.\n        domain (str):\n            The domain of the current sample.\n        sample_index (str):\n            The index of the current sample.\n        used_sample (Dict):\n            The current sample being used for validation.\n    \"\"\"\n    # initialize the sub-dict for each sample\n    if sample_index not in self.epoch_records.keys():\n        self.epoch_records[sample_index] = dict()\n\n    # get the visualization logs for model snapshotting\n    vis_logs = self.model(\n        batch_data=used_sample,\n        epoch=epoch,\n        domain=domain,\n        epoch_records=self.epoch_records,\n        sample_index=sample_index,\n        snapshot_interval=self.visual_snapshot_interval,\n    )\n\n    # put all the visualization logs into the queue\n    self.enqueue(vis_logs)\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.monitor_init","title":"<code>monitor_init(args, model=None)</code>","text":"<p>Initializes the validation monitor with the given arguments and the model. This method is responsible for setting up the general members, tracking best models, early stopping, and last models.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Arguments provided for monitoring.</p> required <code>model</code> <code>Model</code> <p>The model being validated. This parameter must not be None.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided model is None.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def monitor_init(self, args: argparse.Namespace, model: Model = None):\n    \"\"\"Initializes the validation monitor with the given arguments and the model.\n    This method is responsible for setting up the general members, tracking best\n    models, early stopping, and last models.\n\n    Args:\n        args (argparse.Namespace):\n            Arguments provided for monitoring.\n        model (Model, optional):\n            The model being validated. This parameter must not be None.\n\n    Raises:\n        AssertionError: If the provided model is None.\n    \"\"\"\n    assert model is not None, \"Model must be provided and not None.\"\n    # register a pointer of the model\n    self.model = model\n\n    # running mode\n    self.dry_run = args.dry_run\n    self.no_optim = args.no_optim\n    self.mode = \"valid\"\n\n    # best models-related members\n    self.best_model_selection = args.best_model_selection\n    # receive a single metric as a standalone list or tuple\n    if isinstance(self.best_model_selection, (List, tuple)) and isinstance(\n        self.best_model_selection[0], str\n    ):\n        self.best_model_selection = [self.best_model_selection]\n    else:\n        assert isinstance(self.best_model_selection, List), (\n            f\"best_model_selection must be given as a list, \"\n            f\"but got type(best_model_selection)={self.best_model_selection}.\"\n        )\n\n    for i in range(len(self.best_model_selection)):\n        # checking the argument types\n        assert isinstance(self.best_model_selection[i], (List, tuple)), (\n            \"Each element of best_model_selection must be either a list or a tuple, \"\n            f\"but got type={type(self.best_model_selection[i])}.\"\n        )\n        assert len(self.best_model_selection[i]) == 4, (\n            f\"Each element of best_model_selection must be a quad-tuple or qual-list, \"\n            f\"but got length={len(self.best_model_selection[i])}.\"\n        )\n\n        if isinstance(self.best_model_selection[i], tuple):\n            self.best_model_selection[i] = list(self.best_model_selection[i])\n        self.best_model_selection[i][2] = self.best_model_selection[i][2].lower()\n        assert self.best_model_selection[i][2] in [\n            \"max\",\n            \"min\",\n        ], f\"The best_model_mode must be either 'max' or 'min', but got {self.best_model_selection[i][2]}.\"\n\n    # model saving-related members\n    self.best_model_performance = dict()\n    for metric in self.best_model_selection:\n        self.best_model_performance[\"_\".join(metric[:2])] = dict()\n    self.saved_model_epoch = []\n    self.model_save_path = os.path.join(self.result_path, \"models\")\n    if not os.path.exists(self.model_save_path):\n        os.makedirs(self.model_save_path, exist_ok=True)\n\n    # early stopping-related members, the first metric in self.best_model_selection is used\n    self.early_stopping_metric = \"_\".join(self.best_model_selection[0][:2])\n    self.early_stopping_mode = self.best_model_selection[0][2]\n    self.early_stopping_patience = args.early_stopping_patience\n    self.early_stopping_threshold = args.early_stopping_threshold\n    self.early_stopping_epochs = 0\n    self.last_best_performance = (\n        0.0 if self.early_stopping_mode == \"max\" else torch.inf\n    )\n\n    # last models-related members\n    self.last_model_number = args.last_model_number\n    if self.last_model_number &lt; 1:\n        raise ValueError(\n            \"last_model_number cannot be lower than 1, \"\n            \"otherwise the training will not be able to resume.\"\n            f\"Got last_model_number={self.last_model_number}!\"\n        )\n\n    # initialize the snapshooter of this validation monitor\n    self.visual_snapshot_interval = args.visual_snapshot_interval\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.save_aver_model","title":"<code>save_aver_model(epoch_message, metric_pop_flags)</code>","text":"<p>Stores the average model in the ensemble and updates the epoch message based on the model performance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_message</code> <code>str</code> <p>A string message related to the current epoch.</p> required <code>metric_pop_flags</code> <code>Dict[str, bool]</code> <p>A dictionary containing flags related to model performance metrics.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The updated epoch message.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def save_aver_model(self, epoch_message: str, metric_pop_flags: Dict[str, bool]):\n    \"\"\"Stores the average model in the ensemble and updates the epoch message based\n    on the model performance metrics.\n\n    Parameters:\n        epoch_message (str):\n            A string message related to the current epoch.\n        metric_pop_flags (Dict[str, bool]):\n            A dictionary containing flags related to model performance metrics.\n\n    Returns:\n        str: The updated epoch message.\n    \"\"\"\n\n    def save_aver_models(\n        aver_epoch_list: List, aver_num: int, aver_model_name: str\n    ):\n        # no average model is saved if there is only one candidate model\n        if len(aver_epoch_list) == 1:\n            return \"\"\n\n        # sum up the parameters of all best models\n        avg_model = None\n        for epoch in aver_epoch_list:\n            _tgt_model_path = os.path.join(\n                self.model_save_path, f\"epoch_{epoch}.pth\"\n            )\n            # skip if the model doesn't exist\n            if not os.path.exists(_tgt_model_path):\n                continue\n\n            _avg = None\n            # access self.model_save_path from the outer scope\n            if avg_model is not None:\n                _avg = torch.load(_tgt_model_path, map_location=\"cpu\")\n            else:\n                avg_model = torch.load(_tgt_model_path, map_location=\"cpu\")\n\n            if _avg is not None:\n                for key in avg_model.keys():\n                    avg_model[key] += _avg[key]\n\n        # if no average model, skip this function and return an empty string\n        if avg_model is None:\n            return \"\"\n        else:\n            # for the parameters whose dtype is int, averaging is not performed\n            # reference: https://github.com/espnet/espnet/blob/5fa6dcc4e649dc66397c629d0030d09ecef36b80/espnet2/main_funcs/average_nbest_models.py#L90\n            for key in avg_model.keys():\n                if not str(avg_model[key].dtype).startswith(\"torch.int\"):\n                    avg_model[key] /= aver_num\n\n            # save the average model\n            _aver_model_path = os.path.join(self.model_save_path, aver_model_name)\n            torch.save(avg_model, _aver_model_path)\n\n            return f\"{aver_model_name} has been updated to the average of epochs {aver_epoch_list}.\\n\"\n\n    # --- Save the average model for the best models of each metric --- #\n    # loop each metric for best model selection\n    for metric in self.best_model_selection:\n        _metric_name, _metric_mode, _model_num = (\n            \"_\".join(metric[:2]),\n            metric[2],\n            metric[3],\n        )\n\n        # average the recorded best models so far\n        if (\n            len(self.best_model_performance[_metric_name]) == _model_num\n            and metric_pop_flags[_metric_name]\n        ):\n            epoch_message += save_aver_models(\n                aver_epoch_list=list(\n                    self.best_model_performance[_metric_name].keys()\n                ),\n                aver_num=len(self.best_model_performance[_metric_name]),\n                aver_model_name=f\"{_model_num}_{_metric_name}_average.pth\",\n            )\n\n    # --- Save the average model of the last models --- #\n    if self.epoch &gt;= self.last_model_number:\n        epoch_message += save_aver_models(\n            aver_epoch_list=list(\n                range(self.epoch, self.epoch - self.last_model_number, -1)\n            )[::-1],\n            aver_num=self.last_model_number,\n            aver_model_name=f\"{self.last_model_number}_last_average.pth\",\n        )\n\n    return epoch_message\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.start_epoch","title":"<code>start_epoch(epoch)</code>","text":"<p>Prepares the monitor for a new epoch of validation.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def start_epoch(self, epoch: int):\n    \"\"\"Prepares the monitor for a new epoch of validation.\n\n    Args:\n        epoch (int): The current epoch number.\n    \"\"\"\n    # epoch-level information\n    if epoch in self.best_model_performance.keys():\n        self.logger.warning(\n            f\"The record of epoch no.{epoch} has already existed in the monitor! \"\n            f\"It will be overwritten by the new record obtained shortly thereafter.\"\n        )\n    self.epoch = epoch\n    self.epoch_start_time = time.time()\n\n    # refresh the step-level records at the beginning of each epoch\n    self.refresh_step_records()\n\n    # logging the beginning information\n    self.logger.info(f\"The validation part of epoch no.{epoch} starts.\")\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.state_dict","title":"<code>state_dict()</code>","text":"<p>Retrieves the state of the ensemble in dictionary form.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>The current state of the ensemble, typically including model parameters and other related information.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def state_dict(self):\n    \"\"\"Retrieves the state of the ensemble in dictionary form.\n\n    Returns:\n        Dict: The current state of the ensemble, typically including model parameters and other related information.\n    \"\"\"\n    return dict(\n        epoch_records=self.epoch_records,\n        saved_model_epoch=self.saved_model_epoch,\n        best_model_performance=self.best_model_performance,\n        early_stopping_epochs=self.early_stopping_epochs,\n        last_best_performance=self.last_best_performance,\n    )\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.step","title":"<code>step(valid_metrics)</code>","text":"<p>Records information for a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>valid_metrics</code> <code>Dict[str, Tensor]</code> <p>The validation metrics for the current step.</p> required Source code in <code>speechain/monitor.py</code> <pre><code>def step(self, valid_metrics: Dict[str, torch.Tensor]):\n    \"\"\"Records information for a single validation step.\n\n    Args:\n        valid_metrics (Dict[str, torch.Tensor]):\n            The validation metrics for the current step.\n    \"\"\"\n    # accumulate the values of validation criteria\n    if valid_metrics is not None:\n        self.record_step_info(\"criteria\", valid_metrics)\n</code></pre>"},{"location":"reference/monitor/#monitor.ValidMonitor.update_best_and_pop_worst","title":"<code>update_best_and_pop_worst(epoch_message)</code>","text":"<p>Updates the best model in the ensemble and removes the worst model. The best and worst are determined based on the performance metrics. The function also handles logging related to model performance.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_message</code> <code>str</code> <p>A string message related to the current epoch.</p> required <p>Returns:</p> Type Description <p>Tuple[str, bool, Dict[str, bool]]: Returns a tuple containing the updated epoch message, a flag indicating whether early stopping conditions are met, and flags related to model performance metrics.</p> Source code in <code>speechain/monitor.py</code> <pre><code>def update_best_and_pop_worst(self, epoch_message: str):\n    \"\"\"Updates the best model in the ensemble and removes the worst model. The best\n    and worst are determined based on the performance metrics. The function also\n    handles logging related to model performance.\n\n    Parameters:\n        epoch_message (str):\n            A string message related to the current epoch.\n\n    Returns:\n        Tuple[str, bool, Dict[str, bool]]:\n            Returns a tuple containing the updated epoch message, a flag indicating whether early stopping\n            conditions are met, and flags related to model performance metrics.\n    \"\"\"\n\n    def whether_remove(remove_epoch: int):\n        # retain the last several models within self.last_model_number\n        if self.epoch - remove_epoch &lt; self.last_model_number:\n            return False\n        else:\n            remove_flag = True\n            # access metric_epoch_records from the outer scope\n            for _epoch_record in metric_epoch_records.values():\n                if remove_epoch in _epoch_record[\"sorted_epochs\"]:\n                    remove_flag = False\n                    break\n            return remove_flag\n\n    # --- Gather the epoch record information for each metric --- #\n    metric_epoch_records = dict()\n    # loop each metric for best model selection\n    for metric in self.best_model_selection:\n        _metric_name, _metric_mode, _model_num = (\n            \"_\".join(metric[:2]),\n            metric[2],\n            metric[3],\n        )\n\n        # find the best epoch and worst epoch in self.best_model_performance\n        sorted_epochs = dict(\n            sorted(\n                self.best_model_performance[_metric_name].items(),\n                key=lambda x: x[1],\n                reverse=True if _metric_mode == \"max\" else False,\n            )\n        )\n        metric_epoch_records[_metric_name] = dict(\n            sorted_epochs=list(sorted_epochs.keys()),\n            metric_mode=_metric_mode,\n            model_num=_model_num,\n        )\n\n    # --- Pop out the worst model and Update the model symbol links --- #\n    metric_pop_flags = dict()\n    for metric_name, epoch_record in metric_epoch_records.items():\n        # controls whether the worst model has been pooped out or not\n        metric_pop_flags[metric_name] = False\n        # pop out the worst model if there is a redundant one in self.best_model_performance\n        if len(epoch_record[\"sorted_epochs\"]) &gt; epoch_record[\"model_num\"]:\n            # pick up the epoch number of the worst model\n            worst_epoch = epoch_record[\"sorted_epochs\"][-1]\n            self.best_model_performance[metric_name].pop(worst_epoch)\n            epoch_record[\"sorted_epochs\"].remove(worst_epoch)\n            metric_pop_flags[metric_name] = True\n\n        # update the symbol links of all the best models so far\n        for i, epoch in enumerate(epoch_record[\"sorted_epochs\"]):\n            _best_model_pointer = (\n                f\"{metric_name}_best.pth\"\n                if i == 0\n                else f\"{metric_name}_best_{i + 1}.pth\"\n            )\n            # create a soft link from the best model pointer to the model fi le of the current epoch\n            symlink_dst = os.path.join(self.model_save_path, _best_model_pointer)\n            if os.path.islink(symlink_dst) or os.path.exists(symlink_dst):\n                os.unlink(symlink_dst)\n            os.symlink(\n                os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\"),\n                symlink_dst,\n            )\n\n    # update the symbol links of the last several models\n    for epoch in range(self.epoch, max(0, self.epoch - self.last_model_number), -1):\n        _last_model_pointer = (\n            \"latest.pth\"\n            if epoch == self.epoch\n            else f\"last_{self.epoch - epoch + 1}.pth\"\n        )\n        # create a soft link from the best model pointer to the model file of the current epoch\n        symlink_dst = os.path.join(self.model_save_path, _last_model_pointer)\n        if os.path.islink(symlink_dst) or os.path.exists(symlink_dst):\n            os.unlink(symlink_dst)\n        os.symlink(\n            os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\"), symlink_dst\n        )\n\n    # remove the redundant model files\n    saved_epochs = self.saved_model_epoch.copy()\n    for epoch in saved_epochs:\n        epoch_model_path = os.path.join(self.model_save_path, f\"epoch_{epoch}.pth\")\n        if whether_remove(epoch):\n            # remove the record of epoch in the memory\n            self.saved_model_epoch.remove(epoch)\n\n            # remove the model file if it exists\n            if os.path.exists(epoch_model_path):\n                # ensure that the model to be removed is successfully removed\n                while os.path.exists(epoch_model_path):\n                    os.remove(epoch_model_path)\n\n    # --- Early-Stopping epoch number checking for the early-stopping metric --- #\n    if len(metric_epoch_records[self.early_stopping_metric][\"sorted_epochs\"]) != 0:\n        best_epoch = metric_epoch_records[self.early_stopping_metric][\n            \"sorted_epochs\"\n        ][0]\n        # refresh to 0 or add 1 depending on the comparison the best one and the second best one\n        if best_epoch == self.epoch:\n            epoch_message += f\"{self.early_stopping_metric} of the current epoch no.{self.epoch} is the best so far.\\n\"\n\n            # compare the current performance and the last best performance\n            best_performance = self.best_model_performance[\n                self.early_stopping_metric\n            ][best_epoch]\n            if self.is_better(\n                best_performance,\n                self.last_best_performance,\n                mode=self.early_stopping_mode,\n                threshold=self.early_stopping_threshold,\n            ):\n                epoch_message += (\n                    f\"The early-stopping threshold {self.early_stopping_threshold} is reached, \"\n                    \"so the early-stopping epoch number is refreshed.\\n\"\n                )\n                self.early_stopping_epochs = 0\n                self.last_best_performance = best_performance\n            else:\n                epoch_message += (\n                    f\"The early-stopping threshold {self.early_stopping_threshold} is not reached, \"\n                    \"so the early-stopping epoch number keeps increasing.\\n\"\n                )\n                self.early_stopping_epochs += 1\n        # directly add 1 if the current epoch is not the best\n        else:\n            epoch_message += f\"No improvement of {self.early_stopping_metric} in the current epoch no.{self.epoch}.\\n\"\n            self.early_stopping_epochs += 1\n\n        # report the updated early-stopping epoch number\n        epoch_message += f\"The early-stopping epoch number has been updated to {self.early_stopping_epochs}.\\n\"\n\n    # early-stopping check by the patience\n    early_stopping_flag = False\n    if (\n        self.early_stopping_patience is not None\n        and self.early_stopping_epochs &gt; self.early_stopping_patience\n    ):\n        epoch_message += (\n            f\"The early-stopping patience {self.early_stopping_patience} is reached, \"\n            f\"so the training process stops here.\\n\"\n        )\n        early_stopping_flag = True\n    elif self.early_stopping_patience is None:\n        epoch_message += (\n            \"The early-stopping patience is not set by your exp_cfg, \"\n            \"so the training will continue until num_epochs is reached.\\n\"\n        )\n\n    return epoch_message, early_stopping_flag, metric_pop_flags\n</code></pre>"},{"location":"reference/monitor/#monitor.data_saving_logs","title":"<code>data_saving_logs(proc_id, logs_queue, wait_time=1)</code>","text":"<p>Continuously monitors a queue for log data to save. If logs are available, they are retrieved from the queue and saved in the appropriate format. If the queue is empty, the function pauses for a specified wait time before checking again. In case of any exceptions during this process, a warning is issued and the function continues.</p> <p>Parameters:</p> Name Type Description Default <code>proc_id</code> <code>int</code> <p>The identifier of the current process.</p> required <code>logs_queue</code> <code>Queue</code> <p>The queue containing log data to be saved.</p> required <code>wait_time</code> <code>int</code> <p>The number of seconds to wait before checking the queue again if it is empty. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/monitor.py</code> <pre><code>def data_saving_logs(proc_id: int, logs_queue: Queue, wait_time: int = 1):\n    \"\"\"Continuously monitors a queue for log data to save. If logs are available, they\n    are retrieved from the queue and saved in the appropriate format. If the queue is\n    empty, the function pauses for a specified wait time before checking again. In case\n    of any exceptions during this process, a warning is issued and the function\n    continues.\n\n    Args:\n        proc_id (int):\n            The identifier of the current process.\n        logs_queue (Queue):\n            The queue containing log data to be saved.\n        wait_time (int, optional):\n            The number of seconds to wait before checking the queue again if it is empty. Defaults to 1.\n\n    Returns:\n        None\n    \"\"\"\n    while True:\n        try:\n            # Continually check if the logs_queue has items to be saved\n            if not logs_queue.empty():\n                log = logs_queue.get()\n                save_data_by_format(**log)\n            else:\n                # If the queue is empty, pause for a specified wait time\n                time.sleep(wait_time)\n        except Exception as e:\n            warnings.warn(\n                f\"Process {proc_id} encountered an exception in data_saving_logs(): {e}. Continuing operation.\"\n            )\n</code></pre>"},{"location":"reference/runner/","title":"runner","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/runner/#runner.Runner","title":"<code>Runner</code>","text":"<p>               Bases: <code>object</code></p> <p>Runner is the entrance of our toolkit. This static class is made up of several static functions. The whole pipeline is done by all static functions step by step. The reason why the functions are all static is to prevent Runner from becoming the God class after Inheritance. If you are interested in this topic, please refer to https://wiki.c2.com/?GodClass for more details.</p> <p>In this class, we provide an overridable interface add_parse() that enables users to add more arguments they would like their runners to have.</p> <p>Basically, we don't recommend users to override the other functions in this class for robustness. However, in case that the existing functions cannot meet your research requirements, you can override them in your own runners to fit your specific needs. If it happens, we would appreciate it a lot if you could open an issue and let us know.</p> <p>Wish you have a happy usage journey in this toolkit ^_^!</p> Source code in <code>speechain/runner.py</code> <pre><code>class Runner(object):\n    \"\"\"\n    Runner is the entrance of our toolkit. This static class is made up of several static functions. The\n    whole pipeline is done by all static functions step by step. The reason why the functions are all static is to\n    prevent Runner from becoming the God class after Inheritance.\n    If you are interested in this topic, please refer to https://wiki.c2.com/?GodClass for more details.\n\n    In this class, we provide an overridable interface add_parse() that enables users to add more arguments they\n    would like their runners to have.\n\n    Basically, we don't recommend users to override the other functions in this class for robustness.\n    However, in case that the existing functions cannot meet your research requirements, you can override them in your\n    own runners to fit your specific needs. If it happens, we would appreciate it a lot if you could open an issue\n    and let us know.\n\n    Wish you have a happy usage journey in this toolkit ^_^!\n    \"\"\"\n\n    @classmethod\n    def add_parse(cls, parser: argparse.ArgumentParser) -&gt; argparse.ArgumentParser:\n        \"\"\"The interface where users can add their own arguments.\n\n        Args:\n            parser: argparse.ArgumentParser\n                The name space where you want to add your arguments.\n\n        Returns:\n            parser: argparse.ArgumentParser\n                The name space containing your arguments.\n        \"\"\"\n        return parser\n\n    @classmethod\n    def parse(cls):\n        \"\"\"The static function that outputs all the default arguments for the runner.\n\n        Returns:\n            a Dict containing the key-value pairs of all arguments\n        \"\"\"\n        parser = argparse.ArgumentParser()\n\n        # All-in-one configuration setting\n        parser.add_argument(\n            \"--config\",\n            type=str,\n            # default=None,\n            default=\"recipes/asr/librispeech/train-clean-100/exp_cfg/100-bpe5k_conformer-medium_lr2e-3.yaml\",\n            help=\"The path of the all-in-one experiment configuration file. You can write all the arguments in this \"\n            \"all-in-one file instead of giving them to `runner.py` by command lines.\",\n        )\n\n        # Experimental environment\n        group = parser.add_argument_group(\"Group 1: Calculation and System Backend\")\n        group.add_argument(\n            \"--seed\",\n            type=int,\n            default=0,\n            help=\"Initial random seed for the experiment. (default: 0)\",\n        )\n        group.add_argument(\n            \"--cudnn_enabled\",\n            type=str2bool,\n            default=True,\n            help=\"Whether to activate torch.backends.cudnn. (default: True)\",\n        )\n        group.add_argument(\n            \"--cudnn_benchmark\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to activate torch.backends.cudnn.benchmark. \"\n            \"When True, the process of model training will be speed up and the model performance may improve \"\n            \"somewhat. But your results will become less reproducible. (default: False)\",\n        )\n        group.add_argument(\n            \"--cudnn_deterministic\",\n            type=str2bool,\n            default=True,\n            help=\"Whether to activate torch.backends.cudnn.deterministic. \"\n            \"This will improve the reproducibility of your experiments. (default: True)\",\n        )\n        group.add_argument(\n            \"--train_num_workers\",\n            type=int,\n            default=1,\n            help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n            \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n            \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n            \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n            \"value should be within your machine capability (i.e., the number of CPU cores). \"\n            \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n        )\n        group.add_argument(\n            \"--valid_num_workers\",\n            type=int,\n            default=1,\n            help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n            \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n            \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n            \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n            \"value should be within your machine capability (i.e., the number of CPU cores). \"\n            \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n        )\n        group.add_argument(\n            \"--test_num_workers\",\n            type=int,\n            default=1,\n            help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n            \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n            \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n            \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n            \"value should be within your machine capability (i.e., the number of CPU cores). \"\n            \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n        )\n        group.add_argument(\n            \"--pin_memory\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to activate `pin_memory` for the Dataloader of each epoch. \"\n            \"If True, the pinned memory in the dataloaders will be activated and the data loading will be further \"\n            \"speed up. \"\n            \"pin_memory=True is often used together with non_blocking=True. Note that this combination requires a \"\n            \"large amount of memory and CPU cores. (default: False)\",\n        )\n        group.add_argument(\n            \"--non_blocking\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to activate `non_blocking` when transferring data from the memory to GPUs. \"\n            \"If True, the process of model training will be speed up. \"\n            \"non_blocking=True is often used together with pin_memory=True. Note that this combination requires a \"\n            \"large amount of memory and CPU cores. (default: False)\",\n        )\n\n        # gradient descent related\n        group = parser.add_argument_group(\n            \"Group 2: Gradient Calculation and Back-Propagation\"\n        )\n        group.add_argument(\n            \"--use_amp\",\n            type=str2bool,\n            default=True,\n            help=\"Whether activate AMP (Automatic Mixed Precision) during the back-propagation. \"\n            \"If True, the GPU consumption of your model will be smaller so that you can include more data \"\n            \"instances in a single batch. (default: True)\",\n        )\n        group.add_argument(\n            \"--grad_clip\",\n            type=float,\n            default=5.0,\n            help=\"Gradient clipping threshold during the back-propagation. (default: 5.0)\",\n        )\n        group.add_argument(\n            \"--grad_norm_type\",\n            type=float,\n            default=2.0,\n            help=\"Normalization type used when clipping the gradients. (default: 2.0)\",\n        )\n        group.add_argument(\n            \"--accum_grad\",\n            type=int,\n            default=1,\n            help=\"The number of gradient accumulation steps. \"\n            \"To mimic the gradients calculated by large batches with only a small amount of GPUs, please raise \"\n            \"this argument. \"\n            \"The virtual batch size will become (accum_grad * the actual batch size). \"\n            \"Note that the model trained by accum_grad is not identical to the one actually trained by large \"\n            \"batches because of the different randomness in each training step and the existence of BatchNorm. \"\n            \"(default: 1)\",\n        )\n        group.add_argument(\n            \"--ft_factor\",\n            type=float,\n            default=1.0,\n            help=\"The finetuing factor used to scale down learning rates during the parameter optimization. \"\n            \"If `ft_factor` is smaller than 1.0, the learning rates will be proportionally decreased without \"\n            \"changing its scheduling strategy. Usually, ft_factor could be set from 0.1 to 0.5 depending on your \"\n            \"finetuning scenarios. (default: 1.0)\",\n        )\n\n        # multi-GPU distributed training\n        group = parser.add_argument_group(\"Group 3: Multi-GPU Distribution\")\n        group.add_argument(\n            \"--dist_backend\",\n            default=\"nccl\",\n            type=str,\n            help=\"Communication backend for multi-GPU distribution. \"\n            \"If you are using NVIDIA GPUs, we recommend you set this argument to 'nccl'. (default: nccl)\",\n        )\n        group.add_argument(\n            \"--dist_url\",\n            type=str,\n            default=\"tcp://127.0.0.1\",\n            help=\"Communication URL for multi-GPU distribution. \"\n            \"The default value is 'tcp://127.0.0.1' for single-node distributed training and an idle port will be \"\n            \"automatically selected. \"\n            \"The port number cannot be set manually, which means that the argument 'tcp://127.0.0.1:xxxxx' will \"\n            \"have the same effect with 'tcp://127.0.0.1'. \"\n            \"If you want to train your model on multiple nodes, please set dist_url='env://' \"\n            \"(Note: multi-node model distribution is still in beta). \"\n            \"In this case, env values of 'MASTER_PORT', 'MASTER_ADDR', 'WORLD_SIZE', and 'RANK' are referred in \"\n            \"the command line.\",\n        )\n        group.add_argument(\n            \"--world_size\",\n            default=1,\n            type=int,\n            help=\"The number of nodes for model distribution. \"\n            \"This argument is fixed to 1. Currently, we don't recommend you to modify its value.\"\n            \"If you want to conduct multi-node model distribution, please give `world_size` by `WORLD_SIZE=XXX` \"\n            \"in your terminal (Note: multi-node model distribution is still in beta).\",\n        )\n        group.add_argument(\n            \"--rank\",\n            default=0,\n            type=int,\n            help=\"The global rank of the current node for model distribution. \"\n            \"This argument is fixed to 0. Currently, we don't recommend you to modify its value.\"\n            \"If you want to conduct multi-node model distribution, please give `rank` by `RANK=XXX` in your \"\n            \"terminal (Note: multi-node model distribution is still in beta).\",\n        )\n        group.add_argument(\n            \"--ngpu\",\n            type=int,\n            default=1,\n            help=\"The number of GPUs used to run your experiment. \"\n            \"If ngpu is larger than 1, multi-GPU model distribution will be activated. (default: 1)\",\n        )\n        group.add_argument(\n            \"--gpus\",\n            type=str2none,\n            default=None,\n            help=\"This argument specifies the GPUs used to run your experiment. \"\n            \"If you want to specify multiple GPUs, please give this argument in the form of 'x,x,x' \"\n            \"where different GPUs are separated by a comma (please don't end this argument with ','). \"\n            \"Of course, you could also specify your target GPUs by `CUDA_VISIBLE_DEVICES` in the terminal.\"\n            \"If this argument is not given, the framework will automatically select `ngpu` idle GPUs. \",\n        )\n        group.add_argument(\n            \"--same_proc_seed\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to set the same initial random seed for all the GPU processes in DDP mode. \"\n            \"The different random seeds can prevent model distribution from the process homogeneity, \"\n            \"e.g., different GPU processes may have the same on-the-fly data augmentation strategy \"\n            \"(noise addition, SpecAugment, ...) if they have the same initial random seed. \"\n            \"Note: please set this argument to True if you want to use random data selection for your dataloaders \"\n            \"in the DDP mode. (default: False)\",\n        )\n        group.add_argument(\n            \"--ignore_train_exception\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to ignore the exceptions happening during training and validation. \"\n            \"If set to True, your training would not be interrupted by some nonfatal errors, such as occasional \"\n            \"'RuntimeError: CUDA Out of memory', and etc. (default: False)\",\n        )\n        group.add_argument(\n            \"--ignore_test_exception\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to ignore the exceptions happening during testing. \"\n            \"If set to True, your testing would not be interrupted by some nonfatal errors, such as occasional \"\n            \"'RuntimeError: CUDA Out of memory', and etc. (default: False)\",\n        )\n        group.add_argument(\n            \"--enable_syncbatchnorm\",\n            type=str2bool,\n            default=True,\n            help=\"Whether to process the model by 'torch.nn.SyncBatchNorm.convert_sync_batchnorm' for multi-GPU \"\n            \"distributed training. Sometimes your training may be stuck at some points or terminate without being \"\n            \"notified of any errors in the multi-GPU distributed mode. If that happens, you can disable \"\n            \"SyncBatchNorm and debug your codes. (default: True)\",\n        )\n\n        # Training monitoring\n        group = parser.add_argument_group(\"Group 4: Model Training\")\n        group.add_argument(\n            \"--train_result_path\",\n            type=str,\n            default=None,\n            help=\"Where to place all the experiment folder that contains all the result files. \"\n            \"If not given, `train_result_path` wil be automatically initialized by your input `config`. \"\n            \"For example, if your input `config` is \"\n            \"{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/XXXXX.yaml, your `train_result_path` \"\n            \"will be automatically initialized to `{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp/`.\"\n            \"(default: None)\",\n        )\n        group.add_argument(\n            \"--attach_config_folder_to_path\",\n            type=str2bool,\n            default=True,\n            help=\"Whether to attach an additional folder named by your input `--config` at the end of your input \"\n            \"`train_result_path`. (default: True)\",\n        )\n        group.add_argument(\n            \"--train\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to go through the model training branch. (default: False)\",\n        )\n        group.add_argument(\n            \"--dry_run\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to turn on the dry-running mode. \"\n            \"In this mode, only the data loading will be done to see its speed and robustness. \"\n            \"Model calculation and parameter optimization will be skipped. (default: False)\",\n        )\n        group.add_argument(\n            \"--no_optim\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to turn on the no-optimization mode. \"\n            \"In this mode, only the data loading and model calculation will be done to see their speed, \"\n            \"robustness, and memory consumption. (default: False) \"\n            \"(Note: 'dry_run' has the higher priority than 'no_optim'. It means that the model calculation will \"\n            \"be skipped if you give both '--dry_run True' and '--no_optim True'.) \",\n        )\n        group.add_argument(\n            \"--resume\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to resume your model training or testing experiment from the checkpoints. \"\n            \"If True, there must be .pth checkpoint files of your existing experiment in `train_result_path` or \"\n            \"`test_result_path`. This argument is shared by the training and testing branches. (default: False)\",\n        )\n        group.add_argument(\n            \"--start_epoch\",\n            type=int,\n            default=1,\n            help=\"The starting epoch of your experiments. This argument will be automatically initialized by your \"\n            \"checkpoint files if `--resume` is given. (default: 1)\",\n        )\n        group.add_argument(\n            \"--num_epochs\",\n            type=int,\n            default=1000,\n            help=\"The maximum number of training epochs of your experiments. (default: 1000)\",\n        )\n        group.add_argument(\n            \"--valid_per_epochs\",\n            type=int,\n            default=1,\n            help=\"The interval of going through the validation phase during training. \"\n            \"If not given, validation will be done right after parameter optimization in each epoch. (default: 1)\",\n        )\n        group.add_argument(\n            \"--report_per_steps\",\n            type=int,\n            default=0,\n            help=\"The interval of reporting step information logs during model training or testing. \"\n            \"Positive integers mean the absolute reporting intervals that a step report will be made after each \"\n            \"'report_per_steps' steps; \"\n            \"Negative integers mean the relative reporting intervals that there will be -'report_per_steps' \"\n            \"reports in each epoch. \"\n            \"If not given, there will be default 10 reports in each epoch. \",\n        )\n        group.add_argument(\n            \"--best_model_selection\",\n            type=str2list,\n            default=None,\n            help=\"The ways of selecting the best models. This argument should be given as a list of quad-tuples, i.e., \"\n            \"('metric_group', 'metric_name', 'metric_mode', 'model_number'). \"\n            \"'metric_group' can be either 'train' or 'valid' which indicates the group the metric belongs to; \"\n            \"'metric_name' is the name of the metric you select; \"\n            \"'metric_mode' can be either 'min' or 'max' which indicates how to select the models by this metric; \"\n            \"'model_number' indicates how many best models will be saved by this metric. \"\n            \"Note: the metric of the first tuple in the list will be used to do early-stopping for model training.\"\n            \"(default: None)\",\n        )\n        group.add_argument(\n            \"--early_stopping_patience\",\n            type=int,\n            default=None,\n            help=\"The maximum number of epochs when the model doesn't improve its performance before stopping the \"\n            \"model training. If not given, early-stopping will not be adapted. (default: None)\",\n        )\n        group.add_argument(\n            \"--early_stopping_threshold\",\n            type=float,\n            default=0.001,\n            help=\"The threshold to refresh the early-stopping status in the monitor during model training. \"\n            \"Positive float numbers in (0.0, 1.0) mean the relative threshold over the current best performance. \"\n            \"Negative float numbers main the absolute threshold over the current best performance. \"\n            \"early_stopping_threshold=0 means no early-stopping threshold is applied to the current best \"\n            \"performance when deciding whether to refresh the status. (default: 0.005)\",\n        )\n        group.add_argument(\n            \"--last_model_number\",\n            type=int,\n            default=1,\n            help=\"The number of models saved for the last several epochs. \"\n            \"This argument cannot be lower than 1 otherwise the training will not be able to resume. \"\n            \"(default: 1)\",\n        )\n\n        # Training Snapshotting\n        group = parser.add_argument_group(\n            \"Group 5: Real-time Model Visualization Snapshotting\"\n        )\n        group.add_argument(\n            \"--monitor_snapshot_conf\",\n            type=str2dict,\n            default=dict(),\n            help=\"The configuration given to `matploblib.plot()` in `{SPEECHAIN_ROOT/speechain/snapshooter.py}` to \"\n            \"plot curve figures for real-time model visualization during model training. \"\n            \"This argument should be given in the form of a Dict. (default: an empty Dict)\",\n        )\n        group.add_argument(\n            \"--visual_snapshot_number\",\n            type=int,\n            default=0,\n            help=\"The number of the validation data instances used to make snapshots made during model visualization. \"\n            \"This argument should be smaller than the number of your validation data instances. \"\n            \"(default: 0)\",\n        )\n        group.add_argument(\n            \"--visual_snapshot_interval\",\n            type=int,\n            default=5,\n            help=\"The snapshotting interval of model visualization during model training. \"\n            \"This argument should be a positive integer which means that model visualization will be done once \"\n            \"in every `visual_snapshot_interval` epochs. (default: 5)\",\n        )\n\n        # Testing\n        group = parser.add_argument_group(\"Group 6: Model Testing\")\n        group.add_argument(\n            \"--test_result_path\",\n            type=str,\n            default=None,\n            help=\"Where to place all the result files generated during model testing. \"\n            \"If not given, `test_result_path` wil be automatically initialized by your input `train_result_path` \"\n            \"and `test_model`. For example, if your `train_result_path` is \"\n            \"`{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp`, and `test_model` is `MMMMM`, \"\n            \"then your `test_result_path` will be automatically initialized to \"\n            \"`{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp/XXXXX/MMMMM/` where 'XXXXX' is the name of \"\n            \"your configuration file given by `--config`.\",\n        )\n        group.add_argument(\n            \"--test\",\n            type=str2bool,\n            default=False,\n            help=\"Whether to go through the model testing branch. (default: False)\",\n        )\n        group.add_argument(\n            \"--test_model\",\n            type=str2list,\n            default=None,\n            help=\"The names of the model you want to evaluate during model testing. \"\n            \"If given, `{train_result_path}/XXXXX/model/{test_model}.pth` will be used to initialize the parameters \"\n            \"of the Model object. If you only want to evaluate multiple models in one job, please give the \"\n            \"strings of their names in a List. (default: None)\",\n        )\n        group.add_argument(\n            \"--attach_model_folder_when_test\",\n            type=str2bool,\n            default=True,\n            help=\"Whether to attach an additional sub-folder named by your input `--test_model` in the testing \"\n            \"result folder. (default: True)\",\n        )\n        group.add_argument(\n            \"--bad_cases_selection\",\n            type=str2list,\n            default=None,\n            help=\"The selection methods of the top-N bad cases during model testing. \"\n            \"This argument should be given as a list of tri-tuples \"\n            \"('selection_metric', 'selection_mode', 'case_number'). \"\n            \"For example, ('wer', 'max', 50) means 50 testing waveforms with the largest WER will be selected. \"\n            \"Multiple tuples can be given to present different sets of top-n bad cases. (default: None)\",\n        )\n        group.add_argument(\n            \"--saving_proc_num\",\n            type=int,\n            default=1,\n            help=\"The number of daemon processes used to save data generated during testing to the disk. (default: 1)\",\n        )\n\n        # Experiment configuration\n        group = parser.add_argument_group(\n            \"Group 7: Experiment .yaml Configuration File\"\n        )\n        group.add_argument(\n            \"--data_cfg\",\n            type=str2dict,\n            default=None,\n            help=\"The path of the configuration file for data loading and batching. \"\n            \"This argument is required for both model training and testing.\",\n        )\n        group.add_argument(\n            \"--train_cfg\",\n            type=str2dict,\n            default=None,\n            help=\"The path of the configuration file for model construction and parameter optimization. \"\n            \"This argument is required for both model training (both 'model' and 'optim_sche' need to be given) \"\n            \"and testing (only 'model' needs to be given).\",\n        )\n        group.add_argument(\n            \"--infer_cfg\",\n            type=str2dict,\n            default=None,\n            help=\"The configuration file for model inference during model testing. \"\n            \"This argument is required for model testing.\"\n            \"For more details about how to give infer_cfg, please refer to the handbook.md. (default: None)\",\n        )\n\n        # Add customized arguments if needed\n        parser = cls.add_parse(parser)\n        return parser.parse_args()\n\n    @classmethod\n    def build_iterators(\n        cls, data_cfg: Dict[str, Dict], args: argparse.Namespace\n    ) -&gt; Dict[str, Dict[str, Iterator] or Iterator]:\n        \"\"\"This static function builds all iterators used in the experiment. The\n        configuration of iterators is given in your specified 'data_cfg'.\n\n        The iterators are returned as a dictionary where the first-level keys indicate different iterator groups:\n        'train', 'valid', and 'test'. The second-level keys in each group indicates the iterators belonging to the\n        group. In the value of each second-level key, there are two third-level keys: 'type' and 'conf'. 'type'\n        indicates the iterator type and 'conf' indicates the iterator configuration. For more details, please refer\n        to ./speechain/iterator/README.md\n\n        Args:\n            data_cfg: Dict\n                The dictionary containing all the information to initialize the iterators\n            args: argparse.Namespace\n                The arguments of the runner in this experiment.\n\n        Returns:\n            The dictionary of the iterators of all groups (train, valid, test).\n        \"\"\"\n\n        def recur_iterator_init(_data_cfg: Dict, _dset: str):\n            leaf_flag = len(_data_cfg) == 2 and (\n                \"type\" in _data_cfg.keys() and \"conf\" in _data_cfg.keys()\n            )\n            if leaf_flag:\n                iterator_class = import_class(\"speechain.iterator.\" + _data_cfg[\"type\"])\n                return iterator_class(\n                    seed=args.seed,\n                    ngpu=args.ngpu,\n                    num_workers=getattr(args, f\"{_dset}_num_workers\"),\n                    pin_memory=args.pin_memory,\n                    distributed=args.distributed,\n                    **_data_cfg[\"conf\"],\n                )\n            else:\n                return {\n                    key: recur_iterator_init(value, _dset)\n                    for key, value in _data_cfg.items()\n                }\n\n        def recur_batch_num_init(_iterators: Dict or Iterator):\n            leaf_flag = isinstance(_iterators, Iterator)\n            if leaf_flag:\n                return len(_iterators)\n            else:\n                sub_leaf_flag = sum(\n                    [isinstance(value, Iterator) for value in _iterators.values()]\n                ) == len(_iterators)\n                if sub_leaf_flag:\n                    return [len(value) for value in _iterators.values()]\n                else:\n                    return {\n                        key: recur_batch_num_init(value)\n                        for key, value in _iterators.items()\n                    }\n\n        def flatten_dict_to_list(_input: Dict or int or List[int]):\n            leaf_flag = isinstance(_input, (int, List))\n            if leaf_flag:\n                return [_input] if isinstance(_input, int) else _input\n            else:\n                _output = []\n                for value in _input.values():\n                    _output += flatten_dict_to_list(value)\n                return _output\n\n        # get the target groups of the current experiment\n        if args.train:\n            assert (\n                \"train\" in data_cfg.keys() and \"valid\" in data_cfg.keys()\n            ), \"If args.train is set to True, please give 'train' and 'valid' as first-level keys of data_cfg.\"\n            dset_keys = [\"train\", \"valid\"]\n        elif args.test:\n            assert (\n                \"test\" in data_cfg.keys()\n            ), \"If args.test is set to True, please give 'test' as first-level keys of data_cfg.\"\n            dset_keys = [\"test\"]\n        else:\n            raise RuntimeError(\"Please set either args.train or args.test to True!\")\n\n        # recursively initialize all the iterators in the Dict\n        mode = \"train\" if args.train else \"test\"\n        iterators = {\n            dset: recur_iterator_init(data_cfg[dset], dset) for dset in dset_keys\n        }\n        batch_nums = recur_batch_num_init(iterators[mode])\n\n        # set the relative reporting interval during training or testing\n        if args.report_per_steps &lt;= 0:\n            _reports_per_epoch = (\n                10 if args.report_per_steps == 0 else int(-args.report_per_steps)\n            )\n            args.report_per_steps = (\n                min(flatten_dict_to_list(batch_nums)) // _reports_per_epoch\n            )\n        # check the absolute reporting interval during training and testing\n        else:\n            assert int(args.report_per_steps) &lt;= min(\n                flatten_dict_to_list(batch_nums)\n            ), (\n                f\"If args.report_per_steps is given as a positive integer, \"\n                f\"it should be smaller than the minimal {mode} batch number ({min(batch_nums)}). \"\n                f\"But got report_per_steps={int(args.report_per_steps)}!\"\n            )\n\n            # in case that report_per_steps is given as a float number\n            args.report_per_steps = int(args.report_per_steps)\n\n        return iterators\n\n    @classmethod\n    def build_model(\n        cls, model_cfg: Dict[str, Any], args: argparse.Namespace, device: torch.device\n    ) -&gt; Model:\n        \"\"\"This static function builds the model used in the experiment. The\n        configuration of the model is given in the value of the 'model' key in your\n        specified 'model_cfg'.\n\n        Args:\n            model_cfg: Dict\n                Model Configuration\n            args:\n            device:\n\n        Returns:\n            The target Model object initialized by your given configuration\n        \"\"\"\n        assert \"model_type\" in model_cfg.keys(), \"Please specify the model_type!\"\n        assert \"module_conf\" in model_cfg.keys(), \"Please specify the module_conf!\"\n        if \"criterion_conf\" not in model_cfg.keys():\n            model_cfg[\"criterion_conf\"] = None\n\n        model_class = import_class(\"speechain.model.\" + model_cfg[\"model_type\"])\n        return model_class(\n            model_conf=(\n                model_cfg[\"model_conf\"] if \"model_conf\" in model_cfg.keys() else dict()\n            ),\n            module_conf=model_cfg[\"module_conf\"],\n            criterion_conf=model_cfg[\"criterion_conf\"],\n            device=device,\n            result_path=args.train_result_path,\n            non_blocking=args.non_blocking,\n            distributed=args.distributed,\n        ).cuda(device=device)\n\n    @classmethod\n    def build_optim_sches(\n        cls, model: Model, optim_sche_cfg: Dict[str, Any], args: argparse.Namespace\n    ) -&gt; Dict[str, OptimScheduler] or OptimScheduler:\n        \"\"\"This static function builds the OptimSchedulers used in the pipeline. The\n        configuration of the OptimSchedulers is given in the value of 'optim_sches' key\n        in your specified 'train_cfg'.\n\n        This function must be done after DDP wrapping because we need to make sure that the model parameters received\n        by the optimizer in each process are identical. With the identical model parameters, it's safe to consider that\n        the optimizer parameters are also identical.\n\n        Args:\n            model: Model\n                The initialized model.\n            optim_sche_cfg: Dict\n                OptimScheduler Configuration\n            args: argparse.Namespace\n                The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.\n\n        Returns:\n            The Dict of the initialized OptimSchedulers.\n        \"\"\"\n        # single-optimizer scenario\n        if len(optim_sche_cfg) == 2 and (\n            \"type\" in optim_sche_cfg.keys() and \"conf\" in optim_sche_cfg.keys()\n        ):\n            optim_sche_cfg = dict(main=optim_sche_cfg)\n\n        optim_sches = dict()\n        for name, optim_sche in optim_sche_cfg.items():\n            optim_sche_class = import_class(\n                \"speechain.optim_sche.\" + optim_sche[\"type\"]\n            )\n            optim_sches[name] = optim_sche_class(\n                model=model,\n                distributed=args.distributed,\n                use_amp=args.use_amp,\n                accum_grad=args.accum_grad,\n                ft_factor=args.ft_factor,\n                grad_clip=args.grad_clip,\n                grad_norm_type=args.grad_norm_type,\n                **optim_sche[\"conf\"],\n            )\n\n        # multi-optimizer scenario\n        if len(optim_sches) &gt; 1:\n            # adjust whether there are parameter overlapping among updated_modules of all the OptimSchedulers\n            is_all_para = [o.updated_modules is None for o in optim_sches.values()]\n            # updated_modules of all the OptimSchedulers cannot be None at the same time\n            if sum(is_all_para) == len(is_all_para):\n                raise RuntimeError\n            else:\n                # collect the updated_modules of all the OptimScheduler\n                para_list = [o.updated_modules for o in optim_sches.values()]\n                # adjust whether there are redundant keys\n                para_set = set(para_list)\n                # there is parameter overlapping if there are redundant keys\n                if len(para_set) != len(para_list):\n                    raise RuntimeError\n\n        # resuming from an existing checkpoint\n        if args.resume:\n            try:\n                checkpoint = torch.load(\n                    os.path.join(args.train_result_path, \"checkpoint.pth\"),\n                    map_location=model.device,\n                )\n                for name in optim_sches.keys():\n                    optim_sches[name].load_state_dict(checkpoint[\"optim_sches\"][name])\n            except FileNotFoundError:\n                print(\n                    f\"No checkpoint is found in {args.train_result_path}. \"\n                    f\"The training process will start from scratch.\"\n                )\n\n        return optim_sches\n\n    @classmethod\n    def resume(\n        cls, args: argparse.Namespace, model: Model, monitor: TrainValidMonitor\n    ) -&gt; int:\n        \"\"\"Load the model parameters to the current process. This operation is necessary\n        in our toolkit because we need to make sure that the models in all the processes\n        have the same buffer and parameter tensors.\n\n        Args:\n            args: argparse.Namespace\n                The input arguments.\n            model: Model\n                The model to be trained.\n            monitor: TrainValidMonitor\n                The train-valid monitor used to monitor the training phase\n\n        Returns:\n            The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting\n            epoch will be loaded from the checkpoint; otherwise, 1 will be returned.\n        \"\"\"\n        # start the training from the existing checkpoint\n        if args.resume:\n            # load the existing checkpoint\n            checkpoint = torch.load(\n                os.path.join(args.train_result_path, \"checkpoint.pth\"),\n                map_location=model.device,\n            )\n            # load the latest training epoch\n            start_epoch = checkpoint[\"start_epoch\"]\n            # for compatibility with old versions\n            if \"latest_model\" in checkpoint.keys():\n                model.load_state_dict(checkpoint[\"latest_model\"])\n            else:\n                model.load_state_dict(\n                    torch.load(\n                        os.path.join(args.train_result_path, \"models\", \"latest.pth\"),\n                        map_location=model.device,\n                    )\n                )\n\n            # loading the monitor\n            if monitor is not None:\n                # for compatibility with old versions\n                if \"monitor\" not in checkpoint.keys():\n                    monitor.load_state_dict(\n                        dict(\n                            train_monitor=checkpoint[\"train_monitor\"],\n                            valid_monitor=checkpoint[\"valid_monitor\"],\n                        )\n                    )\n                else:\n                    monitor.load_state_dict(checkpoint[\"monitor\"])\n                # info logging\n                monitor.logger.info(\n                    f\"The training process resumes from the epoch no.{start_epoch}.\"\n                )\n\n        # start the training from scratch\n        else:\n            start_epoch = 1\n\n        return start_epoch\n\n    @classmethod\n    def dict_transform(cls, src_dict, transform_func):\n        \"\"\"\n\n        Args:\n            src_dict:\n            transform_func:\n\n        Returns:\n\n        \"\"\"\n        # Multi-dataloader\n        if isinstance(src_dict, Dict):\n            return {key: transform_func(value) for key, value in src_dict.items()}\n        # Single-dataloader\n        else:\n            return transform_func(src_dict)\n\n    @classmethod\n    def measure_time(cls, monitor: Monitor):\n        @contextmanager\n        def empty_context(names=None):\n            yield\n\n        if monitor is None:\n            return empty_context\n        else:\n            return monitor.measure_time\n\n    @classmethod\n    def is_empty_batch(cls, input_batch: Dict):\n        # Single-dataloader case\n        if len(input_batch) == 0:\n            return True\n        # Multi-dataloader case\n        else:\n            for value in input_batch.values():\n                if isinstance(value, Dict):\n                    for sub_value in value.values():\n                        if len(sub_value) == 0:\n                            return True\n                    return False\n\n    @classmethod\n    def train(\n        cls,\n        args: argparse.Namespace,\n        data_cfg: Dict,\n        iterators: Dict[str, Dict[str, Iterator]] or Dict[str, Iterator],\n        model: Model,\n        optim_sches: Dict[str, OptimScheduler] or OptimScheduler,\n        logger,\n        monitor: TrainValidMonitor,\n    ):\n        \"\"\"\n\n        Args:\n            args: argparse.Namespace\n                The input arguments.\n            data_cfg: Dict\n                The data loading configuration. Used to initialize the iterator for model visualization.\n            iterators: Dict\n                The dictionary that contains all the iterators for training and validation.\n            model: Model\n                The model to be trained.\n            optim_sches: Dict\n                The dictionary that contains all the OptimSchedulers used to update the model parameters.\n            logger:\n\n            monitor: TrainValidMonitor\n                The wrapper class for a training monitor and a validation monitor.\n                The training monitor controls the training process of the model and generates the real-time logging\n                information.\n                The validation monitor controls the validation process of the model and generates the real-time\n                logging information.\n\n        \"\"\"\n        assert (\n            args.start_epoch &lt;= args.num_epochs\n        ), \"Your given start_epoch is larger than your given num_epochs!\"\n\n        # --- checking the data lengths of all training iterators --- #\n        # multiple dataloaders scenario\n        if isinstance(iterators[\"train\"], Dict):\n            train_batch_nums = set(\n                [len(iterator) for iterator in iterators[\"train\"].values()]\n            )\n            min_train_batch_num = min(train_batch_nums)\n            if len(train_batch_nums) != 1:\n                logger.info(\n                    f\"Your training iterators have different batch numbers: {train_batch_nums}. \"\n                    f\"The actual batch number during training is set to {min_train_batch_num}!\"\n                )\n        # single dataloader scenario\n        elif isinstance(iterators[\"train\"], Iterator):\n            min_train_batch_num = len(iterators[\"train\"])\n        else:\n            raise RuntimeError(\"Please don't nest data_cfg['train'] more than twice!\")\n\n        # --- checking the data lengths of all validation iterators --- #\n        # multiple dataloaders scenario\n        if isinstance(iterators[\"valid\"], Dict):\n            valid_batch_nums = set(\n                [len(iterator) for iterator in iterators[\"valid\"].values()]\n            )\n            min_valid_batch_num = min(valid_batch_nums)\n            if len(valid_batch_nums) != 1:\n                logger.info(\n                    f\"Your validation iterators have different batch numbers: {valid_batch_nums}. \"\n                    f\"The actual batch number during validation is set to {min_valid_batch_num}!\"\n                )\n        # single dataloader scenario\n        elif isinstance(iterators[\"valid\"], Iterator):\n            min_valid_batch_num = len(iterators[\"valid\"])\n        else:\n            raise RuntimeError(\"Please don't nest data_cfg['valid'] more than twice!\")\n\n        # synchronize the batch numbers across all the distributed processes\n        if args.distributed:\n            _world_size = torch.distributed.get_world_size()\n            # make sure that all processes have the same number of training steps\n            _all_batch_num = torch.LongTensor([0 for _ in range(_world_size)]).cuda(\n                model.device\n            )\n            torch.distributed.all_gather_into_tensor(\n                _all_batch_num,\n                torch.LongTensor([min_train_batch_num]).cuda(model.device),\n            )\n            min_train_batch_num = _all_batch_num.min().item()\n\n            # make sure that all processes have the same number of validation steps\n            _all_batch_num = torch.LongTensor([0 for _ in range(_world_size)]).cuda(\n                model.device\n            )\n            torch.distributed.all_gather_into_tensor(\n                _all_batch_num,\n                torch.LongTensor([min_valid_batch_num]).cuda(model.device),\n            )\n            min_valid_batch_num = _all_batch_num.min().item()\n\n        # --- Initialize the iterator for model visualization --- #\n        if args.visual_snapshot_number &gt; 0:\n            if not args.distributed or args.rank == 0:\n                _valid_keys = list(data_cfg[\"valid\"].keys())\n                if len(_valid_keys) == 2 and (\n                    \"type\" in _valid_keys and \"conf\" in _valid_keys\n                ):\n                    visual_iterator = Iterator(\n                        dataset_type=data_cfg[\"valid\"][\"conf\"][\"dataset_type\"],\n                        dataset_conf=data_cfg[\"valid\"][\"conf\"][\"dataset_conf\"],\n                        batches_per_epoch=args.visual_snapshot_number,\n                        shuffle=False,\n                        ngpu=1,\n                        distributed=False,\n                        is_descending=None,\n                    )\n                else:\n                    visual_domain = _valid_keys[0]\n                    logger.info(\n                        \"There are multiple sub-Dict in your given data_cfg['valid']. \"\n                        f\"The one named {visual_domain} is used to initialize the visualization iterator.\"\n                    )\n                    visual_iterator = {\n                        visual_domain: Iterator(\n                            dataset_type=data_cfg[\"valid\"][visual_domain][\"conf\"][\n                                \"dataset_type\"\n                            ],\n                            dataset_conf=data_cfg[\"valid\"][visual_domain][\"conf\"][\n                                \"dataset_conf\"\n                            ],\n                            batches_per_epoch=args.visual_snapshot_number,\n                            shuffle=False,\n                            ngpu=1,\n                            distributed=False,\n                            is_descending=None,\n                        )\n                    }\n            else:\n                visual_iterator = None\n        else:\n            visual_iterator = None\n\n        # loop each epoch until the end\n        for epoch in range(args.start_epoch, args.num_epochs + 1):\n            # update the random seeds for the current epoch to keep in line with the dataloaders\n            cls.set_random_seeds(args.seed + epoch)\n            # start the current training epoch\n            if monitor is not None:\n                monitor.start_train_epoch(epoch)\n            # initialize all the training dataloaders\n            data_loaders = cls.dict_transform(\n                iterators[\"train\"], lambda x: iter(x.build_loader(epoch))\n            )\n\n            # --- Training Stage --- #\n            model.train()\n            # loop all the training batches\n            for step in range(1, min_train_batch_num + 1):\n                step_num = int(step + (epoch - 1) * min_train_batch_num)\n\n                # --- data loading part --- #\n                with cls.measure_time(\n                    None if monitor is None else monitor.train_monitor\n                )(\"data_load_time\"):\n                    train_batch = cls.dict_transform(\n                        src_dict=data_loaders, transform_func=next\n                    )\n                    # single-GPU case, directly skip the current step when meeting an empty batch\n                    if not args.distributed:\n                        # skip the empty validation batch\n                        if cls.is_empty_batch(train_batch):\n                            continue\n\n                    # multi-GPU case, scatter the skip flag to all nodes\n                    else:\n                        skip_flag_list = torch.LongTensor(\n                            [False for _ in range(torch.distributed.get_world_size())]\n                        ).cuda(model.device)\n                        if cls.is_empty_batch(train_batch):\n                            skip_flag = torch.LongTensor([True]).cuda(model.device)\n                        else:\n                            skip_flag = torch.LongTensor([False]).cuda(model.device)\n                        # as long as one node meets an empty batch, all nodes will simultaneously skip the current step\n                        torch.distributed.all_gather_into_tensor(\n                            skip_flag_list, skip_flag\n                        )\n                        if skip_flag_list.sum() &gt;= 1:\n                            continue\n\n                # forward the batch to get the training criteria and optimize the model\n                train_metrics, optim_lr = None, None\n                # whether to skip the model forward part and model optimization part\n                if not args.dry_run:\n                    # --- model forward part --- #\n                    with autocast(enabled=args.use_amp):\n                        with cls.measure_time(\n                            None if monitor is None else monitor.train_monitor\n                        )(\"model_forward_time\"):\n                            try:\n                                losses, train_metrics = model(\n                                    batch_data=train_batch, epoch=epoch\n                                )\n                            except Exception as e:\n                                if args.ignore_train_exception:\n                                    warnings.warn(\n                                        f\"Rank no.{args.rank} meets error {e}! \"\n                                        f\"no.{step} training step will be skipped!\"\n                                    )\n                                    if logger is not None:\n                                        logger.warning(\n                                            f\"Rank no.{args.rank} meets error {e}! \"\n                                            f\"no.{step} training step will be skipped!\"\n                                        )\n                                    continue\n                                else:\n                                    raise e\n\n                    # whether to skip the model optimization part\n                    if not args.no_optim:\n                        # --- loss backward and optimization part --- #\n                        optim_lr = dict()\n                        for name, optim_sche in optim_sches.items():\n                            optim_sche.step(\n                                losses=losses,\n                                time_func=cls.measure_time(\n                                    None if monitor is None else monitor.train_monitor\n                                ),\n                                optim_name=name,\n                                step_num=step_num,\n                                epoch_num=epoch,\n                                logger=logger,\n                            )\n                            optim_lr[name] = optim_sche.get_lr()\n\n                # log the information of the current training step\n                if monitor is not None:\n                    monitor.train_step(\n                        step_num=step, optim_lr=optim_lr, train_metrics=train_metrics\n                    )\n\n            # finish the current training epoch\n            if monitor is not None:\n                monitor.finish_train_epoch()\n\n            # --- Validation Stage --- #\n            # start the validation part of the current epoch\n            if monitor is not None:\n                monitor.start_valid_epoch(epoch)\n\n            valid_flag = (epoch - 1) % args.valid_per_epochs == 0\n            if valid_flag:\n                # initialize all the validation dataloaders\n                data_loaders = cls.dict_transform(\n                    iterators[\"valid\"], lambda x: iter(x.build_loader(epoch))\n                )\n\n                # make sure that no gradient appears during validation\n                model.eval()\n                with torch.inference_mode():\n                    # loop all validation batches\n                    for step in range(min_valid_batch_num):\n                        # --- data loading part --- #\n                        with cls.measure_time(\n                            None if monitor is None else monitor.valid_monitor\n                        )(\"data_load_time\"):\n                            valid_batch = cls.dict_transform(\n                                src_dict=data_loaders, transform_func=next\n                            )\n                            # single-GPU case, directly skip the current step when meeting an empty batch\n                            if not args.distributed:\n                                # skip the empty validation batch\n                                if cls.is_empty_batch(valid_batch):\n                                    continue\n                            # multi-GPU case, scatter the skip flag to all nodes\n                            else:\n                                skip_flag_list = torch.LongTensor(\n                                    [\n                                        False\n                                        for _ in range(\n                                            torch.distributed.get_world_size()\n                                        )\n                                    ]\n                                ).cuda(model.device)\n                                if cls.is_empty_batch(valid_batch):\n                                    skip_flag = torch.LongTensor([True]).cuda(\n                                        model.device\n                                    )\n                                else:\n                                    skip_flag = torch.LongTensor([False]).cuda(\n                                        model.device\n                                    )\n                                # as long as one node meets an empty batch,\n                                # all nodes will skip the current step at the same time\n                                torch.distributed.all_gather_into_tensor(\n                                    skip_flag_list, skip_flag\n                                )\n                                if skip_flag_list.sum() &gt;= 1:\n                                    continue\n\n                        # forward the batch to get the validation criteria\n                        valid_metrics = None\n                        # whether to skip the model forward part\n                        if not args.dry_run:\n                            # --- model forward part --- #\n                            # with autocast(enabled=args.use_amp) is not used here for accurate validation\n                            with cls.measure_time(\n                                None if monitor is None else monitor.valid_monitor\n                            )(\"model_forward_time\"):\n                                try:\n                                    valid_metrics = model(batch_data=valid_batch)\n                                except Exception as e:\n                                    if args.ignore_train_exception:\n                                        warnings.warn(\n                                            f\"Rank no.{args.rank} meets error {e}! \"\n                                            f\"no.{step} validation step will be skipped!\"\n                                        )\n                                        if logger is not None:\n                                            logger.warning(\n                                                f\"Rank no.{args.rank} meets error {e}! \"\n                                                f\"no.{step} validation step will be skipped!\"\n                                            )\n                                        continue\n                                    else:\n                                        raise e\n\n                        # no step log for the validation step\n                        if monitor is not None:\n                            monitor.valid_step(valid_metrics=valid_metrics)\n\n            # --- Visualization Stage --- #\n            if (\n                args.visual_snapshot_number &gt; 0\n                and (epoch - 1) % args.visual_snapshot_interval == 0\n            ):\n                # make sure that all processes go through the validation phase smoothly\n                if visual_iterator is not None:\n                    if not isinstance(visual_iterator, Dict):\n                        visual_domain = None\n                        visual_dataloader = visual_iterator.build_loader()\n                        visual_indices = visual_iterator.get_batch_indices()\n                    else:\n                        visual_domain = list(visual_iterator.keys())[0]\n                        visual_dataloader = visual_iterator[\n                            visual_domain\n                        ].build_loader()\n                        visual_indices = visual_iterator[\n                            visual_domain\n                        ].get_batch_indices()\n\n                    # make sure that no gradient appears during validation\n                    model.eval()\n                    visual_dataloader = iter(visual_dataloader)\n                    with torch.inference_mode():\n                        for step in range(args.visual_snapshot_number):\n                            visual_sample = next(visual_dataloader)\n                            if cls.is_empty_batch(visual_sample):\n                                logger.info(\n                                    f\"The visual sample {visual_indices[step][0]} is empty, \"\n                                    f\"so its visualization is skipped!\"\n                                )\n                                continue\n                            # feed the current sample to the model\n                            monitor.valid_model_snapshot(\n                                epoch=epoch,\n                                domain=visual_domain,\n                                sample_index=visual_indices[step][0],\n                                used_sample=visual_sample,\n                            )\n                # synchronize all the GPU processes at the end of the visualization stage\n                if args.distributed:\n                    torch.distributed.barrier()\n\n            # finish_valid_epoch() should be called before checkpoint saving\n            finish_valid_flag = None\n            if not args.distributed or args.rank == 0:\n                finish_valid_flag = monitor.finish_valid_epoch(\n                    valid_flag=valid_flag, valid_per_epochs=args.valid_per_epochs\n                )\n\n            # store the checkpoint of the current epoch for later resuming\n            if not args.distributed or args.rank == 0:\n                if not args.dry_run and not args.no_optim:\n                    torch.save(\n                        {\n                            \"start_epoch\": epoch + 1,\n                            \"latest_model\": (\n                                model.state_dict()\n                                if not args.distributed\n                                else model.module.state_dict()\n                            ),\n                            \"monitor\": monitor.state_dict(),\n                            \"optim_sches\": {\n                                name: o.state_dict() for name, o in optim_sches.items()\n                            },\n                        },\n                        os.path.join(args.train_result_path, \"checkpoint.pth\"),\n                    )\n\n            # early-stopping checking for single-GPU\n            if not args.distributed and finish_valid_flag:\n                break\n\n            # early-stopping checking for multi-GPU\n            if args.distributed:\n                stop_flag = torch.BoolTensor([False]).cuda(model.device)\n                flag_list = None\n\n                if args.rank == 0:\n                    if finish_valid_flag:\n                        stop_flag = torch.BoolTensor([True]).cuda(model.device)\n                    flag_list = [\n                        stop_flag for _ in range(torch.distributed.get_world_size())\n                    ]\n\n                torch.distributed.scatter(stop_flag, flag_list)\n                if stop_flag.item():\n                    break\n\n        # check whether all the monitor queues become empty in every minute\n        if not args.distributed or args.rank == 0:\n            monitor.wait_empty_queues()\n\n        # synchronize all the GPU processes at the end\n        if args.distributed:\n            torch.distributed.barrier()\n\n    @classmethod\n    def test(\n        cls,\n        args: argparse.Namespace,\n        test_model: str,\n        iterators: Dict[str, Dict[str, Iterator]],\n        model: Model,\n    ):\n        \"\"\"\n\n        Args:\n            args: argparse.Namespace\n                The input arguments.\n            iterators: Dict\n                The dictionary that contains all the iterators for training and validation.\n            test_model: Model\n                The model to be trained.\n\n        \"\"\"\n\n        # parse infer_cfg depending on different situations\n        if isinstance(args.infer_cfg, str):\n            infer_cfg_dict = {\n                \".\".join(args.infer_cfg.split(\"/\")[-1].split(\".\")[:-1]): load_yaml(\n                    open(args.infer_cfg)\n                )\n            }\n\n        elif isinstance(args.infer_cfg, List):\n            infer_cfg_dict = dict()\n            for cfg in args.infer_cfg:\n                if isinstance(cfg, str):\n                    infer_cfg_dict[\".\".join(cfg.split(\"/\")[-1].split(\".\")[:-1])] = (\n                        load_yaml(open(cfg))\n                    )\n                elif isinstance(cfg, Dict):\n                    cfg = dict(sorted(cfg.items(), key=lambda x: x[0]))\n                    infer_cfg_dict[\n                        \"_\".join([f\"{key}={value}\" for key, value in cfg.items()])\n                    ] = cfg\n                else:\n                    raise TypeError(\n                        \"If infer_cfg is given in the form of a List, \"\n                        \"it must be either a List[str] or a List[Dict]!\"\n                    )\n\n        elif isinstance(args.infer_cfg, Dict):\n            if (\n                \"shared_args\" in args.infer_cfg.keys()\n                and \"exclu_args\" in args.infer_cfg.keys()\n            ):\n                assert isinstance(args.infer_cfg[\"shared_args\"], Dict) and isinstance(\n                    args.infer_cfg[\"exclu_args\"], List\n                ), (\n                    \"If infer_cfg is given by 'shared_args' and 'exclu_args', \"\n                    \"infer_cfg['shared_args'] must be a Dict and infer_cfg['exclu_args'] must be a List.\"\n                )\n                infer_cfg_dict = dict()\n                for cfg in args.infer_cfg[\"exclu_args\"]:\n                    assert isinstance(cfg, Dict), \"\"\n                    for cfg_key in cfg.keys():\n                        if cfg_key in args.infer_cfg[\"shared_args\"].keys():\n                            raise ValueError(\n                                f\"Find a duplicate argument {cfg_key} in both 'shared_args' and 'exclu_args'!\"\n                            )\n\n                    cfg.update(args.infer_cfg[\"shared_args\"])\n                    cfg = dict(sorted(cfg.items(), key=lambda x: x[0]))\n                    infer_cfg_dict[\n                        \"_\".join([f\"{key}={value}\" for key, value in cfg.items()])\n                    ] = cfg\n\n            elif (\n                \"shared_args\" not in args.infer_cfg.keys()\n                and \"exclu_args\" not in args.infer_cfg.keys()\n            ):\n                if len(args.infer_cfg) == 0:\n                    infer_cfg_dict = dict(default_inference=dict())\n                else:\n                    args.infer_cfg = dict(\n                        sorted(args.infer_cfg.items(), key=lambda x: x[0])\n                    )\n                    infer_cfg_dict = {\n                        \"_\".join(\n                            [f\"{key}={value}\" for key, value in args.infer_cfg.items()]\n                        ): args.infer_cfg\n                    }\n\n            else:\n                raise RuntimeError(\n                    \"If infer_cfg is given in the form of a Dict, \"\n                    \"'shared_args' and 'exclu_args' must be or not be in the key list at the same time!\"\n                )\n\n        elif args.infer_cfg is None:\n            infer_cfg_dict = dict(default_inference=dict())\n\n        else:\n            raise TypeError(\n                \"infer_cfg must be given in the form of a string, a List, or a Dict!\"\n            )\n\n        # loop each test configuration\n        for infer_cfg_name, infer_cfg in infer_cfg_dict.items():\n            # configuration-specific result path\n            test_result_path = os.path.join(\n                (\n                    args.train_result_path\n                    if args.test_result_path is None\n                    else args.test_result_path\n                ),\n                infer_cfg_name,\n            )\n            os.makedirs(test_result_path, exist_ok=True)\n\n            # load the existing testing configuration for resuming\n            infer_cfg_path = os.path.join(test_result_path, \"infer_cfg.yaml\")\n            if args.resume and os.path.exists(infer_cfg_path):\n                infer_cfg = load_yaml(open(infer_cfg_path))\n\n            # save the testing configuration file to infer_cfg_path\n            if not args.distributed or args.rank == 0:\n                if len(infer_cfg) &gt; 0:\n                    with open(infer_cfg_path, \"w\", encoding=\"utf-8\") as f:\n                        yaml.dump(infer_cfg, f, sort_keys=False)\n\n            # unlike training and validation, the testing iterators are looped one by one\n            for name, iterator in iterators[\"test\"].items():\n                # replace the slash with a percent symbol\n                name = name.replace(\"/\", \"%\")\n                # add the identity symbol to the path for multi-GPU testing\n                if args.attach_model_folder_when_test:\n                    test_dset_path = os.path.join(test_result_path, test_model, name)\n                else:\n                    test_dset_path = os.path.join(test_result_path, name)\n                test_rank_path = os.path.join(test_dset_path, f\"rank{args.rank}_tmp\")\n                logger = logger_stdout_file(test_rank_path, file_name=\"test\")\n\n                # initialize top-n bad case presentation\n                if args.bad_cases_selection is None:\n                    if model.bad_cases_selection is not None:\n                        args.bad_cases_selection = model.bad_cases_selection\n                    else:\n                        logger.info(\n                            \"There is no configuration of topN bad case selection in either your input \"\n                            \"arguments or default values of your selected model. \"\n                            \"So there will not be any reports about topN bad cases.\"\n                        )\n                # the main testing process\n                if args.bad_cases_selection is not None:\n                    logger.info(\n                        f\"The configuration of topN bad case selection in the current testing process is {args.bad_cases_selection}.\"\n                    )\n\n                # initialize the testing monitor\n                monitor = TestMonitor(\n                    logger=logger, args=args, result_path=test_dset_path\n                )\n\n                # check the resuming status\n                if args.resume:\n                    # loading the existed checkpoint\n                    try:\n                        test_checkpoint = torch.load(\n                            os.path.join(test_rank_path, \"checkpoint.pth\")\n                        )\n                        monitor.load_state_dict(test_checkpoint[\"monitor\"])\n                        start_step = test_checkpoint[\"start_step\"]\n                        logger.info(\n                            f\"The testing process resumes from the step no.{start_step}. \"\n                        )\n                    # checkpoint does not exist\n                    except FileNotFoundError:\n                        start_step = 0\n                        logger.info(\n                            f\"No checkpoint is found in {test_rank_path}. \"\n                            f\"The testing process will start from scratch. \"\n                        )\n                else:\n                    start_step = 0\n                    logger.info(\"The testing process will start from scratch. \")\n\n                # initialize the dataloaders from the given starting point\n                data_loaders = cls.dict_transform(\n                    iterator, lambda x: iter(x.build_loader(start_step=start_step))\n                )\n                test_indices = cls.dict_transform(\n                    iterator, lambda x: x.get_batch_indices()\n                )\n                # if there are multiple dataloaders for the current testing set,\n                # the sample indices of the first element will be used to make the reports\n                if isinstance(test_indices, Dict):\n                    test_indices = test_indices[list(test_indices.keys())[0]]\n                # report the total number of testing steps needed to be done\n                total_step_num = len(test_indices)\n                logger.info(f\"Totally {total_step_num} testing steps.\")\n\n                # make sure that no gradient appears during testing\n                model.eval()\n                with torch.inference_mode():\n                    monitor.start_epoch(total_step_num=total_step_num)\n                    # iterate the testing batches\n                    for i in range(total_step_num):\n                        if i &lt; start_step:\n                            continue\n\n                        # only fetch the testing data right before decoding and evaluation\n                        test_batch = cls.dict_transform(\n                            src_dict=data_loaders, transform_func=next\n                        )\n                        # skip the empty testing batch\n                        if cls.is_empty_batch(test_batch):\n                            continue\n                        # evaluate the current testing batch and get the evaluation results\n                        try:\n                            test_results = model.evaluate(\n                                test_batch=test_batch, infer_conf=infer_cfg\n                            )\n                        # skip the current step if encounter an error (any kind)\n                        except Exception as e:\n                            if args.ignore_test_exception:\n                                logger.warn(\n                                    f\"Rank no.{torch.distributed.get_rank() if args.distributed else '0'} meets the error \"\n                                    f\"{e} at step no.{i}. \"\n                                    f\"Indices of the involved testing samples in this step is {test_indices[i]}.\"\n                                )\n                                continue\n                            else:\n                                raise e\n                        # record evaluation results\n                        monitor.step(\n                            step_num=i + 1,\n                            test_results=test_results,\n                            test_index=test_indices[i],\n                        )\n\n                        # reduce the number of IO operations to speed up the testing\n                        if (\n                            i + 1\n                        ) % monitor.report_per_steps == 0 or i == total_step_num - 1:\n                            # save the checkpoint of the current step for both resuming and multi-GPU evaluation\n                            # the iteration conditions of the test dataloader will also be saved for resuming\n                            torch.save(\n                                dict(start_step=i + 1, monitor=monitor.state_dict()),\n                                os.path.join(test_rank_path, \"checkpoint.pth\"),\n                            )\n\n                # waiting for the data saving daemon process to finish before calling finish_epoch()\n                monitor.wait_empty_queues()\n\n                if not args.distributed or args.rank == 0:\n                    # obtain the group information of the current iterator\n                    group_info = None\n                    if isinstance(iterator, Iterator):\n                        # Dict[str, Dict[str, str]]\n                        group_info = iterator.get_group_info()\n                    elif isinstance(iterator, Dict):\n                        # List[Dict[str, Dict[str, str]]]\n                        group_info_list = [\n                            value.get_group_info() for value in iterator.values()\n                        ]\n                        for group_dict in group_info_list:\n                            if group_dict is not None:\n                                group_info = group_dict\n                                break\n                    else:\n                        raise RuntimeError\n\n                    # finish the evaluation and store the results to the disk\n                    monitor.finish_epoch(meta_info=group_info)\n\n    @classmethod\n    def set_random_seeds(cls, seed: int):\n        \"\"\"Set random seeds for python environment, numpy environment and torch\n        environment.\n\n        Note:\n            1. torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),\n                so it is not necessary to be included here.\n            2. torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on\n                different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.\n        \"\"\"\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    @classmethod\n    def gather_all_iter_ascii(cls, iterator: Iterator, device: torch.device):\n        \"\"\"\n\n        Args:\n            iterator:\n            device:\n\n        Returns:\n\n        \"\"\"\n        # turn the message into ASCII codes and gather the codes length\n        _iter_asc = torch.LongTensor([ord(char) for char in str(iterator)])\n        _iter_asc_len = torch.LongTensor([_iter_asc.size(0)]).cuda(device)\n\n        _iter_asc_lens = torch.LongTensor(\n            [0 for _ in range(torch.distributed.get_world_size())]\n        ).cuda(device)\n        torch.distributed.all_gather_into_tensor(_iter_asc_lens, _iter_asc_len)\n\n        # padding the ASCII codes to the same length and gather them\n        if _iter_asc_len &lt; _iter_asc_lens.max():\n            _iter_asc = torch.cat(\n                (\n                    _iter_asc,\n                    torch.zeros(\n                        _iter_asc_lens.max().item() - _iter_asc_len.item(),\n                        dtype=torch.int64,\n                    ),\n                )\n            )\n\n        _iter_ascs = torch.zeros(\n            (torch.distributed.get_world_size(), _iter_asc_lens.max().item()),\n            dtype=torch.int64,\n            device=device,\n        )\n        torch.distributed.all_gather_into_tensor(_iter_ascs, _iter_asc.cuda(device))\n        return [_iter_asc for _iter_asc in _iter_ascs], _iter_asc_lens\n\n    @classmethod\n    def main_worker(cls, gpu: int, args: argparse.Namespace):\n        \"\"\"The main body of a process on one GPU.\n\n        Args:\n            gpu:\n            args:\n        \"\"\"\n        # --- 0. Random Seed Preparation --- #\n        # set different random seeds for the different GPU processes in DDP mode to avoid the process homogeneity\n        if args.distributed and not args.same_proc_seed:\n            args.seed += gpu\n        cls.set_random_seeds(args.seed)\n\n        # --- 1. Experimental Reproducibility Preparation --- #\n        torch.backends.cudnn.enabled = args.cudnn_enabled\n        torch.backends.cudnn.benchmark = args.cudnn_benchmark\n        torch.backends.cudnn.deterministic = args.cudnn_deterministic\n        # torch.use_deterministic_algorithms(torch.backends.cudnn.deterministic)\n        # For more details about 'CUBLAS_WORKSPACE_CONFIG',\n        # please refer to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n        if V(torch.version.cuda) &gt;= V(\"10.2\"):\n            os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n\n        # --- 2. DDP Model Distribution Initialization --- #\n        if args.distributed:\n            # load the global node rank from the os environment in the multi-node setting\n            if args.dist_url == \"env://\":\n                args.rank = int(os.environ[\"RANK\"])\n\n            if args.ngpu &gt; 1:\n                # Here, the rank is turned from the local node-level rank to the global process-level rank\n                # the input argument 'gpu' is the local rank of the current process in the specific node\n                args.rank = args.rank * args.ngpu + gpu\n            # initialize the distributed environment, connections among all the processes are established here\n            dist.init_process_group(\n                backend=args.dist_backend,\n                init_method=args.dist_url,\n                world_size=args.world_size,\n                rank=args.rank,\n            )\n\n        # --- 3. Experimental Environment Logging --- #\n        if args.config is not None:\n            _config_split = args.config.split(\"/\")\n        else:\n            _config_split = None\n        # automatically decide the result path if not given\n        if args.train_result_path is None:\n            assert (\n                _config_split is not None\n            ), \"If you want to automatically generate train_result_path, please give the configuration by '--config'.\"\n            args.train_result_path = \"/\".join(\n                _config_split[:-2]\n                + [\"exp\", \".\".join(_config_split[-1].split(\".\")[:-1])]\n            )\n        # attach a folder named by args.config to the end of your given result path\n        elif args.attach_config_folder_to_path:\n            # if `--config` is given, attach the name of exp_cfg to the end of train_result_path\n            if _config_split is not None:\n                args.train_result_path = os.path.join(\n                    args.train_result_path, \".\".join(_config_split[-1].split(\".\")[:-1])\n                )\n\n        # initialize the logger and save current script command\n        logger = logger_stdout_file(\n            args.train_result_path,\n            \"train\" if args.train else None,\n            args.distributed,\n            args.rank,\n        )\n\n        # logging the beginning info of the experiment\n        logger.info(f\"Current script command: {' '.join([xi for xi in sys.argv])}\")\n        if args.distributed:\n            logger.info(\n                f\"Multi-GPU distribution information: \"\n                f\"backend={args.dist_backend}, init_method={args.dist_url}, \"\n                f\"nnode={int(args.world_size / args.ngpu)}, ngpu_per_node={args.ngpu}, \"\n                f\"used_gpus={args.gpus}.\"\n            )\n\n        # initialize the computational equipments\n        assert (\n            torch.cuda.is_available()\n        ), \"CUDA is not available! It fails to conduct GPU training.\"\n        # args.gpu is the GPU used in the current process while args.gpus are all the available GPUss\n        args.gpu = args.gpus[gpu] if args.ngpu &gt; 1 else args.gpus\n        device = torch.device(f\"cuda:{args.gpu}\")\n        torch.cuda.device(device)\n        logger.info(f\"Used GPU in the master process: {device}\")\n\n        # --- 4. Configuration Loading --- #\n        # resume from an existing checkpoint, loading the old data and train configurations\n        if args.resume:\n            # loading the existing data and train configurations\n            # But the input data configuration has higher priority than the existing one\n            if args.data_cfg is not None:\n                data_cfg = (\n                    load_yaml(open(args.data_cfg))\n                    if isinstance(args.data_cfg, str)\n                    else args.data_cfg\n                )\n            else:\n                data_cfg = load_yaml(\n                    open(\n                        os.path.join(\n                            args.train_result_path,\n                            f\"{'train' if args.train else 'test'}_data_cfg.yaml\",\n                        )\n                    )\n                )\n            # training configuration will be loaded from the existing file\n            train_cfg = load_yaml(\n                open(os.path.join(args.train_result_path, \"train_cfg.yaml\"))\n            )\n        # start from scratch, loading the new data and train configurations\n        else:\n            assert (\n                args.data_cfg is not None and args.train_cfg is not None\n            ), \"Please specify a data configuration file and a train configuration file!\"\n            data_cfg = (\n                load_yaml(open(args.data_cfg))\n                if isinstance(args.data_cfg, str)\n                else args.data_cfg\n            )\n            train_cfg = (\n                load_yaml(open(args.train_cfg))\n                if isinstance(args.train_cfg, str)\n                else args.train_cfg\n            )\n\n        # --- 5. Data Iterator Initialization --- #\n        iterators = cls.build_iterators(data_cfg=data_cfg, args=args)\n\n        # logging the information of the iterators\n        _iter_message = \"The information of the iterators:\"\n        for dset, iters in iterators.items():\n            # single iterator for the current dataset\n            if isinstance(iters, Iterator):\n                # gather the iterator message from all the process in the multi-GPU distributed training mode\n                if args.distributed:\n                    _iter_ascs, _iter_asc_lens = cls.gather_all_iter_ascii(\n                        iters, device\n                    )\n\n                    # recover the codes from all the processes back to the text\n                    for i, asc in enumerate(_iter_ascs):\n                        _iter_text = \"\".join(\n                            [chr(a) for a in asc[: _iter_asc_lens[i]].tolist()]\n                        )\n                        _iter_message += f\"\\nThe iterator in the {dset} set of the rank no.{i}: {_iter_text}\"\n                # directly report the message in the single-GPU mode\n                else:\n                    _iter_message += f\"\\nThe iterator in the {dset} set: {iters}\"\n\n            # multiple iterators for the current dataset\n            elif isinstance(iters, Dict):\n                for name, iterator in iters.items():\n                    # gather the iterator message from all the process in the multi-GPU distributed training mode\n                    if args.distributed:\n                        _iter_ascs, _iter_asc_lens = cls.gather_all_iter_ascii(\n                            iterator, device\n                        )\n\n                        # recover the codes from all the processes back to the text\n                        for i, asc in enumerate(_iter_ascs):\n                            _iter_text = \"\".join(\n                                [chr(a) for a in asc[: _iter_asc_lens[i]].tolist()]\n                            )\n                            _iter_message += f\"\\nThe {name} iterator in the {dset} set of the rank no.{i}: {_iter_text}\"\n                    # directly report the message in the single-GPU mode\n                    else:\n                        _iter_message += (\n                            f\"\\nThe {name} iterator in the {dset} set: {iterator}\"\n                        )\n\n        logger.info(_iter_message)\n\n        # --- 6. Model Initialization --- #\n        assert (\n            \"model\" in train_cfg.keys()\n        ), \"Please fill in the 'model' tag of your given train_cfg!\"\n        model = cls.build_model(train_cfg[\"model\"], args=args, device=device)\n        logger.info(model_summary(model))\n\n        # for the process of single-GPU training or the rank 0 process of multi-GPUs training\n        if not args.distributed or args.rank == 0:\n            # dumping all the configuration files into train_result_path for resuming\n            with open(\n                os.path.join(args.train_result_path, \"exp_cfg.yaml\"),\n                \"w\",\n                encoding=\"utf-8\",\n            ) as f:\n                yaml.dump(vars(args), f, sort_keys=False)\n            with open(\n                os.path.join(\n                    args.train_result_path,\n                    f\"{'train' if args.train else 'test'}_data_cfg.yaml\",\n                ),\n                \"w\",\n                encoding=\"utf-8\",\n            ) as f:\n                yaml.dump(data_cfg, f, sort_keys=False)\n            with open(\n                os.path.join(args.train_result_path, \"train_cfg.yaml\"),\n                \"w\",\n                encoding=\"utf-8\",\n            ) as f:\n                yaml.dump(train_cfg, f, sort_keys=False)\n\n        # --- 7.1. Model Training Branch --- #\n        if args.train:\n            # initialize the Monitor for training and validation\n            monitor = (\n                None\n                if args.distributed and args.rank != 0\n                else TrainValidMonitor(logger=logger, args=args, model=model)\n            )\n\n            # loading the model from the existing checkpoint for resuming the training process\n            args.start_epoch = cls.resume(args=args, model=model, monitor=monitor)\n\n            # DDP Wrapping of the model must be done after model checkpoint loading\n            if args.distributed:\n                if args.enable_syncbatchnorm:\n                    # turn the batchnorm layers into the sync counterparts\n                    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n                # Here the model buffers and parameters of the master process are broadcast to the other processes\n                model = torch.nn.parallel.DistributedDataParallel(\n                    model, device_ids=[args.gpu], output_device=args.gpu\n                )\n\n            # initialize the OptimSchedulers after DDP wrapping (including optimization resuming)\n            assert (\n                \"optim_sches\" in train_cfg.keys()\n            ), \"Please fill in the 'optim_sches' tag!\"\n            optim_sches = cls.build_optim_sches(\n                model=model, optim_sche_cfg=train_cfg[\"optim_sches\"], args=args\n            )\n\n            # logging the information of the optimschedulers\n            for name, optim_sche in optim_sches.items():\n                logger.info(f\"The {name} OptimScheduler: {optim_sche}\")\n\n            # start the training process\n            cls.train(\n                args=args,\n                data_cfg=data_cfg,\n                iterators=iterators,\n                model=model,\n                optim_sches=optim_sches,\n                logger=logger,\n                monitor=monitor,\n            )\n\n        # --- 7.2. Model Testing Branch --- #\n        elif args.test:\n            if isinstance(args.test_model, str):\n                args.test_model = [args.test_model]\n            elif not isinstance(args.test_model, List):\n                raise ValueError\n\n            # loop each model to be tested\n            for model_name in args.test_model:\n                # get the path of the target model parameters\n                _models_path = os.path.join(args.train_result_path, \"models\")\n                # for compatibility with the older version\n                if os.path.exists(os.path.join(_models_path, f\"{model_name}.mdl\")):\n                    model_path = os.path.join(_models_path, f\"{model_name}.mdl\")\n                elif os.path.exists(os.path.join(_models_path, f\"{model_name}.pth\")):\n                    model_path = os.path.join(_models_path, f\"{model_name}.pth\")\n                else:\n                    raise RuntimeError(\n                        f\"{os.path.join(_models_path, '%s.pth' % model_name)} is not found!\"\n                    )\n\n                # load the target model parameters\n                model.load_state_dict(torch.load(model_path, map_location=model.device))\n\n                # start the testing process\n                cls.test(\n                    args=args, test_model=model_name, iterators=iterators, model=model\n                )\n\n        else:\n            raise RuntimeError(\n                \"train and test in args cannot be False at the same time!\"\n            )\n\n        # --- 8. Release Computational Resource --- #\n        if args.distributed and args.rank == 0:\n            torch.distributed.destroy_process_group()\n        sys.exit(0)\n\n    @classmethod\n    def main(cls, args: argparse.Namespace):\n        \"\"\"The beginning of a experiment branch (training or testing). This function\n        decides the single-GPU or multi-GPU training sub-branch.\n\n        Args:\n            args: argparse.Namespace\n                The input arguments for the experiment.\n        \"\"\"\n        # This block is for safely calling torch.cuda API in the main process\n        try:\n            torch.multiprocessing.set_start_method(\"spawn\")\n        except RuntimeError:\n            pass\n\n        # --- 1. Initialization of the used GPUs in the current experiment --- #\n        # 'CUDA_VISIBLE_DEVICES' has the higher priority than the argument 'gpus'\n        if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n            args.gpus = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n            if not isinstance(args.gpus, str):\n                args.gpus = str(args.gpus)\n\n        # if 'CUDA_VISIBLE_DEVICES' is not given, initialize it by args.gpus\n        elif args.gpus is not None:\n            if isinstance(args.gpus, List):\n                args.gpus = \",\".join(\n                    [\n                        str(g) if not isinstance(g, str) else g\n                        for g in args.gpus\n                        if g != \"\"\n                    ]\n                )\n            elif isinstance(args.gpus, int):\n                args.gpus = str(args.gpus)\n            else:\n                assert isinstance(args.gpus, str)\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n        # if both 'CUDA_VISIBLE_DEVICES' and args.gpus are not given, automatically select available GPUs\n        else:\n            args.gpus = get_idle_gpu(args.ngpu)\n            # make sure that GPU no.0 is the first GPU if it is selected\n            args.gpus = \",\".join(sorted([str(gpu.id) for gpu in args.gpus]))\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n        # convert the GPU absolute number to the GPU relative index to fit 'CUDA_VISIBLE_DEVICES'\n        args.gpus = [idx for idx, _ in enumerate(args.gpus.split(\",\"))]\n\n        # check the GPU configuration\n        assert len(args.gpus) &gt;= args.ngpu, (\n            f\"Error: The visible GPUs {args.gpus} are fewer than the GPUs you would like to use {args.ngpu}, default is 2! \"\n            f\"Use the argument '--gpu' to directly specify your target GPUs.\"\n        )\n        if len(args.gpus) == 1:\n            args.gpus = args.gpus[0]\n\n        # --- 2. Initialization of DDP distribution pipeline --- #\n        # get the world_size from the command line, world_size here means the number of nodes\n        if args.dist_url == \"env://\":\n            args.world_size = int(os.environ[\"WORLD_SIZE\"])\n            raise NotImplementedError(\n                \"Multi-node DDP distributed training is not supported now.....\"\n            )\n\n        # distributed is set to true if multiple GPUs are specified or multiple nodes are specified\n        # args.world_size &gt; 1 means multi-node distribution while args.ngpu &gt; 1 means multi-GPU distribution\n        args.distributed = args.world_size &gt; 1 or args.ngpu &gt; 1\n\n        # multi-GPU distributed training and testing\n        if args.ngpu &gt; 1:\n            # check whether the input number of GPUs is valid\n            ngpus_per_node = torch.cuda.device_count()\n            if args.ngpu &gt; ngpus_per_node:\n                warnings.warn(\n                    f\"Your input args.ngpu ({args.ngpu}) is larger than the GPUs you have on your machine \"\n                    f\"({ngpus_per_node}). Currently, the real args.ngpu becomes {ngpus_per_node}.\"\n                )\n                args.ngpu = ngpus_per_node\n            # here world_size becomes the total number of processes on all nodes\n            args.world_size = args.ngpu * args.world_size\n\n            # automatic port selection if no specified port (only one ':' in args.dist_url)\n            if len(args.dist_url.split(\":\")) &lt; 3:\n                args.dist_url += f\":{get_idle_port()}\"\n            else:\n                raise RuntimeError\n\n            # run one process on each GPU\n            mp.spawn(cls.main_worker, nprocs=args.ngpu, args=(args,))\n\n        # single-GPU training and testing\n        elif args.ngpu == 1:\n            if isinstance(args.gpus, List) and len(args.gpus) &gt; 1:\n                warnings.warn(\n                    f\"Your input args.ngpu {args.gpus} is more than one. \"\n                    f\"Currently, the GPU no.{args.gpus[0]} will be used.\"\n                )\n                args.gpus = args.gpus[0]\n            cls.main_worker(args.gpus, args)\n\n        # CPU testing with the multiprocessing strategy\n        elif args.test:\n            raise NotImplementedError(\n                \"Multiprocessing CPU testing part has not been implemented yet......\"\n            )\n\n        # CPU training is not supported\n        else:\n            raise RuntimeError(\n                \"Our toolkit doesn't support CPU training. Please specify a number of GPUs......\"\n            )\n\n    @classmethod\n    def run(cls):\n        \"\"\"The preparation area of Runner where the configuration is parsed and\n        converted into code-friendly format.\"\"\"\n        # --- 0. Get the Command Line Arguments --- #\n        args = cls.parse()\n\n        # --- 1. Read the Non-Config Arguments from the Command Line --- #\n        # Currently, 'world_size' and 'rank' are not provided to users to set\n        given_args = [\"world_size\", \"rank\"]\n        # The arguments that users give in the command line should not be refreshed by the argument '--config'\n        for i in sys.argv:\n            if i.startswith(\"--\"):\n                given_args.append(i.replace(\"-\", \"\"))\n\n        # check the train and test flags\n        if \"train\" in given_args and \"test\" in given_args:\n            assert (args.train ^ args.test) is True, (\n                \"A running job can only conduct either training process or testing process, \"\n                \"so args.train and args.test cannot be True at the same time. \"\n                \"If you want to conduct training and testing sequentially, \"\n                \"please make two running jobs where the first job has args.train=True and args.test=False and \"\n                \"the second job has args.train=False and args.test=True.\"\n            )\n        elif \"train\" in given_args:\n            given_args.append(\"test\")\n            args.test = not args.train\n        elif \"test\" in given_args:\n            given_args.append(\"train\")\n            args.train = not args.test\n\n        # the command 'CUDA_VISIBLE_DEVICES' has the higher priority than the argument 'gpus'\n        if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n            given_args.append(\"gpus\")\n            args.gpus = None\n\n        # --- 2. Overwrite the Arguments by '--config' --- #\n        # overwrite args from the args.config\n        # Note: the ones given in the command line has the higher priority than args.config\n        if args.config is not None:\n            args.config = parse_path_args(args.config)\n            config = load_yaml(open(args.config, mode=\"r\", encoding=\"utf-8\"))\n            for c in config.keys():\n                if c not in given_args:\n                    # remove the port number in 'dist_url' if given\n                    if c == \"dist_url\":\n                        assert len(config[c].split(\":\")) &lt;= 3\n                        if len(config[c].split(\":\")) == 3:\n                            config[c] = \":\".join(config[c].split(\":\")[:-1])\n                    # skip the existing 'report_per_steps' (either use default value or give it in the command line)\n                    if c == \"report_per_steps\":\n                        continue\n                    # set the argument from config to args\n                    setattr(args, c, config[c])\n\n        # make sure that all the paths are absolute paths\n        if args.train_result_path is not None:\n            args.train_result_path = parse_path_args(args.train_result_path)\n        if args.test_result_path is not None:\n            args.test_result_path = parse_path_args(args.test_result_path)\n        if args.data_cfg is not None and not isinstance(args.data_cfg, Dict):\n            args.data_cfg = parse_path_args(args.data_cfg)\n        if args.train_cfg is not None and not isinstance(args.train_cfg, Dict):\n            args.train_cfg = parse_path_args(args.train_cfg)\n        if args.infer_cfg is not None:\n            if isinstance(args.infer_cfg, str):\n                args.infer_cfg = parse_path_args(args.infer_cfg)\n            elif isinstance(args.infer_cfg, List):\n                args.infer_cfg = [\n                    parse_path_args(cfg) if isinstance(cfg, str) else cfg\n                    for cfg in args.infer_cfg\n                ]\n            elif not isinstance(args.infer_cfg, Dict):\n                raise TypeError(\n                    \"infer_cfg should be either a string, a List, or a Dict, \"\n                    f\"but got type(args.infer_cfg)={type(args.infer_cfg)}.\"\n                )\n\n        # --- 3. Start the Experimental Pipeline --- #\n        assert (args.train ^ args.test) is True, (\n            \"A running job can only conduct either training process or testing process, \"\n            \"so args.train and args.test cannot be True at the same time. \"\n            \"If you want to conduct training and testing sequentially, \"\n            \"please make two running jobs where the first job has args.train=True and args.test=False and \"\n            \"the second job has args.train=False and args.test=True.\"\n        )\n        cls.main(args)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.add_parse","title":"<code>add_parse(parser)</code>  <code>classmethod</code>","text":"<p>The interface where users can add their own arguments.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ArgumentParser</code> <p>argparse.ArgumentParser The name space where you want to add your arguments.</p> required <p>Returns:</p> Name Type Description <code>parser</code> <code>ArgumentParser</code> <p>argparse.ArgumentParser The name space containing your arguments.</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef add_parse(cls, parser: argparse.ArgumentParser) -&gt; argparse.ArgumentParser:\n    \"\"\"The interface where users can add their own arguments.\n\n    Args:\n        parser: argparse.ArgumentParser\n            The name space where you want to add your arguments.\n\n    Returns:\n        parser: argparse.ArgumentParser\n            The name space containing your arguments.\n    \"\"\"\n    return parser\n</code></pre>"},{"location":"reference/runner/#runner.Runner.build_iterators","title":"<code>build_iterators(data_cfg, args)</code>  <code>classmethod</code>","text":"<p>This static function builds all iterators used in the experiment. The configuration of iterators is given in your specified 'data_cfg'.</p> <p>The iterators are returned as a dictionary where the first-level keys indicate different iterator groups: 'train', 'valid', and 'test'. The second-level keys in each group indicates the iterators belonging to the group. In the value of each second-level key, there are two third-level keys: 'type' and 'conf'. 'type' indicates the iterator type and 'conf' indicates the iterator configuration. For more details, please refer to ./speechain/iterator/README.md</p> <p>Parameters:</p> Name Type Description Default <code>data_cfg</code> <code>Dict[str, Dict]</code> <p>Dict The dictionary containing all the information to initialize the iterators</p> required <code>args</code> <code>Namespace</code> <p>argparse.Namespace The arguments of the runner in this experiment.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Iterator] or Iterator]</code> <p>The dictionary of the iterators of all groups (train, valid, test).</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef build_iterators(\n    cls, data_cfg: Dict[str, Dict], args: argparse.Namespace\n) -&gt; Dict[str, Dict[str, Iterator] or Iterator]:\n    \"\"\"This static function builds all iterators used in the experiment. The\n    configuration of iterators is given in your specified 'data_cfg'.\n\n    The iterators are returned as a dictionary where the first-level keys indicate different iterator groups:\n    'train', 'valid', and 'test'. The second-level keys in each group indicates the iterators belonging to the\n    group. In the value of each second-level key, there are two third-level keys: 'type' and 'conf'. 'type'\n    indicates the iterator type and 'conf' indicates the iterator configuration. For more details, please refer\n    to ./speechain/iterator/README.md\n\n    Args:\n        data_cfg: Dict\n            The dictionary containing all the information to initialize the iterators\n        args: argparse.Namespace\n            The arguments of the runner in this experiment.\n\n    Returns:\n        The dictionary of the iterators of all groups (train, valid, test).\n    \"\"\"\n\n    def recur_iterator_init(_data_cfg: Dict, _dset: str):\n        leaf_flag = len(_data_cfg) == 2 and (\n            \"type\" in _data_cfg.keys() and \"conf\" in _data_cfg.keys()\n        )\n        if leaf_flag:\n            iterator_class = import_class(\"speechain.iterator.\" + _data_cfg[\"type\"])\n            return iterator_class(\n                seed=args.seed,\n                ngpu=args.ngpu,\n                num_workers=getattr(args, f\"{_dset}_num_workers\"),\n                pin_memory=args.pin_memory,\n                distributed=args.distributed,\n                **_data_cfg[\"conf\"],\n            )\n        else:\n            return {\n                key: recur_iterator_init(value, _dset)\n                for key, value in _data_cfg.items()\n            }\n\n    def recur_batch_num_init(_iterators: Dict or Iterator):\n        leaf_flag = isinstance(_iterators, Iterator)\n        if leaf_flag:\n            return len(_iterators)\n        else:\n            sub_leaf_flag = sum(\n                [isinstance(value, Iterator) for value in _iterators.values()]\n            ) == len(_iterators)\n            if sub_leaf_flag:\n                return [len(value) for value in _iterators.values()]\n            else:\n                return {\n                    key: recur_batch_num_init(value)\n                    for key, value in _iterators.items()\n                }\n\n    def flatten_dict_to_list(_input: Dict or int or List[int]):\n        leaf_flag = isinstance(_input, (int, List))\n        if leaf_flag:\n            return [_input] if isinstance(_input, int) else _input\n        else:\n            _output = []\n            for value in _input.values():\n                _output += flatten_dict_to_list(value)\n            return _output\n\n    # get the target groups of the current experiment\n    if args.train:\n        assert (\n            \"train\" in data_cfg.keys() and \"valid\" in data_cfg.keys()\n        ), \"If args.train is set to True, please give 'train' and 'valid' as first-level keys of data_cfg.\"\n        dset_keys = [\"train\", \"valid\"]\n    elif args.test:\n        assert (\n            \"test\" in data_cfg.keys()\n        ), \"If args.test is set to True, please give 'test' as first-level keys of data_cfg.\"\n        dset_keys = [\"test\"]\n    else:\n        raise RuntimeError(\"Please set either args.train or args.test to True!\")\n\n    # recursively initialize all the iterators in the Dict\n    mode = \"train\" if args.train else \"test\"\n    iterators = {\n        dset: recur_iterator_init(data_cfg[dset], dset) for dset in dset_keys\n    }\n    batch_nums = recur_batch_num_init(iterators[mode])\n\n    # set the relative reporting interval during training or testing\n    if args.report_per_steps &lt;= 0:\n        _reports_per_epoch = (\n            10 if args.report_per_steps == 0 else int(-args.report_per_steps)\n        )\n        args.report_per_steps = (\n            min(flatten_dict_to_list(batch_nums)) // _reports_per_epoch\n        )\n    # check the absolute reporting interval during training and testing\n    else:\n        assert int(args.report_per_steps) &lt;= min(\n            flatten_dict_to_list(batch_nums)\n        ), (\n            f\"If args.report_per_steps is given as a positive integer, \"\n            f\"it should be smaller than the minimal {mode} batch number ({min(batch_nums)}). \"\n            f\"But got report_per_steps={int(args.report_per_steps)}!\"\n        )\n\n        # in case that report_per_steps is given as a float number\n        args.report_per_steps = int(args.report_per_steps)\n\n    return iterators\n</code></pre>"},{"location":"reference/runner/#runner.Runner.build_model","title":"<code>build_model(model_cfg, args, device)</code>  <code>classmethod</code>","text":"<p>This static function builds the model used in the experiment. The configuration of the model is given in the value of the 'model' key in your specified 'model_cfg'.</p> <p>Parameters:</p> Name Type Description Default <code>model_cfg</code> <code>Dict[str, Any]</code> <p>Dict Model Configuration</p> required <code>args</code> <code>Namespace</code> required <code>device</code> <code>device</code> required <p>Returns:</p> Type Description <code>Model</code> <p>The target Model object initialized by your given configuration</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef build_model(\n    cls, model_cfg: Dict[str, Any], args: argparse.Namespace, device: torch.device\n) -&gt; Model:\n    \"\"\"This static function builds the model used in the experiment. The\n    configuration of the model is given in the value of the 'model' key in your\n    specified 'model_cfg'.\n\n    Args:\n        model_cfg: Dict\n            Model Configuration\n        args:\n        device:\n\n    Returns:\n        The target Model object initialized by your given configuration\n    \"\"\"\n    assert \"model_type\" in model_cfg.keys(), \"Please specify the model_type!\"\n    assert \"module_conf\" in model_cfg.keys(), \"Please specify the module_conf!\"\n    if \"criterion_conf\" not in model_cfg.keys():\n        model_cfg[\"criterion_conf\"] = None\n\n    model_class = import_class(\"speechain.model.\" + model_cfg[\"model_type\"])\n    return model_class(\n        model_conf=(\n            model_cfg[\"model_conf\"] if \"model_conf\" in model_cfg.keys() else dict()\n        ),\n        module_conf=model_cfg[\"module_conf\"],\n        criterion_conf=model_cfg[\"criterion_conf\"],\n        device=device,\n        result_path=args.train_result_path,\n        non_blocking=args.non_blocking,\n        distributed=args.distributed,\n    ).cuda(device=device)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.build_optim_sches","title":"<code>build_optim_sches(model, optim_sche_cfg, args)</code>  <code>classmethod</code>","text":"<p>This static function builds the OptimSchedulers used in the pipeline. The configuration of the OptimSchedulers is given in the value of 'optim_sches' key in your specified 'train_cfg'.</p> <p>This function must be done after DDP wrapping because we need to make sure that the model parameters received by the optimizer in each process are identical. With the identical model parameters, it's safe to consider that the optimizer parameters are also identical.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model The initialized model.</p> required <code>optim_sche_cfg</code> <code>Dict[str, Any]</code> <p>Dict OptimScheduler Configuration</p> required <code>args</code> <code>Namespace</code> <p>argparse.Namespace The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.</p> required <p>Returns:</p> Type Description <code>Dict[str, OptimScheduler] or OptimScheduler</code> <p>The Dict of the initialized OptimSchedulers.</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef build_optim_sches(\n    cls, model: Model, optim_sche_cfg: Dict[str, Any], args: argparse.Namespace\n) -&gt; Dict[str, OptimScheduler] or OptimScheduler:\n    \"\"\"This static function builds the OptimSchedulers used in the pipeline. The\n    configuration of the OptimSchedulers is given in the value of 'optim_sches' key\n    in your specified 'train_cfg'.\n\n    This function must be done after DDP wrapping because we need to make sure that the model parameters received\n    by the optimizer in each process are identical. With the identical model parameters, it's safe to consider that\n    the optimizer parameters are also identical.\n\n    Args:\n        model: Model\n            The initialized model.\n        optim_sche_cfg: Dict\n            OptimScheduler Configuration\n        args: argparse.Namespace\n            The input arguments. Used to pass accum_grad, grad_clip, and grad_norm_type to your optimedulers.\n\n    Returns:\n        The Dict of the initialized OptimSchedulers.\n    \"\"\"\n    # single-optimizer scenario\n    if len(optim_sche_cfg) == 2 and (\n        \"type\" in optim_sche_cfg.keys() and \"conf\" in optim_sche_cfg.keys()\n    ):\n        optim_sche_cfg = dict(main=optim_sche_cfg)\n\n    optim_sches = dict()\n    for name, optim_sche in optim_sche_cfg.items():\n        optim_sche_class = import_class(\n            \"speechain.optim_sche.\" + optim_sche[\"type\"]\n        )\n        optim_sches[name] = optim_sche_class(\n            model=model,\n            distributed=args.distributed,\n            use_amp=args.use_amp,\n            accum_grad=args.accum_grad,\n            ft_factor=args.ft_factor,\n            grad_clip=args.grad_clip,\n            grad_norm_type=args.grad_norm_type,\n            **optim_sche[\"conf\"],\n        )\n\n    # multi-optimizer scenario\n    if len(optim_sches) &gt; 1:\n        # adjust whether there are parameter overlapping among updated_modules of all the OptimSchedulers\n        is_all_para = [o.updated_modules is None for o in optim_sches.values()]\n        # updated_modules of all the OptimSchedulers cannot be None at the same time\n        if sum(is_all_para) == len(is_all_para):\n            raise RuntimeError\n        else:\n            # collect the updated_modules of all the OptimScheduler\n            para_list = [o.updated_modules for o in optim_sches.values()]\n            # adjust whether there are redundant keys\n            para_set = set(para_list)\n            # there is parameter overlapping if there are redundant keys\n            if len(para_set) != len(para_list):\n                raise RuntimeError\n\n    # resuming from an existing checkpoint\n    if args.resume:\n        try:\n            checkpoint = torch.load(\n                os.path.join(args.train_result_path, \"checkpoint.pth\"),\n                map_location=model.device,\n            )\n            for name in optim_sches.keys():\n                optim_sches[name].load_state_dict(checkpoint[\"optim_sches\"][name])\n        except FileNotFoundError:\n            print(\n                f\"No checkpoint is found in {args.train_result_path}. \"\n                f\"The training process will start from scratch.\"\n            )\n\n    return optim_sches\n</code></pre>"},{"location":"reference/runner/#runner.Runner.dict_transform","title":"<code>dict_transform(src_dict, transform_func)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src_dict</code> required <code>transform_func</code> required <p>Returns:</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef dict_transform(cls, src_dict, transform_func):\n    \"\"\"\n\n    Args:\n        src_dict:\n        transform_func:\n\n    Returns:\n\n    \"\"\"\n    # Multi-dataloader\n    if isinstance(src_dict, Dict):\n        return {key: transform_func(value) for key, value in src_dict.items()}\n    # Single-dataloader\n    else:\n        return transform_func(src_dict)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.gather_all_iter_ascii","title":"<code>gather_all_iter_ascii(iterator, device)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>Iterator</code> required <code>device</code> <code>device</code> required <p>Returns:</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef gather_all_iter_ascii(cls, iterator: Iterator, device: torch.device):\n    \"\"\"\n\n    Args:\n        iterator:\n        device:\n\n    Returns:\n\n    \"\"\"\n    # turn the message into ASCII codes and gather the codes length\n    _iter_asc = torch.LongTensor([ord(char) for char in str(iterator)])\n    _iter_asc_len = torch.LongTensor([_iter_asc.size(0)]).cuda(device)\n\n    _iter_asc_lens = torch.LongTensor(\n        [0 for _ in range(torch.distributed.get_world_size())]\n    ).cuda(device)\n    torch.distributed.all_gather_into_tensor(_iter_asc_lens, _iter_asc_len)\n\n    # padding the ASCII codes to the same length and gather them\n    if _iter_asc_len &lt; _iter_asc_lens.max():\n        _iter_asc = torch.cat(\n            (\n                _iter_asc,\n                torch.zeros(\n                    _iter_asc_lens.max().item() - _iter_asc_len.item(),\n                    dtype=torch.int64,\n                ),\n            )\n        )\n\n    _iter_ascs = torch.zeros(\n        (torch.distributed.get_world_size(), _iter_asc_lens.max().item()),\n        dtype=torch.int64,\n        device=device,\n    )\n    torch.distributed.all_gather_into_tensor(_iter_ascs, _iter_asc.cuda(device))\n    return [_iter_asc for _iter_asc in _iter_ascs], _iter_asc_lens\n</code></pre>"},{"location":"reference/runner/#runner.Runner.main","title":"<code>main(args)</code>  <code>classmethod</code>","text":"<p>The beginning of a experiment branch (training or testing). This function decides the single-GPU or multi-GPU training sub-branch.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>argparse.Namespace The input arguments for the experiment.</p> required Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef main(cls, args: argparse.Namespace):\n    \"\"\"The beginning of a experiment branch (training or testing). This function\n    decides the single-GPU or multi-GPU training sub-branch.\n\n    Args:\n        args: argparse.Namespace\n            The input arguments for the experiment.\n    \"\"\"\n    # This block is for safely calling torch.cuda API in the main process\n    try:\n        torch.multiprocessing.set_start_method(\"spawn\")\n    except RuntimeError:\n        pass\n\n    # --- 1. Initialization of the used GPUs in the current experiment --- #\n    # 'CUDA_VISIBLE_DEVICES' has the higher priority than the argument 'gpus'\n    if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n        args.gpus = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n        if not isinstance(args.gpus, str):\n            args.gpus = str(args.gpus)\n\n    # if 'CUDA_VISIBLE_DEVICES' is not given, initialize it by args.gpus\n    elif args.gpus is not None:\n        if isinstance(args.gpus, List):\n            args.gpus = \",\".join(\n                [\n                    str(g) if not isinstance(g, str) else g\n                    for g in args.gpus\n                    if g != \"\"\n                ]\n            )\n        elif isinstance(args.gpus, int):\n            args.gpus = str(args.gpus)\n        else:\n            assert isinstance(args.gpus, str)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    # if both 'CUDA_VISIBLE_DEVICES' and args.gpus are not given, automatically select available GPUs\n    else:\n        args.gpus = get_idle_gpu(args.ngpu)\n        # make sure that GPU no.0 is the first GPU if it is selected\n        args.gpus = \",\".join(sorted([str(gpu.id) for gpu in args.gpus]))\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    # convert the GPU absolute number to the GPU relative index to fit 'CUDA_VISIBLE_DEVICES'\n    args.gpus = [idx for idx, _ in enumerate(args.gpus.split(\",\"))]\n\n    # check the GPU configuration\n    assert len(args.gpus) &gt;= args.ngpu, (\n        f\"Error: The visible GPUs {args.gpus} are fewer than the GPUs you would like to use {args.ngpu}, default is 2! \"\n        f\"Use the argument '--gpu' to directly specify your target GPUs.\"\n    )\n    if len(args.gpus) == 1:\n        args.gpus = args.gpus[0]\n\n    # --- 2. Initialization of DDP distribution pipeline --- #\n    # get the world_size from the command line, world_size here means the number of nodes\n    if args.dist_url == \"env://\":\n        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n        raise NotImplementedError(\n            \"Multi-node DDP distributed training is not supported now.....\"\n        )\n\n    # distributed is set to true if multiple GPUs are specified or multiple nodes are specified\n    # args.world_size &gt; 1 means multi-node distribution while args.ngpu &gt; 1 means multi-GPU distribution\n    args.distributed = args.world_size &gt; 1 or args.ngpu &gt; 1\n\n    # multi-GPU distributed training and testing\n    if args.ngpu &gt; 1:\n        # check whether the input number of GPUs is valid\n        ngpus_per_node = torch.cuda.device_count()\n        if args.ngpu &gt; ngpus_per_node:\n            warnings.warn(\n                f\"Your input args.ngpu ({args.ngpu}) is larger than the GPUs you have on your machine \"\n                f\"({ngpus_per_node}). Currently, the real args.ngpu becomes {ngpus_per_node}.\"\n            )\n            args.ngpu = ngpus_per_node\n        # here world_size becomes the total number of processes on all nodes\n        args.world_size = args.ngpu * args.world_size\n\n        # automatic port selection if no specified port (only one ':' in args.dist_url)\n        if len(args.dist_url.split(\":\")) &lt; 3:\n            args.dist_url += f\":{get_idle_port()}\"\n        else:\n            raise RuntimeError\n\n        # run one process on each GPU\n        mp.spawn(cls.main_worker, nprocs=args.ngpu, args=(args,))\n\n    # single-GPU training and testing\n    elif args.ngpu == 1:\n        if isinstance(args.gpus, List) and len(args.gpus) &gt; 1:\n            warnings.warn(\n                f\"Your input args.ngpu {args.gpus} is more than one. \"\n                f\"Currently, the GPU no.{args.gpus[0]} will be used.\"\n            )\n            args.gpus = args.gpus[0]\n        cls.main_worker(args.gpus, args)\n\n    # CPU testing with the multiprocessing strategy\n    elif args.test:\n        raise NotImplementedError(\n            \"Multiprocessing CPU testing part has not been implemented yet......\"\n        )\n\n    # CPU training is not supported\n    else:\n        raise RuntimeError(\n            \"Our toolkit doesn't support CPU training. Please specify a number of GPUs......\"\n        )\n</code></pre>"},{"location":"reference/runner/#runner.Runner.main_worker","title":"<code>main_worker(gpu, args)</code>  <code>classmethod</code>","text":"<p>The main body of a process on one GPU.</p> <p>Parameters:</p> Name Type Description Default <code>gpu</code> <code>int</code> required <code>args</code> <code>Namespace</code> required Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef main_worker(cls, gpu: int, args: argparse.Namespace):\n    \"\"\"The main body of a process on one GPU.\n\n    Args:\n        gpu:\n        args:\n    \"\"\"\n    # --- 0. Random Seed Preparation --- #\n    # set different random seeds for the different GPU processes in DDP mode to avoid the process homogeneity\n    if args.distributed and not args.same_proc_seed:\n        args.seed += gpu\n    cls.set_random_seeds(args.seed)\n\n    # --- 1. Experimental Reproducibility Preparation --- #\n    torch.backends.cudnn.enabled = args.cudnn_enabled\n    torch.backends.cudnn.benchmark = args.cudnn_benchmark\n    torch.backends.cudnn.deterministic = args.cudnn_deterministic\n    # torch.use_deterministic_algorithms(torch.backends.cudnn.deterministic)\n    # For more details about 'CUBLAS_WORKSPACE_CONFIG',\n    # please refer to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n    if V(torch.version.cuda) &gt;= V(\"10.2\"):\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n\n    # --- 2. DDP Model Distribution Initialization --- #\n    if args.distributed:\n        # load the global node rank from the os environment in the multi-node setting\n        if args.dist_url == \"env://\":\n            args.rank = int(os.environ[\"RANK\"])\n\n        if args.ngpu &gt; 1:\n            # Here, the rank is turned from the local node-level rank to the global process-level rank\n            # the input argument 'gpu' is the local rank of the current process in the specific node\n            args.rank = args.rank * args.ngpu + gpu\n        # initialize the distributed environment, connections among all the processes are established here\n        dist.init_process_group(\n            backend=args.dist_backend,\n            init_method=args.dist_url,\n            world_size=args.world_size,\n            rank=args.rank,\n        )\n\n    # --- 3. Experimental Environment Logging --- #\n    if args.config is not None:\n        _config_split = args.config.split(\"/\")\n    else:\n        _config_split = None\n    # automatically decide the result path if not given\n    if args.train_result_path is None:\n        assert (\n            _config_split is not None\n        ), \"If you want to automatically generate train_result_path, please give the configuration by '--config'.\"\n        args.train_result_path = \"/\".join(\n            _config_split[:-2]\n            + [\"exp\", \".\".join(_config_split[-1].split(\".\")[:-1])]\n        )\n    # attach a folder named by args.config to the end of your given result path\n    elif args.attach_config_folder_to_path:\n        # if `--config` is given, attach the name of exp_cfg to the end of train_result_path\n        if _config_split is not None:\n            args.train_result_path = os.path.join(\n                args.train_result_path, \".\".join(_config_split[-1].split(\".\")[:-1])\n            )\n\n    # initialize the logger and save current script command\n    logger = logger_stdout_file(\n        args.train_result_path,\n        \"train\" if args.train else None,\n        args.distributed,\n        args.rank,\n    )\n\n    # logging the beginning info of the experiment\n    logger.info(f\"Current script command: {' '.join([xi for xi in sys.argv])}\")\n    if args.distributed:\n        logger.info(\n            f\"Multi-GPU distribution information: \"\n            f\"backend={args.dist_backend}, init_method={args.dist_url}, \"\n            f\"nnode={int(args.world_size / args.ngpu)}, ngpu_per_node={args.ngpu}, \"\n            f\"used_gpus={args.gpus}.\"\n        )\n\n    # initialize the computational equipments\n    assert (\n        torch.cuda.is_available()\n    ), \"CUDA is not available! It fails to conduct GPU training.\"\n    # args.gpu is the GPU used in the current process while args.gpus are all the available GPUss\n    args.gpu = args.gpus[gpu] if args.ngpu &gt; 1 else args.gpus\n    device = torch.device(f\"cuda:{args.gpu}\")\n    torch.cuda.device(device)\n    logger.info(f\"Used GPU in the master process: {device}\")\n\n    # --- 4. Configuration Loading --- #\n    # resume from an existing checkpoint, loading the old data and train configurations\n    if args.resume:\n        # loading the existing data and train configurations\n        # But the input data configuration has higher priority than the existing one\n        if args.data_cfg is not None:\n            data_cfg = (\n                load_yaml(open(args.data_cfg))\n                if isinstance(args.data_cfg, str)\n                else args.data_cfg\n            )\n        else:\n            data_cfg = load_yaml(\n                open(\n                    os.path.join(\n                        args.train_result_path,\n                        f\"{'train' if args.train else 'test'}_data_cfg.yaml\",\n                    )\n                )\n            )\n        # training configuration will be loaded from the existing file\n        train_cfg = load_yaml(\n            open(os.path.join(args.train_result_path, \"train_cfg.yaml\"))\n        )\n    # start from scratch, loading the new data and train configurations\n    else:\n        assert (\n            args.data_cfg is not None and args.train_cfg is not None\n        ), \"Please specify a data configuration file and a train configuration file!\"\n        data_cfg = (\n            load_yaml(open(args.data_cfg))\n            if isinstance(args.data_cfg, str)\n            else args.data_cfg\n        )\n        train_cfg = (\n            load_yaml(open(args.train_cfg))\n            if isinstance(args.train_cfg, str)\n            else args.train_cfg\n        )\n\n    # --- 5. Data Iterator Initialization --- #\n    iterators = cls.build_iterators(data_cfg=data_cfg, args=args)\n\n    # logging the information of the iterators\n    _iter_message = \"The information of the iterators:\"\n    for dset, iters in iterators.items():\n        # single iterator for the current dataset\n        if isinstance(iters, Iterator):\n            # gather the iterator message from all the process in the multi-GPU distributed training mode\n            if args.distributed:\n                _iter_ascs, _iter_asc_lens = cls.gather_all_iter_ascii(\n                    iters, device\n                )\n\n                # recover the codes from all the processes back to the text\n                for i, asc in enumerate(_iter_ascs):\n                    _iter_text = \"\".join(\n                        [chr(a) for a in asc[: _iter_asc_lens[i]].tolist()]\n                    )\n                    _iter_message += f\"\\nThe iterator in the {dset} set of the rank no.{i}: {_iter_text}\"\n            # directly report the message in the single-GPU mode\n            else:\n                _iter_message += f\"\\nThe iterator in the {dset} set: {iters}\"\n\n        # multiple iterators for the current dataset\n        elif isinstance(iters, Dict):\n            for name, iterator in iters.items():\n                # gather the iterator message from all the process in the multi-GPU distributed training mode\n                if args.distributed:\n                    _iter_ascs, _iter_asc_lens = cls.gather_all_iter_ascii(\n                        iterator, device\n                    )\n\n                    # recover the codes from all the processes back to the text\n                    for i, asc in enumerate(_iter_ascs):\n                        _iter_text = \"\".join(\n                            [chr(a) for a in asc[: _iter_asc_lens[i]].tolist()]\n                        )\n                        _iter_message += f\"\\nThe {name} iterator in the {dset} set of the rank no.{i}: {_iter_text}\"\n                # directly report the message in the single-GPU mode\n                else:\n                    _iter_message += (\n                        f\"\\nThe {name} iterator in the {dset} set: {iterator}\"\n                    )\n\n    logger.info(_iter_message)\n\n    # --- 6. Model Initialization --- #\n    assert (\n        \"model\" in train_cfg.keys()\n    ), \"Please fill in the 'model' tag of your given train_cfg!\"\n    model = cls.build_model(train_cfg[\"model\"], args=args, device=device)\n    logger.info(model_summary(model))\n\n    # for the process of single-GPU training or the rank 0 process of multi-GPUs training\n    if not args.distributed or args.rank == 0:\n        # dumping all the configuration files into train_result_path for resuming\n        with open(\n            os.path.join(args.train_result_path, \"exp_cfg.yaml\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            yaml.dump(vars(args), f, sort_keys=False)\n        with open(\n            os.path.join(\n                args.train_result_path,\n                f\"{'train' if args.train else 'test'}_data_cfg.yaml\",\n            ),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            yaml.dump(data_cfg, f, sort_keys=False)\n        with open(\n            os.path.join(args.train_result_path, \"train_cfg.yaml\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            yaml.dump(train_cfg, f, sort_keys=False)\n\n    # --- 7.1. Model Training Branch --- #\n    if args.train:\n        # initialize the Monitor for training and validation\n        monitor = (\n            None\n            if args.distributed and args.rank != 0\n            else TrainValidMonitor(logger=logger, args=args, model=model)\n        )\n\n        # loading the model from the existing checkpoint for resuming the training process\n        args.start_epoch = cls.resume(args=args, model=model, monitor=monitor)\n\n        # DDP Wrapping of the model must be done after model checkpoint loading\n        if args.distributed:\n            if args.enable_syncbatchnorm:\n                # turn the batchnorm layers into the sync counterparts\n                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n            # Here the model buffers and parameters of the master process are broadcast to the other processes\n            model = torch.nn.parallel.DistributedDataParallel(\n                model, device_ids=[args.gpu], output_device=args.gpu\n            )\n\n        # initialize the OptimSchedulers after DDP wrapping (including optimization resuming)\n        assert (\n            \"optim_sches\" in train_cfg.keys()\n        ), \"Please fill in the 'optim_sches' tag!\"\n        optim_sches = cls.build_optim_sches(\n            model=model, optim_sche_cfg=train_cfg[\"optim_sches\"], args=args\n        )\n\n        # logging the information of the optimschedulers\n        for name, optim_sche in optim_sches.items():\n            logger.info(f\"The {name} OptimScheduler: {optim_sche}\")\n\n        # start the training process\n        cls.train(\n            args=args,\n            data_cfg=data_cfg,\n            iterators=iterators,\n            model=model,\n            optim_sches=optim_sches,\n            logger=logger,\n            monitor=monitor,\n        )\n\n    # --- 7.2. Model Testing Branch --- #\n    elif args.test:\n        if isinstance(args.test_model, str):\n            args.test_model = [args.test_model]\n        elif not isinstance(args.test_model, List):\n            raise ValueError\n\n        # loop each model to be tested\n        for model_name in args.test_model:\n            # get the path of the target model parameters\n            _models_path = os.path.join(args.train_result_path, \"models\")\n            # for compatibility with the older version\n            if os.path.exists(os.path.join(_models_path, f\"{model_name}.mdl\")):\n                model_path = os.path.join(_models_path, f\"{model_name}.mdl\")\n            elif os.path.exists(os.path.join(_models_path, f\"{model_name}.pth\")):\n                model_path = os.path.join(_models_path, f\"{model_name}.pth\")\n            else:\n                raise RuntimeError(\n                    f\"{os.path.join(_models_path, '%s.pth' % model_name)} is not found!\"\n                )\n\n            # load the target model parameters\n            model.load_state_dict(torch.load(model_path, map_location=model.device))\n\n            # start the testing process\n            cls.test(\n                args=args, test_model=model_name, iterators=iterators, model=model\n            )\n\n    else:\n        raise RuntimeError(\n            \"train and test in args cannot be False at the same time!\"\n        )\n\n    # --- 8. Release Computational Resource --- #\n    if args.distributed and args.rank == 0:\n        torch.distributed.destroy_process_group()\n    sys.exit(0)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.parse","title":"<code>parse()</code>  <code>classmethod</code>","text":"<p>The static function that outputs all the default arguments for the runner.</p> <p>Returns:</p> Type Description <p>a Dict containing the key-value pairs of all arguments</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef parse(cls):\n    \"\"\"The static function that outputs all the default arguments for the runner.\n\n    Returns:\n        a Dict containing the key-value pairs of all arguments\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    # All-in-one configuration setting\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        # default=None,\n        default=\"recipes/asr/librispeech/train-clean-100/exp_cfg/100-bpe5k_conformer-medium_lr2e-3.yaml\",\n        help=\"The path of the all-in-one experiment configuration file. You can write all the arguments in this \"\n        \"all-in-one file instead of giving them to `runner.py` by command lines.\",\n    )\n\n    # Experimental environment\n    group = parser.add_argument_group(\"Group 1: Calculation and System Backend\")\n    group.add_argument(\n        \"--seed\",\n        type=int,\n        default=0,\n        help=\"Initial random seed for the experiment. (default: 0)\",\n    )\n    group.add_argument(\n        \"--cudnn_enabled\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to activate torch.backends.cudnn. (default: True)\",\n    )\n    group.add_argument(\n        \"--cudnn_benchmark\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to activate torch.backends.cudnn.benchmark. \"\n        \"When True, the process of model training will be speed up and the model performance may improve \"\n        \"somewhat. But your results will become less reproducible. (default: False)\",\n    )\n    group.add_argument(\n        \"--cudnn_deterministic\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to activate torch.backends.cudnn.deterministic. \"\n        \"This will improve the reproducibility of your experiments. (default: True)\",\n    )\n    group.add_argument(\n        \"--train_num_workers\",\n        type=int,\n        default=1,\n        help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n        \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n        \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n        \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n        \"value should be within your machine capability (i.e., the number of CPU cores). \"\n        \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n    )\n    group.add_argument(\n        \"--valid_num_workers\",\n        type=int,\n        default=1,\n        help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n        \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n        \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n        \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n        \"value should be within your machine capability (i.e., the number of CPU cores). \"\n        \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n    )\n    group.add_argument(\n        \"--test_num_workers\",\n        type=int,\n        default=1,\n        help=\"The number of worker processes in the `torch.utils.data.DataLoader` of each epoch. \"\n        \"If you have complicated logic of data loading and data augmentation in the memory before passing the \"\n        \"data to the model (e.g., speech speed perturbation, environmental noise addition, ...), raising this \"\n        \"argument may improve the speed of data loading and pre-augmentation. But the choice of the argument \"\n        \"value should be within your machine capability (i.e., the number of CPU cores). \"\n        \"If you want to debug your programs, we recommend you to set this argument to 0. (default: 1)\",\n    )\n    group.add_argument(\n        \"--pin_memory\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to activate `pin_memory` for the Dataloader of each epoch. \"\n        \"If True, the pinned memory in the dataloaders will be activated and the data loading will be further \"\n        \"speed up. \"\n        \"pin_memory=True is often used together with non_blocking=True. Note that this combination requires a \"\n        \"large amount of memory and CPU cores. (default: False)\",\n    )\n    group.add_argument(\n        \"--non_blocking\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to activate `non_blocking` when transferring data from the memory to GPUs. \"\n        \"If True, the process of model training will be speed up. \"\n        \"non_blocking=True is often used together with pin_memory=True. Note that this combination requires a \"\n        \"large amount of memory and CPU cores. (default: False)\",\n    )\n\n    # gradient descent related\n    group = parser.add_argument_group(\n        \"Group 2: Gradient Calculation and Back-Propagation\"\n    )\n    group.add_argument(\n        \"--use_amp\",\n        type=str2bool,\n        default=True,\n        help=\"Whether activate AMP (Automatic Mixed Precision) during the back-propagation. \"\n        \"If True, the GPU consumption of your model will be smaller so that you can include more data \"\n        \"instances in a single batch. (default: True)\",\n    )\n    group.add_argument(\n        \"--grad_clip\",\n        type=float,\n        default=5.0,\n        help=\"Gradient clipping threshold during the back-propagation. (default: 5.0)\",\n    )\n    group.add_argument(\n        \"--grad_norm_type\",\n        type=float,\n        default=2.0,\n        help=\"Normalization type used when clipping the gradients. (default: 2.0)\",\n    )\n    group.add_argument(\n        \"--accum_grad\",\n        type=int,\n        default=1,\n        help=\"The number of gradient accumulation steps. \"\n        \"To mimic the gradients calculated by large batches with only a small amount of GPUs, please raise \"\n        \"this argument. \"\n        \"The virtual batch size will become (accum_grad * the actual batch size). \"\n        \"Note that the model trained by accum_grad is not identical to the one actually trained by large \"\n        \"batches because of the different randomness in each training step and the existence of BatchNorm. \"\n        \"(default: 1)\",\n    )\n    group.add_argument(\n        \"--ft_factor\",\n        type=float,\n        default=1.0,\n        help=\"The finetuing factor used to scale down learning rates during the parameter optimization. \"\n        \"If `ft_factor` is smaller than 1.0, the learning rates will be proportionally decreased without \"\n        \"changing its scheduling strategy. Usually, ft_factor could be set from 0.1 to 0.5 depending on your \"\n        \"finetuning scenarios. (default: 1.0)\",\n    )\n\n    # multi-GPU distributed training\n    group = parser.add_argument_group(\"Group 3: Multi-GPU Distribution\")\n    group.add_argument(\n        \"--dist_backend\",\n        default=\"nccl\",\n        type=str,\n        help=\"Communication backend for multi-GPU distribution. \"\n        \"If you are using NVIDIA GPUs, we recommend you set this argument to 'nccl'. (default: nccl)\",\n    )\n    group.add_argument(\n        \"--dist_url\",\n        type=str,\n        default=\"tcp://127.0.0.1\",\n        help=\"Communication URL for multi-GPU distribution. \"\n        \"The default value is 'tcp://127.0.0.1' for single-node distributed training and an idle port will be \"\n        \"automatically selected. \"\n        \"The port number cannot be set manually, which means that the argument 'tcp://127.0.0.1:xxxxx' will \"\n        \"have the same effect with 'tcp://127.0.0.1'. \"\n        \"If you want to train your model on multiple nodes, please set dist_url='env://' \"\n        \"(Note: multi-node model distribution is still in beta). \"\n        \"In this case, env values of 'MASTER_PORT', 'MASTER_ADDR', 'WORLD_SIZE', and 'RANK' are referred in \"\n        \"the command line.\",\n    )\n    group.add_argument(\n        \"--world_size\",\n        default=1,\n        type=int,\n        help=\"The number of nodes for model distribution. \"\n        \"This argument is fixed to 1. Currently, we don't recommend you to modify its value.\"\n        \"If you want to conduct multi-node model distribution, please give `world_size` by `WORLD_SIZE=XXX` \"\n        \"in your terminal (Note: multi-node model distribution is still in beta).\",\n    )\n    group.add_argument(\n        \"--rank\",\n        default=0,\n        type=int,\n        help=\"The global rank of the current node for model distribution. \"\n        \"This argument is fixed to 0. Currently, we don't recommend you to modify its value.\"\n        \"If you want to conduct multi-node model distribution, please give `rank` by `RANK=XXX` in your \"\n        \"terminal (Note: multi-node model distribution is still in beta).\",\n    )\n    group.add_argument(\n        \"--ngpu\",\n        type=int,\n        default=1,\n        help=\"The number of GPUs used to run your experiment. \"\n        \"If ngpu is larger than 1, multi-GPU model distribution will be activated. (default: 1)\",\n    )\n    group.add_argument(\n        \"--gpus\",\n        type=str2none,\n        default=None,\n        help=\"This argument specifies the GPUs used to run your experiment. \"\n        \"If you want to specify multiple GPUs, please give this argument in the form of 'x,x,x' \"\n        \"where different GPUs are separated by a comma (please don't end this argument with ','). \"\n        \"Of course, you could also specify your target GPUs by `CUDA_VISIBLE_DEVICES` in the terminal.\"\n        \"If this argument is not given, the framework will automatically select `ngpu` idle GPUs. \",\n    )\n    group.add_argument(\n        \"--same_proc_seed\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to set the same initial random seed for all the GPU processes in DDP mode. \"\n        \"The different random seeds can prevent model distribution from the process homogeneity, \"\n        \"e.g., different GPU processes may have the same on-the-fly data augmentation strategy \"\n        \"(noise addition, SpecAugment, ...) if they have the same initial random seed. \"\n        \"Note: please set this argument to True if you want to use random data selection for your dataloaders \"\n        \"in the DDP mode. (default: False)\",\n    )\n    group.add_argument(\n        \"--ignore_train_exception\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to ignore the exceptions happening during training and validation. \"\n        \"If set to True, your training would not be interrupted by some nonfatal errors, such as occasional \"\n        \"'RuntimeError: CUDA Out of memory', and etc. (default: False)\",\n    )\n    group.add_argument(\n        \"--ignore_test_exception\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to ignore the exceptions happening during testing. \"\n        \"If set to True, your testing would not be interrupted by some nonfatal errors, such as occasional \"\n        \"'RuntimeError: CUDA Out of memory', and etc. (default: False)\",\n    )\n    group.add_argument(\n        \"--enable_syncbatchnorm\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to process the model by 'torch.nn.SyncBatchNorm.convert_sync_batchnorm' for multi-GPU \"\n        \"distributed training. Sometimes your training may be stuck at some points or terminate without being \"\n        \"notified of any errors in the multi-GPU distributed mode. If that happens, you can disable \"\n        \"SyncBatchNorm and debug your codes. (default: True)\",\n    )\n\n    # Training monitoring\n    group = parser.add_argument_group(\"Group 4: Model Training\")\n    group.add_argument(\n        \"--train_result_path\",\n        type=str,\n        default=None,\n        help=\"Where to place all the experiment folder that contains all the result files. \"\n        \"If not given, `train_result_path` wil be automatically initialized by your input `config`. \"\n        \"For example, if your input `config` is \"\n        \"{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/XXXXX.yaml, your `train_result_path` \"\n        \"will be automatically initialized to `{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp/`.\"\n        \"(default: None)\",\n    )\n    group.add_argument(\n        \"--attach_config_folder_to_path\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to attach an additional folder named by your input `--config` at the end of your input \"\n        \"`train_result_path`. (default: True)\",\n    )\n    group.add_argument(\n        \"--train\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to go through the model training branch. (default: False)\",\n    )\n    group.add_argument(\n        \"--dry_run\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to turn on the dry-running mode. \"\n        \"In this mode, only the data loading will be done to see its speed and robustness. \"\n        \"Model calculation and parameter optimization will be skipped. (default: False)\",\n    )\n    group.add_argument(\n        \"--no_optim\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to turn on the no-optimization mode. \"\n        \"In this mode, only the data loading and model calculation will be done to see their speed, \"\n        \"robustness, and memory consumption. (default: False) \"\n        \"(Note: 'dry_run' has the higher priority than 'no_optim'. It means that the model calculation will \"\n        \"be skipped if you give both '--dry_run True' and '--no_optim True'.) \",\n    )\n    group.add_argument(\n        \"--resume\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to resume your model training or testing experiment from the checkpoints. \"\n        \"If True, there must be .pth checkpoint files of your existing experiment in `train_result_path` or \"\n        \"`test_result_path`. This argument is shared by the training and testing branches. (default: False)\",\n    )\n    group.add_argument(\n        \"--start_epoch\",\n        type=int,\n        default=1,\n        help=\"The starting epoch of your experiments. This argument will be automatically initialized by your \"\n        \"checkpoint files if `--resume` is given. (default: 1)\",\n    )\n    group.add_argument(\n        \"--num_epochs\",\n        type=int,\n        default=1000,\n        help=\"The maximum number of training epochs of your experiments. (default: 1000)\",\n    )\n    group.add_argument(\n        \"--valid_per_epochs\",\n        type=int,\n        default=1,\n        help=\"The interval of going through the validation phase during training. \"\n        \"If not given, validation will be done right after parameter optimization in each epoch. (default: 1)\",\n    )\n    group.add_argument(\n        \"--report_per_steps\",\n        type=int,\n        default=0,\n        help=\"The interval of reporting step information logs during model training or testing. \"\n        \"Positive integers mean the absolute reporting intervals that a step report will be made after each \"\n        \"'report_per_steps' steps; \"\n        \"Negative integers mean the relative reporting intervals that there will be -'report_per_steps' \"\n        \"reports in each epoch. \"\n        \"If not given, there will be default 10 reports in each epoch. \",\n    )\n    group.add_argument(\n        \"--best_model_selection\",\n        type=str2list,\n        default=None,\n        help=\"The ways of selecting the best models. This argument should be given as a list of quad-tuples, i.e., \"\n        \"('metric_group', 'metric_name', 'metric_mode', 'model_number'). \"\n        \"'metric_group' can be either 'train' or 'valid' which indicates the group the metric belongs to; \"\n        \"'metric_name' is the name of the metric you select; \"\n        \"'metric_mode' can be either 'min' or 'max' which indicates how to select the models by this metric; \"\n        \"'model_number' indicates how many best models will be saved by this metric. \"\n        \"Note: the metric of the first tuple in the list will be used to do early-stopping for model training.\"\n        \"(default: None)\",\n    )\n    group.add_argument(\n        \"--early_stopping_patience\",\n        type=int,\n        default=None,\n        help=\"The maximum number of epochs when the model doesn't improve its performance before stopping the \"\n        \"model training. If not given, early-stopping will not be adapted. (default: None)\",\n    )\n    group.add_argument(\n        \"--early_stopping_threshold\",\n        type=float,\n        default=0.001,\n        help=\"The threshold to refresh the early-stopping status in the monitor during model training. \"\n        \"Positive float numbers in (0.0, 1.0) mean the relative threshold over the current best performance. \"\n        \"Negative float numbers main the absolute threshold over the current best performance. \"\n        \"early_stopping_threshold=0 means no early-stopping threshold is applied to the current best \"\n        \"performance when deciding whether to refresh the status. (default: 0.005)\",\n    )\n    group.add_argument(\n        \"--last_model_number\",\n        type=int,\n        default=1,\n        help=\"The number of models saved for the last several epochs. \"\n        \"This argument cannot be lower than 1 otherwise the training will not be able to resume. \"\n        \"(default: 1)\",\n    )\n\n    # Training Snapshotting\n    group = parser.add_argument_group(\n        \"Group 5: Real-time Model Visualization Snapshotting\"\n    )\n    group.add_argument(\n        \"--monitor_snapshot_conf\",\n        type=str2dict,\n        default=dict(),\n        help=\"The configuration given to `matploblib.plot()` in `{SPEECHAIN_ROOT/speechain/snapshooter.py}` to \"\n        \"plot curve figures for real-time model visualization during model training. \"\n        \"This argument should be given in the form of a Dict. (default: an empty Dict)\",\n    )\n    group.add_argument(\n        \"--visual_snapshot_number\",\n        type=int,\n        default=0,\n        help=\"The number of the validation data instances used to make snapshots made during model visualization. \"\n        \"This argument should be smaller than the number of your validation data instances. \"\n        \"(default: 0)\",\n    )\n    group.add_argument(\n        \"--visual_snapshot_interval\",\n        type=int,\n        default=5,\n        help=\"The snapshotting interval of model visualization during model training. \"\n        \"This argument should be a positive integer which means that model visualization will be done once \"\n        \"in every `visual_snapshot_interval` epochs. (default: 5)\",\n    )\n\n    # Testing\n    group = parser.add_argument_group(\"Group 6: Model Testing\")\n    group.add_argument(\n        \"--test_result_path\",\n        type=str,\n        default=None,\n        help=\"Where to place all the result files generated during model testing. \"\n        \"If not given, `test_result_path` wil be automatically initialized by your input `train_result_path` \"\n        \"and `test_model`. For example, if your `train_result_path` is \"\n        \"`{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp`, and `test_model` is `MMMMM`, \"\n        \"then your `test_result_path` will be automatically initialized to \"\n        \"`{SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp/XXXXX/MMMMM/` where 'XXXXX' is the name of \"\n        \"your configuration file given by `--config`.\",\n    )\n    group.add_argument(\n        \"--test\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to go through the model testing branch. (default: False)\",\n    )\n    group.add_argument(\n        \"--test_model\",\n        type=str2list,\n        default=None,\n        help=\"The names of the model you want to evaluate during model testing. \"\n        \"If given, `{train_result_path}/XXXXX/model/{test_model}.pth` will be used to initialize the parameters \"\n        \"of the Model object. If you only want to evaluate multiple models in one job, please give the \"\n        \"strings of their names in a List. (default: None)\",\n    )\n    group.add_argument(\n        \"--attach_model_folder_when_test\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to attach an additional sub-folder named by your input `--test_model` in the testing \"\n        \"result folder. (default: True)\",\n    )\n    group.add_argument(\n        \"--bad_cases_selection\",\n        type=str2list,\n        default=None,\n        help=\"The selection methods of the top-N bad cases during model testing. \"\n        \"This argument should be given as a list of tri-tuples \"\n        \"('selection_metric', 'selection_mode', 'case_number'). \"\n        \"For example, ('wer', 'max', 50) means 50 testing waveforms with the largest WER will be selected. \"\n        \"Multiple tuples can be given to present different sets of top-n bad cases. (default: None)\",\n    )\n    group.add_argument(\n        \"--saving_proc_num\",\n        type=int,\n        default=1,\n        help=\"The number of daemon processes used to save data generated during testing to the disk. (default: 1)\",\n    )\n\n    # Experiment configuration\n    group = parser.add_argument_group(\n        \"Group 7: Experiment .yaml Configuration File\"\n    )\n    group.add_argument(\n        \"--data_cfg\",\n        type=str2dict,\n        default=None,\n        help=\"The path of the configuration file for data loading and batching. \"\n        \"This argument is required for both model training and testing.\",\n    )\n    group.add_argument(\n        \"--train_cfg\",\n        type=str2dict,\n        default=None,\n        help=\"The path of the configuration file for model construction and parameter optimization. \"\n        \"This argument is required for both model training (both 'model' and 'optim_sche' need to be given) \"\n        \"and testing (only 'model' needs to be given).\",\n    )\n    group.add_argument(\n        \"--infer_cfg\",\n        type=str2dict,\n        default=None,\n        help=\"The configuration file for model inference during model testing. \"\n        \"This argument is required for model testing.\"\n        \"For more details about how to give infer_cfg, please refer to the handbook.md. (default: None)\",\n    )\n\n    # Add customized arguments if needed\n    parser = cls.add_parse(parser)\n    return parser.parse_args()\n</code></pre>"},{"location":"reference/runner/#runner.Runner.resume","title":"<code>resume(args, model, monitor)</code>  <code>classmethod</code>","text":"<p>Load the model parameters to the current process. This operation is necessary in our toolkit because we need to make sure that the models in all the processes have the same buffer and parameter tensors.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>argparse.Namespace The input arguments.</p> required <code>model</code> <code>Model</code> <p>Model The model to be trained.</p> required <code>monitor</code> <code>TrainValidMonitor</code> <p>TrainValidMonitor The train-valid monitor used to monitor the training phase</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting</p> <code>int</code> <p>epoch will be loaded from the checkpoint; otherwise, 1 will be returned.</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef resume(\n    cls, args: argparse.Namespace, model: Model, monitor: TrainValidMonitor\n) -&gt; int:\n    \"\"\"Load the model parameters to the current process. This operation is necessary\n    in our toolkit because we need to make sure that the models in all the processes\n    have the same buffer and parameter tensors.\n\n    Args:\n        args: argparse.Namespace\n            The input arguments.\n        model: Model\n            The model to be trained.\n        monitor: TrainValidMonitor\n            The train-valid monitor used to monitor the training phase\n\n    Returns:\n        The number of the starting epoch. If the training resumes from an existing checkpoint, then the starting\n        epoch will be loaded from the checkpoint; otherwise, 1 will be returned.\n    \"\"\"\n    # start the training from the existing checkpoint\n    if args.resume:\n        # load the existing checkpoint\n        checkpoint = torch.load(\n            os.path.join(args.train_result_path, \"checkpoint.pth\"),\n            map_location=model.device,\n        )\n        # load the latest training epoch\n        start_epoch = checkpoint[\"start_epoch\"]\n        # for compatibility with old versions\n        if \"latest_model\" in checkpoint.keys():\n            model.load_state_dict(checkpoint[\"latest_model\"])\n        else:\n            model.load_state_dict(\n                torch.load(\n                    os.path.join(args.train_result_path, \"models\", \"latest.pth\"),\n                    map_location=model.device,\n                )\n            )\n\n        # loading the monitor\n        if monitor is not None:\n            # for compatibility with old versions\n            if \"monitor\" not in checkpoint.keys():\n                monitor.load_state_dict(\n                    dict(\n                        train_monitor=checkpoint[\"train_monitor\"],\n                        valid_monitor=checkpoint[\"valid_monitor\"],\n                    )\n                )\n            else:\n                monitor.load_state_dict(checkpoint[\"monitor\"])\n            # info logging\n            monitor.logger.info(\n                f\"The training process resumes from the epoch no.{start_epoch}.\"\n            )\n\n    # start the training from scratch\n    else:\n        start_epoch = 1\n\n    return start_epoch\n</code></pre>"},{"location":"reference/runner/#runner.Runner.run","title":"<code>run()</code>  <code>classmethod</code>","text":"<p>The preparation area of Runner where the configuration is parsed and converted into code-friendly format.</p> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef run(cls):\n    \"\"\"The preparation area of Runner where the configuration is parsed and\n    converted into code-friendly format.\"\"\"\n    # --- 0. Get the Command Line Arguments --- #\n    args = cls.parse()\n\n    # --- 1. Read the Non-Config Arguments from the Command Line --- #\n    # Currently, 'world_size' and 'rank' are not provided to users to set\n    given_args = [\"world_size\", \"rank\"]\n    # The arguments that users give in the command line should not be refreshed by the argument '--config'\n    for i in sys.argv:\n        if i.startswith(\"--\"):\n            given_args.append(i.replace(\"-\", \"\"))\n\n    # check the train and test flags\n    if \"train\" in given_args and \"test\" in given_args:\n        assert (args.train ^ args.test) is True, (\n            \"A running job can only conduct either training process or testing process, \"\n            \"so args.train and args.test cannot be True at the same time. \"\n            \"If you want to conduct training and testing sequentially, \"\n            \"please make two running jobs where the first job has args.train=True and args.test=False and \"\n            \"the second job has args.train=False and args.test=True.\"\n        )\n    elif \"train\" in given_args:\n        given_args.append(\"test\")\n        args.test = not args.train\n    elif \"test\" in given_args:\n        given_args.append(\"train\")\n        args.train = not args.test\n\n    # the command 'CUDA_VISIBLE_DEVICES' has the higher priority than the argument 'gpus'\n    if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n        given_args.append(\"gpus\")\n        args.gpus = None\n\n    # --- 2. Overwrite the Arguments by '--config' --- #\n    # overwrite args from the args.config\n    # Note: the ones given in the command line has the higher priority than args.config\n    if args.config is not None:\n        args.config = parse_path_args(args.config)\n        config = load_yaml(open(args.config, mode=\"r\", encoding=\"utf-8\"))\n        for c in config.keys():\n            if c not in given_args:\n                # remove the port number in 'dist_url' if given\n                if c == \"dist_url\":\n                    assert len(config[c].split(\":\")) &lt;= 3\n                    if len(config[c].split(\":\")) == 3:\n                        config[c] = \":\".join(config[c].split(\":\")[:-1])\n                # skip the existing 'report_per_steps' (either use default value or give it in the command line)\n                if c == \"report_per_steps\":\n                    continue\n                # set the argument from config to args\n                setattr(args, c, config[c])\n\n    # make sure that all the paths are absolute paths\n    if args.train_result_path is not None:\n        args.train_result_path = parse_path_args(args.train_result_path)\n    if args.test_result_path is not None:\n        args.test_result_path = parse_path_args(args.test_result_path)\n    if args.data_cfg is not None and not isinstance(args.data_cfg, Dict):\n        args.data_cfg = parse_path_args(args.data_cfg)\n    if args.train_cfg is not None and not isinstance(args.train_cfg, Dict):\n        args.train_cfg = parse_path_args(args.train_cfg)\n    if args.infer_cfg is not None:\n        if isinstance(args.infer_cfg, str):\n            args.infer_cfg = parse_path_args(args.infer_cfg)\n        elif isinstance(args.infer_cfg, List):\n            args.infer_cfg = [\n                parse_path_args(cfg) if isinstance(cfg, str) else cfg\n                for cfg in args.infer_cfg\n            ]\n        elif not isinstance(args.infer_cfg, Dict):\n            raise TypeError(\n                \"infer_cfg should be either a string, a List, or a Dict, \"\n                f\"but got type(args.infer_cfg)={type(args.infer_cfg)}.\"\n            )\n\n    # --- 3. Start the Experimental Pipeline --- #\n    assert (args.train ^ args.test) is True, (\n        \"A running job can only conduct either training process or testing process, \"\n        \"so args.train and args.test cannot be True at the same time. \"\n        \"If you want to conduct training and testing sequentially, \"\n        \"please make two running jobs where the first job has args.train=True and args.test=False and \"\n        \"the second job has args.train=False and args.test=True.\"\n    )\n    cls.main(args)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.set_random_seeds","title":"<code>set_random_seeds(seed)</code>  <code>classmethod</code>","text":"<p>Set random seeds for python environment, numpy environment and torch environment.</p> Note <ol> <li>torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),     so it is not necessary to be included here.</li> <li>torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on     different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.</li> </ol> Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef set_random_seeds(cls, seed: int):\n    \"\"\"Set random seeds for python environment, numpy environment and torch\n    environment.\n\n    Note:\n        1. torch.random.manual_seed(seed) is the same with torch.manual_seed(seed),\n            so it is not necessary to be included here.\n        2. torch.cuda.manual_seed_all(seed) is also not included here because we initialize the processes on\n            different GPUs with different random seeds depending on the GPU number to avoid the process homogeneity.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.test","title":"<code>test(args, test_model, iterators, model)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>argparse.Namespace The input arguments.</p> required <code>iterators</code> <code>Dict[str, Dict[str, Iterator]]</code> <p>Dict The dictionary that contains all the iterators for training and validation.</p> required <code>test_model</code> <code>str</code> <p>Model The model to be trained.</p> required Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef test(\n    cls,\n    args: argparse.Namespace,\n    test_model: str,\n    iterators: Dict[str, Dict[str, Iterator]],\n    model: Model,\n):\n    \"\"\"\n\n    Args:\n        args: argparse.Namespace\n            The input arguments.\n        iterators: Dict\n            The dictionary that contains all the iterators for training and validation.\n        test_model: Model\n            The model to be trained.\n\n    \"\"\"\n\n    # parse infer_cfg depending on different situations\n    if isinstance(args.infer_cfg, str):\n        infer_cfg_dict = {\n            \".\".join(args.infer_cfg.split(\"/\")[-1].split(\".\")[:-1]): load_yaml(\n                open(args.infer_cfg)\n            )\n        }\n\n    elif isinstance(args.infer_cfg, List):\n        infer_cfg_dict = dict()\n        for cfg in args.infer_cfg:\n            if isinstance(cfg, str):\n                infer_cfg_dict[\".\".join(cfg.split(\"/\")[-1].split(\".\")[:-1])] = (\n                    load_yaml(open(cfg))\n                )\n            elif isinstance(cfg, Dict):\n                cfg = dict(sorted(cfg.items(), key=lambda x: x[0]))\n                infer_cfg_dict[\n                    \"_\".join([f\"{key}={value}\" for key, value in cfg.items()])\n                ] = cfg\n            else:\n                raise TypeError(\n                    \"If infer_cfg is given in the form of a List, \"\n                    \"it must be either a List[str] or a List[Dict]!\"\n                )\n\n    elif isinstance(args.infer_cfg, Dict):\n        if (\n            \"shared_args\" in args.infer_cfg.keys()\n            and \"exclu_args\" in args.infer_cfg.keys()\n        ):\n            assert isinstance(args.infer_cfg[\"shared_args\"], Dict) and isinstance(\n                args.infer_cfg[\"exclu_args\"], List\n            ), (\n                \"If infer_cfg is given by 'shared_args' and 'exclu_args', \"\n                \"infer_cfg['shared_args'] must be a Dict and infer_cfg['exclu_args'] must be a List.\"\n            )\n            infer_cfg_dict = dict()\n            for cfg in args.infer_cfg[\"exclu_args\"]:\n                assert isinstance(cfg, Dict), \"\"\n                for cfg_key in cfg.keys():\n                    if cfg_key in args.infer_cfg[\"shared_args\"].keys():\n                        raise ValueError(\n                            f\"Find a duplicate argument {cfg_key} in both 'shared_args' and 'exclu_args'!\"\n                        )\n\n                cfg.update(args.infer_cfg[\"shared_args\"])\n                cfg = dict(sorted(cfg.items(), key=lambda x: x[0]))\n                infer_cfg_dict[\n                    \"_\".join([f\"{key}={value}\" for key, value in cfg.items()])\n                ] = cfg\n\n        elif (\n            \"shared_args\" not in args.infer_cfg.keys()\n            and \"exclu_args\" not in args.infer_cfg.keys()\n        ):\n            if len(args.infer_cfg) == 0:\n                infer_cfg_dict = dict(default_inference=dict())\n            else:\n                args.infer_cfg = dict(\n                    sorted(args.infer_cfg.items(), key=lambda x: x[0])\n                )\n                infer_cfg_dict = {\n                    \"_\".join(\n                        [f\"{key}={value}\" for key, value in args.infer_cfg.items()]\n                    ): args.infer_cfg\n                }\n\n        else:\n            raise RuntimeError(\n                \"If infer_cfg is given in the form of a Dict, \"\n                \"'shared_args' and 'exclu_args' must be or not be in the key list at the same time!\"\n            )\n\n    elif args.infer_cfg is None:\n        infer_cfg_dict = dict(default_inference=dict())\n\n    else:\n        raise TypeError(\n            \"infer_cfg must be given in the form of a string, a List, or a Dict!\"\n        )\n\n    # loop each test configuration\n    for infer_cfg_name, infer_cfg in infer_cfg_dict.items():\n        # configuration-specific result path\n        test_result_path = os.path.join(\n            (\n                args.train_result_path\n                if args.test_result_path is None\n                else args.test_result_path\n            ),\n            infer_cfg_name,\n        )\n        os.makedirs(test_result_path, exist_ok=True)\n\n        # load the existing testing configuration for resuming\n        infer_cfg_path = os.path.join(test_result_path, \"infer_cfg.yaml\")\n        if args.resume and os.path.exists(infer_cfg_path):\n            infer_cfg = load_yaml(open(infer_cfg_path))\n\n        # save the testing configuration file to infer_cfg_path\n        if not args.distributed or args.rank == 0:\n            if len(infer_cfg) &gt; 0:\n                with open(infer_cfg_path, \"w\", encoding=\"utf-8\") as f:\n                    yaml.dump(infer_cfg, f, sort_keys=False)\n\n        # unlike training and validation, the testing iterators are looped one by one\n        for name, iterator in iterators[\"test\"].items():\n            # replace the slash with a percent symbol\n            name = name.replace(\"/\", \"%\")\n            # add the identity symbol to the path for multi-GPU testing\n            if args.attach_model_folder_when_test:\n                test_dset_path = os.path.join(test_result_path, test_model, name)\n            else:\n                test_dset_path = os.path.join(test_result_path, name)\n            test_rank_path = os.path.join(test_dset_path, f\"rank{args.rank}_tmp\")\n            logger = logger_stdout_file(test_rank_path, file_name=\"test\")\n\n            # initialize top-n bad case presentation\n            if args.bad_cases_selection is None:\n                if model.bad_cases_selection is not None:\n                    args.bad_cases_selection = model.bad_cases_selection\n                else:\n                    logger.info(\n                        \"There is no configuration of topN bad case selection in either your input \"\n                        \"arguments or default values of your selected model. \"\n                        \"So there will not be any reports about topN bad cases.\"\n                    )\n            # the main testing process\n            if args.bad_cases_selection is not None:\n                logger.info(\n                    f\"The configuration of topN bad case selection in the current testing process is {args.bad_cases_selection}.\"\n                )\n\n            # initialize the testing monitor\n            monitor = TestMonitor(\n                logger=logger, args=args, result_path=test_dset_path\n            )\n\n            # check the resuming status\n            if args.resume:\n                # loading the existed checkpoint\n                try:\n                    test_checkpoint = torch.load(\n                        os.path.join(test_rank_path, \"checkpoint.pth\")\n                    )\n                    monitor.load_state_dict(test_checkpoint[\"monitor\"])\n                    start_step = test_checkpoint[\"start_step\"]\n                    logger.info(\n                        f\"The testing process resumes from the step no.{start_step}. \"\n                    )\n                # checkpoint does not exist\n                except FileNotFoundError:\n                    start_step = 0\n                    logger.info(\n                        f\"No checkpoint is found in {test_rank_path}. \"\n                        f\"The testing process will start from scratch. \"\n                    )\n            else:\n                start_step = 0\n                logger.info(\"The testing process will start from scratch. \")\n\n            # initialize the dataloaders from the given starting point\n            data_loaders = cls.dict_transform(\n                iterator, lambda x: iter(x.build_loader(start_step=start_step))\n            )\n            test_indices = cls.dict_transform(\n                iterator, lambda x: x.get_batch_indices()\n            )\n            # if there are multiple dataloaders for the current testing set,\n            # the sample indices of the first element will be used to make the reports\n            if isinstance(test_indices, Dict):\n                test_indices = test_indices[list(test_indices.keys())[0]]\n            # report the total number of testing steps needed to be done\n            total_step_num = len(test_indices)\n            logger.info(f\"Totally {total_step_num} testing steps.\")\n\n            # make sure that no gradient appears during testing\n            model.eval()\n            with torch.inference_mode():\n                monitor.start_epoch(total_step_num=total_step_num)\n                # iterate the testing batches\n                for i in range(total_step_num):\n                    if i &lt; start_step:\n                        continue\n\n                    # only fetch the testing data right before decoding and evaluation\n                    test_batch = cls.dict_transform(\n                        src_dict=data_loaders, transform_func=next\n                    )\n                    # skip the empty testing batch\n                    if cls.is_empty_batch(test_batch):\n                        continue\n                    # evaluate the current testing batch and get the evaluation results\n                    try:\n                        test_results = model.evaluate(\n                            test_batch=test_batch, infer_conf=infer_cfg\n                        )\n                    # skip the current step if encounter an error (any kind)\n                    except Exception as e:\n                        if args.ignore_test_exception:\n                            logger.warn(\n                                f\"Rank no.{torch.distributed.get_rank() if args.distributed else '0'} meets the error \"\n                                f\"{e} at step no.{i}. \"\n                                f\"Indices of the involved testing samples in this step is {test_indices[i]}.\"\n                            )\n                            continue\n                        else:\n                            raise e\n                    # record evaluation results\n                    monitor.step(\n                        step_num=i + 1,\n                        test_results=test_results,\n                        test_index=test_indices[i],\n                    )\n\n                    # reduce the number of IO operations to speed up the testing\n                    if (\n                        i + 1\n                    ) % monitor.report_per_steps == 0 or i == total_step_num - 1:\n                        # save the checkpoint of the current step for both resuming and multi-GPU evaluation\n                        # the iteration conditions of the test dataloader will also be saved for resuming\n                        torch.save(\n                            dict(start_step=i + 1, monitor=monitor.state_dict()),\n                            os.path.join(test_rank_path, \"checkpoint.pth\"),\n                        )\n\n            # waiting for the data saving daemon process to finish before calling finish_epoch()\n            monitor.wait_empty_queues()\n\n            if not args.distributed or args.rank == 0:\n                # obtain the group information of the current iterator\n                group_info = None\n                if isinstance(iterator, Iterator):\n                    # Dict[str, Dict[str, str]]\n                    group_info = iterator.get_group_info()\n                elif isinstance(iterator, Dict):\n                    # List[Dict[str, Dict[str, str]]]\n                    group_info_list = [\n                        value.get_group_info() for value in iterator.values()\n                    ]\n                    for group_dict in group_info_list:\n                        if group_dict is not None:\n                            group_info = group_dict\n                            break\n                else:\n                    raise RuntimeError\n\n                # finish the evaluation and store the results to the disk\n                monitor.finish_epoch(meta_info=group_info)\n</code></pre>"},{"location":"reference/runner/#runner.Runner.train","title":"<code>train(args, data_cfg, iterators, model, optim_sches, logger, monitor)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>argparse.Namespace The input arguments.</p> required <code>data_cfg</code> <code>Dict</code> <p>Dict The data loading configuration. Used to initialize the iterator for model visualization.</p> required <code>iterators</code> <code>Dict[str, Dict[str, Iterator]] or Dict[str, Iterator]</code> <p>Dict The dictionary that contains all the iterators for training and validation.</p> required <code>model</code> <code>Model</code> <p>Model The model to be trained.</p> required <code>optim_sches</code> <code>Dict[str, OptimScheduler] or OptimScheduler</code> <p>Dict The dictionary that contains all the OptimSchedulers used to update the model parameters.</p> required <code>logger</code> required <code>monitor</code> <code>TrainValidMonitor</code> <p>TrainValidMonitor The wrapper class for a training monitor and a validation monitor. The training monitor controls the training process of the model and generates the real-time logging information. The validation monitor controls the validation process of the model and generates the real-time logging information.</p> required Source code in <code>speechain/runner.py</code> <pre><code>@classmethod\ndef train(\n    cls,\n    args: argparse.Namespace,\n    data_cfg: Dict,\n    iterators: Dict[str, Dict[str, Iterator]] or Dict[str, Iterator],\n    model: Model,\n    optim_sches: Dict[str, OptimScheduler] or OptimScheduler,\n    logger,\n    monitor: TrainValidMonitor,\n):\n    \"\"\"\n\n    Args:\n        args: argparse.Namespace\n            The input arguments.\n        data_cfg: Dict\n            The data loading configuration. Used to initialize the iterator for model visualization.\n        iterators: Dict\n            The dictionary that contains all the iterators for training and validation.\n        model: Model\n            The model to be trained.\n        optim_sches: Dict\n            The dictionary that contains all the OptimSchedulers used to update the model parameters.\n        logger:\n\n        monitor: TrainValidMonitor\n            The wrapper class for a training monitor and a validation monitor.\n            The training monitor controls the training process of the model and generates the real-time logging\n            information.\n            The validation monitor controls the validation process of the model and generates the real-time\n            logging information.\n\n    \"\"\"\n    assert (\n        args.start_epoch &lt;= args.num_epochs\n    ), \"Your given start_epoch is larger than your given num_epochs!\"\n\n    # --- checking the data lengths of all training iterators --- #\n    # multiple dataloaders scenario\n    if isinstance(iterators[\"train\"], Dict):\n        train_batch_nums = set(\n            [len(iterator) for iterator in iterators[\"train\"].values()]\n        )\n        min_train_batch_num = min(train_batch_nums)\n        if len(train_batch_nums) != 1:\n            logger.info(\n                f\"Your training iterators have different batch numbers: {train_batch_nums}. \"\n                f\"The actual batch number during training is set to {min_train_batch_num}!\"\n            )\n    # single dataloader scenario\n    elif isinstance(iterators[\"train\"], Iterator):\n        min_train_batch_num = len(iterators[\"train\"])\n    else:\n        raise RuntimeError(\"Please don't nest data_cfg['train'] more than twice!\")\n\n    # --- checking the data lengths of all validation iterators --- #\n    # multiple dataloaders scenario\n    if isinstance(iterators[\"valid\"], Dict):\n        valid_batch_nums = set(\n            [len(iterator) for iterator in iterators[\"valid\"].values()]\n        )\n        min_valid_batch_num = min(valid_batch_nums)\n        if len(valid_batch_nums) != 1:\n            logger.info(\n                f\"Your validation iterators have different batch numbers: {valid_batch_nums}. \"\n                f\"The actual batch number during validation is set to {min_valid_batch_num}!\"\n            )\n    # single dataloader scenario\n    elif isinstance(iterators[\"valid\"], Iterator):\n        min_valid_batch_num = len(iterators[\"valid\"])\n    else:\n        raise RuntimeError(\"Please don't nest data_cfg['valid'] more than twice!\")\n\n    # synchronize the batch numbers across all the distributed processes\n    if args.distributed:\n        _world_size = torch.distributed.get_world_size()\n        # make sure that all processes have the same number of training steps\n        _all_batch_num = torch.LongTensor([0 for _ in range(_world_size)]).cuda(\n            model.device\n        )\n        torch.distributed.all_gather_into_tensor(\n            _all_batch_num,\n            torch.LongTensor([min_train_batch_num]).cuda(model.device),\n        )\n        min_train_batch_num = _all_batch_num.min().item()\n\n        # make sure that all processes have the same number of validation steps\n        _all_batch_num = torch.LongTensor([0 for _ in range(_world_size)]).cuda(\n            model.device\n        )\n        torch.distributed.all_gather_into_tensor(\n            _all_batch_num,\n            torch.LongTensor([min_valid_batch_num]).cuda(model.device),\n        )\n        min_valid_batch_num = _all_batch_num.min().item()\n\n    # --- Initialize the iterator for model visualization --- #\n    if args.visual_snapshot_number &gt; 0:\n        if not args.distributed or args.rank == 0:\n            _valid_keys = list(data_cfg[\"valid\"].keys())\n            if len(_valid_keys) == 2 and (\n                \"type\" in _valid_keys and \"conf\" in _valid_keys\n            ):\n                visual_iterator = Iterator(\n                    dataset_type=data_cfg[\"valid\"][\"conf\"][\"dataset_type\"],\n                    dataset_conf=data_cfg[\"valid\"][\"conf\"][\"dataset_conf\"],\n                    batches_per_epoch=args.visual_snapshot_number,\n                    shuffle=False,\n                    ngpu=1,\n                    distributed=False,\n                    is_descending=None,\n                )\n            else:\n                visual_domain = _valid_keys[0]\n                logger.info(\n                    \"There are multiple sub-Dict in your given data_cfg['valid']. \"\n                    f\"The one named {visual_domain} is used to initialize the visualization iterator.\"\n                )\n                visual_iterator = {\n                    visual_domain: Iterator(\n                        dataset_type=data_cfg[\"valid\"][visual_domain][\"conf\"][\n                            \"dataset_type\"\n                        ],\n                        dataset_conf=data_cfg[\"valid\"][visual_domain][\"conf\"][\n                            \"dataset_conf\"\n                        ],\n                        batches_per_epoch=args.visual_snapshot_number,\n                        shuffle=False,\n                        ngpu=1,\n                        distributed=False,\n                        is_descending=None,\n                    )\n                }\n        else:\n            visual_iterator = None\n    else:\n        visual_iterator = None\n\n    # loop each epoch until the end\n    for epoch in range(args.start_epoch, args.num_epochs + 1):\n        # update the random seeds for the current epoch to keep in line with the dataloaders\n        cls.set_random_seeds(args.seed + epoch)\n        # start the current training epoch\n        if monitor is not None:\n            monitor.start_train_epoch(epoch)\n        # initialize all the training dataloaders\n        data_loaders = cls.dict_transform(\n            iterators[\"train\"], lambda x: iter(x.build_loader(epoch))\n        )\n\n        # --- Training Stage --- #\n        model.train()\n        # loop all the training batches\n        for step in range(1, min_train_batch_num + 1):\n            step_num = int(step + (epoch - 1) * min_train_batch_num)\n\n            # --- data loading part --- #\n            with cls.measure_time(\n                None if monitor is None else monitor.train_monitor\n            )(\"data_load_time\"):\n                train_batch = cls.dict_transform(\n                    src_dict=data_loaders, transform_func=next\n                )\n                # single-GPU case, directly skip the current step when meeting an empty batch\n                if not args.distributed:\n                    # skip the empty validation batch\n                    if cls.is_empty_batch(train_batch):\n                        continue\n\n                # multi-GPU case, scatter the skip flag to all nodes\n                else:\n                    skip_flag_list = torch.LongTensor(\n                        [False for _ in range(torch.distributed.get_world_size())]\n                    ).cuda(model.device)\n                    if cls.is_empty_batch(train_batch):\n                        skip_flag = torch.LongTensor([True]).cuda(model.device)\n                    else:\n                        skip_flag = torch.LongTensor([False]).cuda(model.device)\n                    # as long as one node meets an empty batch, all nodes will simultaneously skip the current step\n                    torch.distributed.all_gather_into_tensor(\n                        skip_flag_list, skip_flag\n                    )\n                    if skip_flag_list.sum() &gt;= 1:\n                        continue\n\n            # forward the batch to get the training criteria and optimize the model\n            train_metrics, optim_lr = None, None\n            # whether to skip the model forward part and model optimization part\n            if not args.dry_run:\n                # --- model forward part --- #\n                with autocast(enabled=args.use_amp):\n                    with cls.measure_time(\n                        None if monitor is None else monitor.train_monitor\n                    )(\"model_forward_time\"):\n                        try:\n                            losses, train_metrics = model(\n                                batch_data=train_batch, epoch=epoch\n                            )\n                        except Exception as e:\n                            if args.ignore_train_exception:\n                                warnings.warn(\n                                    f\"Rank no.{args.rank} meets error {e}! \"\n                                    f\"no.{step} training step will be skipped!\"\n                                )\n                                if logger is not None:\n                                    logger.warning(\n                                        f\"Rank no.{args.rank} meets error {e}! \"\n                                        f\"no.{step} training step will be skipped!\"\n                                    )\n                                continue\n                            else:\n                                raise e\n\n                # whether to skip the model optimization part\n                if not args.no_optim:\n                    # --- loss backward and optimization part --- #\n                    optim_lr = dict()\n                    for name, optim_sche in optim_sches.items():\n                        optim_sche.step(\n                            losses=losses,\n                            time_func=cls.measure_time(\n                                None if monitor is None else monitor.train_monitor\n                            ),\n                            optim_name=name,\n                            step_num=step_num,\n                            epoch_num=epoch,\n                            logger=logger,\n                        )\n                        optim_lr[name] = optim_sche.get_lr()\n\n            # log the information of the current training step\n            if monitor is not None:\n                monitor.train_step(\n                    step_num=step, optim_lr=optim_lr, train_metrics=train_metrics\n                )\n\n        # finish the current training epoch\n        if monitor is not None:\n            monitor.finish_train_epoch()\n\n        # --- Validation Stage --- #\n        # start the validation part of the current epoch\n        if monitor is not None:\n            monitor.start_valid_epoch(epoch)\n\n        valid_flag = (epoch - 1) % args.valid_per_epochs == 0\n        if valid_flag:\n            # initialize all the validation dataloaders\n            data_loaders = cls.dict_transform(\n                iterators[\"valid\"], lambda x: iter(x.build_loader(epoch))\n            )\n\n            # make sure that no gradient appears during validation\n            model.eval()\n            with torch.inference_mode():\n                # loop all validation batches\n                for step in range(min_valid_batch_num):\n                    # --- data loading part --- #\n                    with cls.measure_time(\n                        None if monitor is None else monitor.valid_monitor\n                    )(\"data_load_time\"):\n                        valid_batch = cls.dict_transform(\n                            src_dict=data_loaders, transform_func=next\n                        )\n                        # single-GPU case, directly skip the current step when meeting an empty batch\n                        if not args.distributed:\n                            # skip the empty validation batch\n                            if cls.is_empty_batch(valid_batch):\n                                continue\n                        # multi-GPU case, scatter the skip flag to all nodes\n                        else:\n                            skip_flag_list = torch.LongTensor(\n                                [\n                                    False\n                                    for _ in range(\n                                        torch.distributed.get_world_size()\n                                    )\n                                ]\n                            ).cuda(model.device)\n                            if cls.is_empty_batch(valid_batch):\n                                skip_flag = torch.LongTensor([True]).cuda(\n                                    model.device\n                                )\n                            else:\n                                skip_flag = torch.LongTensor([False]).cuda(\n                                    model.device\n                                )\n                            # as long as one node meets an empty batch,\n                            # all nodes will skip the current step at the same time\n                            torch.distributed.all_gather_into_tensor(\n                                skip_flag_list, skip_flag\n                            )\n                            if skip_flag_list.sum() &gt;= 1:\n                                continue\n\n                    # forward the batch to get the validation criteria\n                    valid_metrics = None\n                    # whether to skip the model forward part\n                    if not args.dry_run:\n                        # --- model forward part --- #\n                        # with autocast(enabled=args.use_amp) is not used here for accurate validation\n                        with cls.measure_time(\n                            None if monitor is None else monitor.valid_monitor\n                        )(\"model_forward_time\"):\n                            try:\n                                valid_metrics = model(batch_data=valid_batch)\n                            except Exception as e:\n                                if args.ignore_train_exception:\n                                    warnings.warn(\n                                        f\"Rank no.{args.rank} meets error {e}! \"\n                                        f\"no.{step} validation step will be skipped!\"\n                                    )\n                                    if logger is not None:\n                                        logger.warning(\n                                            f\"Rank no.{args.rank} meets error {e}! \"\n                                            f\"no.{step} validation step will be skipped!\"\n                                        )\n                                    continue\n                                else:\n                                    raise e\n\n                    # no step log for the validation step\n                    if monitor is not None:\n                        monitor.valid_step(valid_metrics=valid_metrics)\n\n        # --- Visualization Stage --- #\n        if (\n            args.visual_snapshot_number &gt; 0\n            and (epoch - 1) % args.visual_snapshot_interval == 0\n        ):\n            # make sure that all processes go through the validation phase smoothly\n            if visual_iterator is not None:\n                if not isinstance(visual_iterator, Dict):\n                    visual_domain = None\n                    visual_dataloader = visual_iterator.build_loader()\n                    visual_indices = visual_iterator.get_batch_indices()\n                else:\n                    visual_domain = list(visual_iterator.keys())[0]\n                    visual_dataloader = visual_iterator[\n                        visual_domain\n                    ].build_loader()\n                    visual_indices = visual_iterator[\n                        visual_domain\n                    ].get_batch_indices()\n\n                # make sure that no gradient appears during validation\n                model.eval()\n                visual_dataloader = iter(visual_dataloader)\n                with torch.inference_mode():\n                    for step in range(args.visual_snapshot_number):\n                        visual_sample = next(visual_dataloader)\n                        if cls.is_empty_batch(visual_sample):\n                            logger.info(\n                                f\"The visual sample {visual_indices[step][0]} is empty, \"\n                                f\"so its visualization is skipped!\"\n                            )\n                            continue\n                        # feed the current sample to the model\n                        monitor.valid_model_snapshot(\n                            epoch=epoch,\n                            domain=visual_domain,\n                            sample_index=visual_indices[step][0],\n                            used_sample=visual_sample,\n                        )\n            # synchronize all the GPU processes at the end of the visualization stage\n            if args.distributed:\n                torch.distributed.barrier()\n\n        # finish_valid_epoch() should be called before checkpoint saving\n        finish_valid_flag = None\n        if not args.distributed or args.rank == 0:\n            finish_valid_flag = monitor.finish_valid_epoch(\n                valid_flag=valid_flag, valid_per_epochs=args.valid_per_epochs\n            )\n\n        # store the checkpoint of the current epoch for later resuming\n        if not args.distributed or args.rank == 0:\n            if not args.dry_run and not args.no_optim:\n                torch.save(\n                    {\n                        \"start_epoch\": epoch + 1,\n                        \"latest_model\": (\n                            model.state_dict()\n                            if not args.distributed\n                            else model.module.state_dict()\n                        ),\n                        \"monitor\": monitor.state_dict(),\n                        \"optim_sches\": {\n                            name: o.state_dict() for name, o in optim_sches.items()\n                        },\n                    },\n                    os.path.join(args.train_result_path, \"checkpoint.pth\"),\n                )\n\n        # early-stopping checking for single-GPU\n        if not args.distributed and finish_valid_flag:\n            break\n\n        # early-stopping checking for multi-GPU\n        if args.distributed:\n            stop_flag = torch.BoolTensor([False]).cuda(model.device)\n            flag_list = None\n\n            if args.rank == 0:\n                if finish_valid_flag:\n                    stop_flag = torch.BoolTensor([True]).cuda(model.device)\n                flag_list = [\n                    stop_flag for _ in range(torch.distributed.get_world_size())\n                ]\n\n            torch.distributed.scatter(stop_flag, flag_list)\n            if stop_flag.item():\n                break\n\n    # check whether all the monitor queues become empty in every minute\n    if not args.distributed or args.rank == 0:\n        monitor.wait_empty_queues()\n\n    # synchronize all the GPU processes at the end\n    if args.distributed:\n        torch.distributed.barrier()\n</code></pre>"},{"location":"reference/snapshooter/","title":"snapshooter","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/snapshooter/#snapshooter.CurvePlotter","title":"<code>CurvePlotter</code>","text":"<p>               Bases: <code>Plotter</code></p> <p>CurvePlotter does the job of plotting the curve of the given material.</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>class CurvePlotter(Plotter):\n    \"\"\"CurvePlotter does the job of plotting the curve of the given material.\"\"\"\n\n    def __init__(self, plot_conf: Dict = None, grid_conf: Dict = None):\n        \"\"\"Initialize the CurvePlotter.\n\n        Args:\n            plot_conf (Dict): the configuration of the plot\n            grid_conf (Dict): the configuration of the grid\n        \"\"\"\n        # default arguments of the plot_conf\n        self.plot_conf = dict(linestyle=\"-\", linewidth=1, marker=\"o\", markersize=5)\n        # overwrite the plot_conf by the input arguments\n        if plot_conf is not None:\n            for key, value in plot_conf.items():\n                self.plot_conf[key] = value\n\n        # default arguments of the plot_conf\n        self.grid_conf = dict(linestyle=\"--\", linewidth=1, color=\"black\", alpha=0.3)\n        # overwrite the plot_conf by the input arguments\n        if grid_conf is not None:\n            for key, value in grid_conf.items():\n                self.grid_conf[key] = value\n\n    def plot(\n        self,\n        ax,\n        material: List or Dict[str, List],\n        fig_name: str,\n        xlabel: str,\n        ylabel: str,\n        x_stride: int = 1,\n    ):\n        \"\"\"Plot the curve of the given material.\n\n        Args:\n            ax (matplotlib.axes._subplots.AxesSubplot): the axis of the figure\n            material (List or Dict[str, List]): the material to be plotted\n            fig_name (str): the name of the figure\n            xlabel (str): the label of x-axis\n            ylabel (str): the label of y-axis\n            x_stride (int): the stride of the x-axis\n\n        Returns:\n            None\n        \"\"\"\n        # set up the figure label of x and y axes\n        ax.set_xlabel(xlabel if xlabel is not None else None)\n        ax.set_ylabel(ylabel if ylabel is not None else fig_name)\n\n        # set the figure grid\n        ax.grid(**self.grid_conf)\n\n        # only one curve in the figure\n        if isinstance(material, List):\n            # for the stand-alone figure, only show up to 5 points at x-axis\n            x_axis = np.arange(0, len(material), dtype=np.int_) * x_stride + 1\n            interval = math.ceil(len(x_axis) / 5)\n            ax.set_xticks(\n                x_axis,\n                [\n                    str(x_axis[i]) if i % interval == 0 else \"\"\n                    for i in range(len(x_axis))\n                ],\n            )\n            ax.plot(x_axis, material, **self.plot_conf)\n\n        # multiple curves in the figure\n        elif isinstance(material, Dict):\n            if len(material) &gt; 0:\n                keys = list(material.keys())\n                # for the summary figure, only show up to 5 points at x-axis\n                x_axis = (\n                    np.arange(1, len(material[keys[0]]) + 1, dtype=np.int_) * x_stride\n                )\n                interval = math.ceil(len(x_axis) / 5)\n                ax.set_xticks(\n                    x_axis,\n                    [\n                        str(x_axis[i]) if i % interval == 0 else \"\"\n                        for i in range(len(x_axis))\n                    ],\n                )\n                for key in keys:\n                    ax.plot(x_axis, material[key], label=key, **self.plot_conf)\n                plt.legend()\n\n    def save(self, materials: Dict, save_path: str, x_stride: int = 1):\n        \"\"\"Save the given material into a specific .txt file for easy visualization.\n\n        Args:\n            materials (Dict): the material to be saved\n            save_path (str): the path to save the materials\n            x_stride (int): the stride of the x-axis\n\n        Returns:\n        \"\"\"\n        # save each material into a specific .txt file for easy visualization\n        for file_name, material in materials.items():\n            # only one file\n            if isinstance(material, List):\n                # initialize the horizontal axis\n                x_axis = np.arange(1, len(material) + 1, dtype=np.int_) * x_stride\n                save_data = np.concatenate(\n                    (x_axis.reshape(-1, 1), np.array(material).reshape(-1, 1)), axis=-1\n                )\n                np.savetxt(f\"{os.path.join(save_path, file_name)}.txt\", save_data)\n            # multiple files in the path\n            elif isinstance(material, Dict):\n                if len(material) &gt; 0:\n                    keys = list(material.keys())\n                    x_axis = (\n                        np.arange(1, len(material[keys[0]]) + 1, dtype=np.int_)\n                        * x_stride\n                    )\n                    for key in keys:\n                        save_data = np.concatenate(\n                            (\n                                x_axis.reshape(-1, 1),\n                                np.array(material[key]).reshape(-1, 1),\n                            ),\n                            axis=-1,\n                        )\n                        np.savetxt(\n                            f\"{os.path.join(save_path, '_'.join([file_name, key]))}.txt\",\n                            save_data,\n                        )\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.CurvePlotter.__init__","title":"<code>__init__(plot_conf=None, grid_conf=None)</code>","text":"<p>Initialize the CurvePlotter.</p> <p>Parameters:</p> Name Type Description Default <code>plot_conf</code> <code>Dict</code> <p>the configuration of the plot</p> <code>None</code> <code>grid_conf</code> <code>Dict</code> <p>the configuration of the grid</p> <code>None</code> Source code in <code>speechain/snapshooter.py</code> <pre><code>def __init__(self, plot_conf: Dict = None, grid_conf: Dict = None):\n    \"\"\"Initialize the CurvePlotter.\n\n    Args:\n        plot_conf (Dict): the configuration of the plot\n        grid_conf (Dict): the configuration of the grid\n    \"\"\"\n    # default arguments of the plot_conf\n    self.plot_conf = dict(linestyle=\"-\", linewidth=1, marker=\"o\", markersize=5)\n    # overwrite the plot_conf by the input arguments\n    if plot_conf is not None:\n        for key, value in plot_conf.items():\n            self.plot_conf[key] = value\n\n    # default arguments of the plot_conf\n    self.grid_conf = dict(linestyle=\"--\", linewidth=1, color=\"black\", alpha=0.3)\n    # overwrite the plot_conf by the input arguments\n    if grid_conf is not None:\n        for key, value in grid_conf.items():\n            self.grid_conf[key] = value\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.CurvePlotter.plot","title":"<code>plot(ax, material, fig_name, xlabel, ylabel, x_stride=1)</code>","text":"<p>Plot the curve of the given material.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>AxesSubplot</code> <p>the axis of the figure</p> required <code>material</code> <code>List or Dict[str, List]</code> <p>the material to be plotted</p> required <code>fig_name</code> <code>str</code> <p>the name of the figure</p> required <code>xlabel</code> <code>str</code> <p>the label of x-axis</p> required <code>ylabel</code> <code>str</code> <p>the label of y-axis</p> required <code>x_stride</code> <code>int</code> <p>the stride of the x-axis</p> <code>1</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def plot(\n    self,\n    ax,\n    material: List or Dict[str, List],\n    fig_name: str,\n    xlabel: str,\n    ylabel: str,\n    x_stride: int = 1,\n):\n    \"\"\"Plot the curve of the given material.\n\n    Args:\n        ax (matplotlib.axes._subplots.AxesSubplot): the axis of the figure\n        material (List or Dict[str, List]): the material to be plotted\n        fig_name (str): the name of the figure\n        xlabel (str): the label of x-axis\n        ylabel (str): the label of y-axis\n        x_stride (int): the stride of the x-axis\n\n    Returns:\n        None\n    \"\"\"\n    # set up the figure label of x and y axes\n    ax.set_xlabel(xlabel if xlabel is not None else None)\n    ax.set_ylabel(ylabel if ylabel is not None else fig_name)\n\n    # set the figure grid\n    ax.grid(**self.grid_conf)\n\n    # only one curve in the figure\n    if isinstance(material, List):\n        # for the stand-alone figure, only show up to 5 points at x-axis\n        x_axis = np.arange(0, len(material), dtype=np.int_) * x_stride + 1\n        interval = math.ceil(len(x_axis) / 5)\n        ax.set_xticks(\n            x_axis,\n            [\n                str(x_axis[i]) if i % interval == 0 else \"\"\n                for i in range(len(x_axis))\n            ],\n        )\n        ax.plot(x_axis, material, **self.plot_conf)\n\n    # multiple curves in the figure\n    elif isinstance(material, Dict):\n        if len(material) &gt; 0:\n            keys = list(material.keys())\n            # for the summary figure, only show up to 5 points at x-axis\n            x_axis = (\n                np.arange(1, len(material[keys[0]]) + 1, dtype=np.int_) * x_stride\n            )\n            interval = math.ceil(len(x_axis) / 5)\n            ax.set_xticks(\n                x_axis,\n                [\n                    str(x_axis[i]) if i % interval == 0 else \"\"\n                    for i in range(len(x_axis))\n                ],\n            )\n            for key in keys:\n                ax.plot(x_axis, material[key], label=key, **self.plot_conf)\n            plt.legend()\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.CurvePlotter.save","title":"<code>save(materials, save_path, x_stride=1)</code>","text":"<p>Save the given material into a specific .txt file for easy visualization.</p> <p>Parameters:</p> Name Type Description Default <code>materials</code> <code>Dict</code> <p>the material to be saved</p> required <code>save_path</code> <code>str</code> <p>the path to save the materials</p> required <code>x_stride</code> <code>int</code> <p>the stride of the x-axis</p> <code>1</code> <p>Returns:</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def save(self, materials: Dict, save_path: str, x_stride: int = 1):\n    \"\"\"Save the given material into a specific .txt file for easy visualization.\n\n    Args:\n        materials (Dict): the material to be saved\n        save_path (str): the path to save the materials\n        x_stride (int): the stride of the x-axis\n\n    Returns:\n    \"\"\"\n    # save each material into a specific .txt file for easy visualization\n    for file_name, material in materials.items():\n        # only one file\n        if isinstance(material, List):\n            # initialize the horizontal axis\n            x_axis = np.arange(1, len(material) + 1, dtype=np.int_) * x_stride\n            save_data = np.concatenate(\n                (x_axis.reshape(-1, 1), np.array(material).reshape(-1, 1)), axis=-1\n            )\n            np.savetxt(f\"{os.path.join(save_path, file_name)}.txt\", save_data)\n        # multiple files in the path\n        elif isinstance(material, Dict):\n            if len(material) &gt; 0:\n                keys = list(material.keys())\n                x_axis = (\n                    np.arange(1, len(material[keys[0]]) + 1, dtype=np.int_)\n                    * x_stride\n                )\n                for key in keys:\n                    save_data = np.concatenate(\n                        (\n                            x_axis.reshape(-1, 1),\n                            np.array(material[key]).reshape(-1, 1),\n                        ),\n                        axis=-1,\n                    )\n                    np.savetxt(\n                        f\"{os.path.join(save_path, '_'.join([file_name, key]))}.txt\",\n                        save_data,\n                    )\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.HistPlotter","title":"<code>HistPlotter</code>","text":"<p>               Bases: <code>Plotter</code></p> <p>HistPlotter does the job of plotting the histogram of the given material.</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>class HistPlotter(Plotter):\n    \"\"\"HistPlotter does the job of plotting the histogram of the given material.\"\"\"\n\n    def __init__(self, plot_conf: Dict = None, grid_conf: Dict = None):\n        \"\"\"Initialize the HistPlotter.\n\n        Args:\n            plot_conf (Dict): the configuration of the plot\n            grid_conf (Dict): the configuration of the grid\n        \"\"\"\n        # default arguments of the plot_conf\n        self.plot_conf = dict(bins=50, edgecolor=\"k\")\n        # overwrite the plot_conf by the input arguments\n        if plot_conf is not None:\n            for key, value in plot_conf.items():\n                self.plot_conf[key] = value\n\n        # default arguments of the plot_conf\n        self.grid_conf = dict(linestyle=\"--\", linewidth=1, color=\"black\", alpha=0.3)\n        # overwrite the plot_conf by the input arguments\n        if grid_conf is not None:\n            for key, value in grid_conf.items():\n                self.grid_conf[key] = value\n\n    def plot(\n        self, ax, material: List, fig_name: str, xlabel: str, ylabel: str, **kwargs\n    ):\n        \"\"\"Plot the histogram of the given material.\n\n        Args:\n            ax:\n            material:\n            fig_name:\n            xlabel:\n            ylabel:\n            **kwargs:\n\n        Returns:\n        \"\"\"\n        # set up the figure label of x and y axes\n        ax.set_xlabel(xlabel if xlabel is not None else fig_name)\n        ax.set_ylabel(ylabel if ylabel is not None else None)\n\n        # set the figure title\n        fig_title = f\"{fig_name}\" if fig_name is not None else f\"{xlabel}\"\n        ax.set_title(\n            fig_title, fontweight=\"bold\", color=\"black\", verticalalignment=\"baseline\"\n        )\n\n        # set the figure grid\n        ax.grid(**self.grid_conf)\n\n        # plot the histogram\n        ax.hist(material, **self.plot_conf)\n\n        # calculate the median (50%), quartiles (25% &amp; 75%) and ten-fold points (10% &amp; 90%) of the distribution\n        sorted_material, material_len = sorted(material), len(material)\n        first_tenfold, first_quat, median, last_quat, last_tenfold = (\n            sorted_material[int(material_len / 10)],\n            sorted_material[int(material_len / 4)],\n            sorted_material[int(material_len / 2)],\n            sorted_material[int(material_len / 4 * 3)],\n            sorted_material[int(material_len / 10 * 9)],\n        )\n        # plot the median as the red vertical line with '--' style\n        ax.axvline(median, c=\"red\", ls=\"--\")\n        # plot the fist and last quartiles as the red vertical line with '-.' style\n        ax.axvline(first_quat, c=\"orange\", ls=\"-.\")\n        ax.axvline(last_quat, c=\"orange\", ls=\"-.\")\n        # plot the fist and last ten-fold points as the red vertical line with ':' style\n        ax.axvline(first_tenfold, c=\"yellow\", ls=\":\")\n        ax.axvline(last_tenfold, c=\"yellow\", ls=\":\")\n\n    def save(self, save_path: str, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.HistPlotter.__init__","title":"<code>__init__(plot_conf=None, grid_conf=None)</code>","text":"<p>Initialize the HistPlotter.</p> <p>Parameters:</p> Name Type Description Default <code>plot_conf</code> <code>Dict</code> <p>the configuration of the plot</p> <code>None</code> <code>grid_conf</code> <code>Dict</code> <p>the configuration of the grid</p> <code>None</code> Source code in <code>speechain/snapshooter.py</code> <pre><code>def __init__(self, plot_conf: Dict = None, grid_conf: Dict = None):\n    \"\"\"Initialize the HistPlotter.\n\n    Args:\n        plot_conf (Dict): the configuration of the plot\n        grid_conf (Dict): the configuration of the grid\n    \"\"\"\n    # default arguments of the plot_conf\n    self.plot_conf = dict(bins=50, edgecolor=\"k\")\n    # overwrite the plot_conf by the input arguments\n    if plot_conf is not None:\n        for key, value in plot_conf.items():\n            self.plot_conf[key] = value\n\n    # default arguments of the plot_conf\n    self.grid_conf = dict(linestyle=\"--\", linewidth=1, color=\"black\", alpha=0.3)\n    # overwrite the plot_conf by the input arguments\n    if grid_conf is not None:\n        for key, value in grid_conf.items():\n            self.grid_conf[key] = value\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.HistPlotter.plot","title":"<code>plot(ax, material, fig_name, xlabel, ylabel, **kwargs)</code>","text":"<p>Plot the histogram of the given material.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> required <code>material</code> <code>List</code> required <code>fig_name</code> <code>str</code> required <code>xlabel</code> <code>str</code> required <code>ylabel</code> <code>str</code> required <code>**kwargs</code> <code>{}</code> <p>Returns:</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def plot(\n    self, ax, material: List, fig_name: str, xlabel: str, ylabel: str, **kwargs\n):\n    \"\"\"Plot the histogram of the given material.\n\n    Args:\n        ax:\n        material:\n        fig_name:\n        xlabel:\n        ylabel:\n        **kwargs:\n\n    Returns:\n    \"\"\"\n    # set up the figure label of x and y axes\n    ax.set_xlabel(xlabel if xlabel is not None else fig_name)\n    ax.set_ylabel(ylabel if ylabel is not None else None)\n\n    # set the figure title\n    fig_title = f\"{fig_name}\" if fig_name is not None else f\"{xlabel}\"\n    ax.set_title(\n        fig_title, fontweight=\"bold\", color=\"black\", verticalalignment=\"baseline\"\n    )\n\n    # set the figure grid\n    ax.grid(**self.grid_conf)\n\n    # plot the histogram\n    ax.hist(material, **self.plot_conf)\n\n    # calculate the median (50%), quartiles (25% &amp; 75%) and ten-fold points (10% &amp; 90%) of the distribution\n    sorted_material, material_len = sorted(material), len(material)\n    first_tenfold, first_quat, median, last_quat, last_tenfold = (\n        sorted_material[int(material_len / 10)],\n        sorted_material[int(material_len / 4)],\n        sorted_material[int(material_len / 2)],\n        sorted_material[int(material_len / 4 * 3)],\n        sorted_material[int(material_len / 10 * 9)],\n    )\n    # plot the median as the red vertical line with '--' style\n    ax.axvline(median, c=\"red\", ls=\"--\")\n    # plot the fist and last quartiles as the red vertical line with '-.' style\n    ax.axvline(first_quat, c=\"orange\", ls=\"-.\")\n    ax.axvline(last_quat, c=\"orange\", ls=\"-.\")\n    # plot the fist and last ten-fold points as the red vertical line with ':' style\n    ax.axvline(first_tenfold, c=\"yellow\", ls=\":\")\n    ax.axvline(last_tenfold, c=\"yellow\", ls=\":\")\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.MatrixPlotter","title":"<code>MatrixPlotter</code>","text":"<p>               Bases: <code>Plotter</code></p> <p>Plot the matrix of the given material.</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>class MatrixPlotter(Plotter):\n    \"\"\"Plot the matrix of the given material.\"\"\"\n\n    def __init__(self, **plot_conf):\n        \"\"\"Initialize the plotter.\n\n        Args:\n            **plot_conf:\n\n        Returns:\n            None\n        \"\"\"\n        # default arguments of the plot_conf\n        self.plot_conf = dict(\n            cmap=\"Blues_r\",\n            cbar=False,\n        )\n        # overwrite the plot_conf by the input arguments\n        if plot_conf is not None:\n            for key, value in plot_conf.items():\n                self.plot_conf[key] = value\n\n        self.plot_conf[\"cmap\"] = plt.get_cmap(self.plot_conf[\"cmap\"])\n\n    def plot(\n        self,\n        ax,\n        material: np.ndarray,\n        fig_name: str,\n        xlabel: str,\n        ylabel: str,\n        flip_y: bool = False,\n        **kwargs,\n    ):\n\n        # set the figure title\n        if ylabel is not None and xlabel is not None:\n            fig_title = f\"{ylabel} vs. {xlabel}\"\n        else:\n            fig_title = ylabel if ylabel is not None else fig_name\n        ax.set_title(\n            fig_title, fontweight=\"bold\", color=\"black\", verticalalignment=\"baseline\"\n        )\n\n        # plot the curve on the figure and save the figure\n        sns.heatmap(\n            data=material[::-1] if flip_y else material,\n            yticklabels=(\n                [\n                    str(i) if (i + 1) % int(material.shape[0] / 5) == 0 else \"\"\n                    for i in range(material.shape[0] - 1, -1, -1)\n                ]\n                if flip_y\n                else \"auto\"\n            ),\n            **self.plot_conf,\n        )\n\n    def save(self, materials: Dict[str, np.ndarray], epoch: int, save_path: str):\n        \"\"\"Save the materials into a .npz file on disk for future usage.\n\n        Args:\n            materials (Dict[str, np.ndarray]): the materials to be saved\n            epoch (int): the epoch of the materials\n            save_path (str): the path to save the materials\n\n        Returns:\n            None\n        \"\"\"\n        np.savez(os.path.join(save_path, f\"epoch{epoch}.npz\"), **materials)\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.MatrixPlotter.__init__","title":"<code>__init__(**plot_conf)</code>","text":"<p>Initialize the plotter.</p> <p>Parameters:</p> Name Type Description Default <code>**plot_conf</code> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def __init__(self, **plot_conf):\n    \"\"\"Initialize the plotter.\n\n    Args:\n        **plot_conf:\n\n    Returns:\n        None\n    \"\"\"\n    # default arguments of the plot_conf\n    self.plot_conf = dict(\n        cmap=\"Blues_r\",\n        cbar=False,\n    )\n    # overwrite the plot_conf by the input arguments\n    if plot_conf is not None:\n        for key, value in plot_conf.items():\n            self.plot_conf[key] = value\n\n    self.plot_conf[\"cmap\"] = plt.get_cmap(self.plot_conf[\"cmap\"])\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.MatrixPlotter.save","title":"<code>save(materials, epoch, save_path)</code>","text":"<p>Save the materials into a .npz file on disk for future usage.</p> <p>Parameters:</p> Name Type Description Default <code>materials</code> <code>Dict[str, ndarray]</code> <p>the materials to be saved</p> required <code>epoch</code> <code>int</code> <p>the epoch of the materials</p> required <code>save_path</code> <code>str</code> <p>the path to save the materials</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def save(self, materials: Dict[str, np.ndarray], epoch: int, save_path: str):\n    \"\"\"Save the materials into a .npz file on disk for future usage.\n\n    Args:\n        materials (Dict[str, np.ndarray]): the materials to be saved\n        epoch (int): the epoch of the materials\n        save_path (str): the path to save the materials\n\n    Returns:\n        None\n    \"\"\"\n    np.savez(os.path.join(save_path, f\"epoch{epoch}.npz\"), **materials)\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.Plotter","title":"<code>Plotter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Plotter is the base class for all plotters.</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>class Plotter(ABC):\n    \"\"\"Plotter is the base class for all plotters.\"\"\"\n\n    @abstractmethod\n    def __init__(self, **kwargs):\n        raise NotImplementedError\n\n    @abstractmethod\n    def plot(\n        self, ax, material: Any, fig_name: str, xlabel: str, ylabel: str, **kwargs\n    ):\n        raise NotImplementedError\n\n    @abstractmethod\n    def save(self, save_path: str, **kwargs):\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.SnapShooter","title":"<code>SnapShooter</code>","text":"<p>SnapShooter does the job of recording snapshots of the model during training or validation.</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>class SnapShooter:\n    \"\"\"SnapShooter does the job of recording snapshots of the model during training or\n    validation.\"\"\"\n\n    def __init__(\n        self,\n        result_path: str,\n        snap_mode: str,\n        fig_width: float = 6.4,\n        fig_height: float = 4.8,\n        curve_plotter_conf: Dict = None,\n        matrix_plotter_conf: Dict = None,\n        hist_plotter_conf: Dict = None,\n    ):\n        \"\"\"Initialize the SnapShooter.\n\n        Args:\n            result_path:\n            fig_width:\n            fig_height:\n            curve_plotter_conf:\n            matrix_plotter_conf:\n            hist_plotter_conf:\n        \"\"\"\n        # initialize the figure plotters and the tensorboard writer\n        if snap_mode is not None:\n            self.figure_path = os.path.join(result_path, \"figures\", snap_mode)\n            self.tb_writer = SummaryWriter(\n                log_dir=os.path.join(result_path, \"tensorboard\", snap_mode)\n            )\n        else:\n            self.figure_path = os.path.join(result_path, \"figures\")\n            self.tb_writer = None\n\n        # initialize the default values of the figure width and height\n        self.fig_width = fig_width\n        self.fig_height = fig_height\n\n        # initialize the built-in plotters\n        # curve plotter\n        curve_plotter_conf = (\n            dict() if curve_plotter_conf is None else curve_plotter_conf\n        )\n        self.curve_plotter = CurvePlotter(**curve_plotter_conf)\n        # matrix plotter\n        matrix_plotter_conf = (\n            dict() if matrix_plotter_conf is None else matrix_plotter_conf\n        )\n        self.matrix_plotter = MatrixPlotter(**matrix_plotter_conf)\n        # histogram plotter\n        hist_plotter_conf = dict() if hist_plotter_conf is None else hist_plotter_conf\n        self.hist_plotter = HistPlotter(**hist_plotter_conf)\n\n    def snapshot(\n        self,\n        materials: Dict,\n        plot_type: str,\n        epoch: int = None,\n        xlabel: str = None,\n        ylabel: str = None,\n        sep_save: bool = True,\n        sum_save: bool = True,\n        tb_write: bool = True,\n        data_save: bool = True,\n        col_num: int = None,\n        row_num: int = None,\n        subfolder_names: List[str] or str = None,\n        **kwargs,\n    ):\n\n        # initialize the list of sub-folder names\n        subfolder_names = [] if subfolder_names is None else subfolder_names\n        subfolder_names = (\n            [subfolder_names] if isinstance(subfolder_names, str) else subfolder_names\n        )\n\n        # initialize the figure saving path\n        if subfolder_names is not None:\n            save_path = os.path.join(self.figure_path, *subfolder_names)\n        else:\n            save_path = self.figure_path\n        os.makedirs(save_path, exist_ok=True)\n\n        # go through different branches based on the data type\n        if plot_type == \"curve\":\n            self.curve_snapshot(\n                save_path=save_path,\n                subfolder_names=subfolder_names,\n                materials=materials,\n                epoch=epoch,\n                xlabel=xlabel,\n                ylabel=ylabel,\n                sep_save=sep_save,\n                sum_save=sum_save,\n                tb_write=tb_write,\n                data_save=data_save,\n                col_num=col_num,\n                row_num=row_num,\n                **kwargs,\n            )\n        elif plot_type == \"matrix\":\n            self.matrix_snapshot(\n                save_path=save_path,\n                subfolder_names=subfolder_names,\n                materials=materials,\n                epoch=epoch,\n                xlabel=xlabel,\n                ylabel=ylabel,\n                sep_save=sep_save,\n                sum_save=sum_save,\n                tb_write=tb_write,\n                data_save=data_save,\n                col_num=col_num,\n                row_num=row_num,\n                **kwargs,\n            )\n        elif plot_type == \"hist\":\n            self.hist_snapshot(\n                save_path=save_path, materials=materials, xlabel=xlabel, ylabel=ylabel\n            )\n        elif plot_type == \"text\":\n            self.text_snapshot(\n                save_path=save_path,\n                subfolder_names=subfolder_names,\n                materials=materials,\n                epoch=epoch,\n                data_save=data_save,\n                **kwargs,\n            )\n        elif plot_type == \"audio\":\n            self.audio_snapshot(\n                save_path=save_path,\n                subfolder_names=subfolder_names,\n                materials=materials,\n                epoch=epoch,\n                data_save=data_save,\n                **kwargs,\n            )\n        else:\n            raise ValueError\n\n        if self.tb_writer is not None:\n            self.tb_writer.flush()\n\n    @contextmanager\n    def sep_figure_context(self, fig_name: str, save_path: str):\n\n        # setup the figure plotter and initialize the figure\n        fig = plt.figure(figsize=[self.fig_width, self.fig_height], num=1)\n        ax = fig.add_subplot(111)\n\n        # return the necessary information for the specific plotter to plot\n        yield ax\n\n        # save the plotted figure\n        plt.savefig(os.path.join(save_path, fig_name + \".png\"))\n        plt.close(fig=fig)\n\n    @contextmanager\n    def sum_figure_context(\n        self,\n        materials: Dict,\n        save_path: str,\n        col_num: int,\n        row_num: int,\n        sum_fig_name: str = \"summary.png\",\n    ):\n\n        # initialize the number of columns and rows in the summary figure\n        if col_num is not None and row_num is not None:\n            assert col_num * row_num &gt;= len(materials)\n        elif col_num is not None:\n            row_num = math.ceil(len(materials) / col_num)\n        elif row_num is not None:\n            col_num = math.ceil(len(materials) / row_num)\n        # if both col_num and row_num are not given, they should be set automatically\n        else:\n            # make sure that the numbers of columns and rows in the summary figure differ by no more than 1\n            material_num = len(materials)\n            for divisor in range(int(math.sqrt(material_num)), material_num + 1):\n                quot = material_num / divisor\n                if abs(divisor - quot) &lt;= 1:\n                    quot = math.ceil(quot)\n                    # the larger number is assigned to the column number\n                    row_num = min(divisor, quot)\n                    col_num = max(divisor, quot)\n                    break\n\n        # setup the figure plotter and initialize the figure\n        # scale the height and width when the number of rows or columns exceeds 3\n        fig_width = self.fig_width * math.ceil(col_num / 3)\n        fig_height = self.fig_height * math.ceil(row_num / 3)\n        # if row_num &gt; col_num:\n        #     fig_height *= (row_num / col_num)\n        # elif col_num &gt; row_num:\n        #     fig_width *= (col_num / row_num)\n\n        fig = plt.figure(figsize=[fig_width, fig_height], num=row_num * col_num)\n        material_keys = list(materials.keys())\n\n        yield fig, material_keys, row_num, col_num\n\n        # save the plotted figure\n        fig.tight_layout()\n        plt.savefig(os.path.join(save_path, sum_fig_name))\n        plt.close(fig=fig)\n\n    def curve_snapshot(\n        self,\n        save_path: str,\n        subfolder_names: List[str],\n        materials: Dict,\n        epoch: int,\n        xlabel: str,\n        ylabel: str,\n        sep_save: bool,\n        sum_save: bool,\n        tb_write: bool,\n        data_save: bool,\n        col_num: int,\n        row_num: int,\n        x_stride: int = 1,\n    ):\n\n        # save each material to a separate figure\n        if sep_save:\n            # loop each file in the given material Dict\n            for fig_name in materials.keys():\n                with self.sep_figure_context(fig_name, save_path) as ax:\n                    # plot the current material into a figure\n                    self.curve_plotter.plot(\n                        ax=ax,\n                        material=materials[fig_name],\n                        fig_name=fig_name,\n                        x_stride=x_stride,\n                        xlabel=xlabel,\n                        ylabel=ylabel,\n                    )\n\n        # save all the materials to a summary figure\n        if sum_save:\n            with self.sum_figure_context(materials, save_path, col_num, row_num) as (\n                fig,\n                material_keys,\n                row_num,\n                col_num,\n            ):\n                # loop each row\n                for row in range(1, row_num + 1):\n                    # loop each column\n                    for col in range(1, col_num + 1):\n                        # initialize the sub-figure\n                        index = (row - 1) * col_num + col\n\n                        # plot each material into the corresponding sub-figure\n                        try:\n                            fig_name = material_keys[index - 1]\n                            ax = fig.add_subplot(row_num, col_num, index)\n                            # plot the figure\n                            self.curve_plotter.plot(\n                                ax=ax,\n                                material=materials[fig_name],\n                                fig_name=fig_name,\n                                x_stride=x_stride,\n                                xlabel=xlabel,\n                                ylabel=ylabel,\n                            )\n                        except IndexError:\n                            pass\n\n        # write to the tensorboard\n        if tb_write and self.tb_writer is not None:\n            for fig_name, material in materials.items():\n                if isinstance(material, List):\n                    self.tb_writer.add_scalar(\n                        tag=os.path.join(*subfolder_names, fig_name),\n                        scalar_value=material[-1],\n                        global_step=epoch,\n                    )\n                elif isinstance(material, Dict):\n                    if len(material) &gt; 0:\n                        self.tb_writer.add_scalars(\n                            main_tag=os.path.join(*subfolder_names, fig_name),\n                            tag_scalar_dict={\n                                key: value[-1] for key, value in material.items()\n                            },\n                            global_step=epoch,\n                        )\n\n        # save all the information into files on disk for future usage\n        if data_save:\n            self.curve_plotter.save(\n                materials=materials, save_path=save_path, x_stride=x_stride\n            )\n\n    def matrix_snapshot(\n        self,\n        save_path: str,\n        subfolder_names: List[str],\n        materials: Dict,\n        epoch: int,\n        xlabel: str,\n        ylabel: str,\n        sep_save: bool,\n        sum_save: bool,\n        tb_write: bool,\n        data_save: bool,\n        col_num: int,\n        row_num: int,\n        flip_y: bool = False,\n    ):\n\n        # save the plotted figure into a specific file\n        if sep_save:\n            # loop each file in the given material Dict\n            for fig_name in materials.keys():\n                with self.sep_figure_context(fig_name, save_path) as ax:\n                    # plot the current material into a figure\n                    self.matrix_plotter.plot(\n                        ax=ax,\n                        material=materials[fig_name],\n                        fig_name=fig_name,\n                        xlabel=xlabel,\n                        ylabel=ylabel,\n                        flip_y=flip_y,\n                    )\n\n        # save all the materials to a summary figure\n        if sum_save:\n            with self.sum_figure_context(\n                materials, save_path, col_num, row_num, f\"epoch{epoch}.png\"\n            ) as (fig, material_keys, row_num, col_num):\n                # loop each row\n                for row in range(1, row_num + 1):\n                    # loop each column\n                    for col in range(1, col_num + 1):\n                        # initialize the sub-figure\n                        index = (row - 1) * col_num + col\n\n                        # plot each material into the corresponding sub-figure\n                        try:\n                            fig_name = material_keys[index - 1]\n                            ax = fig.add_subplot(row_num, col_num, index)\n                            # plot the figure\n                            self.matrix_plotter.plot(\n                                ax=ax,\n                                material=materials[fig_name],\n                                fig_name=fig_name,\n                                xlabel=xlabel,\n                                ylabel=ylabel,\n                                flip_y=flip_y,\n                            )\n                        except IndexError:\n                            pass\n\n        # write the figure to the tensorboard\n        if tb_write and self.tb_writer is not None:\n            img = torchvision.io.read_image(\n                os.path.join(save_path, f\"epoch{epoch}.png\")\n            )\n            self.tb_writer.add_image(\n                tag=os.path.join(*subfolder_names), img_tensor=img, global_step=epoch\n            )\n\n        # save all the matrices into a single .npz file on disk for future usage\n        if data_save:\n            self.matrix_plotter.save(\n                materials=materials, epoch=epoch, save_path=save_path\n            )\n\n    def hist_snapshot(self, save_path: str, materials: Dict, xlabel: str, ylabel: str):\n\n        # loop each file in the given material Dict\n        for fig_name in materials.keys():\n            with self.sep_figure_context(fig_name, save_path) as ax:\n                # plot the current material into a figure\n                self.hist_plotter.plot(\n                    ax=ax,\n                    material=materials[fig_name],\n                    fig_name=fig_name,\n                    xlabel=xlabel,\n                    ylabel=ylabel,\n                )\n\n    def text_snapshot(\n        self,\n        save_path: str,\n        subfolder_names: List[str],\n        materials: Dict[str, List[str]],\n        epoch: int,\n        data_save: bool,\n        x_stride: int = 1,\n    ):\n\n        # loop each file in the given material Dict\n        for file_name, material in materials.items():\n            self.tb_writer.add_text(\n                tag=os.path.join(*subfolder_names, file_name),\n                text_string=material[-1],\n                global_step=epoch,\n            )\n\n            if epoch is not None and data_save:\n                x_axis = np.arange(0, len(material), dtype=np.int_) * x_stride + 1\n                save_data = np.concatenate(\n                    (x_axis.reshape(-1, 1), np.array(material).reshape(-1, 1)), axis=-1\n                )\n\n                # save each material into a specific .txt file for easy visualization\n                np.savetxt(\n                    f\"{os.path.join(save_path, file_name)}.txt\", save_data, fmt=\"%s\"\n                )\n\n    def audio_snapshot(\n        self,\n        save_path: str,\n        subfolder_names: List[str],\n        materials: Dict[str, torch.Tensor],\n        epoch: int,\n        data_save: bool,\n        sample_rate: int,\n        audio_format: str,\n    ):\n\n        for file_name, material in materials.items():\n            self.tb_writer.add_audio(\n                tag=os.path.join(*subfolder_names, file_name),\n                snd_tensor=material,\n                global_step=epoch,\n                sample_rate=sample_rate,\n            )\n\n            # save each audio into a specific file for easy visualization\n            if epoch is not None and data_save:\n                if material.is_cuda:\n                    material = material.detach().cpu()\n                sf.write(\n                    file=f\"{os.path.join(save_path, file_name)}.{audio_format}\",\n                    data=material.numpy(),\n                    samplerate=sample_rate,\n                    subtype=sf.default_subtype(audio_format.upper()),\n                )\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.SnapShooter.__init__","title":"<code>__init__(result_path, snap_mode, fig_width=6.4, fig_height=4.8, curve_plotter_conf=None, matrix_plotter_conf=None, hist_plotter_conf=None)</code>","text":"<p>Initialize the SnapShooter.</p> <p>Parameters:</p> Name Type Description Default <code>result_path</code> <code>str</code> required <code>fig_width</code> <code>float</code> <code>6.4</code> <code>fig_height</code> <code>float</code> <code>4.8</code> <code>curve_plotter_conf</code> <code>Dict</code> <code>None</code> <code>matrix_plotter_conf</code> <code>Dict</code> <code>None</code> <code>hist_plotter_conf</code> <code>Dict</code> <code>None</code> Source code in <code>speechain/snapshooter.py</code> <pre><code>def __init__(\n    self,\n    result_path: str,\n    snap_mode: str,\n    fig_width: float = 6.4,\n    fig_height: float = 4.8,\n    curve_plotter_conf: Dict = None,\n    matrix_plotter_conf: Dict = None,\n    hist_plotter_conf: Dict = None,\n):\n    \"\"\"Initialize the SnapShooter.\n\n    Args:\n        result_path:\n        fig_width:\n        fig_height:\n        curve_plotter_conf:\n        matrix_plotter_conf:\n        hist_plotter_conf:\n    \"\"\"\n    # initialize the figure plotters and the tensorboard writer\n    if snap_mode is not None:\n        self.figure_path = os.path.join(result_path, \"figures\", snap_mode)\n        self.tb_writer = SummaryWriter(\n            log_dir=os.path.join(result_path, \"tensorboard\", snap_mode)\n        )\n    else:\n        self.figure_path = os.path.join(result_path, \"figures\")\n        self.tb_writer = None\n\n    # initialize the default values of the figure width and height\n    self.fig_width = fig_width\n    self.fig_height = fig_height\n\n    # initialize the built-in plotters\n    # curve plotter\n    curve_plotter_conf = (\n        dict() if curve_plotter_conf is None else curve_plotter_conf\n    )\n    self.curve_plotter = CurvePlotter(**curve_plotter_conf)\n    # matrix plotter\n    matrix_plotter_conf = (\n        dict() if matrix_plotter_conf is None else matrix_plotter_conf\n    )\n    self.matrix_plotter = MatrixPlotter(**matrix_plotter_conf)\n    # histogram plotter\n    hist_plotter_conf = dict() if hist_plotter_conf is None else hist_plotter_conf\n    self.hist_plotter = HistPlotter(**hist_plotter_conf)\n</code></pre>"},{"location":"reference/snapshooter/#snapshooter.snapshot_logs","title":"<code>snapshot_logs(logs_queue, event, snapshooter_conf, wait_time=10)</code>","text":"<p>Snapshot the model during training or validation.</p> <p>Parameters:</p> Name Type Description Default <code>logs_queue</code> <code>Queue</code> required <code>event</code> <code>Event</code> required <code>snapshooter_conf</code> <code>Dict</code> required <p>Returns:</p> Source code in <code>speechain/snapshooter.py</code> <pre><code>def snapshot_logs(\n    logs_queue: Queue, event: Event, snapshooter_conf: Dict, wait_time: int = 10\n):\n    \"\"\"Snapshot the model during training or validation.\n\n    Args:\n        logs_queue:\n        event:\n        snapshooter_conf:\n\n    Returns:\n    \"\"\"\n    snapshooter = SnapShooter(**snapshooter_conf)\n    while True:\n        try:\n            # check whether the queue is empty every 10 seconds\n            if not logs_queue.empty():\n                log = logs_queue.get()\n                snapshooter.snapshot(**log)\n            else:\n                event.wait(timeout=wait_time)\n                event.clear()\n        # catch all kinds of Exceptions and continue the process\n        except Exception as e:\n            warnings.warn(f\"snapshot_logs() meets an error: {e}.\")\n</code></pre>"},{"location":"reference/criterion/","title":"criterion","text":""},{"location":"reference/criterion/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/criterion/abs/#criterion.abs.Criterion","title":"<code>Criterion</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Criterion is a Callable object which is the base class for all criterion objects in this toolkit. It serves the role of evaluating the model forward calculation results. Its output can be either a loss function used for training or an evaluation metric used for validation.</p> <p>This base class has two abstract interface functions: criterion_init() for criterion initialization and call() for criterion forward calculation. 1. call() must be overridden if you want to make your own Criterion implementation. 2. criterion_init() is not mandatory to be overridden because some criteria can directly be applied to the input data     without any initialization such as speechain.criterion.accuracy.Accuracy.</p> Source code in <code>speechain/criterion/abs.py</code> <pre><code>class Criterion(ABC):\n    \"\"\"Criterion is a Callable object which is the base class for all criterion objects\n    in this toolkit. It serves the role of evaluating the model forward calculation\n    results. Its output can be either a loss function used for training or an evaluation\n    metric used for validation.\n\n    This base class has two abstract interface functions: criterion_init() for criterion initialization and __call__()\n    for criterion forward calculation.\n    1. __call__() must be overridden if you want to make your own Criterion implementation.\n    2. criterion_init() is not mandatory to be overridden because some criteria can directly be applied to the input data\n        without any initialization such as speechain.criterion.accuracy.Accuracy.\n    \"\"\"\n\n    def __init__(self, **criterion_conf):\n        \"\"\"This initialization function is shared by all Criterion subclasses.\n        Currently, the shared logic only contains calling the initialization function of\n        the parent class.\n\n        Args:\n            **criterion_conf:\n                The arguments used by criterion_init() for your customized Criterion initialization.\n        \"\"\"\n        super(Criterion, self).__init__()\n        self.criterion_init(**criterion_conf)\n\n    def criterion_init(self, **criterion_conf):\n        \"\"\"Abstract interface function for customized initialization of each Criterion\n        subclass. This interface function is not mandatory to be overridden by your\n        implementation.\n\n        Args:\n            **criterion_conf:\n                The arguments used for customized Criterion initialization.\n                For more details, please refer to the docstring of your target Criterion subclass.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, **kwargs):\n        \"\"\"This abstract interface function receives the model forward calculation\n        results and ground-truth labels. The output is a scalar which could be either\n        trainable for parameter optimization or non- trainable for information\n        recording.\n\n        This interface function is mandatory to be overridden by your implementation.\n\n        Args:\n            **kwargs:\n                model forward calculation results and ground-truth labels.\n                For more details, please refer to the docstring of __call__() of your target Criterion subclass.\n\n        Returns:\n            A trainable or non-trainable scalar.\n            For more details, please refer to the docstring of __call__() of your target Criterion subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/criterion/abs/#criterion.abs.Criterion.__call__","title":"<code>__call__(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>This abstract interface function receives the model forward calculation results and ground-truth labels. The output is a scalar which could be either trainable for parameter optimization or non- trainable for information recording.</p> <p>This interface function is mandatory to be overridden by your implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>model forward calculation results and ground-truth labels. For more details, please refer to the docstring of call() of your target Criterion subclass.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A trainable or non-trainable scalar.</p> <p>For more details, please refer to the docstring of call() of your target Criterion subclass.</p> Source code in <code>speechain/criterion/abs.py</code> <pre><code>@abstractmethod\ndef __call__(self, **kwargs):\n    \"\"\"This abstract interface function receives the model forward calculation\n    results and ground-truth labels. The output is a scalar which could be either\n    trainable for parameter optimization or non- trainable for information\n    recording.\n\n    This interface function is mandatory to be overridden by your implementation.\n\n    Args:\n        **kwargs:\n            model forward calculation results and ground-truth labels.\n            For more details, please refer to the docstring of __call__() of your target Criterion subclass.\n\n    Returns:\n        A trainable or non-trainable scalar.\n        For more details, please refer to the docstring of __call__() of your target Criterion subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/criterion/abs/#criterion.abs.Criterion.__init__","title":"<code>__init__(**criterion_conf)</code>","text":"<p>This initialization function is shared by all Criterion subclasses. Currently, the shared logic only contains calling the initialization function of the parent class.</p> <p>Parameters:</p> Name Type Description Default <code>**criterion_conf</code> <p>The arguments used by criterion_init() for your customized Criterion initialization.</p> <code>{}</code> Source code in <code>speechain/criterion/abs.py</code> <pre><code>def __init__(self, **criterion_conf):\n    \"\"\"This initialization function is shared by all Criterion subclasses.\n    Currently, the shared logic only contains calling the initialization function of\n    the parent class.\n\n    Args:\n        **criterion_conf:\n            The arguments used by criterion_init() for your customized Criterion initialization.\n    \"\"\"\n    super(Criterion, self).__init__()\n    self.criterion_init(**criterion_conf)\n</code></pre>"},{"location":"reference/criterion/abs/#criterion.abs.Criterion.criterion_init","title":"<code>criterion_init(**criterion_conf)</code>","text":"<p>Abstract interface function for customized initialization of each Criterion subclass. This interface function is not mandatory to be overridden by your implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**criterion_conf</code> <p>The arguments used for customized Criterion initialization. For more details, please refer to the docstring of your target Criterion subclass.</p> <code>{}</code> Source code in <code>speechain/criterion/abs.py</code> <pre><code>def criterion_init(self, **criterion_conf):\n    \"\"\"Abstract interface function for customized initialization of each Criterion\n    subclass. This interface function is not mandatory to be overridden by your\n    implementation.\n\n    Args:\n        **criterion_conf:\n            The arguments used for customized Criterion initialization.\n            For more details, please refer to the docstring of your target Criterion subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/criterion/accuracy/","title":"accuracy","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/criterion/accuracy/#criterion.accuracy.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>Criterion</code></p> <p>This criterion calculates the accuracy rate (0.0~1.0) between model predictions and target labels.</p> <p>This criterion doesn't have initialization function.</p> Source code in <code>speechain/criterion/accuracy.py</code> <pre><code>class Accuracy(Criterion):\n    \"\"\"This criterion calculates the accuracy rate (0.0~1.0) between model predictions\n    and target labels.\n\n    This criterion doesn't have initialization function.\n    \"\"\"\n\n    def __call__(\n        self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n    ):\n        \"\"\"\n\n        Args:\n            logits: (batch, logits_maxlen, vocab_size)\n                The token predictions from the model\n            text: (batch, text_maxlen)\n                The ground-truth token sequences\n            text_len: (batch,)\n                The length for the token predictions.\n\n        Returns:\n            The accuracy of the token predictions.\n\n        \"\"\"\n        # For the text attached by a &lt;sos/eos&gt; at the beginning\n        if logits.size(1) == text.size(1) - 1:\n            # text_len must match the sequence dimension of text\n            assert text_len.max() == text.size(1), (\n                f\"There is a mismatch of the sentence length between text and text_len. \"\n                f\"Expect text_len.max() is either equal to or 1 smaller than text.size(1), \"\n                f\"but got text_len.max()={text_len.max()} and text.size(1)={text.size(1)}.\"\n            )\n            # remove the &lt;sos/eos&gt; at the beginning\n            text = text[:, 1:].squeeze(dim=-1)\n            # don't use text_len -= 1 here because it will also change the text_len outside this function\n            text_len = text_len - 1\n        # Otherwise, text must not have a &lt;sos/eos&gt; at the beginning (equal in length with logits)\n        elif logits.size(1) != text.size(1):\n            raise RuntimeError\n\n        # mask generation for the input text length\n        text_mask = make_mask_from_len(text_len).squeeze(dim=1)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        # calculate the accuracy by the correct prediction\n        if logits.dim() == text.dim() + 1:\n            logits = logits.argmax(dim=-1)\n        elif logits.dim() != text.dim():\n            raise RuntimeError(\n                f\"logits.shape={logits.shape} but text.shape={text.shape}!\"\n            )\n        correct_num = logits.eq(text).masked_select(text_mask).sum()\n        total_num = text_len.sum()\n        return correct_num / total_num\n</code></pre>"},{"location":"reference/criterion/accuracy/#criterion.accuracy.Accuracy.__call__","title":"<code>__call__(logits, text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>(batch, logits_maxlen, vocab_size) The token predictions from the model</p> required <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The ground-truth token sequences</p> required <code>text_len</code> <code>Tensor</code> <p>(batch,) The length for the token predictions.</p> required <p>Returns:</p> Type Description <p>The accuracy of the token predictions.</p> Source code in <code>speechain/criterion/accuracy.py</code> <pre><code>def __call__(\n    self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n):\n    \"\"\"\n\n    Args:\n        logits: (batch, logits_maxlen, vocab_size)\n            The token predictions from the model\n        text: (batch, text_maxlen)\n            The ground-truth token sequences\n        text_len: (batch,)\n            The length for the token predictions.\n\n    Returns:\n        The accuracy of the token predictions.\n\n    \"\"\"\n    # For the text attached by a &lt;sos/eos&gt; at the beginning\n    if logits.size(1) == text.size(1) - 1:\n        # text_len must match the sequence dimension of text\n        assert text_len.max() == text.size(1), (\n            f\"There is a mismatch of the sentence length between text and text_len. \"\n            f\"Expect text_len.max() is either equal to or 1 smaller than text.size(1), \"\n            f\"but got text_len.max()={text_len.max()} and text.size(1)={text.size(1)}.\"\n        )\n        # remove the &lt;sos/eos&gt; at the beginning\n        text = text[:, 1:].squeeze(dim=-1)\n        # don't use text_len -= 1 here because it will also change the text_len outside this function\n        text_len = text_len - 1\n    # Otherwise, text must not have a &lt;sos/eos&gt; at the beginning (equal in length with logits)\n    elif logits.size(1) != text.size(1):\n        raise RuntimeError\n\n    # mask generation for the input text length\n    text_mask = make_mask_from_len(text_len).squeeze(dim=1)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    # calculate the accuracy by the correct prediction\n    if logits.dim() == text.dim() + 1:\n        logits = logits.argmax(dim=-1)\n    elif logits.dim() != text.dim():\n        raise RuntimeError(\n            f\"logits.shape={logits.shape} but text.shape={text.shape}!\"\n        )\n    correct_num = logits.eq(text).masked_select(text_mask).sum()\n    total_num = text_len.sum()\n    return correct_num / total_num\n</code></pre>"},{"location":"reference/criterion/att_guid/","title":"att_guid","text":""},{"location":"reference/criterion/att_guid/#criterion.att_guid.AttentionGuidance","title":"<code>AttentionGuidance</code>","text":"<p>               Bases: <code>Criterion</code></p> <p>This criterion is the attention guidance loss function.</p> Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention <p>https://arxiv.org/pdf/1710.08969</p> Source code in <code>speechain/criterion/att_guid.py</code> <pre><code>class AttentionGuidance(Criterion):\n    \"\"\"This criterion is the attention guidance loss function.\n\n    References: Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention\n        https://arxiv.org/pdf/1710.08969\n    \"\"\"\n\n    def criterion_init(self, sigma: float = 0.2):\n        \"\"\"\n\n        Args:\n            sigma: float = 0.2\n                The value of the sigma used to calculate the attention guidance loss.\n\n        \"\"\"\n        self.coeff = -1 / (2 * sigma**2)\n\n    def get_weight_matrix(self, X: int, Y: int) -&gt; torch.Tensor:\n        \"\"\"\n\n        Args:\n            X:\n            Y:\n\n        Returns:\n\n        \"\"\"\n        grid_x, grid_y = torch.meshgrid(torch.arange(X), torch.arange(Y))\n        return 1 - torch.exp(self.coeff * torch.pow(grid_x / X - grid_y / Y, 2))\n\n    def __call__(\n        self, att_tensor: torch.Tensor, x_len: torch.Tensor, y_len: torch.Tensor = None\n    ):\n        \"\"\"\n\n        Args:\n            att_tensor: (batch, layer_num * head_num, max_xlen, max_ylen)\n            x_len: (batch,)\n            y_len: (batch,) = None\n\n        Returns:\n\n        \"\"\"\n        # argument checking\n        assert len(att_tensor) == len(x_len)\n        if y_len is None:\n            y_len = x_len\n        else:\n            assert len(att_tensor) == len(y_len)\n\n        # solve length mismatch\n        if x_len.max() &gt; att_tensor.size(2):\n            x_len = x_len - (x_len.max() - att_tensor.size(2))\n        if y_len.max() &gt; att_tensor.size(3):\n            y_len = y_len - (y_len.max() - att_tensor.size(3))\n\n        # guidance weight matrix initialization, (batch, 1, max_xlen, max_ylen)\n        weight_tensor = torch.zeros(\n            (att_tensor.size(0), 1, att_tensor.size(2), att_tensor.size(3)),\n            device=att_tensor.device,\n        )\n        # guidance mask matrix initialization, (batch, 1, max_xlen, max_ylen)\n        mask_flag = torch.zeros(\n            weight_tensor.size(), dtype=torch.bool, device=weight_tensor.device\n        )\n        # loop each utterance and register its weight and mask matrices\n        for i, (X, Y) in enumerate(zip(x_len, y_len)):\n            X, Y = X.item(), Y.item()\n            weight_tensor[i][0][:X, :Y] = self.get_weight_matrix(X, Y).to(\n                att_tensor.device\n            )\n            mask_flag[i][0][:X, :Y] = 1\n\n        # return the mean value of the masked results\n        return torch.mean(torch.masked_select(att_tensor * weight_tensor, mask_flag))\n</code></pre>"},{"location":"reference/criterion/att_guid/#criterion.att_guid.AttentionGuidance.__call__","title":"<code>__call__(att_tensor, x_len, y_len=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>att_tensor</code> <code>Tensor</code> <p>(batch, layer_num * head_num, max_xlen, max_ylen)</p> required <code>x_len</code> <code>Tensor</code> <p>(batch,)</p> required <code>y_len</code> <code>Tensor</code> <p>(batch,) = None</p> <code>None</code> <p>Returns:</p> Source code in <code>speechain/criterion/att_guid.py</code> <pre><code>def __call__(\n    self, att_tensor: torch.Tensor, x_len: torch.Tensor, y_len: torch.Tensor = None\n):\n    \"\"\"\n\n    Args:\n        att_tensor: (batch, layer_num * head_num, max_xlen, max_ylen)\n        x_len: (batch,)\n        y_len: (batch,) = None\n\n    Returns:\n\n    \"\"\"\n    # argument checking\n    assert len(att_tensor) == len(x_len)\n    if y_len is None:\n        y_len = x_len\n    else:\n        assert len(att_tensor) == len(y_len)\n\n    # solve length mismatch\n    if x_len.max() &gt; att_tensor.size(2):\n        x_len = x_len - (x_len.max() - att_tensor.size(2))\n    if y_len.max() &gt; att_tensor.size(3):\n        y_len = y_len - (y_len.max() - att_tensor.size(3))\n\n    # guidance weight matrix initialization, (batch, 1, max_xlen, max_ylen)\n    weight_tensor = torch.zeros(\n        (att_tensor.size(0), 1, att_tensor.size(2), att_tensor.size(3)),\n        device=att_tensor.device,\n    )\n    # guidance mask matrix initialization, (batch, 1, max_xlen, max_ylen)\n    mask_flag = torch.zeros(\n        weight_tensor.size(), dtype=torch.bool, device=weight_tensor.device\n    )\n    # loop each utterance and register its weight and mask matrices\n    for i, (X, Y) in enumerate(zip(x_len, y_len)):\n        X, Y = X.item(), Y.item()\n        weight_tensor[i][0][:X, :Y] = self.get_weight_matrix(X, Y).to(\n            att_tensor.device\n        )\n        mask_flag[i][0][:X, :Y] = 1\n\n    # return the mean value of the masked results\n    return torch.mean(torch.masked_select(att_tensor * weight_tensor, mask_flag))\n</code></pre>"},{"location":"reference/criterion/att_guid/#criterion.att_guid.AttentionGuidance.criterion_init","title":"<code>criterion_init(sigma=0.2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>float = 0.2 The value of the sigma used to calculate the attention guidance loss.</p> <code>0.2</code> Source code in <code>speechain/criterion/att_guid.py</code> <pre><code>def criterion_init(self, sigma: float = 0.2):\n    \"\"\"\n\n    Args:\n        sigma: float = 0.2\n            The value of the sigma used to calculate the attention guidance loss.\n\n    \"\"\"\n    self.coeff = -1 / (2 * sigma**2)\n</code></pre>"},{"location":"reference/criterion/att_guid/#criterion.att_guid.AttentionGuidance.get_weight_matrix","title":"<code>get_weight_matrix(X, Y)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>X</code> <code>int</code> required <code>Y</code> <code>int</code> required <p>Returns:</p> Source code in <code>speechain/criterion/att_guid.py</code> <pre><code>def get_weight_matrix(self, X: int, Y: int) -&gt; torch.Tensor:\n    \"\"\"\n\n    Args:\n        X:\n        Y:\n\n    Returns:\n\n    \"\"\"\n    grid_x, grid_y = torch.meshgrid(torch.arange(X), torch.arange(Y))\n    return 1 - torch.exp(self.coeff * torch.pow(grid_x / X - grid_y / Y, 2))\n</code></pre>"},{"location":"reference/criterion/bce_logits/","title":"bce_logits","text":"<p>Author: Sashi Novitasari Affiliation: NAIST Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/criterion/bce_logits/#criterion.bce_logits.BCELogits","title":"<code>BCELogits</code>","text":"<p>               Bases: <code>Criterion</code></p> Source code in <code>speechain/criterion/bce_logits.py</code> <pre><code>class BCELogits(Criterion):\n    \"\"\"\"\"\"\n\n    def criterion_init(self, pos_weight: float = 1.0, is_normalized: bool = True):\n        \"\"\"\n\n        Args:\n            pos_weight: float = 1.0\n                The weight putted on stop points for stop loss calculation.\n            is_normalized: bool = True\n                Controls whether the sentence normalization is performed for stop loss calculation.\n\n        \"\"\"\n        self.bce_loss = torch.nn.BCEWithLogitsLoss(\n            reduction=\"none\", pos_weight=torch.Tensor([pos_weight])\n        )\n        self.is_normalized = is_normalized\n\n    def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            pred: (batch, text_maxlen)\n                The model predictions for the text\n            tgt: (batch, text_maxlen)\n                The target text labels.\n            tgt_len: (batch,)\n                The text lengths\n\n        Returns:\n            The cross entropy between logits and text\n\n        \"\"\"\n        batch_size, feat_maxlen = pred.size(0), pred.size(1)\n        if tgt.dtype == torch.bool:\n            tgt = tgt.to(dtype=torch.float32)\n\n        # mask production for the target labels\n        tgt_mask = make_mask_from_len(tgt_len).squeeze()\n        if tgt_len.is_cuda:\n            tgt_mask = tgt_mask.cuda(tgt_len.device)\n\n        # BCE loss calculation\n        # make sure that the pos_weight is also on GPU\n        if pred.is_cuda and not self.bce_loss.pos_weight.is_cuda:\n            self.bce_loss.pos_weight = self.bce_loss.pos_weight.cuda(pred.device)\n        # (batch_size, feat_maxlen)\n        loss = self.bce_loss(pred, tgt)\n        # (batch_size, feat_maxlen) -&gt; (batch_size * feat_maxlen)\n        loss = loss.reshape(-1).masked_fill(~tgt_mask.reshape(-1), 0.0)\n\n        # loss reshaping\n        if self.is_normalized:\n            # (batch_size * feat_maxlen) -&gt; (1,)\n            loss = loss.sum() / tgt_mask.sum()\n        else:\n            # (batch_size * feat_maxlen) -&gt; (batch_size, feat_maxlen)\n            loss = loss.reshape(batch_size, feat_maxlen)\n            # (batch_size, feat_maxlen) -&gt; (batch_size,) -&gt; (1,)\n            loss = loss.sum(dim=-1).mean()\n\n        return loss\n</code></pre>"},{"location":"reference/criterion/bce_logits/#criterion.bce_logits.BCELogits.__call__","title":"<code>__call__(pred, tgt, tgt_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>(batch, text_maxlen) The model predictions for the text</p> required <code>tgt</code> <code>Tensor</code> <p>(batch, text_maxlen) The target text labels.</p> required <code>tgt_len</code> <code>Tensor</code> <p>(batch,) The text lengths</p> required <p>Returns:</p> Type Description <p>The cross entropy between logits and text</p> Source code in <code>speechain/criterion/bce_logits.py</code> <pre><code>def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        pred: (batch, text_maxlen)\n            The model predictions for the text\n        tgt: (batch, text_maxlen)\n            The target text labels.\n        tgt_len: (batch,)\n            The text lengths\n\n    Returns:\n        The cross entropy between logits and text\n\n    \"\"\"\n    batch_size, feat_maxlen = pred.size(0), pred.size(1)\n    if tgt.dtype == torch.bool:\n        tgt = tgt.to(dtype=torch.float32)\n\n    # mask production for the target labels\n    tgt_mask = make_mask_from_len(tgt_len).squeeze()\n    if tgt_len.is_cuda:\n        tgt_mask = tgt_mask.cuda(tgt_len.device)\n\n    # BCE loss calculation\n    # make sure that the pos_weight is also on GPU\n    if pred.is_cuda and not self.bce_loss.pos_weight.is_cuda:\n        self.bce_loss.pos_weight = self.bce_loss.pos_weight.cuda(pred.device)\n    # (batch_size, feat_maxlen)\n    loss = self.bce_loss(pred, tgt)\n    # (batch_size, feat_maxlen) -&gt; (batch_size * feat_maxlen)\n    loss = loss.reshape(-1).masked_fill(~tgt_mask.reshape(-1), 0.0)\n\n    # loss reshaping\n    if self.is_normalized:\n        # (batch_size * feat_maxlen) -&gt; (1,)\n        loss = loss.sum() / tgt_mask.sum()\n    else:\n        # (batch_size * feat_maxlen) -&gt; (batch_size, feat_maxlen)\n        loss = loss.reshape(batch_size, feat_maxlen)\n        # (batch_size, feat_maxlen) -&gt; (batch_size,) -&gt; (1,)\n        loss = loss.sum(dim=-1).mean()\n\n    return loss\n</code></pre>"},{"location":"reference/criterion/bce_logits/#criterion.bce_logits.BCELogits.criterion_init","title":"<code>criterion_init(pos_weight=1.0, is_normalized=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>float = 1.0 The weight putted on stop points for stop loss calculation.</p> <code>1.0</code> <code>is_normalized</code> <code>bool</code> <p>bool = True Controls whether the sentence normalization is performed for stop loss calculation.</p> <code>True</code> Source code in <code>speechain/criterion/bce_logits.py</code> <pre><code>def criterion_init(self, pos_weight: float = 1.0, is_normalized: bool = True):\n    \"\"\"\n\n    Args:\n        pos_weight: float = 1.0\n            The weight putted on stop points for stop loss calculation.\n        is_normalized: bool = True\n            Controls whether the sentence normalization is performed for stop loss calculation.\n\n    \"\"\"\n    self.bce_loss = torch.nn.BCEWithLogitsLoss(\n        reduction=\"none\", pos_weight=torch.Tensor([pos_weight])\n    )\n    self.is_normalized = is_normalized\n</code></pre>"},{"location":"reference/criterion/cross_entropy/","title":"cross_entropy","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/criterion/cross_entropy/#criterion.cross_entropy.CrossEntropy","title":"<code>CrossEntropy</code>","text":"<p>               Bases: <code>Criterion</code></p> <p>This criterion calculates the cross entropy between model predictions and target labels.</p> <p>In this implementation, we realize the following functions:     1. Sentence normalization. The loss will be normalized according to the length of each sentence.     2. Label smoothing. The target label will be transformed from a sharp one-hot vector to a smooth distribution vector.     3. Token reweighting. The weight of each token in the cross entropy calculation can be customized manually.     If you want to customize the weights, you need to give the token dictionary.</p> Source code in <code>speechain/criterion/cross_entropy.py</code> <pre><code>class CrossEntropy(Criterion):\n    \"\"\"This criterion calculates the cross entropy between model predictions and target\n    labels.\n\n    In this implementation, we realize the following functions:\n        1. Sentence normalization. The loss will be normalized according to the length of each sentence.\n        2. Label smoothing. The target label will be transformed from a sharp one-hot vector to a smooth distribution vector.\n        3. Token reweighting. The weight of each token in the cross entropy calculation can be customized manually.\n        If you want to customize the weights, you need to give the token dictionary.\n    \"\"\"\n\n    def criterion_init(\n        self,\n        length_normalized: bool = False,\n        label_smoothing: float = 0.0,\n        temperature: float = 1.0,\n        confid_threshold: float = 0.0,\n        confid_level: str = \"sentence\",\n        token_vocab: str = None,\n        new_weights: Dict = None,\n    ):\n        \"\"\"\n\n        Args:\n            length_normalized: bool\n                Controls whether the sentence normalization is performed.\n            label_smoothing: float\n                Controls the scale of label smoothing. 0 means no smoothing.\n            temperature: float\n                Controls the temperature of the Softmax operation.\n            confid_threshold: float\n                Controls whether to ignore the prediction lower than the threshold for loss calculation.\n            confid_level: str\n                The level of confidence calculation. Either 'token' (token_level confidence) or 'sent' (sentence-level\n                confidence). Default to be 'sentence'.\n            token_vocab: str\n                The path of the given token vocabulary list. Necessary if new_weights is not None.\n            new_weights: Dict\n                The customized token weights to calculate the cross entropy. Must be given in the format below:\n                'new_weights:\n                    token1: weight1\n                    token2: weight2\n                    ...'\n        \"\"\"\n\n        assert (\n            0 &lt;= label_smoothing &lt; 1.0\n        ), f\"The value of label_smoothing should be a float number in [0, 1), but got {label_smoothing}!\"\n        assert (\n            temperature &gt;= 0.0\n        ), f\"The value of temperature should be a non-negative float number, but got {temperature}\"\n        assert (\n            0 &lt;= confid_threshold &lt; 1.0\n        ), f\"The value of confid_threshold should be a float number in [0, 1), but got {label_smoothing}!\"\n        assert confid_level in [\n            \"sentence\",\n            \"token\",\n        ], f\"confid_level must be one of ['sentence', 'token'], but got {confid_level}\"\n\n        # para recording\n        self.length_normalized = length_normalized\n        self.label_smoothing = label_smoothing\n        self.temperature = temperature\n        self.confid_threshold = confid_threshold\n        self.confid_level = confid_level\n        self.token_weights = None\n\n        # update the token weights if new_weights is given\n        if new_weights is not None:\n            assert (\n                token_vocab is not None\n            ), \"Please specify a token dictionary by 'token_vocab' if you want to customize the token weights.\"\n\n            token_dict = np.loadtxt(token_vocab, dtype=str, delimiter=\"\\n\")\n            token_dict = dict(zip(token_dict, np.arange(0, token_dict.shape[0])))\n            self.token_weights = torch.ones(len(token_dict)).cuda().detach()\n\n            for token, weight in new_weights.items():\n                self.token_weights[token_dict[token]] = weight\n\n    def __call__(\n        self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n    ):\n        \"\"\"\n\n        Args:\n            logits: (batch, text_maxlen, vocab_size)\n                The model predictions for the text\n            text: (batch, text_maxlen)\n                The target text labels.\n            text_len: (batch,)\n                The text lengths\n\n        Returns:\n            The cross entropy between logits and text\n\n        \"\"\"\n        # For the text attached by a &lt;sos/eos&gt; at the beginning\n        if logits.size(1) == text.size(1) - 1:\n            # text_len must match the sequence dimension of text\n            assert text_len.max() == text.size(1), (\n                f\"There is a mismatch of the sentence length between text and text_len. \"\n                f\"Expect text_len.max() is either equal to or 1 smaller than text.size(1), \"\n                f\"but got text_len.max()={text_len.max()} and text.size(1)={text.size(1)}.\"\n            )\n            # remove the &lt;sos/eos&gt; at the beginning\n            text = text[:, 1:].squeeze(dim=-1)\n            # don't use text_len -= 1 here because it will also change the text_len outside this function\n            text_len = text_len - 1\n        # Otherwise, text must not have a &lt;sos/eos&gt; at the beginning (equal in length with logits)\n        elif logits.size(1) != text.size(1):\n            raise RuntimeError\n\n        # reshape predictions and do log-softmax\n        batch, seq_maxlen, vocab_size = logits.size()\n        log_prob = torch.log_softmax(\n            logits.contiguous().view(batch * seq_maxlen, vocab_size) / self.temperature,\n            dim=-1,\n        )\n\n        # reshape targets and calculate the loss\n        log_prob_target = log_prob.gather(1, text.contiguous().view(-1, 1)).squeeze(\n            dim=-1\n        )\n        if self.label_smoothing &gt; 0:\n            smooth_pos = 1 - self.label_smoothing\n            smooth_neg = self.label_smoothing / vocab_size\n            loss = (log_prob_target * smooth_pos) + (log_prob * smooth_neg).sum(dim=1)\n        else:\n            loss = log_prob_target\n\n        # reweight each token in the calculated loss\n        if self.token_weights is not None:\n            loss = loss * self.token_weights.index_select(0, text.reshape(-1))\n\n        # convert the text length into the bool masks\n        text_mask = make_mask_from_len(text_len, return_3d=False)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        # padding the extra part of each sentence by zeros\n        loss_mask = ~text_mask.reshape(-1)\n        if loss.is_cuda:\n            loss_mask = loss_mask.cuda(loss.device)\n        if self.confid_threshold &gt; 0:\n            # padding the token predictions whose token-level confidences are lower than the threshold\n            if self.confid_level == \"token\":\n                # confid_mask: (batch * seq_maxlen,)\n                confid_mask = log_prob_target &lt;= math.log(self.confid_threshold)\n                loss_mask = torch.logical_or(loss_mask, confid_mask)\n                # update text_len by confid_mask for normalization (mask the extra part of each sentence)\n                # (batch * seq_maxlen,) -&gt; (batch, seq_maxlen) -&gt; (batch,)\n                text_len = (\n                    (~confid_mask.reshape(batch, seq_maxlen))\n                    .masked_fill(~text_mask, False)\n                    .sum(dim=-1)\n                )\n                # whether a sentence is valid (i.e. contains unmasked token prediction): (batch,)\n                valid_sent = text_len &gt; 0\n            # padding the whole sentence predictions whose sentence-level confidences are lower than the threshold\n            elif self.confid_level == \"sentence\":\n                # sent_confid: (batch * seq_maxlen,) -&gt; (batch, seq_maxlen) -&gt; (batch, 1)\n                sent_confid = (\n                    log_prob_target.reshape(batch, seq_maxlen)\n                    .masked_fill(~text_mask, 0.0)\n                    .sum(dim=-1, keepdim=True)\n                )\n                # confid_mask: (batch, 1)\n                confid_mask = sent_confid &lt;= (\n                    text_len * math.log(self.confid_threshold)\n                ).unsqueeze(-1)\n                # confid_mask: (batch, 1) -&gt; (batch, seq_maxlen) -&gt; (batch * seq_maxlen,)\n                loss_mask = torch.logical_or(\n                    loss_mask, confid_mask.expand(-1, seq_maxlen).reshape(-1)\n                )\n                # whether a sentence is valid (i.e. unmasked sentence): (batch, 1) -&gt; (batch,)\n                valid_sent = ~confid_mask.squeeze(-1)\n            else:\n                raise RuntimeError(\n                    f\"confid_level must be one of ['sentence', 'token'], but got {self.confid_level}\"\n                )\n        else:\n            valid_sent = None\n\n        loss = loss.masked_fill(loss_mask, 0.0).reshape(batch, seq_maxlen).sum(dim=-1)\n\n        # normalize the loss by the token sequence length if specified\n        if self.length_normalized:\n            loss /= text_len + 1e-10\n\n        # valid_sent is used to calculate the number of valid sentence included in the loss\n        return (\n            -loss.mean()\n            if self.confid_threshold == 0\n            else -loss.sum() / (torch.sum(valid_sent) + 1e-10)\n        )\n</code></pre>"},{"location":"reference/criterion/cross_entropy/#criterion.cross_entropy.CrossEntropy.__call__","title":"<code>__call__(logits, text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>(batch, text_maxlen, vocab_size) The model predictions for the text</p> required <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The target text labels.</p> required <code>text_len</code> <code>Tensor</code> <p>(batch,) The text lengths</p> required <p>Returns:</p> Type Description <p>The cross entropy between logits and text</p> Source code in <code>speechain/criterion/cross_entropy.py</code> <pre><code>def __call__(\n    self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n):\n    \"\"\"\n\n    Args:\n        logits: (batch, text_maxlen, vocab_size)\n            The model predictions for the text\n        text: (batch, text_maxlen)\n            The target text labels.\n        text_len: (batch,)\n            The text lengths\n\n    Returns:\n        The cross entropy between logits and text\n\n    \"\"\"\n    # For the text attached by a &lt;sos/eos&gt; at the beginning\n    if logits.size(1) == text.size(1) - 1:\n        # text_len must match the sequence dimension of text\n        assert text_len.max() == text.size(1), (\n            f\"There is a mismatch of the sentence length between text and text_len. \"\n            f\"Expect text_len.max() is either equal to or 1 smaller than text.size(1), \"\n            f\"but got text_len.max()={text_len.max()} and text.size(1)={text.size(1)}.\"\n        )\n        # remove the &lt;sos/eos&gt; at the beginning\n        text = text[:, 1:].squeeze(dim=-1)\n        # don't use text_len -= 1 here because it will also change the text_len outside this function\n        text_len = text_len - 1\n    # Otherwise, text must not have a &lt;sos/eos&gt; at the beginning (equal in length with logits)\n    elif logits.size(1) != text.size(1):\n        raise RuntimeError\n\n    # reshape predictions and do log-softmax\n    batch, seq_maxlen, vocab_size = logits.size()\n    log_prob = torch.log_softmax(\n        logits.contiguous().view(batch * seq_maxlen, vocab_size) / self.temperature,\n        dim=-1,\n    )\n\n    # reshape targets and calculate the loss\n    log_prob_target = log_prob.gather(1, text.contiguous().view(-1, 1)).squeeze(\n        dim=-1\n    )\n    if self.label_smoothing &gt; 0:\n        smooth_pos = 1 - self.label_smoothing\n        smooth_neg = self.label_smoothing / vocab_size\n        loss = (log_prob_target * smooth_pos) + (log_prob * smooth_neg).sum(dim=1)\n    else:\n        loss = log_prob_target\n\n    # reweight each token in the calculated loss\n    if self.token_weights is not None:\n        loss = loss * self.token_weights.index_select(0, text.reshape(-1))\n\n    # convert the text length into the bool masks\n    text_mask = make_mask_from_len(text_len, return_3d=False)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    # padding the extra part of each sentence by zeros\n    loss_mask = ~text_mask.reshape(-1)\n    if loss.is_cuda:\n        loss_mask = loss_mask.cuda(loss.device)\n    if self.confid_threshold &gt; 0:\n        # padding the token predictions whose token-level confidences are lower than the threshold\n        if self.confid_level == \"token\":\n            # confid_mask: (batch * seq_maxlen,)\n            confid_mask = log_prob_target &lt;= math.log(self.confid_threshold)\n            loss_mask = torch.logical_or(loss_mask, confid_mask)\n            # update text_len by confid_mask for normalization (mask the extra part of each sentence)\n            # (batch * seq_maxlen,) -&gt; (batch, seq_maxlen) -&gt; (batch,)\n            text_len = (\n                (~confid_mask.reshape(batch, seq_maxlen))\n                .masked_fill(~text_mask, False)\n                .sum(dim=-1)\n            )\n            # whether a sentence is valid (i.e. contains unmasked token prediction): (batch,)\n            valid_sent = text_len &gt; 0\n        # padding the whole sentence predictions whose sentence-level confidences are lower than the threshold\n        elif self.confid_level == \"sentence\":\n            # sent_confid: (batch * seq_maxlen,) -&gt; (batch, seq_maxlen) -&gt; (batch, 1)\n            sent_confid = (\n                log_prob_target.reshape(batch, seq_maxlen)\n                .masked_fill(~text_mask, 0.0)\n                .sum(dim=-1, keepdim=True)\n            )\n            # confid_mask: (batch, 1)\n            confid_mask = sent_confid &lt;= (\n                text_len * math.log(self.confid_threshold)\n            ).unsqueeze(-1)\n            # confid_mask: (batch, 1) -&gt; (batch, seq_maxlen) -&gt; (batch * seq_maxlen,)\n            loss_mask = torch.logical_or(\n                loss_mask, confid_mask.expand(-1, seq_maxlen).reshape(-1)\n            )\n            # whether a sentence is valid (i.e. unmasked sentence): (batch, 1) -&gt; (batch,)\n            valid_sent = ~confid_mask.squeeze(-1)\n        else:\n            raise RuntimeError(\n                f\"confid_level must be one of ['sentence', 'token'], but got {self.confid_level}\"\n            )\n    else:\n        valid_sent = None\n\n    loss = loss.masked_fill(loss_mask, 0.0).reshape(batch, seq_maxlen).sum(dim=-1)\n\n    # normalize the loss by the token sequence length if specified\n    if self.length_normalized:\n        loss /= text_len + 1e-10\n\n    # valid_sent is used to calculate the number of valid sentence included in the loss\n    return (\n        -loss.mean()\n        if self.confid_threshold == 0\n        else -loss.sum() / (torch.sum(valid_sent) + 1e-10)\n    )\n</code></pre>"},{"location":"reference/criterion/cross_entropy/#criterion.cross_entropy.CrossEntropy.criterion_init","title":"<code>criterion_init(length_normalized=False, label_smoothing=0.0, temperature=1.0, confid_threshold=0.0, confid_level='sentence', token_vocab=None, new_weights=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>length_normalized</code> <code>bool</code> <p>bool Controls whether the sentence normalization is performed.</p> <code>False</code> <code>label_smoothing</code> <code>float</code> <p>float Controls the scale of label smoothing. 0 means no smoothing.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>float Controls the temperature of the Softmax operation.</p> <code>1.0</code> <code>confid_threshold</code> <code>float</code> <p>float Controls whether to ignore the prediction lower than the threshold for loss calculation.</p> <code>0.0</code> <code>confid_level</code> <code>str</code> <p>str The level of confidence calculation. Either 'token' (token_level confidence) or 'sent' (sentence-level confidence). Default to be 'sentence'.</p> <code>'sentence'</code> <code>token_vocab</code> <code>str</code> <p>str The path of the given token vocabulary list. Necessary if new_weights is not None.</p> <code>None</code> <code>new_weights</code> <code>Dict</code> <p>Dict The customized token weights to calculate the cross entropy. Must be given in the format below: 'new_weights:     token1: weight1     token2: weight2     ...'</p> <code>None</code> Source code in <code>speechain/criterion/cross_entropy.py</code> <pre><code>def criterion_init(\n    self,\n    length_normalized: bool = False,\n    label_smoothing: float = 0.0,\n    temperature: float = 1.0,\n    confid_threshold: float = 0.0,\n    confid_level: str = \"sentence\",\n    token_vocab: str = None,\n    new_weights: Dict = None,\n):\n    \"\"\"\n\n    Args:\n        length_normalized: bool\n            Controls whether the sentence normalization is performed.\n        label_smoothing: float\n            Controls the scale of label smoothing. 0 means no smoothing.\n        temperature: float\n            Controls the temperature of the Softmax operation.\n        confid_threshold: float\n            Controls whether to ignore the prediction lower than the threshold for loss calculation.\n        confid_level: str\n            The level of confidence calculation. Either 'token' (token_level confidence) or 'sent' (sentence-level\n            confidence). Default to be 'sentence'.\n        token_vocab: str\n            The path of the given token vocabulary list. Necessary if new_weights is not None.\n        new_weights: Dict\n            The customized token weights to calculate the cross entropy. Must be given in the format below:\n            'new_weights:\n                token1: weight1\n                token2: weight2\n                ...'\n    \"\"\"\n\n    assert (\n        0 &lt;= label_smoothing &lt; 1.0\n    ), f\"The value of label_smoothing should be a float number in [0, 1), but got {label_smoothing}!\"\n    assert (\n        temperature &gt;= 0.0\n    ), f\"The value of temperature should be a non-negative float number, but got {temperature}\"\n    assert (\n        0 &lt;= confid_threshold &lt; 1.0\n    ), f\"The value of confid_threshold should be a float number in [0, 1), but got {label_smoothing}!\"\n    assert confid_level in [\n        \"sentence\",\n        \"token\",\n    ], f\"confid_level must be one of ['sentence', 'token'], but got {confid_level}\"\n\n    # para recording\n    self.length_normalized = length_normalized\n    self.label_smoothing = label_smoothing\n    self.temperature = temperature\n    self.confid_threshold = confid_threshold\n    self.confid_level = confid_level\n    self.token_weights = None\n\n    # update the token weights if new_weights is given\n    if new_weights is not None:\n        assert (\n            token_vocab is not None\n        ), \"Please specify a token dictionary by 'token_vocab' if you want to customize the token weights.\"\n\n        token_dict = np.loadtxt(token_vocab, dtype=str, delimiter=\"\\n\")\n        token_dict = dict(zip(token_dict, np.arange(0, token_dict.shape[0])))\n        self.token_weights = torch.ones(len(token_dict)).cuda().detach()\n\n        for token, weight in new_weights.items():\n            self.token_weights[token_dict[token]] = weight\n</code></pre>"},{"location":"reference/criterion/ctc/","title":"ctc","text":""},{"location":"reference/criterion/ctc/#criterion.ctc.CTCLoss","title":"<code>CTCLoss</code>","text":"<p>               Bases: <code>Criterion</code></p> <p>The wrapper class for torch.nn.functional.ctc_loss.</p> Source code in <code>speechain/criterion/ctc.py</code> <pre><code>class CTCLoss(Criterion):\n    \"\"\"The wrapper class for torch.nn.functional.ctc_loss.\"\"\"\n\n    def criterion_init(self, blank: int = 0, zero_infinity: bool = True):\n        \"\"\"\n\n        Args:\n            weight: float\n                The weight on the CTC loss in the overall ASR loss. Used to balance the loss terms outside this class.\n            blank: int = 0\n                The blank label for CTC modeling. In order to use CuDNN, blank must be set to 0.\n            zero_infinity: bool = True\n                Whether to zero infinite losses and the associated gradients when calculating the CTC loss.\n\n        \"\"\"\n        # arguments checking\n        self.blank = blank\n        self.zero_infinity = zero_infinity\n\n    def __call__(\n        self,\n        ctc_logits: torch.Tensor,\n        enc_feat_len: torch.Tensor,\n        text: torch.Tensor,\n        text_len: torch.Tensor,\n    ):\n        \"\"\"\n\n        Args:\n            ctc_logits: (batch, enc_feat_len, vocab)\n                The model output from the CTC layer before the softmax operation.\n            enc_feat_len: (batch,)\n                The length of encoder feature sequences (&lt;= the length of acoustic feature sequence)\n            text: (batch, text_len)\n                The grount-truth token index sequences.\n            text_len: (batch,)\n                The length of each token index sequence.\n\n        \"\"\"\n        batch, enc_feat_maxlen = ctc_logits.size(0), enc_feat_len.max().item()\n\n        # (batch, enc_feat_len, vocab) -&gt; (enc_feat_len, batch, vocab)\n        ctc_logits = ctc_logits.transpose(0, 1).log_softmax(dim=-1)\n\n        # remove the &lt;sos/eos&gt; at the beginning and end of each sentence\n        text, text_len = text[:, 1:-1], text_len - 2\n        if len(text.shape) == 1:\n            text = text.unsqueeze(-1)\n        text = torch.cat([text[i, : text_len[i]] for i in range(batch)])\n\n        # obtain the ctc loss for each data instances in the given batch\n        loss = torch.nn.functional.ctc_loss(\n            ctc_logits,\n            text,\n            enc_feat_len,\n            text_len,\n            blank=self.blank,\n            reduction=\"none\",\n            zero_infinity=self.zero_infinity,\n        )\n        return loss.mean()\n\n    def recover(self, ctc_text: torch.Tensor, ctc_text_len: torch.Tensor):\n\n        if len(ctc_text.shape) == 3:\n            ctc_text = torch.argmax(ctc_text, dim=-1)\n\n        text = [[] for _ in range(ctc_text.size(0))]\n        for i in range(ctc_text.size(0)):\n            for j in range(ctc_text_len[i]):\n                token = ctc_text[i][j].item()\n                if (j == 0 or token != ctc_text[i][j - 1]) and token != self.blank:\n                    text[i].append(token)\n\n        return text\n</code></pre>"},{"location":"reference/criterion/ctc/#criterion.ctc.CTCLoss.__call__","title":"<code>__call__(ctc_logits, enc_feat_len, text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ctc_logits</code> <code>Tensor</code> <p>(batch, enc_feat_len, vocab) The model output from the CTC layer before the softmax operation.</p> required <code>enc_feat_len</code> <code>Tensor</code> <p>(batch,) The length of encoder feature sequences (&lt;= the length of acoustic feature sequence)</p> required <code>text</code> <code>Tensor</code> <p>(batch, text_len) The grount-truth token index sequences.</p> required <code>text_len</code> <code>Tensor</code> <p>(batch,) The length of each token index sequence.</p> required Source code in <code>speechain/criterion/ctc.py</code> <pre><code>def __call__(\n    self,\n    ctc_logits: torch.Tensor,\n    enc_feat_len: torch.Tensor,\n    text: torch.Tensor,\n    text_len: torch.Tensor,\n):\n    \"\"\"\n\n    Args:\n        ctc_logits: (batch, enc_feat_len, vocab)\n            The model output from the CTC layer before the softmax operation.\n        enc_feat_len: (batch,)\n            The length of encoder feature sequences (&lt;= the length of acoustic feature sequence)\n        text: (batch, text_len)\n            The grount-truth token index sequences.\n        text_len: (batch,)\n            The length of each token index sequence.\n\n    \"\"\"\n    batch, enc_feat_maxlen = ctc_logits.size(0), enc_feat_len.max().item()\n\n    # (batch, enc_feat_len, vocab) -&gt; (enc_feat_len, batch, vocab)\n    ctc_logits = ctc_logits.transpose(0, 1).log_softmax(dim=-1)\n\n    # remove the &lt;sos/eos&gt; at the beginning and end of each sentence\n    text, text_len = text[:, 1:-1], text_len - 2\n    if len(text.shape) == 1:\n        text = text.unsqueeze(-1)\n    text = torch.cat([text[i, : text_len[i]] for i in range(batch)])\n\n    # obtain the ctc loss for each data instances in the given batch\n    loss = torch.nn.functional.ctc_loss(\n        ctc_logits,\n        text,\n        enc_feat_len,\n        text_len,\n        blank=self.blank,\n        reduction=\"none\",\n        zero_infinity=self.zero_infinity,\n    )\n    return loss.mean()\n</code></pre>"},{"location":"reference/criterion/ctc/#criterion.ctc.CTCLoss.criterion_init","title":"<code>criterion_init(blank=0, zero_infinity=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>weight</code> <p>float The weight on the CTC loss in the overall ASR loss. Used to balance the loss terms outside this class.</p> required <code>blank</code> <code>int</code> <p>int = 0 The blank label for CTC modeling. In order to use CuDNN, blank must be set to 0.</p> <code>0</code> <code>zero_infinity</code> <code>bool</code> <p>bool = True Whether to zero infinite losses and the associated gradients when calculating the CTC loss.</p> <code>True</code> Source code in <code>speechain/criterion/ctc.py</code> <pre><code>def criterion_init(self, blank: int = 0, zero_infinity: bool = True):\n    \"\"\"\n\n    Args:\n        weight: float\n            The weight on the CTC loss in the overall ASR loss. Used to balance the loss terms outside this class.\n        blank: int = 0\n            The blank label for CTC modeling. In order to use CuDNN, blank must be set to 0.\n        zero_infinity: bool = True\n            Whether to zero infinite losses and the associated gradients when calculating the CTC loss.\n\n    \"\"\"\n    # arguments checking\n    self.blank = blank\n    self.zero_infinity = zero_infinity\n</code></pre>"},{"location":"reference/criterion/error_rate/","title":"error_rate","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/criterion/error_rate/#criterion.error_rate.ErrorRate","title":"<code>ErrorRate</code>","text":"<p>               Bases: <code>Criterion</code></p> Source code in <code>speechain/criterion/error_rate.py</code> <pre><code>class ErrorRate(Criterion):\n    \"\"\"\"\"\"\n\n    def criterion_init(self, tokenizer: Tokenizer = None, do_aver: bool = False):\n        \"\"\"\n\n        Args:\n            tokenizer: Tokenizer\n            do_aver: bool\n\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.do_aver = do_aver\n\n    def __call__(\n        self,\n        hypo_text: torch.Tensor or List[str] or str,\n        real_text: torch.Tensor or List[str] or str,\n        tokenizer: Tokenizer = None,\n        do_aver: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            hypo_text (torch.Tensor or List[str] or str): the hypothesis text\n            real_text (torch.Tensor or List[str] or str): the reference text\n            tokenizer (Tokenizer): the tokenizer\n            do_aver (bool): whether to average the error rate over the batch\n\n        Returns:\n\n        \"\"\"\n        if tokenizer is None:\n            assert self.tokenizer is not None\n            tokenizer = self.tokenizer\n\n        # make sure that hypo_text is a 2-dim tensor or a list of strings\n        if isinstance(hypo_text, torch.Tensor) and hypo_text.dim() == 1:\n            hypo_text = hypo_text.unsqueeze(0)\n        elif isinstance(hypo_text, str):\n            hypo_text = [hypo_text]\n        # make sure that real_text is a 2-dim tensor or a list of strings\n        if isinstance(real_text, torch.Tensor) and real_text.dim() == 1:\n            real_text = real_text.unsqueeze(0)\n        elif isinstance(real_text, str):\n            real_text = [real_text]\n\n        cer_dist, cer_len, wer_dist, wer_len = [], [], [], []\n        for i in range(len(hypo_text)):\n            # obtain the strings\n            hypo_string = text_preprocess(hypo_text[i], tokenizer)\n            real_string = text_preprocess(real_text[i], tokenizer)\n\n            # calculate CER\n            hypo_chars = hypo_string.replace(\" \", \"\")\n            real_chars = real_string.replace(\" \", \"\")\n            cer_dist.append(editdistance.eval(hypo_chars, real_chars))\n            cer_len.append(len(real_chars))\n\n            # calculate WER\n            # Note that split(\" \") is not equivalent to split() here\n            # because split(\" \") will give an extra '' at the end of the list if the string ends with a \" \"\n            # while split() doesn't\n            hypo_words = hypo_string.split()\n            real_words = real_string.split()\n            wer_dist.append(editdistance.eval(hypo_words, real_words))\n            wer_len.append(len(real_words))\n\n        cer, wer = [], []\n        for i in range(len(cer_dist)):\n            cer.append(cer_dist[i] / cer_len[i])\n            wer.append(wer_dist[i] / wer_len[i])\n        if do_aver:\n            cer = sum(cer) / len(cer)\n            wer = sum(wer) / len(wer)\n\n        return cer, wer\n</code></pre>"},{"location":"reference/criterion/error_rate/#criterion.error_rate.ErrorRate.__call__","title":"<code>__call__(hypo_text, real_text, tokenizer=None, do_aver=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hypo_text</code> <code>Tensor or List[str] or str</code> <p>the hypothesis text</p> required <code>real_text</code> <code>Tensor or List[str] or str</code> <p>the reference text</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>the tokenizer</p> <code>None</code> <code>do_aver</code> <code>bool</code> <p>whether to average the error rate over the batch</p> <code>False</code> <p>Returns:</p> Source code in <code>speechain/criterion/error_rate.py</code> <pre><code>def __call__(\n    self,\n    hypo_text: torch.Tensor or List[str] or str,\n    real_text: torch.Tensor or List[str] or str,\n    tokenizer: Tokenizer = None,\n    do_aver: bool = False,\n):\n    \"\"\"\n\n    Args:\n        hypo_text (torch.Tensor or List[str] or str): the hypothesis text\n        real_text (torch.Tensor or List[str] or str): the reference text\n        tokenizer (Tokenizer): the tokenizer\n        do_aver (bool): whether to average the error rate over the batch\n\n    Returns:\n\n    \"\"\"\n    if tokenizer is None:\n        assert self.tokenizer is not None\n        tokenizer = self.tokenizer\n\n    # make sure that hypo_text is a 2-dim tensor or a list of strings\n    if isinstance(hypo_text, torch.Tensor) and hypo_text.dim() == 1:\n        hypo_text = hypo_text.unsqueeze(0)\n    elif isinstance(hypo_text, str):\n        hypo_text = [hypo_text]\n    # make sure that real_text is a 2-dim tensor or a list of strings\n    if isinstance(real_text, torch.Tensor) and real_text.dim() == 1:\n        real_text = real_text.unsqueeze(0)\n    elif isinstance(real_text, str):\n        real_text = [real_text]\n\n    cer_dist, cer_len, wer_dist, wer_len = [], [], [], []\n    for i in range(len(hypo_text)):\n        # obtain the strings\n        hypo_string = text_preprocess(hypo_text[i], tokenizer)\n        real_string = text_preprocess(real_text[i], tokenizer)\n\n        # calculate CER\n        hypo_chars = hypo_string.replace(\" \", \"\")\n        real_chars = real_string.replace(\" \", \"\")\n        cer_dist.append(editdistance.eval(hypo_chars, real_chars))\n        cer_len.append(len(real_chars))\n\n        # calculate WER\n        # Note that split(\" \") is not equivalent to split() here\n        # because split(\" \") will give an extra '' at the end of the list if the string ends with a \" \"\n        # while split() doesn't\n        hypo_words = hypo_string.split()\n        real_words = real_string.split()\n        wer_dist.append(editdistance.eval(hypo_words, real_words))\n        wer_len.append(len(real_words))\n\n    cer, wer = [], []\n    for i in range(len(cer_dist)):\n        cer.append(cer_dist[i] / cer_len[i])\n        wer.append(wer_dist[i] / wer_len[i])\n    if do_aver:\n        cer = sum(cer) / len(cer)\n        wer = sum(wer) / len(wer)\n\n    return cer, wer\n</code></pre>"},{"location":"reference/criterion/error_rate/#criterion.error_rate.ErrorRate.criterion_init","title":"<code>criterion_init(tokenizer=None, do_aver=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer</p> <code>None</code> <code>do_aver</code> <code>bool</code> <p>bool</p> <code>False</code> Source code in <code>speechain/criterion/error_rate.py</code> <pre><code>def criterion_init(self, tokenizer: Tokenizer = None, do_aver: bool = False):\n    \"\"\"\n\n    Args:\n        tokenizer: Tokenizer\n        do_aver: bool\n\n    \"\"\"\n    self.tokenizer = tokenizer\n    self.do_aver = do_aver\n</code></pre>"},{"location":"reference/criterion/fbeta_score/","title":"fbeta_score","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/criterion/fbeta_score/#criterion.fbeta_score.FBetaScore","title":"<code>FBetaScore</code>","text":"<p>               Bases: <code>Criterion</code></p> Source code in <code>speechain/criterion/fbeta_score.py</code> <pre><code>class FBetaScore(Criterion):\n    \"\"\"\"\"\"\n\n    def criterion_init(self, beta: int = 1):\n        \"\"\"\n\n        Args:\n            beta: float\n\n        \"\"\"\n        self.beta = beta\n\n    def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            pred:\n            tgt:\n            tgt_len:\n\n        Returns:\n\n        \"\"\"\n        # mask generation for the input text length\n        tgt_mask = make_mask_from_len(tgt_len).squeeze()\n        if tgt.is_cuda:\n            tgt_mask = tgt_mask.cuda(tgt.device)\n\n        # mask the pred and tgt, calculate the necessary components for precision and recall\n        pred, tgt = pred.masked_select(tgt_mask), tgt.masked_select(tgt_mask)\n        pred_pos, pred_neg, tgt_pos, tgt_neg = pred == 1, pred == 0, tgt == 1, tgt == 0\n        true_pos, true_neg, false_pos, false_neg = (\n            torch.logical_and(pred_pos, tgt_pos).sum(),\n            torch.logical_and(pred_neg, tgt_neg).sum(),\n            torch.logical_and(pred_pos, tgt_neg).sum(),\n            torch.logical_and(pred_neg, tgt_pos).sum(),\n        )\n\n        # calculate f_beta by precision and recall\n        precision = true_pos / (true_pos + false_pos + 1e-10)\n        recall = true_pos / (true_pos + false_neg + 1e-10)\n        return (\n            (1 + self.beta**2)\n            * (precision * recall)\n            / (self.beta**2 * precision + recall + 1e-10)\n        )\n</code></pre>"},{"location":"reference/criterion/fbeta_score/#criterion.fbeta_score.FBetaScore.__call__","title":"<code>__call__(pred, tgt, tgt_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> required <code>tgt</code> <code>Tensor</code> required <code>tgt_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/criterion/fbeta_score.py</code> <pre><code>def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        pred:\n        tgt:\n        tgt_len:\n\n    Returns:\n\n    \"\"\"\n    # mask generation for the input text length\n    tgt_mask = make_mask_from_len(tgt_len).squeeze()\n    if tgt.is_cuda:\n        tgt_mask = tgt_mask.cuda(tgt.device)\n\n    # mask the pred and tgt, calculate the necessary components for precision and recall\n    pred, tgt = pred.masked_select(tgt_mask), tgt.masked_select(tgt_mask)\n    pred_pos, pred_neg, tgt_pos, tgt_neg = pred == 1, pred == 0, tgt == 1, tgt == 0\n    true_pos, true_neg, false_pos, false_neg = (\n        torch.logical_and(pred_pos, tgt_pos).sum(),\n        torch.logical_and(pred_neg, tgt_neg).sum(),\n        torch.logical_and(pred_pos, tgt_neg).sum(),\n        torch.logical_and(pred_neg, tgt_pos).sum(),\n    )\n\n    # calculate f_beta by precision and recall\n    precision = true_pos / (true_pos + false_pos + 1e-10)\n    recall = true_pos / (true_pos + false_neg + 1e-10)\n    return (\n        (1 + self.beta**2)\n        * (precision * recall)\n        / (self.beta**2 * precision + recall + 1e-10)\n    )\n</code></pre>"},{"location":"reference/criterion/fbeta_score/#criterion.fbeta_score.FBetaScore.criterion_init","title":"<code>criterion_init(beta=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>beta</code> <code>int</code> <p>float</p> <code>1</code> Source code in <code>speechain/criterion/fbeta_score.py</code> <pre><code>def criterion_init(self, beta: int = 1):\n    \"\"\"\n\n    Args:\n        beta: float\n\n    \"\"\"\n    self.beta = beta\n</code></pre>"},{"location":"reference/criterion/least_error/","title":"least_error","text":"<p>Author: Sashi Novitasari Affiliation: NAIST Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/criterion/least_error/#criterion.least_error.LeastError","title":"<code>LeastError</code>","text":"<p>               Bases: <code>Criterion</code></p> Source code in <code>speechain/criterion/least_error.py</code> <pre><code>class LeastError(Criterion):\n    \"\"\"\"\"\"\n\n    def criterion_init(\n        self,\n        loss_type: str = \"L2\",\n        is_normalized: bool = True,\n        update_range: int or float = None,\n    ):\n        \"\"\"\n\n        Args:\n            loss_type: str = 'L2'\n                The type of acoustic feature prediction loss. Should be either 'L1', 'L2', and 'L1+L2'.\n            is_normalized: bool = True\n                Controls whether the sentence normalization is performed for feature loss calculation.\n            update_range: int or float = None\n                The updating range of the dimension of acoustic features for feature loss calculation.\n\n        \"\"\"\n        assert loss_type in [\n            \"L1\",\n            \"L2\",\n            \"L1+L2\",\n        ], f\"You input loss_type must be one of 'L1', 'L2', and 'L1+L2'. But got loss_type={loss_type}.\"\n        if isinstance(update_range, int):\n            assert (\n                update_range &lt; 0\n            ), f\"For setting absolute updating range, you must input a negative integer, but got {update_range}.\"\n        elif isinstance(update_range, float):\n            assert 0 &lt; update_range &lt; 1, (\n                f\"For setting relative updating range, you must input a postive float number between 0 and 1, \"\n                f\"but got {update_range}.\"\n            )\n        elif update_range is not None:\n            raise TypeError\n\n        self.loss_type = loss_type\n        self.l1_loss = torch.nn.L1Loss(reduction=\"none\")\n        self.l2_loss = torch.nn.MSELoss(reduction=\"none\")\n\n        self.is_normalized = is_normalized\n        self.update_range = update_range\n\n    def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            pred: (batch, feat_maxlen, feat_dim) or (batch, feat_maxlen)\n                The model predictions for the feature\n            tgt: (batch, feat_maxlen, feat_dim) or (batch, feat_maxlen)\n                The target feature labels.\n            tgt_len: (batch,)\n                The target label lengths\n\n        \"\"\"\n        # prediction reshape, make sure it is a 3d tensor\n        if len(pred.shape) == 2:\n            pred = pred.unsqueeze(-1)\n        elif len(pred.shape) != 3:\n            raise RuntimeError\n\n        # target reshape, make sure it is a 3d tensor\n        if len(tgt.shape) == 2:\n            tgt = tgt.unsqueeze(-1)\n        elif len(tgt.shape) != 3:\n            raise RuntimeError\n\n        batch_size, feat_maxlen, feat_dim = pred.size(0), pred.size(1), pred.size(2)\n        # updating range restriction, ndim is the dimension selected to be updated\n        if self.update_range is not None:\n            ndim = (\n                feat_dim * self.update_range\n                if self.update_range &gt; 0\n                else -self.update_range\n            )\n            assert ndim &lt; feat_dim\n\n            pred = pred[:, :, :ndim]\n            tgt = tgt[:, :, :ndim]\n\n        # mask production for the target labels\n        tgt_mask = make_mask_from_len(tgt_len, return_3d=False)\n\n        # MSE &amp; MAE loss calculation\n        # (batch_size, feat_maxlen, ndim)\n        if self.loss_type == \"L1\":\n            loss = self.l1_loss(pred, tgt)\n        elif self.loss_type == \"L2\":\n            loss = self.l2_loss(pred, tgt)\n        elif self.loss_type == \"L1+L2\":\n            loss = self.l1_loss(pred, tgt) + self.l2_loss(pred, tgt)\n        else:\n            raise RuntimeError\n\n        # loss reshaping\n        # (batch_size, feat_maxlen, ndim) -&gt; (batch_size, feat_maxlen)\n        loss = loss.mean(dim=-1)\n        # (batch_size, feat_maxlen) -&gt; (batch_size * feat_maxlen)\n        loss = loss.reshape(-1).masked_fill(~tgt_mask.reshape(-1), 0.0)\n        if self.is_normalized:\n            # (batch_size * feat_maxlen) -&gt; (1,)\n            loss = loss.sum() / tgt_mask.sum()\n        else:\n            # (batch_size * feat_maxlen) -&gt; (batch_size, feat_maxlen)\n            loss = loss.reshape(batch_size, feat_maxlen)\n            # (batch_size, feat_maxlen) -&gt; (batch_size,) -&gt; (1,)\n            loss = loss.sum(dim=-1).mean()\n\n        return loss\n\n    def extra_repr(self) -&gt; str:\n        return f\"loss_type={self.loss_type}, is_normalized={self.is_normalized}\"\n</code></pre>"},{"location":"reference/criterion/least_error/#criterion.least_error.LeastError.__call__","title":"<code>__call__(pred, tgt, tgt_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) or (batch, feat_maxlen) The model predictions for the feature</p> required <code>tgt</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) or (batch, feat_maxlen) The target feature labels.</p> required <code>tgt_len</code> <code>Tensor</code> <p>(batch,) The target label lengths</p> required Source code in <code>speechain/criterion/least_error.py</code> <pre><code>def __call__(self, pred: torch.Tensor, tgt: torch.Tensor, tgt_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        pred: (batch, feat_maxlen, feat_dim) or (batch, feat_maxlen)\n            The model predictions for the feature\n        tgt: (batch, feat_maxlen, feat_dim) or (batch, feat_maxlen)\n            The target feature labels.\n        tgt_len: (batch,)\n            The target label lengths\n\n    \"\"\"\n    # prediction reshape, make sure it is a 3d tensor\n    if len(pred.shape) == 2:\n        pred = pred.unsqueeze(-1)\n    elif len(pred.shape) != 3:\n        raise RuntimeError\n\n    # target reshape, make sure it is a 3d tensor\n    if len(tgt.shape) == 2:\n        tgt = tgt.unsqueeze(-1)\n    elif len(tgt.shape) != 3:\n        raise RuntimeError\n\n    batch_size, feat_maxlen, feat_dim = pred.size(0), pred.size(1), pred.size(2)\n    # updating range restriction, ndim is the dimension selected to be updated\n    if self.update_range is not None:\n        ndim = (\n            feat_dim * self.update_range\n            if self.update_range &gt; 0\n            else -self.update_range\n        )\n        assert ndim &lt; feat_dim\n\n        pred = pred[:, :, :ndim]\n        tgt = tgt[:, :, :ndim]\n\n    # mask production for the target labels\n    tgt_mask = make_mask_from_len(tgt_len, return_3d=False)\n\n    # MSE &amp; MAE loss calculation\n    # (batch_size, feat_maxlen, ndim)\n    if self.loss_type == \"L1\":\n        loss = self.l1_loss(pred, tgt)\n    elif self.loss_type == \"L2\":\n        loss = self.l2_loss(pred, tgt)\n    elif self.loss_type == \"L1+L2\":\n        loss = self.l1_loss(pred, tgt) + self.l2_loss(pred, tgt)\n    else:\n        raise RuntimeError\n\n    # loss reshaping\n    # (batch_size, feat_maxlen, ndim) -&gt; (batch_size, feat_maxlen)\n    loss = loss.mean(dim=-1)\n    # (batch_size, feat_maxlen) -&gt; (batch_size * feat_maxlen)\n    loss = loss.reshape(-1).masked_fill(~tgt_mask.reshape(-1), 0.0)\n    if self.is_normalized:\n        # (batch_size * feat_maxlen) -&gt; (1,)\n        loss = loss.sum() / tgt_mask.sum()\n    else:\n        # (batch_size * feat_maxlen) -&gt; (batch_size, feat_maxlen)\n        loss = loss.reshape(batch_size, feat_maxlen)\n        # (batch_size, feat_maxlen) -&gt; (batch_size,) -&gt; (1,)\n        loss = loss.sum(dim=-1).mean()\n\n    return loss\n</code></pre>"},{"location":"reference/criterion/least_error/#criterion.least_error.LeastError.criterion_init","title":"<code>criterion_init(loss_type='L2', is_normalized=True, update_range=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>loss_type</code> <code>str</code> <p>str = 'L2' The type of acoustic feature prediction loss. Should be either 'L1', 'L2', and 'L1+L2'.</p> <code>'L2'</code> <code>is_normalized</code> <code>bool</code> <p>bool = True Controls whether the sentence normalization is performed for feature loss calculation.</p> <code>True</code> <code>update_range</code> <code>int or float</code> <p>int or float = None The updating range of the dimension of acoustic features for feature loss calculation.</p> <code>None</code> Source code in <code>speechain/criterion/least_error.py</code> <pre><code>def criterion_init(\n    self,\n    loss_type: str = \"L2\",\n    is_normalized: bool = True,\n    update_range: int or float = None,\n):\n    \"\"\"\n\n    Args:\n        loss_type: str = 'L2'\n            The type of acoustic feature prediction loss. Should be either 'L1', 'L2', and 'L1+L2'.\n        is_normalized: bool = True\n            Controls whether the sentence normalization is performed for feature loss calculation.\n        update_range: int or float = None\n            The updating range of the dimension of acoustic features for feature loss calculation.\n\n    \"\"\"\n    assert loss_type in [\n        \"L1\",\n        \"L2\",\n        \"L1+L2\",\n    ], f\"You input loss_type must be one of 'L1', 'L2', and 'L1+L2'. But got loss_type={loss_type}.\"\n    if isinstance(update_range, int):\n        assert (\n            update_range &lt; 0\n        ), f\"For setting absolute updating range, you must input a negative integer, but got {update_range}.\"\n    elif isinstance(update_range, float):\n        assert 0 &lt; update_range &lt; 1, (\n            f\"For setting relative updating range, you must input a postive float number between 0 and 1, \"\n            f\"but got {update_range}.\"\n        )\n    elif update_range is not None:\n        raise TypeError\n\n    self.loss_type = loss_type\n    self.l1_loss = torch.nn.L1Loss(reduction=\"none\")\n    self.l2_loss = torch.nn.MSELoss(reduction=\"none\")\n\n    self.is_normalized = is_normalized\n    self.update_range = update_range\n</code></pre>"},{"location":"reference/criterion/perplexity/","title":"perplexity","text":""},{"location":"reference/criterion/perplexity/#criterion.perplexity.Perplexity","title":"<code>Perplexity</code>","text":"<p>               Bases: <code>Criterion</code></p> Source code in <code>speechain/criterion/perplexity.py</code> <pre><code>class Perplexity(Criterion):\n    \"\"\"\"\"\"\n\n    def __call__(\n        self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n    ):\n        \"\"\"\n\n        Args:\n            logits:\n            text:\n            text_len:\n\n        Returns:\n\n        \"\"\"\n        # mask generation for the input text\n        text_mask = make_mask_from_len(text_len - 1, return_3d=False)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        # perplexity calculation\n        log_prob = torch.log_softmax(logits, dim=-1)\n        text_prob = log_prob.gather(-1, text[:, 1:].view(text.size(0), -1, 1)).squeeze(\n            dim=-1\n        )\n        text_prob = text_prob.masked_fill(~text_mask, 0.0)\n        text_ppl = torch.exp(\n            torch.sum(text_prob, dim=-1) * (-1 / (text_len - 1))\n        ).mean()\n\n        return text_ppl\n</code></pre>"},{"location":"reference/criterion/perplexity/#criterion.perplexity.Perplexity.__call__","title":"<code>__call__(logits, text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> required <code>text</code> <code>Tensor</code> required <code>text_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/criterion/perplexity.py</code> <pre><code>def __call__(\n    self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n):\n    \"\"\"\n\n    Args:\n        logits:\n        text:\n        text_len:\n\n    Returns:\n\n    \"\"\"\n    # mask generation for the input text\n    text_mask = make_mask_from_len(text_len - 1, return_3d=False)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    # perplexity calculation\n    log_prob = torch.log_softmax(logits, dim=-1)\n    text_prob = log_prob.gather(-1, text[:, 1:].view(text.size(0), -1, 1)).squeeze(\n        dim=-1\n    )\n    text_prob = text_prob.masked_fill(~text_mask, 0.0)\n    text_ppl = torch.exp(\n        torch.sum(text_prob, dim=-1) * (-1 / (text_len - 1))\n    ).mean()\n\n    return text_ppl\n</code></pre>"},{"location":"reference/dataset/","title":"dataset","text":""},{"location":"reference/dataset/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Dataset</code>, <code>ABC</code></p> <p>Base class for reading and packaging data instances from disk into memory for model training or testing.</p> <p>The Dataset receives indices of selected data instances from a Dataloader object, created by a high-level Iterator. Post-processing steps may need to be executed in the Model object later as the output batches might not be fully processed.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>class Dataset(torch.utils.data.Dataset, ABC):\n    \"\"\"Base class for reading and packaging data instances from disk into memory for\n    model training or testing.\n\n    The Dataset receives indices of selected data instances from a Dataloader object,\n    created by a high-level Iterator. Post-processing steps may need to be executed in\n    the Model object later as the output batches might not be fully processed.\n    \"\"\"\n\n    def __init__(\n        self,\n        main_data: Dict[str, Union[str, List[str]]],\n        data_selection: Optional[List[Union[List[str], str]]] = None,\n        **dataset_conf,\n    ):\n        \"\"\"This initialization function reads the main body of the data instances into\n        the memory. The main body is used to extract individual data instances from the\n        disk to form a batch during model training or testing.\n\n        The hook dataset_init_fn() is executed here after reading the main body of data instances.\n\n        Args:\n            main_data (Dict[str, Union[str, List[str]]]):\n                Dictionary containing data instances used in the Dataset object. Each key-value pair consists of a data\n                variable name and an absolute path to the corresponding 'idx2data' file. The value can be a single path string\n                or a list of multiple path strings.\n\n            data_selection (Optional[List[Union[List[str], str]]]):\n                Strategies for data selection to limit used data instances during iterator initialization. Multiple strategies\n                can be specified in a list. Each data selection strategy should be either a bi-list (non-meta strategy)\n                or tri-list (meta strategy). Refer to the function docstring of data_selection() for more details on\n                the selection strategies.\n\n            **dataset_conf: Additional configuration arguments for custom Dataset initialization.\n\n            data_selection: List[List[str] or str] = None\n                The strategies for data selection during the iterator initialization to shrink the used data instances.\n                Multiple strategies can be specified in a list. Each data selection strategy must be either a bi-list\n                (non-meta strategy) or tri-list (meta strategy).\n                1. non-meta strategy:\n                    The selection strategies that don't involve metadata. These strategies should be given as a bi-list,\n                    i.e., ['selection mode', 'selection number']. 'selection mode' indicates the way to select data\n                    instances while 'selection number' indicates how many data instances to be selected.\n                    Currently, available non-meta selection modes include:\n                        1. 'order': Select the given number of data instances from the beginning.\n                        2. 'rev_order': Select the given number of data instances from the end.\n                        3. 'random': Randomly select the given number of data instances.\n                            Note: You should keep the same random seeds for all the GPU processes in the DDP mode to\n                            ensure that the selected data instances are the same in each process. In this case, please\n                            set the 'same_proc_seed' argument to True in your configuration given to speechain.runner\n                2. meta strategy:\n                    The selection strategies that involves metadata. These strategies should be given as a tri-list,\n                    i.e., ['selection mode', 'selection threshold', 'metadata path']. 'selection mode' indicates the\n                    way to select data instances, 'selection threshold' indicates the metadata threshold to select data\n                    instances, and 'metadata path' indicates where is the metadata used for selection.\n                    Currently, available meta selection modes include:\n                        1. 'min': Select the data instances whose metadata is smaller than the threshold.\n                        2. 'max': Select the data instances whose metadata is larger than the threshold.\n                        3. 'middle': Remove the data instances whose metadata is the largest and smallest.\n        \"\"\"\n        super(Dataset, self).__init__()\n\n        # Validate main_data\n        if not isinstance(main_data, Dict):\n            raise TypeError(\n                f\"Expected main_data to be a Dict, but got {type(main_data)}\"\n            )\n\n        # Load main body of data instances\n        self.main_data, self.data_index = read_idx2data_file_to_dict(main_data)\n\n        # Apply data selection if specified\n        if data_selection is not None:\n            # Ensure data_selection is a list of lists\n            if sum([isinstance(i, List) for i in data_selection]) != len(\n                data_selection\n            ):\n                data_selection = [data_selection]\n\n            # Iterate through each selection strategy\n            for i in data_selection:\n                # Non-meta selection\n                if len(i) == 2:\n                    selection_mode, selection_num, meta_info = i[0], i[1], None\n                    if selection_mode not in [\"random\", \"order\", \"rev_order\"]:\n                        raise ValueError(\n                            f\"For non-meta selection, mode must be 'random', 'order', or 'rev_order'. Got {selection_mode}\"\n                        )\n                # Meta-required selection\n                elif len(i) == 3:\n                    selection_mode, selection_num, meta_info = i[0], i[1], i[2]\n                    if selection_mode not in [\"min\", \"max\", \"middle\", \"group\"]:\n                        raise ValueError(\n                            f\"For meta selection, mode must be 'min', 'max', 'middle', or 'group'. Got {selection_mode}\"\n                        )\n                else:\n                    raise ValueError(\n                        \"Each element of data_selection should be either a 2-element or 3-element list\"\n                    )\n\n                # Validate selection_num\n                if isinstance(selection_num, str):\n                    # Non-numerical contents are turned into a list for identification\n                    if (\n                        not selection_num.isdigit()\n                        and not selection_num.replace(\".\", \"\").isdigit()\n                    ):\n                        assert selection_mode == \"group\"\n                        selection_num = [selection_num]\n\n                valid_selection_num = (\n                    (isinstance(selection_num, float) and 0 &lt; selection_num &lt; 1)\n                    or (\n                        isinstance(selection_num, int)\n                        and -len(self.data_index) &lt; selection_num &lt; 0\n                    )\n                    or isinstance(selection_num, (str, List))\n                )\n                if not valid_selection_num:\n                    raise ValueError(\n                        \"Data selection number should be a float number between 0 and 1, a negative integer, \"\n                        \"a string, or a list of strings\"\n                    )\n\n                if (isinstance(selection_num, (int, float)) and selection_num &lt; 0) and (\n                    -selection_num &gt;= len(self.data_index)\n                ):\n                    raise ValueError(\n                        \"Data selection amount cannot be larger than total number of data instances\"\n                    )\n\n                # Apply the data selection\n                self.data_index = self.data_selection(\n                    self.data_index, selection_mode, selection_num, meta_info\n                )\n\n        # Custom initialization for subclasses\n        self.data_len = self.data_len_register_fn(self.main_data)\n        self.dataset_init_fn(**dataset_conf)\n\n    @staticmethod\n    def data_len_register_fn(\n        main_data: Dict[str, Dict[str, str]],\n    ) -&gt; Union[Dict[str, Union[int, float]], None]:\n        \"\"\"Static hook function that registers default information about the length of\n        each data instance.\n\n        By default, this function does nothing. If you need to decide the data length on-the-fly, override this function\n        with your own implementation.\n\n        Args:\n            main_data (Dict[str, Dict[str, str]]): Dictionary of main data from which length information is derived.\n\n        Returns:\n            Dict[str, Union[int, float]] or None: Dictionary mapping data instances to their lengths, or None if not implemented.\n        \"\"\"\n        return None\n\n    def dataset_init_fn(self, **dataset_conf):\n        \"\"\"Hook function that initializes the custom parts of dataset implementations.\n\n        By default, this function does nothing. If your Dataset subclass has custom parts, override this function\n        with your own implementation.\n\n        Args:\n            **dataset_conf: Arguments for the custom initialization of the Dataset subclass.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def data_selection(\n        data_index: List[str],\n        selection_mode: str,\n        selection_num: Union[float, int, str],\n        meta_info: Union[List[str], str, None] = None,\n    ) -&gt; List:\n        \"\"\"Selects data instances based on the provided selection strategy.\n\n        Returns a new list of selected data instances.\n\n        Args:\n            data_index (List[str]):\n                List of data instance indices prior to data selection.\n            selection_num (Union[float, int, str]):\n                Indicates how many data instances to select, varying with its data type.\n                float: Represents the relative number of data instances to select (between 0 and 1).\n                int: Represents the absolute number of data instances to select. If negative, its absolute value is taken.\n                str: Represents the metadata threshold for data selection. Only 'min' and 'max' modes support this.\n                    You can use the !-suffixed representer `!str` to convert a float or integer number to a string in your .yaml file.\n            selection_mode: str\n                Defines the selection strategy:\n                1. non-meta strategy:\n                   Rule-based selection strategies that do not involve metadata. Includes:\n                     1. 'order': Selects the given number of data instances from the beginning.\n                     2. 'rev_order': Selects the given number of data instances from the end.\n                     3. 'random': Selects the given number of data instances randomly.\n                         Note: You should keep the same random seeds for all the GPU processes in the DDP mode to ensure\n                         that the selected data instances are the same in each process. In this case, please set the\n                         'same_proc_seed' argument to True in your configuration given to speechain.runner.py.\n                2. meta strategy:\n                   Selection strategies that involve metadata. Includes:\n                     1. 'min': Selects the data instances whose metadata is smaller than the threshold.\n                     2. 'max': Selects the data instances whose metadata is larger than the threshold.\n                     3. 'middle': Removes the data instances whose metadata is the largest and smallest.\n            meta_info (Union[List[str], str, None], optional):\n                Path to metadata information used for selection. Defaults to None.\n\n        Returns: List[str]\n            List[str]: A list of selected data instance indices.\n        \"\"\"\n        # Convert data_index to numpy.array for easier manipulation\n        sorted_data = np.array(data_index, dtype=str)\n\n        # Non-metadata selection strategies\n        if meta_info is None:\n            assert isinstance(selection_num, (int, float))\n            # Determine absolute or relative number of instances to select\n            selection_num = int(\n                -selection_num\n                if selection_num &lt; 0\n                else len(sorted_data) * selection_num\n            )\n            # Selection from the beginning\n            if selection_mode == \"order\":\n                sorted_data = sorted_data[:selection_num]\n            # Selection from the end\n            elif selection_mode == \"rev_order\":\n                sorted_data = sorted_data[-selection_num:]\n            # Random selection\n            elif selection_mode == \"random\":\n                sorted_data = sorted_data[\n                    np.random.randint(0, len(sorted_data), selection_num)\n                ]\n\n        # Metadata-based selection strategies\n        else:\n            # Load metadata information\n            meta_info = load_idx2data_file(meta_info)\n            meta_info = np.array([[key, value] for key, value in meta_info.items()])\n            # Initialize sorted indices and metadata values\n            try:\n                meta_sorted_data = meta_info[:, 0][\n                    np.argsort(meta_info[:, 1].astype(float))\n                ]\n                meta_sorted_value = np.sort(meta_info[:, 1].astype(float))\n            # Catch conversion errors\n            except ValueError:\n                meta_sorted_data = meta_info[:, 0]\n                meta_sorted_value = meta_info[:, 1]\n\n            # Only retain data instances present in both datasets\n            retain_flags = np.in1d(meta_sorted_data, sorted_data)\n            meta_sorted_data, meta_sorted_value = (\n                meta_sorted_data[retain_flags],\n                meta_sorted_value[retain_flags],\n            )\n\n            # Process selection based on provided selection_num\n            if isinstance(selection_num, (int, float)):\n                # 0 &lt; selection_num &lt; 1 means that we relatively select data instances by a percentage number\n                # selection_num &lt; 0 means that we absolutely select data instances by the given value\n                selection_num = int(\n                    -selection_num\n                    if selection_num &lt; 0\n                    else len(meta_sorted_data) * selection_num\n                )\n                # 'min' means the instances with the minimal meta values will be selected\n                if selection_mode == \"min\":\n                    removed_sorted_data = meta_sorted_data[selection_num:]\n                # 'max' means the instances with the maximal meta values will be selected\n                elif selection_mode == \"max\":\n                    removed_sorted_data = meta_sorted_data[:-selection_num]\n                # 'middle' means the instances with the minimal and maximal meta values will be excluded\n                elif selection_mode == \"middle\":\n                    removed_sorted_data = np.concatenate(\n                        (\n                            meta_sorted_data[\n                                : int((meta_sorted_data.shape[0] - selection_num) / 2)\n                            ],\n                            meta_sorted_data[\n                                -int((meta_sorted_data.shape[0] - selection_num) / 2) :\n                            ],\n                        ),\n                        axis=0,\n                    )\n                else:\n                    raise RuntimeError(\n                        f\"If selection_num is given in a integer or float number ({selection_num}), \"\n                        f\"selection_mode must be one of ['min', 'max', 'middle']. \"\n                        f\"But got {selection_mode}.\"\n                    )\n\n            # select the data instances by a given threshold\n            elif isinstance(selection_num, str):\n                selection_num = float(selection_num)\n                # 'min' means the instances whose metadata is lower than the given threshold will be selected\n                if selection_mode == \"min\":\n                    removed_sorted_data = meta_sorted_data[\n                        meta_sorted_value &gt; selection_num\n                    ]\n                # 'max' means the instances whose metadata is larger than the given threshold will be selected\n                elif selection_mode == \"max\":\n                    removed_sorted_data = meta_sorted_data[\n                        meta_sorted_value &lt; selection_num\n                    ]\n                # 'middle' is not supported for the threshold selection\n                else:\n                    raise RuntimeError(\n                        f\"If selection_num is given in a string ({selection_num}), selection_mode must \"\n                        f\"be one of ['min', 'max']. But got {selection_mode}.\"\n                    )\n\n            # other strings mean the target groups of data instances\n            elif isinstance(selection_num, List):\n                removed_sorted_data = meta_sorted_data[\n                    [\n                        True if value not in selection_num else False\n                        for value in meta_sorted_value\n                    ]\n                ]\n\n            else:\n                raise ValueError(\"Invalid type for selection_num.\")\n\n            # Remove undesired instances from sorted_data\n            sorted_data = np.setdiff1d(sorted_data, removed_sorted_data)\n\n        # Return selected indices as list\n        return sorted_data.tolist()\n\n    def get_data_index(self) -&gt; List[str]:\n        \"\"\"This function is designed to make users know the data indices of this Dataset\n        object without accessing its members for lower coupling.\n\n        Returns: List[str]\n            The list of the indices of all data instances in this dataset.\n        \"\"\"\n        return self.data_index\n\n    def remove_data_by_index(self, index: str):\n        \"\"\"This function removes the corresponding data instance from this Dataset\n        object by the given index.\n\n        It's mainly used for solving the index mismatch of data instances with the high-\n        level Iterator object.\n        \"\"\"\n        # remove the data instances with the given index from self.main_data\n        for data_type in self.main_data.keys():\n            if index in self.main_data[data_type].keys():\n                self.main_data[data_type].pop(index)\n\n    def __getitem__(self, index: str) -&gt; Dict[str, Any]:\n        \"\"\"This function is the implementation of the one in the parent class\n        `torch.utils.data.Dataset`.  This function is activated by the _Dataloader_\n        object one data instance a time. In each time, this function receives an index\n        and returns the selected data instance.\n\n        The hook `proc_main_data_fn()` is executed here after extracting the main body of the selected data instance.\n\n        Args:\n            index: str\n                The index of the selected data instance given by the Dataloader object.\n\n        Returns: Dict[str, Any]\n            A dictionary containing a data instance.\n        \"\"\"\n        # pre-extract the data instances from self.main_data dictionary by the given index\n        outputs = {key: value[index] for key, value in self.main_data.items()}\n\n        # process the main body of data instances by the hook interface implementation\n        outputs = self.extract_main_data_fn(outputs)\n        return outputs\n\n    def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n        \"\"\"This hook function extracts the selected data instance from the disk to the\n        memory. If you want to implement your own data instance extraction, please\n        override this hook function and give your logic here.\n\n        Args:\n            main_data: Dict[str, str]\n                The dictionary containing necessary information for extracting the data instance from the disk to the\n                memory. For example, the audio file path for the waveform data and the feature file path for the speaker\n                embedding.\n\n        Returns: Dict[str, Any]\n            The dictionary containing the extracted data instance.\n        \"\"\"\n        return main_data\n\n    def collate_fn(self, batch: List[Dict]) -&gt; Dict[str, Any]:\n        \"\"\"This hook function decides how to preprocess a list of extracted data\n        instance dictionary before giving them to the model. This hook function is used\n        as the value of the argument collate_fn for initializing Dataloader object at\n        the beginning of each epoch.\n\n        If you have your own batch collating strategy, we don't recommend you to override this hook but another hook\n        named `collate_main_data_fn()`.\n\n        This function should return the processed batch data in the form of a dictionary.\n\n        Args:\n            batch: List[Dict[str, Any]]\n                The tuple of data instance dictionaries extracted by `extract_main_data_fn()`.\n\n        Returns: Dict[str, Any]\n            The batch dictionary that will be passed to the model.\n        \"\"\"\n        # preprocess List[Dict[str, Any]] to Dict[str, List[Any]]\n        outputs = dict()\n        while len(batch) != 0:\n            ele_dict = batch[0]\n            if ele_dict is not None:\n                for key in ele_dict.keys():\n                    if key not in outputs.keys():\n                        outputs[key] = []\n                    outputs[key].append(ele_dict[key])\n            # remove the redundant data for memory safety\n            batch.remove(ele_dict)\n\n        # postprocess Dict[str, List[Any]] by the hook implementation\n        return self.collate_main_data_fn(outputs)\n\n    def collate_main_data_fn(\n        self, batch_dict: Dict[str, List]\n    ) -&gt; Dict[str, torch.Tensor or List]:\n        \"\"\"This hook function decides how to preprocess a dictionary of the extracted\n        batch of data instances before giving them to the model. The original hook in\n        the base class packages all the elements other than strings of the batch into a\n        `torch.Tensor`. Therefore, the `torch.Tensor` elements must have the same shape.\n        The string elements will remain a list.\n\n        If you have your own batch collating strategy, please override this hook function and give your logic here.\n\n        Args:\n            batch_dict: Dict[str, List]\n                The reshaped dictionary of the extracted batch. In each key-value item, the key is the name of the data\n                variable that will be passed to the model and the value is the list of unorganized data from all the\n                elements in the batch.\n\n        Returns: Dict[str, torch.Tensor or List]\n            The dictionary containing the collated batch of data instances.\n        \"\"\"\n        # extract the main body of data instances by the hook interface implementation\n        for key in batch_dict.keys():\n            # List[torch.Tensor] -&gt; torch.Tensor\n            if isinstance(batch_dict[key][0], torch.Tensor):\n                batch_dict[key] = torch.stack([ele for ele in batch_dict[key]])\n            # List[numpy.ndarry] -&gt; List[torch.Tensor] -&gt; torch.Tensor\n            elif isinstance(batch_dict[key][0], np.ndarray):\n                batch_dict[key] = torch.stack(\n                    [torch.tensor(ele) for ele in batch_dict[key]]\n                )\n            # List[int] -&gt; torch.LongTensor\n            elif isinstance(batch_dict[key][0], int):\n                batch_dict[key] = torch.LongTensor(batch_dict[key])\n            # List[float] -&gt; torch.FloatTensor\n            elif isinstance(batch_dict[key][0], float):\n                batch_dict[key] = torch.FloatTensor(batch_dict[key])\n            # List[str] remains List[str]\n            elif not isinstance(batch_dict[key][0], str):\n                raise RuntimeError\n\n        return batch_dict\n\n    def __repr__(self):\n        return self.__class__.__name__\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>This function is the implementation of the one in the parent class <code>torch.utils.data.Dataset</code>.  This function is activated by the Dataloader object one data instance a time. In each time, this function receives an index and returns the selected data instance.</p> <p>The hook <code>proc_main_data_fn()</code> is executed here after extracting the main body of the selected data instance.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>str</code> <p>str The index of the selected data instance given by the Dataloader object.</p> required <p>Dict[str, Any]</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing a data instance.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def __getitem__(self, index: str) -&gt; Dict[str, Any]:\n    \"\"\"This function is the implementation of the one in the parent class\n    `torch.utils.data.Dataset`.  This function is activated by the _Dataloader_\n    object one data instance a time. In each time, this function receives an index\n    and returns the selected data instance.\n\n    The hook `proc_main_data_fn()` is executed here after extracting the main body of the selected data instance.\n\n    Args:\n        index: str\n            The index of the selected data instance given by the Dataloader object.\n\n    Returns: Dict[str, Any]\n        A dictionary containing a data instance.\n    \"\"\"\n    # pre-extract the data instances from self.main_data dictionary by the given index\n    outputs = {key: value[index] for key, value in self.main_data.items()}\n\n    # process the main body of data instances by the hook interface implementation\n    outputs = self.extract_main_data_fn(outputs)\n    return outputs\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.__init__","title":"<code>__init__(main_data, data_selection=None, **dataset_conf)</code>","text":"<p>This initialization function reads the main body of the data instances into the memory. The main body is used to extract individual data instances from the disk to form a batch during model training or testing.</p> <p>The hook dataset_init_fn() is executed here after reading the main body of data instances.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict[str, Union[str, List[str]]]</code> <p>Dictionary containing data instances used in the Dataset object. Each key-value pair consists of a data variable name and an absolute path to the corresponding 'idx2data' file. The value can be a single path string or a list of multiple path strings.</p> required <code>data_selection</code> <code>Optional[List[Union[List[str], str]]]</code> <p>Strategies for data selection to limit used data instances during iterator initialization. Multiple strategies can be specified in a list. Each data selection strategy should be either a bi-list (non-meta strategy) or tri-list (meta strategy). Refer to the function docstring of data_selection() for more details on the selection strategies.</p> <code>None</code> <code>**dataset_conf</code> <p>Additional configuration arguments for custom Dataset initialization.</p> <code>{}</code> <code>data_selection</code> <code>Optional[List[Union[List[str], str]]]</code> <p>List[List[str] or str] = None The strategies for data selection during the iterator initialization to shrink the used data instances. Multiple strategies can be specified in a list. Each data selection strategy must be either a bi-list (non-meta strategy) or tri-list (meta strategy). 1. non-meta strategy:     The selection strategies that don't involve metadata. These strategies should be given as a bi-list,     i.e., ['selection mode', 'selection number']. 'selection mode' indicates the way to select data     instances while 'selection number' indicates how many data instances to be selected.     Currently, available non-meta selection modes include:         1. 'order': Select the given number of data instances from the beginning.         2. 'rev_order': Select the given number of data instances from the end.         3. 'random': Randomly select the given number of data instances.             Note: You should keep the same random seeds for all the GPU processes in the DDP mode to             ensure that the selected data instances are the same in each process. In this case, please             set the 'same_proc_seed' argument to True in your configuration given to speechain.runner 2. meta strategy:     The selection strategies that involves metadata. These strategies should be given as a tri-list,     i.e., ['selection mode', 'selection threshold', 'metadata path']. 'selection mode' indicates the     way to select data instances, 'selection threshold' indicates the metadata threshold to select data     instances, and 'metadata path' indicates where is the metadata used for selection.     Currently, available meta selection modes include:         1. 'min': Select the data instances whose metadata is smaller than the threshold.         2. 'max': Select the data instances whose metadata is larger than the threshold.         3. 'middle': Remove the data instances whose metadata is the largest and smallest.</p> <code>None</code> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def __init__(\n    self,\n    main_data: Dict[str, Union[str, List[str]]],\n    data_selection: Optional[List[Union[List[str], str]]] = None,\n    **dataset_conf,\n):\n    \"\"\"This initialization function reads the main body of the data instances into\n    the memory. The main body is used to extract individual data instances from the\n    disk to form a batch during model training or testing.\n\n    The hook dataset_init_fn() is executed here after reading the main body of data instances.\n\n    Args:\n        main_data (Dict[str, Union[str, List[str]]]):\n            Dictionary containing data instances used in the Dataset object. Each key-value pair consists of a data\n            variable name and an absolute path to the corresponding 'idx2data' file. The value can be a single path string\n            or a list of multiple path strings.\n\n        data_selection (Optional[List[Union[List[str], str]]]):\n            Strategies for data selection to limit used data instances during iterator initialization. Multiple strategies\n            can be specified in a list. Each data selection strategy should be either a bi-list (non-meta strategy)\n            or tri-list (meta strategy). Refer to the function docstring of data_selection() for more details on\n            the selection strategies.\n\n        **dataset_conf: Additional configuration arguments for custom Dataset initialization.\n\n        data_selection: List[List[str] or str] = None\n            The strategies for data selection during the iterator initialization to shrink the used data instances.\n            Multiple strategies can be specified in a list. Each data selection strategy must be either a bi-list\n            (non-meta strategy) or tri-list (meta strategy).\n            1. non-meta strategy:\n                The selection strategies that don't involve metadata. These strategies should be given as a bi-list,\n                i.e., ['selection mode', 'selection number']. 'selection mode' indicates the way to select data\n                instances while 'selection number' indicates how many data instances to be selected.\n                Currently, available non-meta selection modes include:\n                    1. 'order': Select the given number of data instances from the beginning.\n                    2. 'rev_order': Select the given number of data instances from the end.\n                    3. 'random': Randomly select the given number of data instances.\n                        Note: You should keep the same random seeds for all the GPU processes in the DDP mode to\n                        ensure that the selected data instances are the same in each process. In this case, please\n                        set the 'same_proc_seed' argument to True in your configuration given to speechain.runner\n            2. meta strategy:\n                The selection strategies that involves metadata. These strategies should be given as a tri-list,\n                i.e., ['selection mode', 'selection threshold', 'metadata path']. 'selection mode' indicates the\n                way to select data instances, 'selection threshold' indicates the metadata threshold to select data\n                instances, and 'metadata path' indicates where is the metadata used for selection.\n                Currently, available meta selection modes include:\n                    1. 'min': Select the data instances whose metadata is smaller than the threshold.\n                    2. 'max': Select the data instances whose metadata is larger than the threshold.\n                    3. 'middle': Remove the data instances whose metadata is the largest and smallest.\n    \"\"\"\n    super(Dataset, self).__init__()\n\n    # Validate main_data\n    if not isinstance(main_data, Dict):\n        raise TypeError(\n            f\"Expected main_data to be a Dict, but got {type(main_data)}\"\n        )\n\n    # Load main body of data instances\n    self.main_data, self.data_index = read_idx2data_file_to_dict(main_data)\n\n    # Apply data selection if specified\n    if data_selection is not None:\n        # Ensure data_selection is a list of lists\n        if sum([isinstance(i, List) for i in data_selection]) != len(\n            data_selection\n        ):\n            data_selection = [data_selection]\n\n        # Iterate through each selection strategy\n        for i in data_selection:\n            # Non-meta selection\n            if len(i) == 2:\n                selection_mode, selection_num, meta_info = i[0], i[1], None\n                if selection_mode not in [\"random\", \"order\", \"rev_order\"]:\n                    raise ValueError(\n                        f\"For non-meta selection, mode must be 'random', 'order', or 'rev_order'. Got {selection_mode}\"\n                    )\n            # Meta-required selection\n            elif len(i) == 3:\n                selection_mode, selection_num, meta_info = i[0], i[1], i[2]\n                if selection_mode not in [\"min\", \"max\", \"middle\", \"group\"]:\n                    raise ValueError(\n                        f\"For meta selection, mode must be 'min', 'max', 'middle', or 'group'. Got {selection_mode}\"\n                    )\n            else:\n                raise ValueError(\n                    \"Each element of data_selection should be either a 2-element or 3-element list\"\n                )\n\n            # Validate selection_num\n            if isinstance(selection_num, str):\n                # Non-numerical contents are turned into a list for identification\n                if (\n                    not selection_num.isdigit()\n                    and not selection_num.replace(\".\", \"\").isdigit()\n                ):\n                    assert selection_mode == \"group\"\n                    selection_num = [selection_num]\n\n            valid_selection_num = (\n                (isinstance(selection_num, float) and 0 &lt; selection_num &lt; 1)\n                or (\n                    isinstance(selection_num, int)\n                    and -len(self.data_index) &lt; selection_num &lt; 0\n                )\n                or isinstance(selection_num, (str, List))\n            )\n            if not valid_selection_num:\n                raise ValueError(\n                    \"Data selection number should be a float number between 0 and 1, a negative integer, \"\n                    \"a string, or a list of strings\"\n                )\n\n            if (isinstance(selection_num, (int, float)) and selection_num &lt; 0) and (\n                -selection_num &gt;= len(self.data_index)\n            ):\n                raise ValueError(\n                    \"Data selection amount cannot be larger than total number of data instances\"\n                )\n\n            # Apply the data selection\n            self.data_index = self.data_selection(\n                self.data_index, selection_mode, selection_num, meta_info\n            )\n\n    # Custom initialization for subclasses\n    self.data_len = self.data_len_register_fn(self.main_data)\n    self.dataset_init_fn(**dataset_conf)\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.collate_fn","title":"<code>collate_fn(batch)</code>","text":"<p>This hook function decides how to preprocess a list of extracted data instance dictionary before giving them to the model. This hook function is used as the value of the argument collate_fn for initializing Dataloader object at the beginning of each epoch.</p> <p>If you have your own batch collating strategy, we don't recommend you to override this hook but another hook named <code>collate_main_data_fn()</code>.</p> <p>This function should return the processed batch data in the form of a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict]</code> <p>List[Dict[str, Any]] The tuple of data instance dictionaries extracted by <code>extract_main_data_fn()</code>.</p> required <p>Dict[str, Any]</p> Type Description <code>Dict[str, Any]</code> <p>The batch dictionary that will be passed to the model.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def collate_fn(self, batch: List[Dict]) -&gt; Dict[str, Any]:\n    \"\"\"This hook function decides how to preprocess a list of extracted data\n    instance dictionary before giving them to the model. This hook function is used\n    as the value of the argument collate_fn for initializing Dataloader object at\n    the beginning of each epoch.\n\n    If you have your own batch collating strategy, we don't recommend you to override this hook but another hook\n    named `collate_main_data_fn()`.\n\n    This function should return the processed batch data in the form of a dictionary.\n\n    Args:\n        batch: List[Dict[str, Any]]\n            The tuple of data instance dictionaries extracted by `extract_main_data_fn()`.\n\n    Returns: Dict[str, Any]\n        The batch dictionary that will be passed to the model.\n    \"\"\"\n    # preprocess List[Dict[str, Any]] to Dict[str, List[Any]]\n    outputs = dict()\n    while len(batch) != 0:\n        ele_dict = batch[0]\n        if ele_dict is not None:\n            for key in ele_dict.keys():\n                if key not in outputs.keys():\n                    outputs[key] = []\n                outputs[key].append(ele_dict[key])\n        # remove the redundant data for memory safety\n        batch.remove(ele_dict)\n\n    # postprocess Dict[str, List[Any]] by the hook implementation\n    return self.collate_main_data_fn(outputs)\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.collate_main_data_fn","title":"<code>collate_main_data_fn(batch_dict)</code>","text":"<p>This hook function decides how to preprocess a dictionary of the extracted batch of data instances before giving them to the model. The original hook in the base class packages all the elements other than strings of the batch into a <code>torch.Tensor</code>. Therefore, the <code>torch.Tensor</code> elements must have the same shape. The string elements will remain a list.</p> <p>If you have your own batch collating strategy, please override this hook function and give your logic here.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dict</code> <code>Dict[str, List]</code> <p>Dict[str, List] The reshaped dictionary of the extracted batch. In each key-value item, the key is the name of the data variable that will be passed to the model and the value is the list of unorganized data from all the elements in the batch.</p> required <p>Dict[str, torch.Tensor or List]</p> Type Description <code>Dict[str, Tensor or List]</code> <p>The dictionary containing the collated batch of data instances.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def collate_main_data_fn(\n    self, batch_dict: Dict[str, List]\n) -&gt; Dict[str, torch.Tensor or List]:\n    \"\"\"This hook function decides how to preprocess a dictionary of the extracted\n    batch of data instances before giving them to the model. The original hook in\n    the base class packages all the elements other than strings of the batch into a\n    `torch.Tensor`. Therefore, the `torch.Tensor` elements must have the same shape.\n    The string elements will remain a list.\n\n    If you have your own batch collating strategy, please override this hook function and give your logic here.\n\n    Args:\n        batch_dict: Dict[str, List]\n            The reshaped dictionary of the extracted batch. In each key-value item, the key is the name of the data\n            variable that will be passed to the model and the value is the list of unorganized data from all the\n            elements in the batch.\n\n    Returns: Dict[str, torch.Tensor or List]\n        The dictionary containing the collated batch of data instances.\n    \"\"\"\n    # extract the main body of data instances by the hook interface implementation\n    for key in batch_dict.keys():\n        # List[torch.Tensor] -&gt; torch.Tensor\n        if isinstance(batch_dict[key][0], torch.Tensor):\n            batch_dict[key] = torch.stack([ele for ele in batch_dict[key]])\n        # List[numpy.ndarry] -&gt; List[torch.Tensor] -&gt; torch.Tensor\n        elif isinstance(batch_dict[key][0], np.ndarray):\n            batch_dict[key] = torch.stack(\n                [torch.tensor(ele) for ele in batch_dict[key]]\n            )\n        # List[int] -&gt; torch.LongTensor\n        elif isinstance(batch_dict[key][0], int):\n            batch_dict[key] = torch.LongTensor(batch_dict[key])\n        # List[float] -&gt; torch.FloatTensor\n        elif isinstance(batch_dict[key][0], float):\n            batch_dict[key] = torch.FloatTensor(batch_dict[key])\n        # List[str] remains List[str]\n        elif not isinstance(batch_dict[key][0], str):\n            raise RuntimeError\n\n    return batch_dict\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.data_len_register_fn","title":"<code>data_len_register_fn(main_data)</code>  <code>staticmethod</code>","text":"<p>Static hook function that registers default information about the length of each data instance.</p> <p>By default, this function does nothing. If you need to decide the data length on-the-fly, override this function with your own implementation.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict[str, Dict[str, str]]</code> <p>Dictionary of main data from which length information is derived.</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Union[int, float]], None]</code> <p>Dict[str, Union[int, float]] or None: Dictionary mapping data instances to their lengths, or None if not implemented.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>@staticmethod\ndef data_len_register_fn(\n    main_data: Dict[str, Dict[str, str]],\n) -&gt; Union[Dict[str, Union[int, float]], None]:\n    \"\"\"Static hook function that registers default information about the length of\n    each data instance.\n\n    By default, this function does nothing. If you need to decide the data length on-the-fly, override this function\n    with your own implementation.\n\n    Args:\n        main_data (Dict[str, Dict[str, str]]): Dictionary of main data from which length information is derived.\n\n    Returns:\n        Dict[str, Union[int, float]] or None: Dictionary mapping data instances to their lengths, or None if not implemented.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.data_selection","title":"<code>data_selection(data_index, selection_mode, selection_num, meta_info=None)</code>  <code>staticmethod</code>","text":"<p>Selects data instances based on the provided selection strategy.</p> <p>Returns a new list of selected data instances.</p> <p>Parameters:</p> Name Type Description Default <code>data_index</code> <code>List[str]</code> <p>List of data instance indices prior to data selection.</p> required <code>selection_num</code> <code>Union[float, int, str]</code> <p>Indicates how many data instances to select, varying with its data type. float: Represents the relative number of data instances to select (between 0 and 1). int: Represents the absolute number of data instances to select. If negative, its absolute value is taken. str: Represents the metadata threshold for data selection. Only 'min' and 'max' modes support this.     You can use the !-suffixed representer <code>!str</code> to convert a float or integer number to a string in your .yaml file.</p> required <code>selection_mode</code> <code>str</code> <p>str Defines the selection strategy: 1. non-meta strategy:    Rule-based selection strategies that do not involve metadata. Includes:      1. 'order': Selects the given number of data instances from the beginning.      2. 'rev_order': Selects the given number of data instances from the end.      3. 'random': Selects the given number of data instances randomly.          Note: You should keep the same random seeds for all the GPU processes in the DDP mode to ensure          that the selected data instances are the same in each process. In this case, please set the          'same_proc_seed' argument to True in your configuration given to speechain.runner.py. 2. meta strategy:    Selection strategies that involve metadata. Includes:      1. 'min': Selects the data instances whose metadata is smaller than the threshold.      2. 'max': Selects the data instances whose metadata is larger than the threshold.      3. 'middle': Removes the data instances whose metadata is the largest and smallest.</p> required <code>meta_info</code> <code>Union[List[str], str, None]</code> <p>Path to metadata information used for selection. Defaults to None.</p> <code>None</code> <p>List[str]</p> Type Description <code>List</code> <p>List[str]: A list of selected data instance indices.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>@staticmethod\ndef data_selection(\n    data_index: List[str],\n    selection_mode: str,\n    selection_num: Union[float, int, str],\n    meta_info: Union[List[str], str, None] = None,\n) -&gt; List:\n    \"\"\"Selects data instances based on the provided selection strategy.\n\n    Returns a new list of selected data instances.\n\n    Args:\n        data_index (List[str]):\n            List of data instance indices prior to data selection.\n        selection_num (Union[float, int, str]):\n            Indicates how many data instances to select, varying with its data type.\n            float: Represents the relative number of data instances to select (between 0 and 1).\n            int: Represents the absolute number of data instances to select. If negative, its absolute value is taken.\n            str: Represents the metadata threshold for data selection. Only 'min' and 'max' modes support this.\n                You can use the !-suffixed representer `!str` to convert a float or integer number to a string in your .yaml file.\n        selection_mode: str\n            Defines the selection strategy:\n            1. non-meta strategy:\n               Rule-based selection strategies that do not involve metadata. Includes:\n                 1. 'order': Selects the given number of data instances from the beginning.\n                 2. 'rev_order': Selects the given number of data instances from the end.\n                 3. 'random': Selects the given number of data instances randomly.\n                     Note: You should keep the same random seeds for all the GPU processes in the DDP mode to ensure\n                     that the selected data instances are the same in each process. In this case, please set the\n                     'same_proc_seed' argument to True in your configuration given to speechain.runner.py.\n            2. meta strategy:\n               Selection strategies that involve metadata. Includes:\n                 1. 'min': Selects the data instances whose metadata is smaller than the threshold.\n                 2. 'max': Selects the data instances whose metadata is larger than the threshold.\n                 3. 'middle': Removes the data instances whose metadata is the largest and smallest.\n        meta_info (Union[List[str], str, None], optional):\n            Path to metadata information used for selection. Defaults to None.\n\n    Returns: List[str]\n        List[str]: A list of selected data instance indices.\n    \"\"\"\n    # Convert data_index to numpy.array for easier manipulation\n    sorted_data = np.array(data_index, dtype=str)\n\n    # Non-metadata selection strategies\n    if meta_info is None:\n        assert isinstance(selection_num, (int, float))\n        # Determine absolute or relative number of instances to select\n        selection_num = int(\n            -selection_num\n            if selection_num &lt; 0\n            else len(sorted_data) * selection_num\n        )\n        # Selection from the beginning\n        if selection_mode == \"order\":\n            sorted_data = sorted_data[:selection_num]\n        # Selection from the end\n        elif selection_mode == \"rev_order\":\n            sorted_data = sorted_data[-selection_num:]\n        # Random selection\n        elif selection_mode == \"random\":\n            sorted_data = sorted_data[\n                np.random.randint(0, len(sorted_data), selection_num)\n            ]\n\n    # Metadata-based selection strategies\n    else:\n        # Load metadata information\n        meta_info = load_idx2data_file(meta_info)\n        meta_info = np.array([[key, value] for key, value in meta_info.items()])\n        # Initialize sorted indices and metadata values\n        try:\n            meta_sorted_data = meta_info[:, 0][\n                np.argsort(meta_info[:, 1].astype(float))\n            ]\n            meta_sorted_value = np.sort(meta_info[:, 1].astype(float))\n        # Catch conversion errors\n        except ValueError:\n            meta_sorted_data = meta_info[:, 0]\n            meta_sorted_value = meta_info[:, 1]\n\n        # Only retain data instances present in both datasets\n        retain_flags = np.in1d(meta_sorted_data, sorted_data)\n        meta_sorted_data, meta_sorted_value = (\n            meta_sorted_data[retain_flags],\n            meta_sorted_value[retain_flags],\n        )\n\n        # Process selection based on provided selection_num\n        if isinstance(selection_num, (int, float)):\n            # 0 &lt; selection_num &lt; 1 means that we relatively select data instances by a percentage number\n            # selection_num &lt; 0 means that we absolutely select data instances by the given value\n            selection_num = int(\n                -selection_num\n                if selection_num &lt; 0\n                else len(meta_sorted_data) * selection_num\n            )\n            # 'min' means the instances with the minimal meta values will be selected\n            if selection_mode == \"min\":\n                removed_sorted_data = meta_sorted_data[selection_num:]\n            # 'max' means the instances with the maximal meta values will be selected\n            elif selection_mode == \"max\":\n                removed_sorted_data = meta_sorted_data[:-selection_num]\n            # 'middle' means the instances with the minimal and maximal meta values will be excluded\n            elif selection_mode == \"middle\":\n                removed_sorted_data = np.concatenate(\n                    (\n                        meta_sorted_data[\n                            : int((meta_sorted_data.shape[0] - selection_num) / 2)\n                        ],\n                        meta_sorted_data[\n                            -int((meta_sorted_data.shape[0] - selection_num) / 2) :\n                        ],\n                    ),\n                    axis=0,\n                )\n            else:\n                raise RuntimeError(\n                    f\"If selection_num is given in a integer or float number ({selection_num}), \"\n                    f\"selection_mode must be one of ['min', 'max', 'middle']. \"\n                    f\"But got {selection_mode}.\"\n                )\n\n        # select the data instances by a given threshold\n        elif isinstance(selection_num, str):\n            selection_num = float(selection_num)\n            # 'min' means the instances whose metadata is lower than the given threshold will be selected\n            if selection_mode == \"min\":\n                removed_sorted_data = meta_sorted_data[\n                    meta_sorted_value &gt; selection_num\n                ]\n            # 'max' means the instances whose metadata is larger than the given threshold will be selected\n            elif selection_mode == \"max\":\n                removed_sorted_data = meta_sorted_data[\n                    meta_sorted_value &lt; selection_num\n                ]\n            # 'middle' is not supported for the threshold selection\n            else:\n                raise RuntimeError(\n                    f\"If selection_num is given in a string ({selection_num}), selection_mode must \"\n                    f\"be one of ['min', 'max']. But got {selection_mode}.\"\n                )\n\n        # other strings mean the target groups of data instances\n        elif isinstance(selection_num, List):\n            removed_sorted_data = meta_sorted_data[\n                [\n                    True if value not in selection_num else False\n                    for value in meta_sorted_value\n                ]\n            ]\n\n        else:\n            raise ValueError(\"Invalid type for selection_num.\")\n\n        # Remove undesired instances from sorted_data\n        sorted_data = np.setdiff1d(sorted_data, removed_sorted_data)\n\n    # Return selected indices as list\n    return sorted_data.tolist()\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.dataset_init_fn","title":"<code>dataset_init_fn(**dataset_conf)</code>","text":"<p>Hook function that initializes the custom parts of dataset implementations.</p> <p>By default, this function does nothing. If your Dataset subclass has custom parts, override this function with your own implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**dataset_conf</code> <p>Arguments for the custom initialization of the Dataset subclass.</p> <code>{}</code> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def dataset_init_fn(self, **dataset_conf):\n    \"\"\"Hook function that initializes the custom parts of dataset implementations.\n\n    By default, this function does nothing. If your Dataset subclass has custom parts, override this function\n    with your own implementation.\n\n    Args:\n        **dataset_conf: Arguments for the custom initialization of the Dataset subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.extract_main_data_fn","title":"<code>extract_main_data_fn(main_data)</code>","text":"<p>This hook function extracts the selected data instance from the disk to the memory. If you want to implement your own data instance extraction, please override this hook function and give your logic here.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict</code> <p>Dict[str, str] The dictionary containing necessary information for extracting the data instance from the disk to the memory. For example, the audio file path for the waveform data and the feature file path for the speaker embedding.</p> required <p>Dict[str, Any]</p> Type Description <code>Dict[str, Any] or None</code> <p>The dictionary containing the extracted data instance.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n    \"\"\"This hook function extracts the selected data instance from the disk to the\n    memory. If you want to implement your own data instance extraction, please\n    override this hook function and give your logic here.\n\n    Args:\n        main_data: Dict[str, str]\n            The dictionary containing necessary information for extracting the data instance from the disk to the\n            memory. For example, the audio file path for the waveform data and the feature file path for the speaker\n            embedding.\n\n    Returns: Dict[str, Any]\n        The dictionary containing the extracted data instance.\n    \"\"\"\n    return main_data\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.get_data_index","title":"<code>get_data_index()</code>","text":"<p>This function is designed to make users know the data indices of this Dataset object without accessing its members for lower coupling.</p> <p>List[str]</p> Type Description <code>List[str]</code> <p>The list of the indices of all data instances in this dataset.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def get_data_index(self) -&gt; List[str]:\n    \"\"\"This function is designed to make users know the data indices of this Dataset\n    object without accessing its members for lower coupling.\n\n    Returns: List[str]\n        The list of the indices of all data instances in this dataset.\n    \"\"\"\n    return self.data_index\n</code></pre>"},{"location":"reference/dataset/abs/#dataset.abs.Dataset.remove_data_by_index","title":"<code>remove_data_by_index(index)</code>","text":"<p>This function removes the corresponding data instance from this Dataset object by the given index.</p> <p>It's mainly used for solving the index mismatch of data instances with the high- level Iterator object.</p> Source code in <code>speechain/dataset/abs.py</code> <pre><code>def remove_data_by_index(self, index: str):\n    \"\"\"This function removes the corresponding data instance from this Dataset\n    object by the given index.\n\n    It's mainly used for solving the index mismatch of data instances with the high-\n    level Iterator object.\n    \"\"\"\n    # remove the data instances with the given index from self.main_data\n    for data_type in self.main_data.keys():\n        if index in self.main_data[data_type].keys():\n            self.main_data[data_type].pop(index)\n</code></pre>"},{"location":"reference/dataset/speech_text/","title":"speech_text","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.RandomSpkFeatDataset","title":"<code>RandomSpkFeatDataset</code>","text":"<p>               Bases: <code>SpeechTextDataset</code></p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>class RandomSpkFeatDataset(SpeechTextDataset):\n    \"\"\"\"\"\"\n\n    def dataset_init_fn(\n        self,\n        spk_feat: List[str] or str = None,\n        use_aver_feat: bool = True,\n        mixup_number: int = 1,\n        **super_conf,\n    ):\n\n        super(RandomSpkFeatDataset, self).dataset_init_fn(**super_conf)\n\n        assert (\n            spk_feat is not None\n        ), f\"spk_feat cannot be None. Please specify it in {self.__class__.__name__}!\"\n        assert (\n            isinstance(mixup_number, int) and mixup_number &gt;= 1\n        ), f\"mixup_number must be a positive integer, but got {mixup_number}!\"\n        self.mixup_number = mixup_number\n\n        # List[str] or str -&gt; List[str]\n        if not isinstance(spk_feat, List):\n            spk_feat = [spk_feat]\n        metadata_dir = [os.path.dirname(s_f) for s_f in spk_feat]\n        spk_emb_model = [\n            os.path.basename(s_f).split(\"2\")[-1].split(\"_\")[0] for s_f in spk_feat\n        ]\n\n        # register the list of available speaker IDs\n        self.idx2spk = load_idx2data_file(\n            [os.path.join(m_d, \"idx2spk\") for m_d in metadata_dir]\n        )\n        self.spk_ids_list = sorted(set(self.idx2spk.values()))\n        self.spk_num = len(self.spk_ids_list)\n        self.spk2freq = {spk_id: 0 for spk_id in self.spk_ids_list}\n\n        # speaker embedding file reading, List[str] -&gt; Dict[str, str]\n        idx2spk_feat = load_idx2data_file(spk_feat)\n        self.spk2spk_feat = {\n            spk_id: {\n                spk_feat_id: idx2spk_feat[spk_feat_id]\n                for spk_feat_id in idx2spk_feat.keys()\n                if self.idx2spk[spk_feat_id] == spk_id\n            }\n            for spk_id in self.spk_ids_list\n        }\n        if use_aver_feat:\n            self.spk2aver_spk_feat = load_idx2data_file(\n                [\n                    os.path.join(m_d, f\"spk2aver_{s_e_m}_spk_feat\")\n                    for m_d, s_e_m in zip(metadata_dir, spk_emb_model)\n                ]\n            )\n\n    def extract_main_data_fn(self, main_data: Dict[str, str]) -&gt; Dict[str, Any] or None:\n        \"\"\"This hook function randomly pick up a speaker embedding feature from the\n        given spk_feat file as the reference.\n\n        The randomness is controlled by the `seed` you give in the exp_cfg.\n        \"\"\"\n        assert \"spk_ids\" not in main_data.keys(), (\n            f\"Please don't give spk_ids to main_data of {self.__class__.__name__}. \"\n            f\"This Dataset is used to evaluate open-set multi-speaker TTS that uses external speaker embedding.\"\n        )\n        assert \"spk_feat\" not in main_data.keys(), (\n            f\"Please don't give spk_feat to main_data of {self.__class__.__name__}. \"\n            f\"Your spk_feat should be given outside the main_data.\"\n        )\n\n        # process 'feat' and 'text' by the parent class\n        main_data = super(RandomSpkFeatDataset, self).extract_main_data_fn(main_data)\n        # None means empty batch received from the parent class\n        if main_data is None:\n            return main_data\n\n        chosen_spk_feat_ids, chosen_spk_ids = [], []\n        while len(chosen_spk_feat_ids) &lt; self.mixup_number:\n            random_spk_id, self.spk2freq = get_min_indices_by_freq(\n                self.spk2freq,\n                freq_weights=(\n                    len(main_data[\"text\"]) if \"text\" in main_data.keys() else None\n                ),\n            )\n            random_spk_id = random_spk_id[0]\n\n            # randomly pick up a speaker embedding feature vector\n            spk_feat = self.spk2spk_feat[random_spk_id]\n            spk_feat_id_list = list(spk_feat.keys())\n            random_spk_feat_id = spk_feat_id_list[\n                random.randint(0, len(spk_feat_id_list) - 1)\n            ]\n            if not hasattr(self, \"spk2aver_spk_feat\"):\n                spk_feat = read_data_by_path(\n                    spk_feat[random_spk_feat_id], return_tensor=True\n                )\n            else:\n                # randomly pick up a useless spk_feat_id for the same randomness results\n                random_spk_feat_id = \"aver_spk_feat\"\n                spk_feat = read_data_by_path(\n                    self.spk2aver_spk_feat[random_spk_id], return_tensor=True\n                )\n\n            if \"spk_feat\" not in main_data.keys():\n                main_data[\"spk_feat\"] = spk_feat\n            else:\n                main_data[\"spk_feat\"] += spk_feat\n\n            chosen_spk_feat_ids.append(random_spk_feat_id)\n            chosen_spk_ids.append(random_spk_id)\n\n        # take the average of the chose speaker embedding features\n        if self.mixup_number &gt; 1:\n            main_data[\"spk_feat\"] /= self.mixup_number\n            # sort all the IDs of spk_feat and spk to make sure the naming uniqueness\n            main_data[\"spk_feat_ids\"] = \"+\".join(sorted(chosen_spk_feat_ids))\n            main_data[\"spk_ids\"] = \"+\".join(sorted(chosen_spk_ids))\n        else:\n            main_data[\"spk_feat_ids\"] = chosen_spk_feat_ids[0]\n            main_data[\"spk_ids\"] = chosen_spk_ids[0]\n\n        return main_data\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.RandomSpkFeatDataset.extract_main_data_fn","title":"<code>extract_main_data_fn(main_data)</code>","text":"<p>This hook function randomly pick up a speaker embedding feature from the given spk_feat file as the reference.</p> <p>The randomness is controlled by the <code>seed</code> you give in the exp_cfg.</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def extract_main_data_fn(self, main_data: Dict[str, str]) -&gt; Dict[str, Any] or None:\n    \"\"\"This hook function randomly pick up a speaker embedding feature from the\n    given spk_feat file as the reference.\n\n    The randomness is controlled by the `seed` you give in the exp_cfg.\n    \"\"\"\n    assert \"spk_ids\" not in main_data.keys(), (\n        f\"Please don't give spk_ids to main_data of {self.__class__.__name__}. \"\n        f\"This Dataset is used to evaluate open-set multi-speaker TTS that uses external speaker embedding.\"\n    )\n    assert \"spk_feat\" not in main_data.keys(), (\n        f\"Please don't give spk_feat to main_data of {self.__class__.__name__}. \"\n        f\"Your spk_feat should be given outside the main_data.\"\n    )\n\n    # process 'feat' and 'text' by the parent class\n    main_data = super(RandomSpkFeatDataset, self).extract_main_data_fn(main_data)\n    # None means empty batch received from the parent class\n    if main_data is None:\n        return main_data\n\n    chosen_spk_feat_ids, chosen_spk_ids = [], []\n    while len(chosen_spk_feat_ids) &lt; self.mixup_number:\n        random_spk_id, self.spk2freq = get_min_indices_by_freq(\n            self.spk2freq,\n            freq_weights=(\n                len(main_data[\"text\"]) if \"text\" in main_data.keys() else None\n            ),\n        )\n        random_spk_id = random_spk_id[0]\n\n        # randomly pick up a speaker embedding feature vector\n        spk_feat = self.spk2spk_feat[random_spk_id]\n        spk_feat_id_list = list(spk_feat.keys())\n        random_spk_feat_id = spk_feat_id_list[\n            random.randint(0, len(spk_feat_id_list) - 1)\n        ]\n        if not hasattr(self, \"spk2aver_spk_feat\"):\n            spk_feat = read_data_by_path(\n                spk_feat[random_spk_feat_id], return_tensor=True\n            )\n        else:\n            # randomly pick up a useless spk_feat_id for the same randomness results\n            random_spk_feat_id = \"aver_spk_feat\"\n            spk_feat = read_data_by_path(\n                self.spk2aver_spk_feat[random_spk_id], return_tensor=True\n            )\n\n        if \"spk_feat\" not in main_data.keys():\n            main_data[\"spk_feat\"] = spk_feat\n        else:\n            main_data[\"spk_feat\"] += spk_feat\n\n        chosen_spk_feat_ids.append(random_spk_feat_id)\n        chosen_spk_ids.append(random_spk_id)\n\n    # take the average of the chose speaker embedding features\n    if self.mixup_number &gt; 1:\n        main_data[\"spk_feat\"] /= self.mixup_number\n        # sort all the IDs of spk_feat and spk to make sure the naming uniqueness\n        main_data[\"spk_feat_ids\"] = \"+\".join(sorted(chosen_spk_feat_ids))\n        main_data[\"spk_ids\"] = \"+\".join(sorted(chosen_spk_ids))\n    else:\n        main_data[\"spk_feat_ids\"] = chosen_spk_feat_ids[0]\n        main_data[\"spk_ids\"] = chosen_spk_ids[0]\n\n    return main_data\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.SpeechTextDataset","title":"<code>SpeechTextDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This Dataset subclass is mainly used by ASR and TTS models.</p> <p>In this subclass, each data instance is made up of an utterance and a sentence as well as the speaker information (speaker ID + speaker embedding feature).</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>class SpeechTextDataset(Dataset):\n    \"\"\"This Dataset subclass is mainly used by ASR and TTS models.\n\n    In this subclass, each data instance is made up of an utterance and a sentence as\n    well as the speaker information (speaker ID + speaker embedding feature).\n    \"\"\"\n\n    def dataset_init_fn(\n        self,\n        use_g2p: bool = False,\n        unk_mask_prob: float = 0.0,\n        use_speed_perturb: bool = False,\n        sample_rate: int = 16000,\n        perturb_range: List[float] = [0.9, 1.0, 1.1],\n        pitch_conf: Dict = None,\n    ):\n        \"\"\"Dataset initialization function.\n\n        Args:\n            use_g2p (bool, optional): Whether to process the raw string by G2P. We don't\n                recommend you to turn it on because on-the-fly transformer from string to\n                phoneme list consumes a lot of CPU resources. Defaults to False.\n\n            unk_mask_prob (float, optional): Probability of masking tokens as unknown.\n                Defaults to 0.0.\n            use_speed_perturb (bool, optional): Whether to perturb the speed of the\n                waveforms. Defaults to False.\n            sample_rate (int, optional): Audio sampling rate in Hz. Defaults to 16000.\n            perturb_range (List[float], optional): Range of speed perturbation factors.\n                Defaults to [0.9, 1.0, 1.1].\n\n            pitch_conf (Dict, optional): The configuration given to convert_wav_to_pitch()\n                for pitch extraction. If not given, pitch extraction will not be done\n                on-the-fly. Defaults to None.\n\n        Note:\n            Phoneme related: use_g2p\n            Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range\n            Pitch related: pitch_conf\n        \"\"\"\n        # register sampling rate for later check\n        self.sample_rate = sample_rate\n        warnings.warn(\n            f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n            f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n            f\"Please make sure that {sample_rate} is the same with your model! \"\n            f\"If this is not your target sampling rate, \"\n            f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n            f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n        )\n\n        assert (\n            0 &lt;= unk_mask_prob &lt;= 1\n        ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n        self.unk_mask_prob = unk_mask_prob\n\n        # phoneme extraction\n        if use_g2p:\n            self.g2p = G2p()\n\n        if use_speed_perturb:\n            self.perturb_range = perturb_range\n            self.speed_resampler_list = [\n                torchaudio.transforms.Resample(\n                    orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n                )\n                for factor in perturb_range\n            ]\n\n        # pitch extraction\n        if pitch_conf is not None:\n            if \"sr\" in pitch_conf.keys():\n                assert pitch_conf[\"sr\"] == self.sample_rate, (\n                    f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                    f\"given sample_rate ({self.sample_rate})!\"\n                )\n            pitch_conf[\"sr\"] = self.sample_rate\n            self.pitch_extract_fn = partial(\n                convert_wav_to_pitch, return_tensor=True, **pitch_conf\n            )\n\n    @staticmethod\n    def data_len_register_fn(\n        main_data: Dict[str, Dict[str, str]],\n    ) -&gt; Dict[str, int or float] or None:\n        \"\"\"\n\n        Returns:\n            If 'text' is given in main_data, return the number of characters in each sentence.\n            Otherwise, return None\n\n        \"\"\"\n        if \"text\" in main_data.keys():\n            return {key: len(value) for key, value in main_data[\"text\"].items()}\n        else:\n            return None\n\n    def collate_main_data_fn(\n        self, batch_dict: Dict[str, List]\n    ) -&gt; Dict[str, torch.Tensor or List]:\n        \"\"\"The utterances used for training ASR and TTS models may have different\n        lengths, so we need to do the padding operations to make them equal in length.\n\n        The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n        vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n        Args:\n            batch_dict (Dict[str, List]): The keys of the input `batch_dict` dictionary should be one of the following:\n                1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n                2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n                3. `text`: a List of text strings.\n                4. `spk_ids`: a List of speaker ID strings.\n                5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n        Returns:\n            A dictionary mapping strings to either torch.Tensor or List, where:\n                - feat and spk_feat are three-dimensional torch.Tensor\n                - text and spk_ids are lists of raw strings whose discretization is done in the Model object\n        \"\"\"\n\n        # --- 1. Pad Speech Data and Stack them together --- #\n        if \"feat\" in batch_dict.keys():\n            # para init\n            feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n            batch_size, feat_maxlen, feat_dim = (\n                len(batch_dict[\"feat\"]),\n                feat_len.max().item(),\n                batch_dict[\"feat\"][0].shape[-1],\n            )\n\n            # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n            # overwrite the padding matrix with each feat vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                    feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n                elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                    feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            # update 'feat' and attach 'feat_len' for later model forward\n            batch_dict[\"feat\"] = feat\n            batch_dict[\"feat_len\"] = feat_len\n\n        # --- 2. Pad Pitch Data and Stack them together --- #\n        if \"pitch\" in batch_dict.keys():\n            # para init\n            pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n            batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n            # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n            pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n            # overwrite the padding matrix with each pitch vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                    pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n                elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                    pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            batch_dict[\"pitch\"] = pitch\n            batch_dict[\"pitch_len\"] = pitch_len\n\n        # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n        if \"duration\" in batch_dict.keys():\n            # para init\n            batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n                [len(ele) for ele in batch_dict[\"duration\"]]\n            )\n\n            # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            duration = torch.zeros(\n                (batch_size, duration_len.max().item()), dtype=torch.float32\n            )\n            # overwrite the padding matrix with each duration vector\n            for i in range(batch_size):\n                # process duration data based on data type\n                if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                    duration[i][: duration_len[i]] = torch.tensor(\n                        batch_dict[\"duration\"][i]\n                    )\n                elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                    duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n                else:\n                    raise TypeError(\n                        f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                    )\n\n            # attach 'duration' and 'duration_len' for model forward\n            batch_dict[\"duration\"] = duration\n            batch_dict[\"duration_len\"] = duration_len\n\n        # --- 4. Stack Speaker Embedding Feature together --- #\n        if \"spk_feat\" in batch_dict.keys():\n            batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n        return batch_dict\n\n    def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n        \"\"\"The function that loads speech-text data from the disk. If the speech is in\n        the form of raw waveforms, the last dimension should be expanded to 1 of raw\n        speech for compatibility with acoustic feature.\n\n        Args:\n            main_data: Dict[str, str]\n                The keys of the input main_data dictionary should be one of the following:\n                    1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                    2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                    TTS models.\n                    3. 'duration': phoneme durations. used for training fastspeech2 model.\n                    4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                    ASR and TTS models.\n                    5. 'spk_feat': speaker embedding features.\n                `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n                in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n                However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n                CPU burden.\n\n        Returns:\n            `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n            `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n        \"\"\"\n        assert (\n            \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n        ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n        for key in main_data.keys():\n            if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n                raise RuntimeError(\n                    f\"Unknown data name {key}! \"\n                    f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                    \"'feat' (for paths of raw waveforms or acoustic features), \"\n                    \"'text' (for transcript text data), \"\n                    \"'duration' (for phoneme duration data), \"\n                    \"'spk_ids' (for speaker IDs), \"\n                    \"'spk_feat' (for speaker embedding features).\"\n                )\n\n        # --- 1. Speech Data Extraction --- #\n        if \"feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"feat\"], sample_rate = read_data_by_path(\n                main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n            )\n            # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n            if main_data[\"feat\"].size(0) == 0:\n                return None\n\n            # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n            if sample_rate &gt; self.sample_rate:\n                if not hasattr(self, \"wav_resampler_dict\"):\n                    self.wav_resampler_dict = {\n                        sample_rate: torchaudio.transforms.Resample(\n                            orig_freq=sample_rate, new_freq=self.sample_rate\n                        )\n                    }\n                main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n            # extracted waveforms could not have lower sampling rate than the built-in one\n            elif sample_rate &lt; self.sample_rate:\n                raise RuntimeError(\n                    f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n                )\n\n            # perturb the speed of the extracted speech if specified\n            if hasattr(self, \"speed_resampler_list\"):\n                assert sample_rate == self.sample_rate, (\n                    f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                    f\"waveform ({sample_rate})!\"\n                )\n                resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n                main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n\n            # extract the pitch from the speech on-the-fly\n            if hasattr(self, \"pitch_extract_fn\"):\n                try:\n                    main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n                # IndexError means all the pitch values are unvoiced (=0.0)\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 2. Transcript Text Extraction --- #\n        if \"text\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"text\"], str\n            ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n            # for the text data in the format of a list\n            if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n                main_data[\"text\"] = main_data[\"text\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"text\"] = main_data[\"text\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"text\"] = [\n                    (\n                        token[1:-1]\n                        if token.startswith(\"'\") and token.endswith(\"'\")\n                        else token\n                    )\n                    for token in main_data[\"text\"]\n                ]\n            # process the raw string by G2P if specified\n            elif hasattr(self, \"g2p\"):\n                phn_list = self.g2p(main_data[\"text\"])\n                main_data[\"text\"] = [\n                    phn if phn != \" \" else \"&lt;space&gt;\"\n                    for phn in phn_list\n                    if phn not in abnormal_phns\n                ]\n\n        # --- 3. Phoneme Duration Extraction --- #\n        if \"duration\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"duration\"], str\n            ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n            # for the text data in the format of a list\n            if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n                \"]\"\n            ):\n                main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"duration\"] = [\n                    (\n                        float(duration[1:-1])\n                        if duration.startswith(\"'\") and duration.endswith(\"'\")\n                        else float(duration)\n                    )\n                    for duration in main_data[\"duration\"]\n                ]\n            else:\n                raise RuntimeError(\n                    \"The 'duration' string should be surrounded by a pair of square brackets!\"\n                )\n\n        # --- 4. Silence Trimming at the two ends --- #\n        # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n        if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n            main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n        ):\n            # trim both feat and text\n            if \"feat\" in main_data.keys():\n                assert \"duration\" in main_data.keys(), (\n                    \"If you want to trim the silence at two ends of speech, \"\n                    \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n                )\n                front_trim_len, tail_trim_len, total_duration = (\n                    0,\n                    0,\n                    sum(main_data[\"duration\"]),\n                )\n                try:\n                    # sum up all the silence tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        front_trim_len += main_data[\"duration\"][0]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][1:],\n                            main_data[\"duration\"][1:],\n                        )\n                    # sum up all the silence tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        tail_trim_len += main_data[\"duration\"][-1]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][:-1],\n                            main_data[\"duration\"][:-1],\n                        )\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n                # normalize the trimming lengths by the total duration length\n                front_trim_len, tail_trim_len = (\n                    front_trim_len / total_duration,\n                    tail_trim_len / total_duration,\n                )\n                # trim the extra silence in feat (waveforms or acoustic features)\n                feat_start, feat_end = int(\n                    front_trim_len * len(main_data[\"feat\"])\n                ), int(tail_trim_len * len(main_data[\"feat\"]))\n                main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n                if feat_end &gt; 0:\n                    main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n                # also trim the two ends of pitch values if extracted\n                if \"pitch\" in main_data.keys():\n                    pitch_start, pitch_end = int(\n                        front_trim_len * len(main_data[\"pitch\"])\n                    ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                    main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                    if pitch_end &gt; 0:\n                        main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n            # only trim text if feat is not given\n            else:\n                try:\n                    # sum up all the &lt;space&gt; tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][1:]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][1:]\n                    # sum up all the &lt;space&gt; tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][:-1]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][:-1]\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n        if self.unk_mask_prob &gt; 0:\n            assert \"text\" in main_data.keys() and isinstance(\n                main_data[\"text\"], List\n            ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n            # Get the start and end indices of words based on the positions of space tokens\n            space_indices = [\n                i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n            ]\n            word_start_indices, word_end_indices = [0] + [\n                s_i + 1 for s_i in space_indices\n            ], space_indices + [len(main_data[\"text\"])]\n\n            # Determine which words to mask\n            word_mask_flags = (\n                np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n            )\n\n            _tmp_text, _tmp_duration = [], []\n            for i in range(len(word_mask_flags)):\n                # If the word should be masked, add an '&lt;unk&gt;' token\n                if word_mask_flags[i]:\n                    _tmp_text.append(\"&lt;unk&gt;\")\n                    if \"duration\" in main_data.keys():\n                        _sum_duration = sum(\n                            main_data[\"duration\"][\n                                word_start_indices[i] : word_end_indices[i]\n                            ]\n                        )\n                        _tmp_duration.append(round(_sum_duration, 2))\n\n                # If the word shouldn't be masked, add the original tokens of the word\n                else:\n                    _tmp_text += main_data[\"text\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration += main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n\n                # Add space tokens and their durations between words, except for the last word\n                if i != len(word_mask_flags) - 1:\n                    _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n            # Update main_data with the new text and duration information\n            main_data[\"text\"] = _tmp_text\n            if \"duration\" in main_data.keys():\n                main_data[\"duration\"] = _tmp_duration\n\n        # --- 6. Speaker ID Extraction --- #\n        if \"spk_ids\" in main_data.keys():\n            # the speaker ID here is just a raw string\n            assert isinstance(\n                main_data[\"spk_ids\"], str\n            ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n        # --- 7. Speaker Embedding Feature --- #\n        if \"spk_feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"spk_feat\"] = read_data_by_path(\n                main_data[\"spk_feat\"], return_tensor=True\n            )\n\n        return main_data\n\n    def __repr__(self):\n        outputs = f\"{self.__class__.__name__}(sample_rate={self.sample_rate}\"\n        if hasattr(self, \"g2p\"):\n            outputs += \", use_g2p=True\"\n        if hasattr(self, \"speed_resampler_list\"):\n            outputs += f\", speed_perturb_range={self.perturb_range}\"\n        if hasattr(self, \"pitch_extract_fn\"):\n            outputs += \", pitch_extract=True\"\n        if self.unk_mask_prob &gt; 0:\n            outputs += f\", unk_mask_prob={self.unk_mask_prob}\"\n        return outputs + \")\"\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.SpeechTextDataset.collate_main_data_fn","title":"<code>collate_main_data_fn(batch_dict)</code>","text":"<p>The utterances used for training ASR and TTS models may have different lengths, so we need to do the padding operations to make them equal in length.</p> <p>The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dict</code> <code>Dict[str, List]</code> <p>The keys of the input <code>batch_dict</code> dictionary should be one of the following: 1. <code>feat</code>: a List of 2d <code>torch.Tensor</code> with different lengths. 2. <code>pitch</code>: a List of 1d <code>torch.Tensor</code> with different lengths. 3. <code>text</code>: a List of text strings. 4. <code>spk_ids</code>: a List of speaker ID strings. 5. <code>spk_feat</code>: a List of 2d <code>torch.Tensor</code> with equal lengths.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor or List]</code> <p>A dictionary mapping strings to either torch.Tensor or List, where: - feat and spk_feat are three-dimensional torch.Tensor - text and spk_ids are lists of raw strings whose discretization is done in the Model object</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def collate_main_data_fn(\n    self, batch_dict: Dict[str, List]\n) -&gt; Dict[str, torch.Tensor or List]:\n    \"\"\"The utterances used for training ASR and TTS models may have different\n    lengths, so we need to do the padding operations to make them equal in length.\n\n    The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n    vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n    Args:\n        batch_dict (Dict[str, List]): The keys of the input `batch_dict` dictionary should be one of the following:\n            1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n            2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n            3. `text`: a List of text strings.\n            4. `spk_ids`: a List of speaker ID strings.\n            5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n    Returns:\n        A dictionary mapping strings to either torch.Tensor or List, where:\n            - feat and spk_feat are three-dimensional torch.Tensor\n            - text and spk_ids are lists of raw strings whose discretization is done in the Model object\n    \"\"\"\n\n    # --- 1. Pad Speech Data and Stack them together --- #\n    if \"feat\" in batch_dict.keys():\n        # para init\n        feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n        batch_size, feat_maxlen, feat_dim = (\n            len(batch_dict[\"feat\"]),\n            feat_len.max().item(),\n            batch_dict[\"feat\"][0].shape[-1],\n        )\n\n        # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n        # overwrite the padding matrix with each feat vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n            elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        # update 'feat' and attach 'feat_len' for later model forward\n        batch_dict[\"feat\"] = feat\n        batch_dict[\"feat_len\"] = feat_len\n\n    # --- 2. Pad Pitch Data and Stack them together --- #\n    if \"pitch\" in batch_dict.keys():\n        # para init\n        pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n        batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n        # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n        pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n        # overwrite the padding matrix with each pitch vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n            elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        batch_dict[\"pitch\"] = pitch\n        batch_dict[\"pitch_len\"] = pitch_len\n\n    # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n    if \"duration\" in batch_dict.keys():\n        # para init\n        batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n            [len(ele) for ele in batch_dict[\"duration\"]]\n        )\n\n        # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        duration = torch.zeros(\n            (batch_size, duration_len.max().item()), dtype=torch.float32\n        )\n        # overwrite the padding matrix with each duration vector\n        for i in range(batch_size):\n            # process duration data based on data type\n            if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                duration[i][: duration_len[i]] = torch.tensor(\n                    batch_dict[\"duration\"][i]\n                )\n            elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n            else:\n                raise TypeError(\n                    f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                )\n\n        # attach 'duration' and 'duration_len' for model forward\n        batch_dict[\"duration\"] = duration\n        batch_dict[\"duration_len\"] = duration_len\n\n    # --- 4. Stack Speaker Embedding Feature together --- #\n    if \"spk_feat\" in batch_dict.keys():\n        batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n    return batch_dict\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.SpeechTextDataset.data_len_register_fn","title":"<code>data_len_register_fn(main_data)</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, int or float] or None</code> <p>If 'text' is given in main_data, return the number of characters in each sentence.</p> <code>Dict[str, int or float] or None</code> <p>Otherwise, return None</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>@staticmethod\ndef data_len_register_fn(\n    main_data: Dict[str, Dict[str, str]],\n) -&gt; Dict[str, int or float] or None:\n    \"\"\"\n\n    Returns:\n        If 'text' is given in main_data, return the number of characters in each sentence.\n        Otherwise, return None\n\n    \"\"\"\n    if \"text\" in main_data.keys():\n        return {key: len(value) for key, value in main_data[\"text\"].items()}\n    else:\n        return None\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.SpeechTextDataset.dataset_init_fn","title":"<code>dataset_init_fn(use_g2p=False, unk_mask_prob=0.0, use_speed_perturb=False, sample_rate=16000, perturb_range=[0.9, 1.0, 1.1], pitch_conf=None)</code>","text":"<p>Dataset initialization function.</p> <p>Parameters:</p> Name Type Description Default <code>use_g2p</code> <code>bool</code> <p>Whether to process the raw string by G2P. We don't recommend you to turn it on because on-the-fly transformer from string to phoneme list consumes a lot of CPU resources. Defaults to False.</p> <code>False</code> <code>unk_mask_prob</code> <code>float</code> <p>Probability of masking tokens as unknown. Defaults to 0.0.</p> <code>0.0</code> <code>use_speed_perturb</code> <code>bool</code> <p>Whether to perturb the speed of the waveforms. Defaults to False.</p> <code>False</code> <code>sample_rate</code> <code>int</code> <p>Audio sampling rate in Hz. Defaults to 16000.</p> <code>16000</code> <code>perturb_range</code> <code>List[float]</code> <p>Range of speed perturbation factors. Defaults to [0.9, 1.0, 1.1].</p> <code>[0.9, 1.0, 1.1]</code> <code>pitch_conf</code> <code>Dict</code> <p>The configuration given to convert_wav_to_pitch() for pitch extraction. If not given, pitch extraction will not be done on-the-fly. Defaults to None.</p> <code>None</code> Note <p>Phoneme related: use_g2p Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range Pitch related: pitch_conf</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def dataset_init_fn(\n    self,\n    use_g2p: bool = False,\n    unk_mask_prob: float = 0.0,\n    use_speed_perturb: bool = False,\n    sample_rate: int = 16000,\n    perturb_range: List[float] = [0.9, 1.0, 1.1],\n    pitch_conf: Dict = None,\n):\n    \"\"\"Dataset initialization function.\n\n    Args:\n        use_g2p (bool, optional): Whether to process the raw string by G2P. We don't\n            recommend you to turn it on because on-the-fly transformer from string to\n            phoneme list consumes a lot of CPU resources. Defaults to False.\n\n        unk_mask_prob (float, optional): Probability of masking tokens as unknown.\n            Defaults to 0.0.\n        use_speed_perturb (bool, optional): Whether to perturb the speed of the\n            waveforms. Defaults to False.\n        sample_rate (int, optional): Audio sampling rate in Hz. Defaults to 16000.\n        perturb_range (List[float], optional): Range of speed perturbation factors.\n            Defaults to [0.9, 1.0, 1.1].\n\n        pitch_conf (Dict, optional): The configuration given to convert_wav_to_pitch()\n            for pitch extraction. If not given, pitch extraction will not be done\n            on-the-fly. Defaults to None.\n\n    Note:\n        Phoneme related: use_g2p\n        Waveform related: unk_mask_prob, use_speed_perturb, sample_rate, perturb_range\n        Pitch related: pitch_conf\n    \"\"\"\n    # register sampling rate for later check\n    self.sample_rate = sample_rate\n    warnings.warn(\n        f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n        f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n        f\"Please make sure that {sample_rate} is the same with your model! \"\n        f\"If this is not your target sampling rate, \"\n        f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n        f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n    )\n\n    assert (\n        0 &lt;= unk_mask_prob &lt;= 1\n    ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n    self.unk_mask_prob = unk_mask_prob\n\n    # phoneme extraction\n    if use_g2p:\n        self.g2p = G2p()\n\n    if use_speed_perturb:\n        self.perturb_range = perturb_range\n        self.speed_resampler_list = [\n            torchaudio.transforms.Resample(\n                orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n            )\n            for factor in perturb_range\n        ]\n\n    # pitch extraction\n    if pitch_conf is not None:\n        if \"sr\" in pitch_conf.keys():\n            assert pitch_conf[\"sr\"] == self.sample_rate, (\n                f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                f\"given sample_rate ({self.sample_rate})!\"\n            )\n        pitch_conf[\"sr\"] = self.sample_rate\n        self.pitch_extract_fn = partial(\n            convert_wav_to_pitch, return_tensor=True, **pitch_conf\n        )\n</code></pre>"},{"location":"reference/dataset/speech_text/#dataset.speech_text.SpeechTextDataset.extract_main_data_fn","title":"<code>extract_main_data_fn(main_data)</code>","text":"<p>The function that loads speech-text data from the disk. If the speech is in the form of raw waveforms, the last dimension should be expanded to 1 of raw speech for compatibility with acoustic feature.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict</code> <p>Dict[str, str] The keys of the input main_data dictionary should be one of the following:     1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.     2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and     TTS models.     3. 'duration': phoneme durations. used for training fastspeech2 model.     4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the     ASR and TTS models.     5. 'spk_feat': speaker embedding features. <code>spk_ids</code> and <code>spk_feat</code> are designed for multi-speaker TTS model and are not mandatory to be included in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training. However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the CPU burden.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] or None</code> <p><code>feat</code> and <code>spk_feat</code> are in the form of two-dimensional <code>torch.Tensor</code>;</p> <code>Dict[str, Any] or None</code> <p><code>text</code> and <code>spk_ids</code> are in the form of raw strings whose discretization is done in the Model object.</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n    \"\"\"The function that loads speech-text data from the disk. If the speech is in\n    the form of raw waveforms, the last dimension should be expanded to 1 of raw\n    speech for compatibility with acoustic feature.\n\n    Args:\n        main_data: Dict[str, str]\n            The keys of the input main_data dictionary should be one of the following:\n                1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                TTS models.\n                3. 'duration': phoneme durations. used for training fastspeech2 model.\n                4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                ASR and TTS models.\n                5. 'spk_feat': speaker embedding features.\n            `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n            in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n            However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n            CPU burden.\n\n    Returns:\n        `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n        `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n    \"\"\"\n    assert (\n        \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n    ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n    for key in main_data.keys():\n        if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n            raise RuntimeError(\n                f\"Unknown data name {key}! \"\n                f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                \"'feat' (for paths of raw waveforms or acoustic features), \"\n                \"'text' (for transcript text data), \"\n                \"'duration' (for phoneme duration data), \"\n                \"'spk_ids' (for speaker IDs), \"\n                \"'spk_feat' (for speaker embedding features).\"\n            )\n\n    # --- 1. Speech Data Extraction --- #\n    if \"feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"feat\"], sample_rate = read_data_by_path(\n            main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n        )\n        # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n        if main_data[\"feat\"].size(0) == 0:\n            return None\n\n        # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n        if sample_rate &gt; self.sample_rate:\n            if not hasattr(self, \"wav_resampler_dict\"):\n                self.wav_resampler_dict = {\n                    sample_rate: torchaudio.transforms.Resample(\n                        orig_freq=sample_rate, new_freq=self.sample_rate\n                    )\n                }\n            main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n        # extracted waveforms could not have lower sampling rate than the built-in one\n        elif sample_rate &lt; self.sample_rate:\n            raise RuntimeError(\n                f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n            )\n\n        # perturb the speed of the extracted speech if specified\n        if hasattr(self, \"speed_resampler_list\"):\n            assert sample_rate == self.sample_rate, (\n                f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                f\"waveform ({sample_rate})!\"\n            )\n            resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n            main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n\n        # extract the pitch from the speech on-the-fly\n        if hasattr(self, \"pitch_extract_fn\"):\n            try:\n                main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n            # IndexError means all the pitch values are unvoiced (=0.0)\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 2. Transcript Text Extraction --- #\n    if \"text\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"text\"], str\n        ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n        # for the text data in the format of a list\n        if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n            main_data[\"text\"] = main_data[\"text\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"text\"] = main_data[\"text\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"text\"] = [\n                (\n                    token[1:-1]\n                    if token.startswith(\"'\") and token.endswith(\"'\")\n                    else token\n                )\n                for token in main_data[\"text\"]\n            ]\n        # process the raw string by G2P if specified\n        elif hasattr(self, \"g2p\"):\n            phn_list = self.g2p(main_data[\"text\"])\n            main_data[\"text\"] = [\n                phn if phn != \" \" else \"&lt;space&gt;\"\n                for phn in phn_list\n                if phn not in abnormal_phns\n            ]\n\n    # --- 3. Phoneme Duration Extraction --- #\n    if \"duration\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"duration\"], str\n        ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n        # for the text data in the format of a list\n        if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n            \"]\"\n        ):\n            main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"duration\"] = [\n                (\n                    float(duration[1:-1])\n                    if duration.startswith(\"'\") and duration.endswith(\"'\")\n                    else float(duration)\n                )\n                for duration in main_data[\"duration\"]\n            ]\n        else:\n            raise RuntimeError(\n                \"The 'duration' string should be surrounded by a pair of square brackets!\"\n            )\n\n    # --- 4. Silence Trimming at the two ends --- #\n    # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n    if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n        main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n    ):\n        # trim both feat and text\n        if \"feat\" in main_data.keys():\n            assert \"duration\" in main_data.keys(), (\n                \"If you want to trim the silence at two ends of speech, \"\n                \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n            )\n            front_trim_len, tail_trim_len, total_duration = (\n                0,\n                0,\n                sum(main_data[\"duration\"]),\n            )\n            try:\n                # sum up all the silence tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    front_trim_len += main_data[\"duration\"][0]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][1:],\n                        main_data[\"duration\"][1:],\n                    )\n                # sum up all the silence tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    tail_trim_len += main_data[\"duration\"][-1]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][:-1],\n                        main_data[\"duration\"][:-1],\n                    )\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n            # normalize the trimming lengths by the total duration length\n            front_trim_len, tail_trim_len = (\n                front_trim_len / total_duration,\n                tail_trim_len / total_duration,\n            )\n            # trim the extra silence in feat (waveforms or acoustic features)\n            feat_start, feat_end = int(\n                front_trim_len * len(main_data[\"feat\"])\n            ), int(tail_trim_len * len(main_data[\"feat\"]))\n            main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n            if feat_end &gt; 0:\n                main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n            # also trim the two ends of pitch values if extracted\n            if \"pitch\" in main_data.keys():\n                pitch_start, pitch_end = int(\n                    front_trim_len * len(main_data[\"pitch\"])\n                ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                if pitch_end &gt; 0:\n                    main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n        # only trim text if feat is not given\n        else:\n            try:\n                # sum up all the &lt;space&gt; tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][1:]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][1:]\n                # sum up all the &lt;space&gt; tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][:-1]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][:-1]\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n    if self.unk_mask_prob &gt; 0:\n        assert \"text\" in main_data.keys() and isinstance(\n            main_data[\"text\"], List\n        ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n        # Get the start and end indices of words based on the positions of space tokens\n        space_indices = [\n            i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n        ]\n        word_start_indices, word_end_indices = [0] + [\n            s_i + 1 for s_i in space_indices\n        ], space_indices + [len(main_data[\"text\"])]\n\n        # Determine which words to mask\n        word_mask_flags = (\n            np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n        )\n\n        _tmp_text, _tmp_duration = [], []\n        for i in range(len(word_mask_flags)):\n            # If the word should be masked, add an '&lt;unk&gt;' token\n            if word_mask_flags[i]:\n                _tmp_text.append(\"&lt;unk&gt;\")\n                if \"duration\" in main_data.keys():\n                    _sum_duration = sum(\n                        main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n                    )\n                    _tmp_duration.append(round(_sum_duration, 2))\n\n            # If the word shouldn't be masked, add the original tokens of the word\n            else:\n                _tmp_text += main_data[\"text\"][\n                    word_start_indices[i] : word_end_indices[i]\n                ]\n                if \"duration\" in main_data.keys():\n                    _tmp_duration += main_data[\"duration\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n\n            # Add space tokens and their durations between words, except for the last word\n            if i != len(word_mask_flags) - 1:\n                _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                if \"duration\" in main_data.keys():\n                    _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n        # Update main_data with the new text and duration information\n        main_data[\"text\"] = _tmp_text\n        if \"duration\" in main_data.keys():\n            main_data[\"duration\"] = _tmp_duration\n\n    # --- 6. Speaker ID Extraction --- #\n    if \"spk_ids\" in main_data.keys():\n        # the speaker ID here is just a raw string\n        assert isinstance(\n            main_data[\"spk_ids\"], str\n        ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n    # --- 7. Speaker Embedding Feature --- #\n    if \"spk_feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"spk_feat\"] = read_data_by_path(\n            main_data[\"spk_feat\"], return_tensor=True\n        )\n\n    return main_data\n</code></pre>"},{"location":"reference/infer_func/","title":"infer_func","text":""},{"location":"reference/infer_func/beam_search/","title":"beam_search","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p> <p>Modified from https://github.com/huggingface/transformers/blob/518bd02c9b71291333ef374f055a4d1ac3042654/src/transformers/generation_beam_search.py</p>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.BeamHypotheses","title":"<code>BeamHypotheses</code>","text":"<p>               Bases: <code>object</code></p> <p>Beam Hypothesis Container.</p> Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>class BeamHypotheses(object):\n    \"\"\"Beam Hypothesis Container.\"\"\"\n\n    def __init__(self, beam_size: int, max_length: int, length_penalty: float):\n        \"\"\"Initialize n-best list of hypotheses.\n\n        Args:\n            beam_size: int\n                The number of beams used in this container\n            max_length: int\n                The maximal length of the generated hypotheses\n            length_penalty: float\n                The penalty you put on the hypothesis lengths.\n                The larger length_penalty is, the longer your hypotheses will be.\n                length_penalty=1 means no penalty on lengths.\n        \"\"\"\n        # static variables\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.beam_size = beam_size\n        self.length_penalty = (\n            length_penalty  # length_penalty=1 means no penalty on length\n        )\n\n        # dynamic variables\n        self.beams = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"Number of hypotheses generated so far.\"\"\"\n        return len(self.beams)\n\n    def add(self, hyp: torch.Tensor, sum_logprobs: float):\n        \"\"\"Add a new hypothesis to the container.\n\n        Args:\n            hyp: (hypo_len,)\n                The generated hypothesis transcript.\n            sum_logprobs: float\n                The sum of log probability of each token prediction in the hypothesis.\n        \"\"\"\n        # normalize the sum of log probability by the hypothesis length\n        score = sum_logprobs / ((len(hyp) + eps) ** self.length_penalty)\n\n        # some beams remain undone or the score is better than the worst score so far\n        if len(self) &lt; self.beam_size or score &gt; self.worst_score:\n            self.beams.append((score, hyp))\n\n            # remove the worst hypothesis and update the worst score\n            if len(self) &gt; self.beam_size:\n                sorted_scores = sorted(\n                    [(s, idx) for idx, (s, _) in enumerate(self.beams)]\n                )\n                del self.beams[sorted_scores[0][1]]\n                self.worst_score = sorted_scores[1][0]\n            # update the worst score\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs: float, curr_len: int = None):\n        \"\"\"Whether the beam searching of this container is done or not.\n\n        Args:\n            best_sum_logprobs: float\n                The best log-prob sum we get in the current time step\n            curr_len: int\n                The length of the current input hypothesis\n\n        Returns:\n            A flag that indicates whether the beam searching of this container is done.\n            True means the container already has 'beam_size' hypotheses and the current hypothesis is not better than anyone of them.\n            False means either the container has some empty beams or the current input hypothesis is better than the worst hypothesis.\n        \"\"\"\n        # some beams are empty\n        if len(self) &lt; self.beam_size:\n            return False\n        # no beam is empty\n        else:\n            if curr_len is None:\n                curr_len = self.max_length\n            curr_score = best_sum_logprobs / ((curr_len + eps) ** self.length_penalty)\n\n            # whether the current score is worse than the worst score\n            return curr_score &lt; self.worst_score\n</code></pre>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.BeamHypotheses.__init__","title":"<code>__init__(beam_size, max_length, length_penalty)</code>","text":"<p>Initialize n-best list of hypotheses.</p> <p>Parameters:</p> Name Type Description Default <code>beam_size</code> <code>int</code> <p>int The number of beams used in this container</p> required <code>max_length</code> <code>int</code> <p>int The maximal length of the generated hypotheses</p> required <code>length_penalty</code> <code>float</code> <p>float The penalty you put on the hypothesis lengths. The larger length_penalty is, the longer your hypotheses will be. length_penalty=1 means no penalty on lengths.</p> required Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>def __init__(self, beam_size: int, max_length: int, length_penalty: float):\n    \"\"\"Initialize n-best list of hypotheses.\n\n    Args:\n        beam_size: int\n            The number of beams used in this container\n        max_length: int\n            The maximal length of the generated hypotheses\n        length_penalty: float\n            The penalty you put on the hypothesis lengths.\n            The larger length_penalty is, the longer your hypotheses will be.\n            length_penalty=1 means no penalty on lengths.\n    \"\"\"\n    # static variables\n    self.max_length = max_length - 1  # ignoring bos_token\n    self.beam_size = beam_size\n    self.length_penalty = (\n        length_penalty  # length_penalty=1 means no penalty on length\n    )\n\n    # dynamic variables\n    self.beams = []\n    self.worst_score = 1e9\n</code></pre>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.BeamHypotheses.__len__","title":"<code>__len__()</code>","text":"<p>Number of hypotheses generated so far.</p> Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>def __len__(self):\n    \"\"\"Number of hypotheses generated so far.\"\"\"\n    return len(self.beams)\n</code></pre>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.BeamHypotheses.add","title":"<code>add(hyp, sum_logprobs)</code>","text":"<p>Add a new hypothesis to the container.</p> <p>Parameters:</p> Name Type Description Default <code>hyp</code> <code>Tensor</code> <p>(hypo_len,) The generated hypothesis transcript.</p> required <code>sum_logprobs</code> <code>float</code> <p>float The sum of log probability of each token prediction in the hypothesis.</p> required Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>def add(self, hyp: torch.Tensor, sum_logprobs: float):\n    \"\"\"Add a new hypothesis to the container.\n\n    Args:\n        hyp: (hypo_len,)\n            The generated hypothesis transcript.\n        sum_logprobs: float\n            The sum of log probability of each token prediction in the hypothesis.\n    \"\"\"\n    # normalize the sum of log probability by the hypothesis length\n    score = sum_logprobs / ((len(hyp) + eps) ** self.length_penalty)\n\n    # some beams remain undone or the score is better than the worst score so far\n    if len(self) &lt; self.beam_size or score &gt; self.worst_score:\n        self.beams.append((score, hyp))\n\n        # remove the worst hypothesis and update the worst score\n        if len(self) &gt; self.beam_size:\n            sorted_scores = sorted(\n                [(s, idx) for idx, (s, _) in enumerate(self.beams)]\n            )\n            del self.beams[sorted_scores[0][1]]\n            self.worst_score = sorted_scores[1][0]\n        # update the worst score\n        else:\n            self.worst_score = min(score, self.worst_score)\n</code></pre>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.BeamHypotheses.is_done","title":"<code>is_done(best_sum_logprobs, curr_len=None)</code>","text":"<p>Whether the beam searching of this container is done or not.</p> <p>Parameters:</p> Name Type Description Default <code>best_sum_logprobs</code> <code>float</code> <p>float The best log-prob sum we get in the current time step</p> required <code>curr_len</code> <code>int</code> <p>int The length of the current input hypothesis</p> <code>None</code> <p>Returns:</p> Type Description <p>A flag that indicates whether the beam searching of this container is done.</p> <p>True means the container already has 'beam_size' hypotheses and the current hypothesis is not better than anyone of them.</p> <p>False means either the container has some empty beams or the current input hypothesis is better than the worst hypothesis.</p> Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>def is_done(self, best_sum_logprobs: float, curr_len: int = None):\n    \"\"\"Whether the beam searching of this container is done or not.\n\n    Args:\n        best_sum_logprobs: float\n            The best log-prob sum we get in the current time step\n        curr_len: int\n            The length of the current input hypothesis\n\n    Returns:\n        A flag that indicates whether the beam searching of this container is done.\n        True means the container already has 'beam_size' hypotheses and the current hypothesis is not better than anyone of them.\n        False means either the container has some empty beams or the current input hypothesis is better than the worst hypothesis.\n    \"\"\"\n    # some beams are empty\n    if len(self) &lt; self.beam_size:\n        return False\n    # no beam is empty\n    else:\n        if curr_len is None:\n            curr_len = self.max_length\n        curr_score = best_sum_logprobs / ((curr_len + eps) ** self.length_penalty)\n\n        # whether the current score is worse than the worst score\n        return curr_score &lt; self.worst_score\n</code></pre>"},{"location":"reference/infer_func/beam_search/#infer_func.beam_search.beam_searching","title":"<code>beam_searching(enc_feat, enc_feat_mask, asr_decode_fn, vocab_size, sos_eos=None, padding_idx=0, beam_size=1, min_f2t_ratio=3.0, length_penalty=1.0, temperature=1.0, eos_filtering=False, eos_threshold=1.5, ctc_weight=0.0, ctc_decode_fn=None, ctc_temperature=1.0, lm_weight=0.2, lm_temperature=1.0, lm_decode_fn=None, lm_window_size=None, ilm_sub_weight=0.0, sent_per_beam=1)</code>","text":"<p>Batch version of beam searching to enable parallel computation. The basic idea is reshaping batch_size sentences into (batch_size * beam_size) sentences.</p> <p>However, the hypothesis text probabilities calculated by a batch of inputs and a single input are slightly different due to the model accuracy. in rare cases, the best hypothesis with the highest probability may be different when the beam searching process is performed in the batch level.</p> <p>Therefore, batch-level beam searching is mainly used to speed up the pseudo text generation. For model visualization during training or evaluation after training, we recommend you to perform the beam searching for each single input.</p> <p>Parameters:</p> Name Type Description Default <code>enc_feat</code> <code>Tensor</code> <p>(batch_size, feat_maxlen, enc_dim) The final hidden representations from the encoder.</p> required <code>enc_feat_mask</code> <code>Tensor</code> <p>(batch_size, 1, feat_maxlen) The masks for the encoder representations.</p> required <code>asr_decode_fn</code> <p>The function that decodes the hypothesis for one time step and get the next prediction.</p> required <code>vocab_size</code> <code>int</code> <p>int The number of tokens in the vocabulary dictionary</p> required <code>sos_eos</code> <code>int</code> <p>int The index of the  token. <code>None</code> <code>padding_idx</code> <code>int</code> <p>int The index of the padding token.</p> <code>0</code> <code>beam_size</code> <code>int</code> <p>int The number of beams used for each hypothesis sentence.</p> <code>1</code> <code>min_f2t_ratio</code> <code>float or int</code> <p>float The ratio of the hypothesis max length to encoder representation length (feat_maxlen). Postive values mean the relative ratio. Negative values mean the absolute max length of the hypothesis sentence.</p> <code>3.0</code> <code>length_penalty</code> <code>float</code> <p>float The hypothesis score is divided by its length to prevent the short hypothesis. length_penalty is the penalty on the hypothesis length. The larger the value is, the longer hypothesis you will get.</p> <code>1.0</code> <code>temperature</code> <code>float</code> <p>float The temperature coefficient used for calculating the log-softmax probability for the ASR decoder. The higher temperature is (&gt;1), the more even token probability distribution you will get. Vice versa for the lower temperature (&lt;1). Usually, raising the temperature up helps ASR in better decoding. The temperature value needs to be tuned on your validation sets. The common practice is to start from 1.5 and search other values in (1.0, 2.0).</p> <code>1.0</code> <code>eos_filtering</code> <code>bool</code> <p>bool Controls whether the eos filtering is performed. If True, the algorithm will only emit an eos when the eos probability is larger than some times the maximum probability of the other tokens. This function is default to be off since it may reversely aggravate the n-gram phrase repeating problem. It's better to turn it on only when your model suffers from meeting eos very early on many testing cases. reference: 'Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions'     Section 3.1.2 in https://arxiv.org/pdf/1904.02619</p> <code>False</code> <code>eos_threshold</code> <code>float</code> <p>float The eos filtering threshold used for eos filtering. This threshold will be multiplied with the maximum probability of the other tokens when deciding whether to emit an eos. The larger this threshold is (&gt;1), the easier the hypothesis emits an eos. Vice versa for the smaller temperature (&lt;1). The default value 1.5 comes from the reference paper above.</p> <code>1.5</code> <code>ctc_weight</code> <code>float</code> <p>float = 0.0 The weight putted on the CTC scores at each decoding step.</p> <code>0.0</code> <code>ctc_decode_fn</code> <p>= None The CTC forward function for decoding the encoder hidden features.</p> <code>None</code> <code>lm_weight</code> <code>float</code> <p>float = 0.0 The weight putted on the LM scores at each decoding step.</p> <code>0.2</code> <code>lm_temperature</code> <code>float</code> <p>float = 1.0 The temperature coefficient used for calculating the log-softmax probability for the LM decoder.</p> <code>1.0</code> <code>lm_decode_fn</code> <code>LM</code> <p>LM = None The LM forward function for decoding the current partial hypothesis.</p> <code>None</code> <code>ilm_sub_weight</code> <code>float</code> <p>float = 0.0 The weight putted on the subtraction of the inner LM of the ASR model. The inner LM subtraction is used for ASR-LM decoding. ilm_sub_weight is better to be tuned in [0.5*lm_weight, lm_weight].</p> <code>0.0</code> <code>sent_per_beam</code> <code>int</code> <p>int The number of sentences in each beam that are returned in this function. sent_per_beam &gt; 1 is mainly used for data augmentation (under development).</p> <code>1</code> Source code in <code>speechain/infer_func/beam_search.py</code> <pre><code>def beam_searching(\n    enc_feat: torch.Tensor,\n    enc_feat_mask: torch.Tensor,\n    asr_decode_fn,\n    vocab_size: int,\n    sos_eos: int = None,\n    padding_idx: int = 0,\n    beam_size: int = 1,\n    min_f2t_ratio: float or int = 3.0,\n    length_penalty: float = 1.0,\n    temperature: float = 1.0,\n    eos_filtering: bool = False,\n    eos_threshold: float = 1.5,\n    ctc_weight: float = 0.0,\n    ctc_decode_fn=None,\n    ctc_temperature: float = 1.0,\n    lm_weight: float = 0.2,\n    lm_temperature: float = 1.0,\n    lm_decode_fn: LM = None,\n    lm_window_size: int = None,\n    ilm_sub_weight: float = 0.0,\n    sent_per_beam: int = 1,\n):\n    \"\"\"Batch version of beam searching to enable parallel computation. The basic idea is\n    reshaping batch_size sentences into (batch_size * beam_size) sentences.\n\n    However, the hypothesis text probabilities calculated by a batch of inputs and a single input are slightly\n    different due to the model accuracy. in rare cases, the best hypothesis with the highest probability may be different\n    when the beam searching process is performed in the batch level.\n\n    Therefore, batch-level beam searching is mainly used to speed up the pseudo text generation. For model visualization\n    during training or evaluation after training, we recommend you to perform the beam searching for each single input.\n\n    Args:\n        enc_feat: (batch_size, feat_maxlen, enc_dim)\n            The final hidden representations from the encoder.\n        enc_feat_mask: (batch_size, 1, feat_maxlen)\n            The masks for the encoder representations.\n        asr_decode_fn:\n            The function that decodes the hypothesis for one time step and get the next prediction.\n        vocab_size: int\n            The number of tokens in the vocabulary dictionary\n        sos_eos: int\n            The index of the &lt;sos/eos&gt; token.\n        padding_idx: int\n            The index of the padding token.\n        beam_size: int\n            The number of beams used for each hypothesis sentence.\n        min_f2t_ratio: float\n            The ratio of the hypothesis max length to encoder representation length (feat_maxlen).\n            Postive values mean the relative ratio.\n            Negative values mean the absolute max length of the hypothesis sentence.\n        length_penalty: float\n            The hypothesis score is divided by its length to prevent the short hypothesis.\n            length_penalty is the penalty on the hypothesis length.\n            The larger the value is, the longer hypothesis you will get.\n        temperature: float\n            The temperature coefficient used for calculating the log-softmax probability for the ASR decoder. The\n            higher temperature is (&gt;1), the more even token probability distribution you will get. Vice versa for the\n            lower temperature (&lt;1). Usually, raising the temperature up helps ASR in better decoding. The temperature\n            value needs to be tuned on your validation sets. The common practice is to start from 1.5 and search other\n            values in (1.0, 2.0).\n        eos_filtering: bool\n            Controls whether the eos filtering is performed.\n            If True, the algorithm will only emit an eos when the eos probability is larger than some times the\n            maximum probability of the other tokens.\n            This function is default to be off since it may reversely aggravate the n-gram phrase repeating problem.\n            It's better to turn it on only when your model suffers from meeting eos very early on many testing cases.\n            reference: 'Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions'\n                Section 3.1.2 in https://arxiv.org/pdf/1904.02619\n        eos_threshold: float\n            The eos filtering threshold used for eos filtering. This threshold will be multiplied with the\n            maximum probability of the other tokens when deciding whether to emit an eos. The larger this threshold is (&gt;1),\n            the easier the hypothesis emits an eos. Vice versa for the smaller temperature (&lt;1).\n            The default value 1.5 comes from the reference paper above.\n        ctc_weight: float = 0.0\n            The weight putted on the CTC scores at each decoding step.\n        ctc_decode_fn: = None\n            The CTC forward function for decoding the encoder hidden features.\n        lm_weight: float = 0.0\n            The weight putted on the LM scores at each decoding step.\n        lm_temperature: float = 1.0\n            The temperature coefficient used for calculating the log-softmax probability for the LM decoder.\n        lm_decode_fn: LM = None\n            The LM forward function for decoding the current partial hypothesis.\n        ilm_sub_weight: float = 0.0\n            The weight putted on the subtraction of the inner LM of the ASR model. The inner LM subtraction is used for\n            ASR-LM decoding. ilm_sub_weight is better to be tuned in [0.5*lm_weight, lm_weight].\n        sent_per_beam: int\n            The number of sentences in each beam that are returned in this function.\n            sent_per_beam &gt; 1 is mainly used for data augmentation (under development).\n    \"\"\"\n    # --- 0. Arguments Checking --- #\n    if sent_per_beam &gt; 1:\n        raise NotImplementedError(\"Currently, sent_per_beam &gt; 1 is not supported....\")\n    assert (\n        0.0 &lt;= ctc_weight\n    ), f\"ctc_weight should be a non-negative float number, but got ctc_weight={ctc_weight}!\"\n    assert (\n        0.0 &lt;= lm_weight\n    ), f\"lm_weight should be a non-negative float number, but got lm_weight={lm_weight}!\"\n    assert (\n        0.0 &lt;= ilm_sub_weight\n    ), f\"ilm_sub_weight should be a non-negative float number, but got ilm_sub_weight={ilm_sub_weight}!\"\n\n    assert (\n        ctc_temperature &gt; 0.0\n    ), f\"ctc_temperature should be a positive float number, but got ctc_temperature={ctc_temperature}\"\n    assert (\n        temperature &gt; 0.0\n    ), f\"temperature should be a positive float number, but got temperature={temperature}\"\n    assert (\n        lm_temperature &gt; 0.0\n    ), f\"lm_temperature should be a positive float number, but got lm_temperature={lm_temperature}\"\n\n    # --- 1. Reference Initialization --- #\n    batch_size = enc_feat.size(0)\n    enc_feat_len = enc_feat_mask.squeeze(dim=1).sum(dim=-1)\n\n    # hypo_maxlen is uniformly calculated for all the sentences by the longest utterance\n    # Since the utterances in a batch usually have the similar lengths, it won't be a big problem for text decoding\n    feat_maxlen = enc_feat.size(1)\n    hypo_maxlen = (\n        int(feat_maxlen / min_f2t_ratio) if min_f2t_ratio &gt; 0 else int(-min_f2t_ratio)\n    )\n    cuda_device = enc_feat.device\n    if sos_eos is None:\n        sos_eos = vocab_size - 1\n\n    # --- 2. Encoder Feature Reshaping --- #\n    # (batch_size, feat_maxlen, edim) -&gt; (batch_size, 1, feat_maxlen , edim)\n    enc_feat = enc_feat.unsqueeze(1).contiguous()\n    # (batch_size, 1, feat_maxlen , edim) -&gt; (batch_size, beam_size, feat_maxlen, edim)\n    enc_feat = enc_feat.repeat(1, beam_size, 1, 1)\n    # (batch_size, beam_size, feat_maxlen, edim) -&gt; (batch_size \u00d7 beam_size, feat_maxlen, edim)\n    enc_feat = enc_feat.view(-1, enc_feat.size(2), enc_feat.size(3)).contiguous()\n\n    # (batch_size, 1, feat_maxlen) -&gt; (batch_size, 1, 1, feat_maxlen)\n    enc_feat_mask = enc_feat_mask.unsqueeze(1).contiguous()\n    # (batch_size, 1, 1, feat_maxlen) -&gt; (batch_size, beam_size, 1, feat_maxlen)\n    enc_feat_mask = enc_feat_mask.repeat(1, beam_size, 1, 1)\n    # (batch_size, beam_size, 1, feat_maxlen) -&gt; (batch_size \u00d7 beam_size, 1, feat_maxlen)\n    enc_feat_mask = enc_feat_mask.view(\n        -1, enc_feat_mask.size(2), enc_feat_mask.size(3)\n    ).contiguous()\n\n    # --- 3. CTC Initialization --- #\n    if ctc_decode_fn is not None and ctc_weight &gt; 0:\n        ctc_logits = ctc_decode_fn(enc_feat)\n        # CTC should not have any predictions on the sos_eos token\n        ctc_logits[:, :, sos_eos] = minus_inf\n        ctc_scorer = CTCPrefixScorer(\n            x=torch.log_softmax(ctc_logits / ctc_temperature, dim=-1),\n            enc_lens=make_len_from_mask(enc_feat_mask),\n            batch_size=batch_size,\n            beam_size=beam_size,\n            blank_index=padding_idx,\n            eos_index=sos_eos,\n        )\n    else:\n        ctc_scorer = None\n    ctc_memory = None\n\n    # --- 4. Registers Initialization --- #\n    # build a hypothesis container for each sentence in the batch\n    generated_hyps = [\n        BeamHypotheses(beam_size, hypo_maxlen, length_penalty)\n        for _ in range(batch_size)\n    ]\n    # Done flags for all sentences in the batch\n    done = [False for _ in range(batch_size)]\n    # scores of all beam containers (batch_size \u00d7 beam_size,)\n    beam_scores = torch.empty(\n        (batch_size * beam_size,), dtype=torch.float, device=cuda_device\n    )\n    beam_scores.fill_(float(\"-inf\"))\n    # keep only the first to make sure no redundancy.\n    beam_scores.index_fill_(\n        0, torch.arange(batch_size, device=cuda_device) * beam_size, 0.0\n    )\n\n    # start tokens for all sentences in a batch (batch_size \u00d7 beam_size, 1)\n    hypo_text = torch.full(\n        (batch_size * beam_size, 1), sos_eos, dtype=torch.long, device=cuda_device\n    )\n    hypo_text_len = torch.ones(\n        (batch_size * beam_size,), dtype=torch.long, device=cuda_device\n    )\n\n    # --- 5. Beam Searching Algorithm --- #\n    while hypo_text_len.max() &lt; hypo_maxlen:\n        # --- 5.1. Attention-based Decoder Forward --- #\n        # (batch_size \u00d7 beam_size, curr_len) -&gt; (batch_size \u00d7 beam_size, curr_len, vocab_size)\n        curr_outputs = asr_decode_fn(\n            enc_feat=enc_feat,\n            enc_feat_mask=enc_feat_mask,\n            text=hypo_text,\n            text_len=hypo_text_len,\n        )[0].detach()\n        # (batch_size \u00d7 beam_size, curr_len, vocab_size) -&gt; (batch_size \u00d7 beam_size, vocab_size)\n        curr_outputs = curr_outputs[:, -1, :]\n        next_token_scores = torch.log_softmax(curr_outputs / temperature, dim=-1)\n\n        # --- 5.2. CTC Scorer Forward --- #\n        if ctc_scorer is not None:\n            # block blank token\n            next_token_scores[:, padding_idx] = minus_inf\n            ctc_token_scores, ctc_memory = ctc_scorer.forward_step(\n                g=hypo_text[:, 1:], state=ctc_memory\n            )\n            next_token_scores = (\n                1 - ctc_weight\n            ) * next_token_scores + ctc_weight * ctc_token_scores\n\n        # --- 5.3. LM Forward --- #\n        if lm_decode_fn is not None and lm_weight &gt; 0:\n            # (batch_size \u00d7 beam_size, curr_len) -&gt; (batch_size \u00d7 beam_size, curr_len, vocab_size)\n            lm_outputs = lm_decode_fn(\n                text=(\n                    hypo_text\n                    if lm_window_size is None\n                    else hypo_text[:, -lm_window_size:]\n                ),\n                text_len=(\n                    hypo_text_len\n                    if lm_window_size is None\n                    else torch.clamp(hypo_text_len, max=lm_window_size)\n                ),\n            )[0].detach()\n            # (batch_size \u00d7 beam_size, curr_len, vocab_size) -&gt; (batch_size \u00d7 beam_size, vocab_size)\n            lm_next_token_scores = torch.log_softmax(\n                lm_outputs[:, -1, :] / lm_temperature, dim=-1\n            )\n            next_token_scores = next_token_scores + lm_weight * lm_next_token_scores\n\n            # --- 5.3.1. Internal LM Subtraction (only available in LM forward) --- #\n            if ilm_sub_weight &gt; 0:\n                # (batch_size \u00d7 beam_size, curr_len) -&gt; (batch_size \u00d7 beam_size, curr_len, vocab_size)\n                ilm_outputs = asr_decode_fn(\n                    # enc_feat=torch.zeros_like(enc_feat),\n                    enc_feat=torch.zeros(\n                        enc_feat.size(0), 1, enc_feat.size(2), device=enc_feat.device\n                    ),\n                    # enc_feat_mask=enc_feat_mask,\n                    enc_feat_mask=torch.ones(\n                        enc_feat_mask.size(0),\n                        enc_feat_mask.size(1),\n                        1,\n                        dtype=torch.bool,\n                        device=enc_feat_mask.device,\n                    ),\n                    # the window size of internal LM is set to the same one with the external LM\n                    text=(\n                        hypo_text\n                        if lm_window_size is None\n                        else hypo_text[:, -lm_window_size:]\n                    ),\n                    text_len=(\n                        hypo_text_len\n                        if lm_window_size is None\n                        else torch.clamp(hypo_text_len, max=lm_window_size)\n                    ),\n                )[0].detach()\n                # (batch_size \u00d7 beam_size, curr_len, vocab_size) -&gt; (batch_size \u00d7 beam_size, vocab_size)\n                ilm_next_token_scores = torch.log_softmax(ilm_outputs[:, -1, :], dim=-1)\n                next_token_scores = (\n                    next_token_scores - ilm_sub_weight * ilm_next_token_scores\n                )\n\n        # Calculate the score of the obtained token sequences so far\n        next_scores = next_token_scores + beam_scores.unsqueeze(-1).expand_as(\n            next_token_scores\n        )\n\n        # Arrange all beams of the same sentence into a single row to pick up the best predictions across all beams.\n        # (batch_size \u00d7 beam_size, vocab_size) -&gt; (batch_size, beam_size \u00d7 vocab_size)\n        next_scores = next_scores.view(batch_size, -1)\n\n        # Pick up beam_size \u00d7 (beam_size + 1) tokens from (beam_size * vocab_size) candidates for algorithm robustness\n        # mainly two usage:\n        #   1. for different tokens of each beam in the first time step\n        #   2. when meeting an eos token, complement the beams with the rest predictions\n        assert (\n            beam_size + 1 &lt;= vocab_size\n        ), \"beam_size cannot be larger than vocab_size - 1 (exclude &lt;sos/eos&gt;)!\"\n        # next_scores, next_tokens = torch.topk(next_scores, beam_size * (beam_size + 1), dim=1, largest=True, sorted=True)\n        next_scores, next_tokens = torch.topk(\n            next_scores, 2 * beam_size, dim=1, largest=True, sorted=True\n        )\n\n        # batch-level beam results for all sentence, each element is a tri-tuple (score, token_id, effective_beam_id)\n        next_batch_beam = []\n        # looping each sentence in a batch\n        for batch_idx in range(batch_size):\n            # if the current sentence is already finished, adding a padding token and continue\n            if done[batch_idx]:\n                # for the finished sentence, the padding token is added\n                next_batch_beam.extend([(0, padding_idx, 0)] * beam_size)\n                continue\n\n            # sentence-level beam results for all beams in the current sentence,\n            # each element is a tri-tuple (score, token_id, effective_beam_id)\n            next_sent_beam = []\n            # looping each beam, there are (beam_size * batch_size) candidate predictions in total.\n            for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(\n                zip(next_tokens[batch_idx], next_scores[batch_idx])\n            ):\n                # the index number of the beam in a single sentence, range from 0 to beam_size-1\n                # beam_id = beam_token_id // vocab_size\n                beam_id = torch.div(beam_token_id, vocab_size, rounding_mode=\"floor\")\n                # the index number of the real token, range from 0 to vocab_size-1\n                token_id = beam_token_id % vocab_size\n                # the index number of the beam across the current batch\n                # \u2208 {batch_idx * beam_size, ..., (batch_idx + 1) * beam_size - 1}\n                effective_beam_id = batch_idx * beam_size + beam_id\n\n                # if the eos token is met, the predictions will be either saved as a hypothesis or simple removed.\n                if token_id.item() == sos_eos:\n                    # the top beam_size elements in next_tokens have the largest prob,\n                    # so the ones other than the first beam_size elements will be ignored even though eos is met.\n                    if beam_token_rank &gt;= beam_size:\n                        continue\n                    # conduct EOS filtering\n                    elif eos_filtering:\n                        eos_score = next_token_scores[effective_beam_id][sos_eos]\n                        # the largest token score other than eos is the reference score\n                        vocab_idx = torch.arange(0, vocab_size, dtype=torch.int)\n                        ref_score = next_token_scores[effective_beam_id][\n                            vocab_idx != sos_eos\n                        ].max()\n                        # only consider the eos whose score is larger than eos_threshold * reference score\n                        if eos_score &lt;= eos_threshold * ref_score:\n                            continue\n\n                    generated_hyps[batch_idx].add(\n                        hyp=hypo_text[effective_beam_id][1:].detach().cpu(),\n                        sum_logprobs=beam_token_score.item(),\n                    )\n                # add the predictions into the temporary results.\n                else:\n                    # # make sure that different tokens are selected for different beams in the first time step\n                    # add_flag = True\n                    # if hypo_text_len.max() == 1 and len(next_sent_beam) != 0:\n                    #     for item in next_sent_beam:\n                    #         if item[1] == token_id or item[2] == effective_beam_id:\n                    #             add_flag = False\n                    #             break\n                    # if add_flag:\n                    #     next_sent_beam.append((beam_token_score, token_id, effective_beam_id))\n                    next_sent_beam.append(\n                        (beam_token_score, token_id, effective_beam_id)\n                    )\n\n                # only get beam_size predictions from beam_size * beam_size candidates\n                if len(next_sent_beam) == beam_size:\n                    break\n\n            # update the done flag of the current sentence\n            if not done[batch_idx]:\n                done[batch_idx] = generated_hyps[batch_idx].is_done(\n                    best_sum_logprobs=next_scores[batch_idx].max().item(),\n                    curr_len=hypo_text_len[\n                        batch_idx * beam_size : (batch_idx + 1) * beam_size\n                    ]\n                    .max()\n                    .item()\n                    - 1,\n                )\n            next_batch_beam.extend(next_sent_beam)\n\n        if all(done):\n            break\n\n        # Summary of the results of the selected predictions for all beams in the current time step\n        # (batch_size * beam_size), beam score and beam index\n        beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n        beam_tokens = hypo_text.new([x[1] for x in next_batch_beam])\n        beam_idx = hypo_text.new([x[2] for x in next_batch_beam])\n\n        # Update the length of generated sentences\n        hypo_text = torch.cat([hypo_text[beam_idx], beam_tokens.unsqueeze(1)], dim=1)\n        hypo_text_len = torch.sum(hypo_text != padding_idx, dim=-1)\n\n        # align encoder_out with input_tokens\n        enc_feat, enc_feat_mask = enc_feat[beam_idx], enc_feat_mask[beam_idx]\n        if ctc_memory is not None:\n            token_idx = beam_tokens.view(batch_size, beam_size) + (\n                torch.arange(beam_size, device=cuda_device) * vocab_size\n            ).unsqueeze(0)\n            ctc_memory = ctc_scorer.permute_mem(ctc_memory, beam_idx, token_idx)\n\n    # --- 6. Post-processing --- #\n    # for the predictions that end without an eos token at the end because of the max length\n    for batch_idx in range(batch_size):\n        # skip the sentences that get enough hypotheses ending with eos\n        if done[batch_idx]:\n            continue\n        # check whether they can be added into final beam searching results\n        for beam_id in range(beam_size):\n            effective_beam_id = batch_idx * beam_size + beam_id\n            final_score = beam_scores[effective_beam_id].item()\n            final_tokens = hypo_text[effective_beam_id][1:].detach().cpu()\n            generated_hyps[batch_idx].add(final_tokens, final_score)\n\n    # --- 7. Length Calculation --- #\n    hypo_text_len = hypo_text_len.new(batch_size * sent_per_beam)\n    hypo_text_list = []\n    hypo_text_confid = []\n    # looping each sentence in the current batch\n    for i, hypotheses in enumerate(generated_hyps):\n        sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])\n        # looping each beam for the given sentence\n        for j in range(sent_per_beam):\n            effective_batch_idx = sent_per_beam * i + j\n            _hypo = sorted_hyps.pop()\n\n            # remove the sos tokens at the beginning of hyp\n            best_hyp = _hypo[1]\n            hypo_text_len[effective_batch_idx] = len(best_hyp)\n            hypo_text_list.append(best_hyp)\n            hypo_text_confid.append(_hypo[0])\n\n    # --- 8. Padding --- #\n    # some sentences are shorter than the maximal length\n    if hypo_text_len.min().item() != hypo_text_len.max().item():\n        sent_max_len = min(hypo_text_len.max().item(), hypo_maxlen)\n        hypo_text = torch.full(\n            (batch_size * sent_per_beam, sent_max_len),\n            padding_idx,\n            dtype=torch.long,\n            device=cuda_device,\n        )\n        for i in range(len(hypo_text_list)):\n            # padding pseudo transcripts\n            hypo_text[i, : hypo_text_len[i]] = hypo_text_list[i]\n    # all the sentences are equally long\n    else:\n        hypo_text = torch.stack(hypo_text_list).to(cuda_device)\n\n    return dict(\n        hypo_text=hypo_text,\n        hypo_text_len=hypo_text_len,\n        feat_token_len_ratio=enc_feat_len / (hypo_text_len + 1e-10),\n        hypo_text_confid=torch.Tensor(hypo_text_confid),\n    )\n</code></pre>"},{"location":"reference/infer_func/ctc_decoding/","title":"ctc_decoding","text":""},{"location":"reference/infer_func/ctc_decoding/#infer_func.ctc_decoding.CTCPrefixScorer","title":"<code>CTCPrefixScorer</code>","text":"<p>This class implements the CTC prefix scorer of Algorithm 2 in reference: https://www.merl.com/publications/docs/TR2017-190.pdf. Official implementation: https://github.com/espnet/espnet/blob/master/espnet/nets/ctc_prefix_score.py Arguments</p> <p>x : torch.Tensor     The encoder states. enc_lens : torch.Tensor     The actual length of each enc_states sequence. batch_size : int     The size of the batch. beam_size : int     The width of beam. blank_index : int     The index of the blank token. eos_index : int     The index of the end-of-sequence (eos) token. ctc_window_size: int     Compute the ctc scores over the time frames using windowing based on attention peaks.     If 0, no windowing applied.</p> Source code in <code>speechain/infer_func/ctc_decoding.py</code> <pre><code>class CTCPrefixScorer:\n    \"\"\"This class implements the CTC prefix scorer of Algorithm 2 in\n    reference: https://www.merl.com/publications/docs/TR2017-190.pdf.\n    Official implementation: https://github.com/espnet/espnet/blob/master/espnet/nets/ctc_prefix_score.py\n    Arguments\n    ---------\n    x : torch.Tensor\n        The encoder states.\n    enc_lens : torch.Tensor\n        The actual length of each enc_states sequence.\n    batch_size : int\n        The size of the batch.\n    beam_size : int\n        The width of beam.\n    blank_index : int\n        The index of the blank token.\n    eos_index : int\n        The index of the end-of-sequence (eos) token.\n    ctc_window_size: int\n        Compute the ctc scores over the time frames using windowing based on attention peaks.\n        If 0, no windowing applied.\n    \"\"\"\n\n    def __init__(\n        self,\n        x,\n        enc_lens,\n        batch_size,\n        beam_size,\n        blank_index,\n        eos_index,\n    ):\n        self.blank_index = blank_index\n        self.eos_index = eos_index\n        self.max_enc_len = x.size(1)\n        self.batch_size = batch_size\n        self.beam_size = beam_size\n        self.vocab_size = x.size(-1)\n        self.device = x.device\n        self.minus_inf = -1e20\n        self.last_frame_index = enc_lens - 1\n\n        # mask frames &gt; enc_lens\n        mask = (\n            ~make_mask_from_len(enc_lens, return_3d=False)\n            .unsqueeze(-1)\n            .expand(-1, -1, self.vocab_size)\n        )\n        x.masked_fill_(mask, self.minus_inf)\n        x[:, :, self.blank_index] = x[:, :, self.blank_index].masked_fill_(\n            mask[:, :, self.blank_index], 0\n        )\n\n        # dim=0: xnb, nonblank posteriors, dim=1: xb, blank posteriors\n        xnb = x.transpose(0, 1)\n        xb = xnb[:, :, self.blank_index].unsqueeze(2).expand(-1, -1, self.vocab_size)\n\n        # (2, L, batch_size * beam_size, vocab_size)\n        self.x = torch.stack([xnb, xb])\n\n        # The first index of each sentence.\n        self.beam_offset = torch.arange(batch_size, device=self.device) * self.beam_size\n        # The first index of each candidates.\n        self.cand_offset = (\n            torch.arange(batch_size, device=self.device) * self.vocab_size\n        )\n\n    def forward_step(self, g, state):\n        \"\"\"This method if one step of forwarding operation for the prefix ctc scorer.\n\n        Arguments\n        ---------\n        g : torch.Tensor\n            The tensor of prefix label sequences, h = g + c.\n        state : tuple\n            Previous ctc states.\n        \"\"\"\n\n        prefix_length = g.size(1)\n        last_char = [gi[-1] for gi in g] if prefix_length &gt; 0 else [0] * len(g)\n        if state is None:\n            # r_prev: (L, 2, batch_size * beam_size)\n            r_prev = torch.full(\n                (self.max_enc_len, 2, self.batch_size * self.beam_size),\n                self.minus_inf,\n                device=self.device,\n            )\n\n            # r_prev[:, 0] = r^n(g), r_prev[:, 1] = r^b(g)\n            # Accumulate blank posteriors at each step\n            r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank_index], dim=0)\n            psi_prev = 0.0\n        else:\n            r_prev, psi_prev = state\n\n        # Prepare forward probs\n        # r[:, 0] = r^n(h), r[:, 1] = r^b(h)\n        r = torch.full(\n            (\n                self.max_enc_len,\n                2,\n                self.batch_size * self.beam_size,\n                self.vocab_size,\n            ),\n            self.minus_inf,\n            device=self.device,\n        )\n\n        # (Alg.2-6)\n        if prefix_length == 0:\n            r[0, 0] = self.x[0, 0]\n        # (Alg.2-10): phi = prev_nonblank + prev_blank = r_t-1^nb(g) + r_t-1^b(g)\n        r_sum = torch.logsumexp(r_prev, 1)\n        phi = r_sum.unsqueeze(2).repeat(1, 1, self.vocab_size)\n\n        # (Alg.2-10): if last token of prefix g in candidates, phi = prev_b + 0\n        for i in range(self.batch_size * self.beam_size):\n            phi[:, i, last_char[i]] = r_prev[:, 1, i]\n\n        # Start, end frames for scoring (|g| &lt; |h|).\n        # Scoring based on attn peak\n        start = max(1, prefix_length)\n        end = self.max_enc_len\n\n        # Compute forward prob log(r_t^nb(h)) and log(r_t^b(h)):\n        for t in range(start, end):\n            # (Alg.2-11): dim=0, p(h|cur step is nonblank) = [p(prev step=y) + phi] * p(c)\n            rnb_prev = r[t - 1, 0]\n            # (Alg.2-12): dim=1, p(h|cur step is blank) = [p(prev step is blank) + p(prev step is nonblank)] * p(blank)\n            rb_prev = r[t - 1, 1]\n            r_ = torch.stack([rnb_prev, phi[t - 1], rnb_prev, rb_prev]).view(\n                2, 2, self.batch_size * self.beam_size, self.vocab_size\n            )\n            r[t] = torch.logsumexp(r_, 1) + self.x[:, t]\n\n        # Compute the predix prob, psi\n        psi_init = r[start - 1, 0].unsqueeze(0)\n        # phi is prob at t-1 step, shift one frame and add it to the current prob p(c)\n        phix = torch.cat((phi[0].unsqueeze(0), phi[:-1]), dim=0) + self.x[0]\n        # (Alg.2-13): psi = psi + phi * p(c)\n        psi = torch.logsumexp(torch.cat((phix[start:end], psi_init), dim=0), dim=0)\n\n        # (Alg.2-3): if c = &lt;eos&gt;, psi = log(r_T^n(g) + r_T^b(g)), where T is the length of max frames\n        for i in range(self.batch_size * self.beam_size):\n            psi[i, self.eos_index] = r_sum[\n                self.last_frame_index[i // self.beam_size], i\n            ]\n\n        # Exclude blank probs for joint scoring\n        psi[:, self.blank_index] = self.minus_inf\n\n        return psi - psi_prev, (r, psi)\n\n    def permute_mem(self, memory, beam_idx, token_idx):\n        \"\"\"This method permutes the CTC model memory to synchronize the memory index\n        with the current output.\n\n        Arguments\n        ---------\n        memory : No limit\n            The memory variable to be permuted.\n        index : torch.Tensor\n            The index of the previous path.\n        Return\n        ------\n        The variable of the memory being permuted.\n        \"\"\"\n        r, psi = memory\n        r, psi = r[:, :, beam_idx], psi[beam_idx]\n        # The index of top-K vocab came from in (t-1) timesteps.\n        best_index = (\n            token_idx\n            + (self.beam_offset.unsqueeze(1).expand_as(token_idx) * self.vocab_size)\n        ).view(-1)\n        # synchronize forward prob\n        psi = torch.index_select(psi.view(-1), dim=0, index=best_index)\n        psi = (\n            psi.view(-1, 1)\n            .repeat(1, self.vocab_size)\n            .view(self.batch_size * self.beam_size, self.vocab_size)\n        )\n\n        # synchronize ctc states\n        r = torch.index_select(\n            r.view(-1, 2, self.batch_size * self.beam_size * self.vocab_size),\n            dim=-1,\n            index=best_index,\n        )\n        r = r.view(-1, 2, self.batch_size * self.beam_size)\n\n        return r, psi\n</code></pre>"},{"location":"reference/infer_func/ctc_decoding/#infer_func.ctc_decoding.CTCPrefixScorer.forward_step","title":"<code>forward_step(g, state)</code>","text":"<p>This method if one step of forwarding operation for the prefix ctc scorer.</p>"},{"location":"reference/infer_func/ctc_decoding/#infer_func.ctc_decoding.CTCPrefixScorer.forward_step--arguments","title":"Arguments","text":"<p>g : torch.Tensor     The tensor of prefix label sequences, h = g + c. state : tuple     Previous ctc states.</p> Source code in <code>speechain/infer_func/ctc_decoding.py</code> <pre><code>def forward_step(self, g, state):\n    \"\"\"This method if one step of forwarding operation for the prefix ctc scorer.\n\n    Arguments\n    ---------\n    g : torch.Tensor\n        The tensor of prefix label sequences, h = g + c.\n    state : tuple\n        Previous ctc states.\n    \"\"\"\n\n    prefix_length = g.size(1)\n    last_char = [gi[-1] for gi in g] if prefix_length &gt; 0 else [0] * len(g)\n    if state is None:\n        # r_prev: (L, 2, batch_size * beam_size)\n        r_prev = torch.full(\n            (self.max_enc_len, 2, self.batch_size * self.beam_size),\n            self.minus_inf,\n            device=self.device,\n        )\n\n        # r_prev[:, 0] = r^n(g), r_prev[:, 1] = r^b(g)\n        # Accumulate blank posteriors at each step\n        r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank_index], dim=0)\n        psi_prev = 0.0\n    else:\n        r_prev, psi_prev = state\n\n    # Prepare forward probs\n    # r[:, 0] = r^n(h), r[:, 1] = r^b(h)\n    r = torch.full(\n        (\n            self.max_enc_len,\n            2,\n            self.batch_size * self.beam_size,\n            self.vocab_size,\n        ),\n        self.minus_inf,\n        device=self.device,\n    )\n\n    # (Alg.2-6)\n    if prefix_length == 0:\n        r[0, 0] = self.x[0, 0]\n    # (Alg.2-10): phi = prev_nonblank + prev_blank = r_t-1^nb(g) + r_t-1^b(g)\n    r_sum = torch.logsumexp(r_prev, 1)\n    phi = r_sum.unsqueeze(2).repeat(1, 1, self.vocab_size)\n\n    # (Alg.2-10): if last token of prefix g in candidates, phi = prev_b + 0\n    for i in range(self.batch_size * self.beam_size):\n        phi[:, i, last_char[i]] = r_prev[:, 1, i]\n\n    # Start, end frames for scoring (|g| &lt; |h|).\n    # Scoring based on attn peak\n    start = max(1, prefix_length)\n    end = self.max_enc_len\n\n    # Compute forward prob log(r_t^nb(h)) and log(r_t^b(h)):\n    for t in range(start, end):\n        # (Alg.2-11): dim=0, p(h|cur step is nonblank) = [p(prev step=y) + phi] * p(c)\n        rnb_prev = r[t - 1, 0]\n        # (Alg.2-12): dim=1, p(h|cur step is blank) = [p(prev step is blank) + p(prev step is nonblank)] * p(blank)\n        rb_prev = r[t - 1, 1]\n        r_ = torch.stack([rnb_prev, phi[t - 1], rnb_prev, rb_prev]).view(\n            2, 2, self.batch_size * self.beam_size, self.vocab_size\n        )\n        r[t] = torch.logsumexp(r_, 1) + self.x[:, t]\n\n    # Compute the predix prob, psi\n    psi_init = r[start - 1, 0].unsqueeze(0)\n    # phi is prob at t-1 step, shift one frame and add it to the current prob p(c)\n    phix = torch.cat((phi[0].unsqueeze(0), phi[:-1]), dim=0) + self.x[0]\n    # (Alg.2-13): psi = psi + phi * p(c)\n    psi = torch.logsumexp(torch.cat((phix[start:end], psi_init), dim=0), dim=0)\n\n    # (Alg.2-3): if c = &lt;eos&gt;, psi = log(r_T^n(g) + r_T^b(g)), where T is the length of max frames\n    for i in range(self.batch_size * self.beam_size):\n        psi[i, self.eos_index] = r_sum[\n            self.last_frame_index[i // self.beam_size], i\n        ]\n\n    # Exclude blank probs for joint scoring\n    psi[:, self.blank_index] = self.minus_inf\n\n    return psi - psi_prev, (r, psi)\n</code></pre>"},{"location":"reference/infer_func/ctc_decoding/#infer_func.ctc_decoding.CTCPrefixScorer.permute_mem","title":"<code>permute_mem(memory, beam_idx, token_idx)</code>","text":"<p>This method permutes the CTC model memory to synchronize the memory index with the current output.</p>"},{"location":"reference/infer_func/ctc_decoding/#infer_func.ctc_decoding.CTCPrefixScorer.permute_mem--arguments","title":"Arguments","text":"<p>memory : No limit     The memory variable to be permuted. index : torch.Tensor     The index of the previous path. Return</p> <p>The variable of the memory being permuted.</p> Source code in <code>speechain/infer_func/ctc_decoding.py</code> <pre><code>def permute_mem(self, memory, beam_idx, token_idx):\n    \"\"\"This method permutes the CTC model memory to synchronize the memory index\n    with the current output.\n\n    Arguments\n    ---------\n    memory : No limit\n        The memory variable to be permuted.\n    index : torch.Tensor\n        The index of the previous path.\n    Return\n    ------\n    The variable of the memory being permuted.\n    \"\"\"\n    r, psi = memory\n    r, psi = r[:, :, beam_idx], psi[beam_idx]\n    # The index of top-K vocab came from in (t-1) timesteps.\n    best_index = (\n        token_idx\n        + (self.beam_offset.unsqueeze(1).expand_as(token_idx) * self.vocab_size)\n    ).view(-1)\n    # synchronize forward prob\n    psi = torch.index_select(psi.view(-1), dim=0, index=best_index)\n    psi = (\n        psi.view(-1, 1)\n        .repeat(1, self.vocab_size)\n        .view(self.batch_size * self.beam_size, self.vocab_size)\n    )\n\n    # synchronize ctc states\n    r = torch.index_select(\n        r.view(-1, 2, self.batch_size * self.beam_size * self.vocab_size),\n        dim=-1,\n        index=best_index,\n    )\n    r = r.view(-1, 2, self.batch_size * self.beam_size)\n\n    return r, psi\n</code></pre>"},{"location":"reference/infer_func/tts_decoding/","title":"tts_decoding","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/infer_func/tts_decoding/#infer_func.tts_decoding.auto_regression","title":"<code>auto_regression(enc_text, enc_text_mask, reduction_factor, decode_one_step, feat_dim, spk_ids=None, spk_feat=None, stop_threshold=0.5, maxlen_ratio=10.0, continual_steps=0, use_before=False)</code>","text":"<p>Auto-regressive acoustic feature generation using a transformer-based TTS model.</p> <p>Parameters:</p> Name Type Description Default <code>enc_text</code> <code>Tensor</code> <p>Encoded text tensor.</p> required <code>enc_text_mask</code> <code>Tensor</code> <p>Mask for encoded text tensor.</p> required <code>reduction_factor</code> <code>int</code> <p>Reduction factor for acoustic features.</p> required <code>decode_one_step</code> <code>callable</code> <p>Function for decoding one step of the model.</p> required <code>feat_dim</code> <code>int</code> <p>Dimensionality of acoustic features.</p> required <code>spk_ids</code> <code>Tensor</code> <p>Speaker ID tensor.</p> <code>None</code> <code>spk_feat</code> <code>Tensor</code> <p>Speaker feature tensor.</p> <code>None</code> <code>stop_threshold</code> <code>float</code> <p>Threshold for stop token prediction.</p> <code>0.5</code> <code>maxlen_ratio</code> <code>float</code> <p>Maximum length ratio for generated features.</p> <code>10.0</code> <code>continual_steps</code> <code>int</code> <p>Number of steps to continue generation after stop token is predicted.</p> <code>0</code> <code>use_before</code> <code>bool</code> <p>Whether to use the decoder's \"before\" features for generation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary containing synthetic acoustic features, their lengths, and the ratio of feature lengths to</p> <p>input text lengths.</p> Source code in <code>speechain/infer_func/tts_decoding.py</code> <pre><code>def auto_regression(\n    enc_text: torch.Tensor,\n    enc_text_mask: torch.Tensor,\n    reduction_factor: int,\n    decode_one_step,\n    feat_dim: int,\n    spk_ids: torch.Tensor = None,\n    spk_feat: torch.Tensor = None,\n    stop_threshold: float = 0.5,\n    maxlen_ratio: int or float = 10.0,\n    continual_steps: int = 0,\n    use_before: bool = False,\n):\n    \"\"\"Auto-regressive acoustic feature generation using a transformer-based TTS model.\n\n    Args:\n        enc_text (torch.Tensor):\n            Encoded text tensor.\n        enc_text_mask (torch.Tensor):\n            Mask for encoded text tensor.\n        reduction_factor (int):\n            Reduction factor for acoustic features.\n        decode_one_step (callable):\n            Function for decoding one step of the model.\n        feat_dim (int):\n            Dimensionality of acoustic features.\n        spk_ids (torch.Tensor):\n            Speaker ID tensor.\n        spk_feat (torch.Tensor):\n            Speaker feature tensor.\n        stop_threshold (float):\n            Threshold for stop token prediction.\n        maxlen_ratio (float):\n            Maximum length ratio for generated features.\n        continual_steps (int):\n            Number of steps to continue generation after stop token is predicted.\n        use_before (bool):\n            Whether to use the decoder's \"before\" features for generation.\n\n    Returns:\n        dict: Dictionary containing synthetic acoustic features, their lengths, and the ratio of feature lengths to\n        input text lengths.\n    \"\"\"\n    # --- Initialization Stage --- #\n    batch_size = enc_text.size(0)\n    enc_text_len = enc_text_mask.sum(dim=-1).squeeze()\n    logits_threshold = -math.log(1 / stop_threshold - 1)\n\n    # Different from the beam searching, syn_feat_maxlen is individually calculated for each utterance.\n    # Since the utterances in a batch usually have the similar lengths for efficient batch-level decoding,\n    # the text lengths are very likely to vary in a large range especially for subword and word tokenizers.\n    # + 1 here is to consider the silence frames at the beginning\n    hypo_feat_maxlen = enc_text_len * maxlen_ratio / reduction_factor + 1\n    cuda_device = enc_text.device\n\n    # Initial silence inputs as the first frames at the beginning of TTS decoding\n    hypo_feat = torch.zeros(\n        (batch_size, 1, feat_dim), dtype=torch.float, device=cuda_device\n    )\n    hypo_feat_len = torch.ones(batch_size, dtype=torch.int, device=cuda_device)\n\n    # --- Auto-Regressive Acoustic Feature Generation --- #\n    stop_flags = torch.zeros(batch_size, dtype=torch.bool, device=cuda_device)\n    stop_points = torch.zeros(batch_size, dtype=torch.int, device=cuda_device)\n    # keep looping until all the synthetic utterances in the batch meet their stop flags\n    while stop_flags.sum() &lt; batch_size:\n        pred_stop, pred_feat_before, pred_feat_after, _, _, _, _, _ = decode_one_step(\n            enc_text=enc_text,\n            enc_text_mask=enc_text_mask,\n            feat=hypo_feat,\n            feat_len=hypo_feat_len,\n            spk_feat=spk_feat,\n            spk_ids=spk_ids,\n            is_test=True,\n        )\n\n        # attach the new synthetic frames to the end of synthetic frames obtained so far\n        # (batch_size, curr_len, feat_dim) + (batch_size, 1, feat_dim) = (batch_size, curr_len + 1, feat_dim)\n        pred_feat = (\n            pred_feat_before[:, -1].unsqueeze(1)\n            if use_before\n            else pred_feat_after[:, -1].unsqueeze(1)\n        )\n        # attach the silence to the utterances that has already been finished\n        pred_feat[stop_flags] = 0\n        hypo_feat = torch.cat([hypo_feat, pred_feat], dim=1)\n        hypo_feat_len[~stop_flags] += 1\n\n        # update the stop flags for all the utterances\n        curr_steps = hypo_feat.size(1)\n        pred_stop = pred_stop[:, -1].squeeze()\n        # update the stop points where the stop token is met at the first time only\n        stop_points[(pred_stop &gt; logits_threshold) &amp; (stop_points == 0)] = curr_steps\n        # there are two stop conditions:\n        # 1. stop token is met and continual_steps of frames have been generated\n        # 2. maxlen of this utterance is met\n        stop_flags = (\n            (stop_points != 0) &amp; (curr_steps &gt;= stop_points + continual_steps)\n        ) | (hypo_feat_len &gt;= hypo_feat_maxlen)\n\n    # remove the redundant silence parts at the beginning of the synthetic frames\n    # the silence parts at the end are not removed here\n    # hypo_feat should be kept in the form of a matrix for a faster vocoder processing\n    hypo_feat, hypo_feat_len = hypo_feat[:, 1:], hypo_feat_len - 1\n\n    # reduction_factor recovery\n    if reduction_factor &gt; 1:\n        assert feat_dim % reduction_factor == 0\n        hypo_feat = hypo_feat.reshape(\n            batch_size,\n            hypo_feat.size(1) * reduction_factor,\n            feat_dim // reduction_factor,\n        )\n        hypo_feat_len *= reduction_factor\n\n    return dict(\n        hypo_feat=hypo_feat,\n        hypo_feat_len=hypo_feat_len,\n        feat_token_len_ratio=hypo_feat_len / (enc_text_len + 1e-10),\n    )\n</code></pre>"},{"location":"reference/iterator/","title":"iterator","text":""},{"location":"reference/iterator/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator","title":"<code>Iterator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Iterator is the base class that takes charge of grouping data instances into batches for training or testing models. Each iterator has a built-in speechain.dataset.Dataset object as one of its member variables. Actually, an Iterator object cannot directly access the data instances in the built- in Dataset object but maintains a batching view of the indices of the data instances used for model training or testing.</p> <p>The initialization of the built-in Dataset object is done automatically during the initialization of the iterator. At the beginning of each epoch, the iterator generates a <code>torch.utils.data.DataLoader</code> object to fetch the batches of data instances from the disk.</p> <p>The iterators are divided into 3 groups: train, valid, and test. In each group, 2 or more iterator objects can be constructed so that there could be multiple data-label pairs in a single batch.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>class Iterator(ABC):\n    \"\"\"Iterator is the base class that takes charge of grouping data instances into\n    batches for training or testing models. Each iterator has a built-in\n    speechain.dataset.Dataset object as one of its member variables. Actually, an\n    Iterator object cannot directly access the data instances in the built- in Dataset\n    object but maintains a batching view of the indices of the data instances used for\n    model training or testing.\n\n    The initialization of the built-in Dataset object is done automatically during the initialization of the iterator.\n    At the beginning of each epoch, the iterator generates a `torch.utils.data.DataLoader` object to fetch the batches\n    of data instances from the disk.\n\n    The iterators are divided into 3 groups: train, valid, and test. In each group, 2 or more iterator objects can be\n    constructed so that there could be multiple data-label pairs in a single batch.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_type: str,\n        dataset_conf: Dict,\n        batches_per_epoch: int = None,\n        data_len: str or List[str] = None,\n        group_info: Dict[str, str or List[str]] = None,\n        is_descending: bool or None = True,\n        shuffle: bool = True,\n        seed: int = 0,\n        ngpu: int = 1,\n        num_workers: int = 1,\n        same_worker_seed: bool = False,\n        pin_memory: bool = True,\n        distributed: bool = False,\n        **iter_conf,\n    ):\n        \"\"\"The general initialization function of all the Iterator classes. Dataset\n        initialization is automatically done here by the given dataset_type and\n        dataset_conf.\n\n        In this initialization function, each iterator subclass should override a hook function batches_generate_fn()\n        to generate the batching view of data instances in the built-in Dataset object based on their own data batching\n        strategy.\n\n        Args:\n            dataset_type: str\n                Query string to pick up the target Dataset subclass in `speechain/dataset/`\n            dataset_conf: Dict\n                Dataset configuration for its automatic initialization\n            batches_per_epoch: int = None\n                The number of batches in each epoch. This number can be either smaller or larger than the real batch\n                number. If not given (None), all batches will be used in each epoch.\n            is_descending: bool = True\n                Whether the batches are sorted in the descending order by the length (True) or in the ascending order\n                (False). If this argument is given as None, no sorting is done for involved data instances.\n            data_len: str or List[str] = None\n                The absolute path of the data length file. Multiple length files can be given in a list, but they\n                should contain non-overlapping data instances.\n            group_info: Dict[str, str or List[str]] = None\n                The dictionary of paths for the 'idx2data' files used for group-wise evaluation results visualization.\n            shuffle: bool = True\n                Whether the batches are shuffled at the beginning of each epoch.\n            seed: int = 0\n                Random seed for iterator initialization.\n                It will be used to\n                    1. shuffle batches before giving to the Dataloader of each epoch.\n                    2. initialize the workers of the Dataloader for the reproducibility.\n                This argument is automatically given by the experiment environment configuration.\n            ngpu: int = 1\n                The number of GPUs used to train or test models. The GPU number is used to ensure that each GPU process\n                in the DDP mode has the batches with the same number of data instances.\n                This argument is automatically given by the experiment environment configuration.\n            num_workers: int = 1\n                Number of workers for the Dataloader.\n                This argument is automatically given by the experiment environment configuration.\n            pin_memory: bool = False\n                Whether pin_memory trick is used in the Dataloader.\n                This argument is automatically given by the experiment environment configuration.\n            distributed: bool = False\n                Whether DDP is used to distribute the model.\n                This argument is automatically given by the experiment environment configuration.\n            **iter_conf:\n                iterator configuration for customized batch generation\n        \"\"\"\n        # initialize the built-in dataset of the iterator\n        dataset_class = import_class(\"speechain.dataset.\" + dataset_type)\n        self.dataset = dataset_class(**dataset_conf)\n\n        # initialize the general part of the iterator\n        if batches_per_epoch is not None:\n            assert (\n                batches_per_epoch &gt; 0\n            ), f\"batches_per_epoch must be a positive number, but got {batches_per_epoch}.\"\n        self.batches_per_epoch = (\n            int(batches_per_epoch)\n            if batches_per_epoch is not None\n            else batches_per_epoch\n        )\n        self.is_descending = is_descending\n        self.shuffle = shuffle\n        self.seed = seed\n        self.ngpu = ngpu\n        self.num_workers = num_workers\n        self.same_worker_seed = same_worker_seed\n        self.pin_memory = pin_memory\n        self.distributed = distributed\n\n        # --- 1. Loading the Data Length Information --- #\n        if data_len is None:\n            data_len = self.dataset.data_len\n\n        # initialize the data lengths if given\n        if data_len is not None:\n            # remain the original order of the data indices if is_descending not specified\n            self.data_len = (\n                load_idx2data_file(data_len, int)\n                if not isinstance(data_len, Dict)\n                else data_len\n            )\n\n            # check the data index in data_len and self.dataset\n            data_len_keys, dataset_keys = set(self.data_len.keys()), set(\n                self.dataset.get_data_index()\n            )\n            # delete the redundant key-value pairs in data_len\n            redundant_keys = data_len_keys.difference(dataset_keys)\n            if len(redundant_keys) &gt; 0:\n                warnings.warn(\n                    f\"There are {len(redundant_keys)} redundant keys that exist in data_len but not in main_data! \"\n                    f\"If you are using data_selection in data_cfg, this may not be a problem.\"\n                )\n                for redundant_key in redundant_keys:\n                    self.data_len.pop(redundant_key)\n            # delete the redundant key-value pairs in self.dataset\n            redundant_keys = dataset_keys.difference(data_len_keys)\n            if len(redundant_keys) &gt; 0:\n                warnings.warn(\n                    f\"There are {len(redundant_keys)} redundant keys that exist in main_data but not in data_len! \"\n                    f\"If you are using data_selection in data_cfg, this may not be a problem.\"\n                )\n                for redundant_key in dataset_keys.difference(data_len_keys):\n                    self.dataset.remove_data_by_index(redundant_key)\n        else:\n            self.data_len = None\n\n        # remain the original order of the data indices if data_len not specified\n        self.sorted_data = self.dataset.get_data_index()\n\n        # --- 2. Sorting the Data instances in order --- #\n        # sorting the data indices by their lengths if specified\n        if self.data_len is not None and self.is_descending is not None:\n            # shrink the data_len by sorted_data if necessary\n            if len(self.data_len) &gt; len(self.sorted_data):\n                self.data_len = {\n                    index: self.data_len[index] for index in self.sorted_data\n                }\n            self.data_len = dict(\n                sorted(\n                    self.data_len.items(),\n                    key=lambda x: x[1],\n                    reverse=self.is_descending,\n                )\n            )\n\n            # record the keys of the data instances for batch generation\n            self.sorted_data = list(self.data_len.keys())\n\n        # --- 3. Initialize the Customized Part (batching strategy) of the Iterator --- #\n        # initialize the customized part of the iterator and get the batches of data indices\n        self.batches = self.batches_generate_fn(\n            self.sorted_data, self.data_len, **iter_conf\n        )\n        assert len(self.batches) &gt; 0, (\n            f\"There is no batch generated in {self.__class__.__name__}! \"\n            f\"It's probably because there is a index mismatch between you given main_data in the dataset.\"\n        )\n\n        # make sure that each batch has self.ngpu data indices for even workload on each GPU\n        if self.ngpu &gt; 1:\n            _tmp_indices = None\n            for i in range(len(self.batches)):\n                # attach the redundant ones from the last batch to the beginning of the current batch\n                if _tmp_indices is not None:\n                    self.batches[i] = _tmp_indices + self.batches[i]\n                    _tmp_indices = None\n                # check whether there are some redundant ones in the current batch\n                _remain = len(self.batches[i]) % self.ngpu\n                if _remain != 0:\n                    _tmp_indices = self.batches[i][-_remain:]\n                    self.batches[i] = self.batches[i][:-_remain]\n            # check whether there are extra ones not included\n            if _tmp_indices is not None:\n                self.batches.append(_tmp_indices)\n\n        # --- 4. Separate the Dataset into Multiple Non-overlapping Sections in the DDP Mode --- #\n        # clip the batch view for distributed training\n        if self.distributed:\n            # set stride to the number of processes\n            stride = torch.distributed.get_world_size()\n            # set the start point to the global rank of the current process\n            # make sure that the batches on GPU no.0 have the least data size (for more memory on no.0 GPU)\n            start_point = (\n                stride - torch.distributed.get_rank() - 1\n                if self.is_descending or self.is_descending is None\n                else torch.distributed.get_rank()\n            )\n            self.batches = [batch[start_point::stride] for batch in self.batches]\n\n            # delete all the empty elements in the multi-GPU distributed mode\n            while [] in self.batches:\n                self.batches.remove([])\n\n        # --- 5. Extract the Metadata Information from the Disk to the Memory --- #\n        if group_info is not None:\n            # --- 6.1. Loading the Group Information of Data Instances from the Disk to the Memory --- #\n            assert isinstance(\n                group_info, Dict\n            ), f\"group_info must be given in Dict, but got type(main_data)={type(group_info)}\"\n            self.group_info, self.data_index = read_idx2data_file_to_dict(group_info)\n\n            # --- 6.2. Data Instance Index Checking between self.group_info and self.dataset.main_data --- #\n            # check the data index in self.group_info and self.dataset\n            group_info_keys, dataset_keys = set(self.data_index), set(\n                self.dataset.get_data_index()\n            )\n            # delete the redundant key-value pairs in self.group_info\n            for redundant_key in group_info_keys.difference(dataset_keys):\n                for group_name in self.group_info.keys():\n                    self.group_info[group_name].pop(redundant_key)\n            # delete the redundant key-value pairs in self.dataset\n            for redundant_key in dataset_keys.difference(group_info_keys):\n                self.dataset.remove_data_by_index(redundant_key)\n        else:\n            self.group_info, self.data_index = None, self.dataset.get_data_index()\n\n    def batches_generate_fn(\n        self, data_index: List[str], data_len: Dict[str, int], batch_size: int = None\n    ) -&gt; List[List[str]]:\n        \"\"\"This hook interface function generates the batching view based on a specific\n        batch generation strategy.\n\n        Your overridden function should return the batches of instance indices as a List[List[str]] where each sub-list\n        corresponds to a batch of data instances. Each element in the sub-list is the index of a data instance.\n\n        In this original hook implementation, all the data instances in the built-in Dataset object will be grouped\n        into batches with exactly the same amount of instances. data_len is not used in this hook function but used for\n        sorting all the instances in the general initialization function of the iterator. The sorted data instances make\n        sure that the instances in a single batch have similar lengths.\n\n        Args:\n            data_index: List[str]\n                The list of indices of all the data instances available to generate the batching view.\n            data_len: Dict[str, int]\n                The dictionary that indicates the data length of each available data instance in data_index.\n            batch_size: int = None\n                How many data instances does a batch should have. If not given, it will be the number of GPUs (ngpu) to\n                ensure that the model validation or testing is done one data instance at each step on a single GPU\n                process.\n\n        Returns:\n            A list of batches generated by your batching strategy. This List[List[str]] is called the batching view of\n            the iterator object. Each batch in the returned list is a sub-list whose elements are the indices of data\n            instances in the corresponding batch.\n        \"\"\"\n        # batch_size is default to be the number of used GPUs to ensure that the model validation or testing is done one\n        # data instance at each step on a single GPU process\n        if batch_size is None:\n            batch_size = self.ngpu\n        # argument checking\n        if not isinstance(batch_size, int):\n            batch_size = int(batch_size)\n        assert (\n            batch_size &gt; 0\n        ), f\"batch_size must be a positive integer, but got {batch_size}.\"\n\n        # divide the data into individual batches with equal amount of instances\n        batches = [\n            data_index[i : i + batch_size]\n            for i in range(0, len(data_index) - batch_size + 1, batch_size)\n        ]\n        # in case that there are several uncovered instances at the end of self.sorted_data\n        remaining = len(data_index) % batch_size\n        if remaining != 0:\n            batches.append(data_index[-remaining:])\n\n        return batches\n\n    def __len__(self):\n        \"\"\"\n\n        Returns:\n            The real number of batches the iterator will load.\n            If batches_per_epoch is given, it will be returned; otherwise, the total number of all the batches in the\n            built-in Dataset object will be returned.\n\n        \"\"\"\n        if self.batches_per_epoch is not None:\n            return self.batches_per_epoch\n        else:\n            return len(self.batches)\n\n    def get_batch_indices(self) -&gt; List[List[str]]:\n        \"\"\"This function return the current batching view of the iterator object.\n\n        Returns: List[List[str]]\n            The batching view generated by the customized hook interface batches_generate_fn(). Each element of the\n            returned batching view list is a sub-list of data indices where each index corresponds to a data instance\n            in the built-in Dataset object.\n        \"\"\"\n        return self.batches\n\n    def get_group_info(self) -&gt; Dict[str, Dict[str, str]] or None:\n        \"\"\"This function returns the metadata information of the built-in Dataset\n        object. The returned metadata is mainly used for group-wise testing results\n        visualization.\n\n        Returns:\n            If metadata information is not initialized in the built-in Dataset object, None will be returned.\n            Otherwise, the meta_info member of the built-in Dataset object will be returned which is a dictionary.\n        \"\"\"\n        return self.group_info\n\n    def build_loader(self, epoch: int = 1, start_step: int = 0):\n        \"\"\"This function generate a torch.util.data.DataLoader to load the batches of\n        data instances for the input epoch.\n\n        If batches_per_epoch is not given, all the batches in self.batches will be used to generate the Dataloader;\n        If batches_per_epoch is given, 'batches_per_epoch' batches will be generated by self.batches according to the\n        difference between batches_per_epoch and the number of existing batches.\n\n        batches_per_epoch can be either larger or smaller than the total number of batches.\n        For a smaller batches_per_epoch, a part of self.batches will be used as the batch clip;\n        For a larger batches_per_epoch, self.batches will be supplemented by a part of itself to form the batch clip.\n\n        Args:\n            epoch: int = 1\n                The number of the current epoch. Used as part of the random seed to shuffle the batches.\n            start_step: int = 0\n                The start point for the dataloader of the current epoch. Used for resuming from a checkpoint during\n                testing.\n\n        Returns:\n            A DataLoader built on the batch clip of the current epoch.\n            If batches_per_epoch is not given, the batch clip is self.batches.\n        \"\"\"\n        # no cut off when batches_per_epoch is not given\n        if (\n            self.batches_per_epoch is None\n            or len(self.batches) == self.batches_per_epoch\n        ):\n            batches = self.batches\n\n        # the amount of batches is larger than the given batches_per_epoch\n        elif len(self.batches) &gt; self.batches_per_epoch:\n            # where to start cutting off the batches in this epoch\n            cursor = (self.batches_per_epoch * (epoch - 1)) % len(self.batches)\n            # the remaining part of existing batches is enough for this epoch\n            if len(self.batches) - cursor &gt;= self.batches_per_epoch:\n                batches = self.batches[cursor : cursor + self.batches_per_epoch]\n            # the remaining part is not enough, we need to go back to the beginning of existing batches\n            else:\n                batches = (\n                    self.batches[cursor:]\n                    + self.batches[\n                        : self.batches_per_epoch - len(self.batches) + cursor\n                    ]\n                )\n\n        # the amount of batches is smaller than the given batches_per_epoch\n        elif len(self.batches) &lt; self.batches_per_epoch:\n            # same way to get the starting point (cursor)\n            cursor = (self.batches_per_epoch * (epoch - 1)) % len(self.batches)\n            current_batch_size = 0\n            batches = []\n            # looping until we get enough batches\n            while current_batch_size &lt; self.batches_per_epoch:\n                # the remaining part of existing batches is enough for us\n                if (\n                    current_batch_size + len(self.batches) - cursor\n                    &gt;= self.batches_per_epoch\n                ):\n                    last_remain = self.batches_per_epoch - current_batch_size\n                    batches += self.batches[cursor : cursor + last_remain]\n                    current_batch_size += last_remain\n                # the remaining is not enough, we need to go to the beginning and do again\n                else:\n                    batches += self.batches[cursor:]\n                    current_batch_size += len(self.batches) - cursor\n                    cursor = 0\n        else:\n            raise RuntimeError\n\n        if self.shuffle:\n            np.random.RandomState(epoch + self.seed).shuffle(batches)\n\n        if start_step &gt; 0:\n            batches = batches[start_step:]\n\n        return DataLoader(\n            dataset=self.dataset,\n            batch_sampler=batches,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            collate_fn=self.dataset.collate_fn,\n            worker_init_fn=partial(\n                worker_init_fn,\n                base_seed=epoch + self.seed,\n                same_worker_seed=self.same_worker_seed,\n            ),\n        )\n\n    def __repr__(self):\n        batch_len = [len(batch) for batch in self.batches]\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"dataset=({str(self.dataset)}), \"\n            f\"seed={self.seed}, \"\n            f\"ngpu={self.ngpu}, \"\n            f\"num_workers={self.num_workers}, \"\n            f\"same_worker_seed={self.same_worker_seed}, \"\n            f\"pin_memory={self.pin_memory}, \"\n            f\"is_descending={self.is_descending}, \"\n            f\"shuffle={self.shuffle}, \"\n            f\"total_batches={len(self.batches)}, \"\n            f\"batches_per_epoch={len(self)}, \"\n            f\"max_batch={max(batch_len)}, \"\n            f\"min_batch={min(batch_len)}, \"\n            f\"mean_batch={sum(batch_len) / len(batch_len):.1f})\"\n        )\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.__init__","title":"<code>__init__(dataset_type, dataset_conf, batches_per_epoch=None, data_len=None, group_info=None, is_descending=True, shuffle=True, seed=0, ngpu=1, num_workers=1, same_worker_seed=False, pin_memory=True, distributed=False, **iter_conf)</code>","text":"<p>The general initialization function of all the Iterator classes. Dataset initialization is automatically done here by the given dataset_type and dataset_conf.</p> <p>In this initialization function, each iterator subclass should override a hook function batches_generate_fn() to generate the batching view of data instances in the built-in Dataset object based on their own data batching strategy.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>str</code> <p>str Query string to pick up the target Dataset subclass in <code>speechain/dataset/</code></p> required <code>dataset_conf</code> <code>Dict</code> <p>Dict Dataset configuration for its automatic initialization</p> required <code>batches_per_epoch</code> <code>int</code> <p>int = None The number of batches in each epoch. This number can be either smaller or larger than the real batch number. If not given (None), all batches will be used in each epoch.</p> <code>None</code> <code>is_descending</code> <code>bool or None</code> <p>bool = True Whether the batches are sorted in the descending order by the length (True) or in the ascending order (False). If this argument is given as None, no sorting is done for involved data instances.</p> <code>True</code> <code>data_len</code> <code>str or List[str]</code> <p>str or List[str] = None The absolute path of the data length file. Multiple length files can be given in a list, but they should contain non-overlapping data instances.</p> <code>None</code> <code>group_info</code> <code>Dict[str, str or List[str]]</code> <p>Dict[str, str or List[str]] = None The dictionary of paths for the 'idx2data' files used for group-wise evaluation results visualization.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>bool = True Whether the batches are shuffled at the beginning of each epoch.</p> <code>True</code> <code>seed</code> <code>int</code> <p>int = 0 Random seed for iterator initialization. It will be used to     1. shuffle batches before giving to the Dataloader of each epoch.     2. initialize the workers of the Dataloader for the reproducibility. This argument is automatically given by the experiment environment configuration.</p> <code>0</code> <code>ngpu</code> <code>int</code> <p>int = 1 The number of GPUs used to train or test models. The GPU number is used to ensure that each GPU process in the DDP mode has the batches with the same number of data instances. This argument is automatically given by the experiment environment configuration.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>int = 1 Number of workers for the Dataloader. This argument is automatically given by the experiment environment configuration.</p> <code>1</code> <code>pin_memory</code> <code>bool</code> <p>bool = False Whether pin_memory trick is used in the Dataloader. This argument is automatically given by the experiment environment configuration.</p> <code>True</code> <code>distributed</code> <code>bool</code> <p>bool = False Whether DDP is used to distribute the model. This argument is automatically given by the experiment environment configuration.</p> <code>False</code> <code>**iter_conf</code> <p>iterator configuration for customized batch generation</p> <code>{}</code> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def __init__(\n    self,\n    dataset_type: str,\n    dataset_conf: Dict,\n    batches_per_epoch: int = None,\n    data_len: str or List[str] = None,\n    group_info: Dict[str, str or List[str]] = None,\n    is_descending: bool or None = True,\n    shuffle: bool = True,\n    seed: int = 0,\n    ngpu: int = 1,\n    num_workers: int = 1,\n    same_worker_seed: bool = False,\n    pin_memory: bool = True,\n    distributed: bool = False,\n    **iter_conf,\n):\n    \"\"\"The general initialization function of all the Iterator classes. Dataset\n    initialization is automatically done here by the given dataset_type and\n    dataset_conf.\n\n    In this initialization function, each iterator subclass should override a hook function batches_generate_fn()\n    to generate the batching view of data instances in the built-in Dataset object based on their own data batching\n    strategy.\n\n    Args:\n        dataset_type: str\n            Query string to pick up the target Dataset subclass in `speechain/dataset/`\n        dataset_conf: Dict\n            Dataset configuration for its automatic initialization\n        batches_per_epoch: int = None\n            The number of batches in each epoch. This number can be either smaller or larger than the real batch\n            number. If not given (None), all batches will be used in each epoch.\n        is_descending: bool = True\n            Whether the batches are sorted in the descending order by the length (True) or in the ascending order\n            (False). If this argument is given as None, no sorting is done for involved data instances.\n        data_len: str or List[str] = None\n            The absolute path of the data length file. Multiple length files can be given in a list, but they\n            should contain non-overlapping data instances.\n        group_info: Dict[str, str or List[str]] = None\n            The dictionary of paths for the 'idx2data' files used for group-wise evaluation results visualization.\n        shuffle: bool = True\n            Whether the batches are shuffled at the beginning of each epoch.\n        seed: int = 0\n            Random seed for iterator initialization.\n            It will be used to\n                1. shuffle batches before giving to the Dataloader of each epoch.\n                2. initialize the workers of the Dataloader for the reproducibility.\n            This argument is automatically given by the experiment environment configuration.\n        ngpu: int = 1\n            The number of GPUs used to train or test models. The GPU number is used to ensure that each GPU process\n            in the DDP mode has the batches with the same number of data instances.\n            This argument is automatically given by the experiment environment configuration.\n        num_workers: int = 1\n            Number of workers for the Dataloader.\n            This argument is automatically given by the experiment environment configuration.\n        pin_memory: bool = False\n            Whether pin_memory trick is used in the Dataloader.\n            This argument is automatically given by the experiment environment configuration.\n        distributed: bool = False\n            Whether DDP is used to distribute the model.\n            This argument is automatically given by the experiment environment configuration.\n        **iter_conf:\n            iterator configuration for customized batch generation\n    \"\"\"\n    # initialize the built-in dataset of the iterator\n    dataset_class = import_class(\"speechain.dataset.\" + dataset_type)\n    self.dataset = dataset_class(**dataset_conf)\n\n    # initialize the general part of the iterator\n    if batches_per_epoch is not None:\n        assert (\n            batches_per_epoch &gt; 0\n        ), f\"batches_per_epoch must be a positive number, but got {batches_per_epoch}.\"\n    self.batches_per_epoch = (\n        int(batches_per_epoch)\n        if batches_per_epoch is not None\n        else batches_per_epoch\n    )\n    self.is_descending = is_descending\n    self.shuffle = shuffle\n    self.seed = seed\n    self.ngpu = ngpu\n    self.num_workers = num_workers\n    self.same_worker_seed = same_worker_seed\n    self.pin_memory = pin_memory\n    self.distributed = distributed\n\n    # --- 1. Loading the Data Length Information --- #\n    if data_len is None:\n        data_len = self.dataset.data_len\n\n    # initialize the data lengths if given\n    if data_len is not None:\n        # remain the original order of the data indices if is_descending not specified\n        self.data_len = (\n            load_idx2data_file(data_len, int)\n            if not isinstance(data_len, Dict)\n            else data_len\n        )\n\n        # check the data index in data_len and self.dataset\n        data_len_keys, dataset_keys = set(self.data_len.keys()), set(\n            self.dataset.get_data_index()\n        )\n        # delete the redundant key-value pairs in data_len\n        redundant_keys = data_len_keys.difference(dataset_keys)\n        if len(redundant_keys) &gt; 0:\n            warnings.warn(\n                f\"There are {len(redundant_keys)} redundant keys that exist in data_len but not in main_data! \"\n                f\"If you are using data_selection in data_cfg, this may not be a problem.\"\n            )\n            for redundant_key in redundant_keys:\n                self.data_len.pop(redundant_key)\n        # delete the redundant key-value pairs in self.dataset\n        redundant_keys = dataset_keys.difference(data_len_keys)\n        if len(redundant_keys) &gt; 0:\n            warnings.warn(\n                f\"There are {len(redundant_keys)} redundant keys that exist in main_data but not in data_len! \"\n                f\"If you are using data_selection in data_cfg, this may not be a problem.\"\n            )\n            for redundant_key in dataset_keys.difference(data_len_keys):\n                self.dataset.remove_data_by_index(redundant_key)\n    else:\n        self.data_len = None\n\n    # remain the original order of the data indices if data_len not specified\n    self.sorted_data = self.dataset.get_data_index()\n\n    # --- 2. Sorting the Data instances in order --- #\n    # sorting the data indices by their lengths if specified\n    if self.data_len is not None and self.is_descending is not None:\n        # shrink the data_len by sorted_data if necessary\n        if len(self.data_len) &gt; len(self.sorted_data):\n            self.data_len = {\n                index: self.data_len[index] for index in self.sorted_data\n            }\n        self.data_len = dict(\n            sorted(\n                self.data_len.items(),\n                key=lambda x: x[1],\n                reverse=self.is_descending,\n            )\n        )\n\n        # record the keys of the data instances for batch generation\n        self.sorted_data = list(self.data_len.keys())\n\n    # --- 3. Initialize the Customized Part (batching strategy) of the Iterator --- #\n    # initialize the customized part of the iterator and get the batches of data indices\n    self.batches = self.batches_generate_fn(\n        self.sorted_data, self.data_len, **iter_conf\n    )\n    assert len(self.batches) &gt; 0, (\n        f\"There is no batch generated in {self.__class__.__name__}! \"\n        f\"It's probably because there is a index mismatch between you given main_data in the dataset.\"\n    )\n\n    # make sure that each batch has self.ngpu data indices for even workload on each GPU\n    if self.ngpu &gt; 1:\n        _tmp_indices = None\n        for i in range(len(self.batches)):\n            # attach the redundant ones from the last batch to the beginning of the current batch\n            if _tmp_indices is not None:\n                self.batches[i] = _tmp_indices + self.batches[i]\n                _tmp_indices = None\n            # check whether there are some redundant ones in the current batch\n            _remain = len(self.batches[i]) % self.ngpu\n            if _remain != 0:\n                _tmp_indices = self.batches[i][-_remain:]\n                self.batches[i] = self.batches[i][:-_remain]\n        # check whether there are extra ones not included\n        if _tmp_indices is not None:\n            self.batches.append(_tmp_indices)\n\n    # --- 4. Separate the Dataset into Multiple Non-overlapping Sections in the DDP Mode --- #\n    # clip the batch view for distributed training\n    if self.distributed:\n        # set stride to the number of processes\n        stride = torch.distributed.get_world_size()\n        # set the start point to the global rank of the current process\n        # make sure that the batches on GPU no.0 have the least data size (for more memory on no.0 GPU)\n        start_point = (\n            stride - torch.distributed.get_rank() - 1\n            if self.is_descending or self.is_descending is None\n            else torch.distributed.get_rank()\n        )\n        self.batches = [batch[start_point::stride] for batch in self.batches]\n\n        # delete all the empty elements in the multi-GPU distributed mode\n        while [] in self.batches:\n            self.batches.remove([])\n\n    # --- 5. Extract the Metadata Information from the Disk to the Memory --- #\n    if group_info is not None:\n        # --- 6.1. Loading the Group Information of Data Instances from the Disk to the Memory --- #\n        assert isinstance(\n            group_info, Dict\n        ), f\"group_info must be given in Dict, but got type(main_data)={type(group_info)}\"\n        self.group_info, self.data_index = read_idx2data_file_to_dict(group_info)\n\n        # --- 6.2. Data Instance Index Checking between self.group_info and self.dataset.main_data --- #\n        # check the data index in self.group_info and self.dataset\n        group_info_keys, dataset_keys = set(self.data_index), set(\n            self.dataset.get_data_index()\n        )\n        # delete the redundant key-value pairs in self.group_info\n        for redundant_key in group_info_keys.difference(dataset_keys):\n            for group_name in self.group_info.keys():\n                self.group_info[group_name].pop(redundant_key)\n        # delete the redundant key-value pairs in self.dataset\n        for redundant_key in dataset_keys.difference(group_info_keys):\n            self.dataset.remove_data_by_index(redundant_key)\n    else:\n        self.group_info, self.data_index = None, self.dataset.get_data_index()\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Type Description <p>The real number of batches the iterator will load.</p> <p>If batches_per_epoch is given, it will be returned; otherwise, the total number of all the batches in the</p> <p>built-in Dataset object will be returned.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def __len__(self):\n    \"\"\"\n\n    Returns:\n        The real number of batches the iterator will load.\n        If batches_per_epoch is given, it will be returned; otherwise, the total number of all the batches in the\n        built-in Dataset object will be returned.\n\n    \"\"\"\n    if self.batches_per_epoch is not None:\n        return self.batches_per_epoch\n    else:\n        return len(self.batches)\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.batches_generate_fn","title":"<code>batches_generate_fn(data_index, data_len, batch_size=None)</code>","text":"<p>This hook interface function generates the batching view based on a specific batch generation strategy.</p> <p>Your overridden function should return the batches of instance indices as a List[List[str]] where each sub-list corresponds to a batch of data instances. Each element in the sub-list is the index of a data instance.</p> <p>In this original hook implementation, all the data instances in the built-in Dataset object will be grouped into batches with exactly the same amount of instances. data_len is not used in this hook function but used for sorting all the instances in the general initialization function of the iterator. The sorted data instances make sure that the instances in a single batch have similar lengths.</p> <p>Parameters:</p> Name Type Description Default <code>data_index</code> <code>List[str]</code> <p>List[str] The list of indices of all the data instances available to generate the batching view.</p> required <code>data_len</code> <code>Dict[str, int]</code> <p>Dict[str, int] The dictionary that indicates the data length of each available data instance in data_index.</p> required <code>batch_size</code> <code>int</code> <p>int = None How many data instances does a batch should have. If not given, it will be the number of GPUs (ngpu) to ensure that the model validation or testing is done one data instance at each step on a single GPU process.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>A list of batches generated by your batching strategy. This List[List[str]] is called the batching view of</p> <code>List[List[str]]</code> <p>the iterator object. Each batch in the returned list is a sub-list whose elements are the indices of data</p> <code>List[List[str]]</code> <p>instances in the corresponding batch.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def batches_generate_fn(\n    self, data_index: List[str], data_len: Dict[str, int], batch_size: int = None\n) -&gt; List[List[str]]:\n    \"\"\"This hook interface function generates the batching view based on a specific\n    batch generation strategy.\n\n    Your overridden function should return the batches of instance indices as a List[List[str]] where each sub-list\n    corresponds to a batch of data instances. Each element in the sub-list is the index of a data instance.\n\n    In this original hook implementation, all the data instances in the built-in Dataset object will be grouped\n    into batches with exactly the same amount of instances. data_len is not used in this hook function but used for\n    sorting all the instances in the general initialization function of the iterator. The sorted data instances make\n    sure that the instances in a single batch have similar lengths.\n\n    Args:\n        data_index: List[str]\n            The list of indices of all the data instances available to generate the batching view.\n        data_len: Dict[str, int]\n            The dictionary that indicates the data length of each available data instance in data_index.\n        batch_size: int = None\n            How many data instances does a batch should have. If not given, it will be the number of GPUs (ngpu) to\n            ensure that the model validation or testing is done one data instance at each step on a single GPU\n            process.\n\n    Returns:\n        A list of batches generated by your batching strategy. This List[List[str]] is called the batching view of\n        the iterator object. Each batch in the returned list is a sub-list whose elements are the indices of data\n        instances in the corresponding batch.\n    \"\"\"\n    # batch_size is default to be the number of used GPUs to ensure that the model validation or testing is done one\n    # data instance at each step on a single GPU process\n    if batch_size is None:\n        batch_size = self.ngpu\n    # argument checking\n    if not isinstance(batch_size, int):\n        batch_size = int(batch_size)\n    assert (\n        batch_size &gt; 0\n    ), f\"batch_size must be a positive integer, but got {batch_size}.\"\n\n    # divide the data into individual batches with equal amount of instances\n    batches = [\n        data_index[i : i + batch_size]\n        for i in range(0, len(data_index) - batch_size + 1, batch_size)\n    ]\n    # in case that there are several uncovered instances at the end of self.sorted_data\n    remaining = len(data_index) % batch_size\n    if remaining != 0:\n        batches.append(data_index[-remaining:])\n\n    return batches\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.build_loader","title":"<code>build_loader(epoch=1, start_step=0)</code>","text":"<p>This function generate a torch.util.data.DataLoader to load the batches of data instances for the input epoch.</p> <p>If batches_per_epoch is not given, all the batches in self.batches will be used to generate the Dataloader; If batches_per_epoch is given, 'batches_per_epoch' batches will be generated by self.batches according to the difference between batches_per_epoch and the number of existing batches.</p> <p>batches_per_epoch can be either larger or smaller than the total number of batches. For a smaller batches_per_epoch, a part of self.batches will be used as the batch clip; For a larger batches_per_epoch, self.batches will be supplemented by a part of itself to form the batch clip.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>int = 1 The number of the current epoch. Used as part of the random seed to shuffle the batches.</p> <code>1</code> <code>start_step</code> <code>int</code> <p>int = 0 The start point for the dataloader of the current epoch. Used for resuming from a checkpoint during testing.</p> <code>0</code> <p>Returns:</p> Type Description <p>A DataLoader built on the batch clip of the current epoch.</p> <p>If batches_per_epoch is not given, the batch clip is self.batches.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def build_loader(self, epoch: int = 1, start_step: int = 0):\n    \"\"\"This function generate a torch.util.data.DataLoader to load the batches of\n    data instances for the input epoch.\n\n    If batches_per_epoch is not given, all the batches in self.batches will be used to generate the Dataloader;\n    If batches_per_epoch is given, 'batches_per_epoch' batches will be generated by self.batches according to the\n    difference between batches_per_epoch and the number of existing batches.\n\n    batches_per_epoch can be either larger or smaller than the total number of batches.\n    For a smaller batches_per_epoch, a part of self.batches will be used as the batch clip;\n    For a larger batches_per_epoch, self.batches will be supplemented by a part of itself to form the batch clip.\n\n    Args:\n        epoch: int = 1\n            The number of the current epoch. Used as part of the random seed to shuffle the batches.\n        start_step: int = 0\n            The start point for the dataloader of the current epoch. Used for resuming from a checkpoint during\n            testing.\n\n    Returns:\n        A DataLoader built on the batch clip of the current epoch.\n        If batches_per_epoch is not given, the batch clip is self.batches.\n    \"\"\"\n    # no cut off when batches_per_epoch is not given\n    if (\n        self.batches_per_epoch is None\n        or len(self.batches) == self.batches_per_epoch\n    ):\n        batches = self.batches\n\n    # the amount of batches is larger than the given batches_per_epoch\n    elif len(self.batches) &gt; self.batches_per_epoch:\n        # where to start cutting off the batches in this epoch\n        cursor = (self.batches_per_epoch * (epoch - 1)) % len(self.batches)\n        # the remaining part of existing batches is enough for this epoch\n        if len(self.batches) - cursor &gt;= self.batches_per_epoch:\n            batches = self.batches[cursor : cursor + self.batches_per_epoch]\n        # the remaining part is not enough, we need to go back to the beginning of existing batches\n        else:\n            batches = (\n                self.batches[cursor:]\n                + self.batches[\n                    : self.batches_per_epoch - len(self.batches) + cursor\n                ]\n            )\n\n    # the amount of batches is smaller than the given batches_per_epoch\n    elif len(self.batches) &lt; self.batches_per_epoch:\n        # same way to get the starting point (cursor)\n        cursor = (self.batches_per_epoch * (epoch - 1)) % len(self.batches)\n        current_batch_size = 0\n        batches = []\n        # looping until we get enough batches\n        while current_batch_size &lt; self.batches_per_epoch:\n            # the remaining part of existing batches is enough for us\n            if (\n                current_batch_size + len(self.batches) - cursor\n                &gt;= self.batches_per_epoch\n            ):\n                last_remain = self.batches_per_epoch - current_batch_size\n                batches += self.batches[cursor : cursor + last_remain]\n                current_batch_size += last_remain\n            # the remaining is not enough, we need to go to the beginning and do again\n            else:\n                batches += self.batches[cursor:]\n                current_batch_size += len(self.batches) - cursor\n                cursor = 0\n    else:\n        raise RuntimeError\n\n    if self.shuffle:\n        np.random.RandomState(epoch + self.seed).shuffle(batches)\n\n    if start_step &gt; 0:\n        batches = batches[start_step:]\n\n    return DataLoader(\n        dataset=self.dataset,\n        batch_sampler=batches,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        collate_fn=self.dataset.collate_fn,\n        worker_init_fn=partial(\n            worker_init_fn,\n            base_seed=epoch + self.seed,\n            same_worker_seed=self.same_worker_seed,\n        ),\n    )\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.get_batch_indices","title":"<code>get_batch_indices()</code>","text":"<p>This function return the current batching view of the iterator object.</p> <p>List[List[str]]</p> Type Description <code>List[List[str]]</code> <p>The batching view generated by the customized hook interface batches_generate_fn(). Each element of the</p> <code>List[List[str]]</code> <p>returned batching view list is a sub-list of data indices where each index corresponds to a data instance</p> <code>List[List[str]]</code> <p>in the built-in Dataset object.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def get_batch_indices(self) -&gt; List[List[str]]:\n    \"\"\"This function return the current batching view of the iterator object.\n\n    Returns: List[List[str]]\n        The batching view generated by the customized hook interface batches_generate_fn(). Each element of the\n        returned batching view list is a sub-list of data indices where each index corresponds to a data instance\n        in the built-in Dataset object.\n    \"\"\"\n    return self.batches\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.Iterator.get_group_info","title":"<code>get_group_info()</code>","text":"<p>This function returns the metadata information of the built-in Dataset object. The returned metadata is mainly used for group-wise testing results visualization.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, str]] or None</code> <p>If metadata information is not initialized in the built-in Dataset object, None will be returned.</p> <code>Dict[str, Dict[str, str]] or None</code> <p>Otherwise, the meta_info member of the built-in Dataset object will be returned which is a dictionary.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def get_group_info(self) -&gt; Dict[str, Dict[str, str]] or None:\n    \"\"\"This function returns the metadata information of the built-in Dataset\n    object. The returned metadata is mainly used for group-wise testing results\n    visualization.\n\n    Returns:\n        If metadata information is not initialized in the built-in Dataset object, None will be returned.\n        Otherwise, the meta_info member of the built-in Dataset object will be returned which is a dictionary.\n    \"\"\"\n    return self.group_info\n</code></pre>"},{"location":"reference/iterator/abs/#iterator.abs.worker_init_fn","title":"<code>worker_init_fn(worker_id, base_seed, same_worker_seed)</code>","text":"<p>Set random seed for each worker in DataLoader to ensure the reproducibility.</p> Source code in <code>speechain/iterator/abs.py</code> <pre><code>def worker_init_fn(worker_id: int, base_seed: int, same_worker_seed: bool):\n    \"\"\"Set random seed for each worker in DataLoader to ensure the reproducibility.\"\"\"\n    seed = base_seed if same_worker_seed else base_seed + worker_id\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"reference/iterator/block/","title":"block","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/iterator/block/#iterator.block.BlockIterator","title":"<code>BlockIterator</code>","text":"<p>               Bases: <code>Iterator</code></p> <p>The strategy of this iterator is to generate batches with the same amount of data lengths. For sequence-to-sequence tasks, the data instances are usually different in data length. If there is a fixed number of data instances in each batch, the data volume of a single batch may constantly change during training. This may either cause a CUDA memory error (out of GPU memory) or large idle GPU memories.</p> <p>It can be considered as the strategy that always gives 'rectangles' with similar 'areas' if we treat the number of data instances in a batch as the rectangle length and the maximal data length as the rectangle width.</p> Source code in <code>speechain/iterator/block.py</code> <pre><code>class BlockIterator(Iterator):\n    \"\"\"The strategy of this iterator is to generate batches with the same amount of data\n    lengths. For sequence-to-sequence tasks, the data instances are usually different in\n    data length. If there is a fixed number of data instances in each batch, the data\n    volume of a single batch may constantly change during training. This may either\n    cause a CUDA memory error (out of GPU memory) or large idle GPU memories.\n\n    It can be considered as the strategy that always gives 'rectangles' with similar\n    'areas' if we treat the number of data instances in a batch as the rectangle length\n    and the maximal data length as the rectangle width.\n    \"\"\"\n\n    def batches_generate_fn(\n        self, data_index: List[str], data_len: Dict[str, int], batch_len: int = None\n    ) -&gt; List[List[str]]:\n        \"\"\"All the data instances in the built-in Dataset object will be grouped into\n        batches with the same total lengths. The lengths used for grouping is given in\n        data_len. The customized argument batch_len specifies the total length that each\n        batch should have.\n\n        Args:\n            data_index\n            data_len\n            batch_len: int = None\n                The total data length of all the data instances in a batch.\n                If the data is in the format of audio waveforms, batch_len is the amount of sampling points.\n                If the data is in the format of acoustic features, batch_len is the amount of time frames.\n        \"\"\"\n        assert batch_len is not None, \"batch_len cannot be None and must be specified!\"\n        if not isinstance(batch_len, int):\n            batch_len = int(batch_len)\n        # configuration initialization\n        assert (\n            batch_len &gt; 0\n        ), f\"batch_len must be a positive integer, but got {batch_len}.\"\n\n        # divide the data into individual batches by their lengths\n        batches = []\n        current_batch_frames = 0\n        current_batch = []\n        for index in data_index:\n            current_batch.append(index)\n            current_batch_frames += data_len[index]\n\n            if current_batch_frames &gt;= batch_len:\n                batches.append(current_batch)\n                current_batch = []\n                current_batch_frames = 0\n\n        # add the remaining instances as a single batch\n        if len(current_batch) &gt; 0:\n            batches.append(current_batch)\n\n        return batches\n</code></pre>"},{"location":"reference/iterator/block/#iterator.block.BlockIterator.batches_generate_fn","title":"<code>batches_generate_fn(data_index, data_len, batch_len=None)</code>","text":"<p>All the data instances in the built-in Dataset object will be grouped into batches with the same total lengths. The lengths used for grouping is given in data_len. The customized argument batch_len specifies the total length that each batch should have.</p> <p>Parameters:</p> Name Type Description Default <code>batch_len</code> <code>int</code> <p>int = None The total data length of all the data instances in a batch. If the data is in the format of audio waveforms, batch_len is the amount of sampling points. If the data is in the format of acoustic features, batch_len is the amount of time frames.</p> <code>None</code> Source code in <code>speechain/iterator/block.py</code> <pre><code>def batches_generate_fn(\n    self, data_index: List[str], data_len: Dict[str, int], batch_len: int = None\n) -&gt; List[List[str]]:\n    \"\"\"All the data instances in the built-in Dataset object will be grouped into\n    batches with the same total lengths. The lengths used for grouping is given in\n    data_len. The customized argument batch_len specifies the total length that each\n    batch should have.\n\n    Args:\n        data_index\n        data_len\n        batch_len: int = None\n            The total data length of all the data instances in a batch.\n            If the data is in the format of audio waveforms, batch_len is the amount of sampling points.\n            If the data is in the format of acoustic features, batch_len is the amount of time frames.\n    \"\"\"\n    assert batch_len is not None, \"batch_len cannot be None and must be specified!\"\n    if not isinstance(batch_len, int):\n        batch_len = int(batch_len)\n    # configuration initialization\n    assert (\n        batch_len &gt; 0\n    ), f\"batch_len must be a positive integer, but got {batch_len}.\"\n\n    # divide the data into individual batches by their lengths\n    batches = []\n    current_batch_frames = 0\n    current_batch = []\n    for index in data_index:\n        current_batch.append(index)\n        current_batch_frames += data_len[index]\n\n        if current_batch_frames &gt;= batch_len:\n            batches.append(current_batch)\n            current_batch = []\n            current_batch_frames = 0\n\n    # add the remaining instances as a single batch\n    if len(current_batch) &gt; 0:\n        batches.append(current_batch)\n\n    return batches\n</code></pre>"},{"location":"reference/model/","title":"model","text":""},{"location":"reference/model/abs/","title":"abs","text":"<p>Abstract base class for all models.</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/model/abs/#model.abs.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Model is the base class for all models in this toolkit. The main job of a model includes:     1. (optional) preprocess the input batch data to the trainable format     2. calculate the model prediction results by the Module members     3. evaluate the prediction results by the Criterion members</p> <p>Each model has several built-in Module members that make up the neural network structure of the model. These Module members will be initialized by the <code>module_conf</code> given in your configuration.</p> <p>There are a built-in dictionary named <code>init_class_dict</code> and a built-in list named <code>default_init_modules</code> in the base class. init_class_dict<code>contains all the available initialization functions of the model parameters while</code>default_init_modules` includes the network layers that have their own initialization functions.</p> <p>Attributes:</p> Name Type Description <code>init_class_dict</code> <code>Dict</code> <p>Available parameter initialization functions</p> <code>default_init_modules</code> <code>List</code> <p>Network layers with own initialization functions</p> Source code in <code>speechain/model/abs.py</code> <pre><code>class Model(torch.nn.Module, ABC):\n    \"\"\"\n    Model is the base class for all models in this toolkit. The main job of a model includes:\n        1. (optional) preprocess the input batch data to the trainable format\n        2. calculate the model prediction results by the Module members\n        3. evaluate the prediction results by the Criterion members\n\n    Each model has several built-in Module members that make up the neural network structure of the model. These Module\n    members will be initialized by the `module_conf` given in your configuration.\n\n    There are a built-in dictionary named `init_class_dict` and a built-in list named `default_init_modules` in the\n    base class. init_class_dict` contains all the available initialization functions of the model parameters while\n    `default_init_modules` includes the network layers that have their own initialization functions.\n\n    Attributes:\n        init_class_dict (Dict): Available parameter initialization functions\n        default_init_modules (List): Network layers with own initialization functions\n\n    \"\"\"\n\n    # available parameter initialization functions\n    init_class_dict: Dict = {\n        \"xavier\": torch.nn.init.xavier_normal_,\n        \"xavier_normal\": torch.nn.init.xavier_normal_,\n        \"xavier_uniform\": torch.nn.init.xavier_uniform_,\n        \"kaiming\": torch.nn.init.kaiming_normal_,\n        \"kaiming_normal\": torch.nn.init.kaiming_normal_,\n        \"kaiming_uniform\": torch.nn.init.kaiming_uniform_,\n        \"uniform\": torch.nn.init.uniform_,\n        \"normal\": torch.nn.init.normal_,\n        \"zeros\": torch.nn.init.zeros_,\n    }\n\n    # some modules have their own parameter initialization methods\n    default_init_modules: List = [  # explicitely defined\n        torch.nn.Embedding,\n        torch.nn.LayerNorm,\n        torch.nn.BatchNorm1d,\n        torch.nn.BatchNorm2d,\n        PositionalEncoding,\n    ]\n\n    def __init__(\n        self,\n        device: torch.device,\n        module_conf: Dict,\n        result_path: str,\n        model_conf: Dict = None,\n        criterion_conf: Dict = None,\n        non_blocking: bool = False,\n        distributed: bool = False,\n    ):\n        \"\"\"In this initialization function, there are two parts of\n        initialization: model-specific customized initialization and model-\n        independent general initialization.\n\n        Model-specific customized initialization is done by two interface functions: module_init() and criterion_init().\n        module_init() initializes the neural network structure of the model while criterion_init() initializes the\n        criteria used to optimize (loss functions) and evaluate (validation metrics) the model.\n\n        After the customized initialization, there are 3 steps for general initialization:\n            1. Pretrained parameters will be loaded into your model if the key `pretrained_model` is given. Multiple\n            pretrained models can be specified and each of them can be loaded into different parts of your model. The\n            mismatch between the names of pretrained parameters and the parameters of your model is handled by the key\n            'mapping'. The value of the key `mapping` is a dictionary where each key-value item corresponds to a mapping\n            of parameter names. The key is the parameter name in the pretrained parameters while the value is the\n            parameter name of your model.\n\n            2. If `pretrained_model` is not given, the parameters of your model will be initialized by the function that\n            matches your input query 'init'. Please refer to the built-in dictionary `init_class_dict` for the available\n            initialization functions. If `init` is not given, the default initialization function\n            `torch.nn.init.xavier_normal_` will be used to initialize your model.\n\n            3. Finally, the specified parts of your model will be frozen if 'frozen_modules' is given. If there is only\n            one frozen module, you can directly give the string of its name to 'frozen_modules' like\n            'frozen_modules: {module_name}'; if there are multiple modules you want to freeze, you can give their names\n            in a list as\n            ```\n            frozen_modules:\n              - {module_name1}\n              - {module_name2}\n              - ...\n            ```\n            Moreover, the frozen granularity depends on your input `frozen_modules`.\n            For example,\n                1. If you give 'frozen_modules: encoder_prenet', all parameters of the prenet of your encoder will be\n                frozen\n                2. If you give 'frozen_modules: encoder_prenet.conv', only the convolution layers of the prenet of your\n                encoder will be frozen\n                3. If you give 'frozen_modules: encoder_prenet.conv.0', only the first convolution layer of the prenet\n                of your encoder will be frozen\n                4. If you give 'frozen_modules: encoder_prenet.conv.0.bias', only the bias vector of the first\n                convolution layer of the prenet of your encoder will be frozen\n\n        Args:\n            device (torch.device):\n                The computational device used for model calculation in the current GPU process.\n            model_conf (Dict):\n                The model configuration used for general model initialization.\n            module_conf (Dict):\n                The module configuration used for network structure initialization.\n            criterion_conf (Dict):\n                The criterion configuration used for criterion (loss functions and evaluation metrics) initialization.\n        \"\"\"\n        super(Model, self).__init__()\n\n        # input argument checking\n        assert module_conf is not None, \"module_conf cannot be None!\"\n        # model_conf is default to be an empty dictionary\n        model_conf = dict() if model_conf is None else model_conf\n        # criterion_conf is default to be an empty dictionary\n        criterion_conf = dict() if criterion_conf is None else criterion_conf\n        # customize_conf is default to be an empty dictionary\n        if \"customize_conf\" not in model_conf.keys():\n            model_conf[\"customize_conf\"] = dict()\n\n        # general argument registration\n        self.non_blocking = non_blocking\n        self.distributed = distributed\n        self.device = device\n\n        # snapshotting-related argument registration\n        self.result_path = result_path\n        if \"visual_infer_conf\" in model_conf.keys():\n            # configuration is given as a .yaml file\n            if isinstance(model_conf[\"visual_infer_conf\"], str):\n                self.visual_infer_conf = load_yaml(\n                    open(parse_path_args(model_conf[\"visual_infer_conf\"]))\n                )\n            # configuration is explicitly given\n            elif isinstance(model_conf[\"visual_infer_conf\"], Dict):\n                self.visual_infer_conf = model_conf[\"visual_infer_conf\"]\n            else:\n                raise RuntimeError(\n                    \"model_conf['visual_infer_conf'] must be given as either a string or a Dict.\"\n                )\n        else:\n            self.visual_infer_conf = dict()\n\n        # --- 1. Model Construction --- #\n        self.module_init(**module_conf, **model_conf[\"customize_conf\"])\n        self.criterion_init(**criterion_conf)\n        # initialize the bad case selection methods by the hook function\n        self.bad_cases_selection = self.bad_cases_selection_init_fn()\n\n        # --- 2.1. Pretrained Model Loading --- #\n        pretrained_model = (\n            model_conf[\"pretrained_model\"]\n            if \"pretrained_model\" in model_conf.keys()\n            else None\n        )\n        if pretrained_model is not None:\n            pretrained_model = (\n                pretrained_model\n                if isinstance(pretrained_model, list)\n                else [pretrained_model]\n            )\n\n            for ptm in pretrained_model:\n                # argument checking\n                if isinstance(ptm, str):\n                    ptm = dict(path=parse_path_args(ptm))\n                elif isinstance(ptm, Dict):\n                    assert \"path\" in ptm.keys(), (\n                        \"If model['model_conf']['pretrained_model'] is given as a Dict, \"\n                        \"please give a key named 'path' to specify where your pretrained model is placed.\"\n                    )\n                    if os.path.exists(ptm[\"path\"]):\n                        raise RuntimeError(\n                            f\"The specified path of your pretrained model {ptm['path']} doesn't exist! \"\n                            f\"Please check the input path.\"\n                        )\n                else:\n                    raise TypeError(\n                        f\"The elements in model['model_conf']['pretrained_model'] must be either a string \"\n                        f\"or a Dict, but got {ptm}\"\n                    )\n\n                _pt_model = torch.load(\n                    parse_path_args(ptm[\"path\"]), map_location=self.device\n                )\n                mapping = ptm[\"mapping\"] if \"mapping\" in ptm.keys() else None\n                if mapping is None:\n                    self.load_state_dict(\n                        _pt_model,\n                        strict=True if \"strict\" not in ptm.keys() else ptm[\"strict\"],\n                    )\n                else:\n                    assert isinstance(mapping, dict) and len(mapping) &gt;= 1, (\n                        f\"mapping must be given as a dict and cannot be empty! \"\n                        f\"Got type(mapping)={type(mapping)} and len(mapping)={len(mapping)}\"\n                    )\n\n                    _src_modules = OrderedDict()\n                    # loop each name-parameter pair in the model\n                    for name, para in _pt_model.items():\n                        # loop each source-target mapping pair\n                        for src, tgt in mapping.items():\n                            # attach '.' to the end is for making the name unique\n                            src, tgt = src + \".\", tgt + \".\"\n                            # change the parameter name in the middle\n                            if src in name:\n                                name = name.replace(src, tgt)\n                        # record the parameter no matter whether its name is modified or not\n                        _src_modules[name] = para\n                    self.load_state_dict(\n                        _src_modules,\n                        strict=True if \"strict\" not in ptm.keys() else ptm[\"strict\"],\n                    )\n\n        # --- 2.2. Model Parameter Initialization --- #\n        else:\n            # the default initialization method is xavier (i.e. xavier_normal)\n            init = model_conf[\"init\"] if \"init\" in model_conf.keys() else \"xavier\"\n            assert (\n                init in self.init_class_dict.keys()\n            ), f\"Only the initialization methods {self.init_class_dict.keys()} are supported, but got init={init}.\"\n\n            for name, para in self.named_parameters():\n                # initialize all the bias vectors to zero\n                if \".bias\" in name and para.dim() == 1:\n                    torch.nn.init.zeros_(para)\n                # initialize all the weight vectors except for those of normalization layers (BatchNorm &amp; LayerNorm)\n                elif para.dim() &gt; 1:\n                    self.init_class_dict[init](para)\n\n            # initialize the modules that have their own default init methods\n            for module in self.modules():\n                if isinstance(module, tuple(self.default_init_modules)):\n                    module.reset_parameters()\n\n        # --- 3. Model Parameter Freezing --- #\n        frozen_modules = (\n            model_conf[\"frozen_modules\"]\n            if \"frozen_modules\" in model_conf.keys()\n            else None\n        )\n        if frozen_modules is not None:\n            if frozen_modules != \"all\":\n                frozen_modules = (\n                    frozen_modules\n                    if isinstance(frozen_modules, list)\n                    else [frozen_modules]\n                )\n\n            for name, para in self.named_parameters():\n                frozen_flag = False\n                if frozen_modules != \"all\":\n                    for module in frozen_modules:\n                        frozen_flag = name.startswith(module + \".\")\n                else:\n                    frozen_flag = True\n\n                if frozen_flag:\n                    para.requires_grad = False\n                else:\n                    raise RuntimeError(\n                        f\"frozen_modules: Parameters of {name} are not found in the model!\"\n                    )\n\n    @abstractmethod\n    def module_init(self, **kwargs) -&gt; None:\n        \"\"\"The interface function that initializes the Module members of the model.\n        These Module members make up the neural network structure of the model. Some\n        models have their customized part that also needs to be initialization in this\n        function, e.g. the tokenizer of ASR and TTS models.\n\n        Note: This interface function must be overridden for each Model subclass.\n\n        Args:\n            **kwargs:\n                The combination of the arguments in your given `module_conf` and `model_conf['customize_conf']`.\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    @abstractmethod\n    def criterion_init(self, **criterion_conf) -&gt; None:\n        \"\"\"\n        The interface function that initializes the Criterion members of the model. These Criterion members can be\n        divided into two parts: the loss functions used for training and the evaluation metrics used for validation.\n\n        Args:\n            **criterion_conf:\n                The arguments in your given `criterion_conf`.\n\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    @staticmethod\n    def bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n        \"\"\"This hook function returns the default bad case selection method of each\n        Model object. This default value will be referred by the _Runner_ to present the\n        top-N bad cases.\n\n        The original hook implementation in the base Model class returns None which means no default value.\n\n        Returns: List[List[str or int]]\n            The returned default value should be a list of tri-list where each tri-list is in the form of\n            [`selection_metric`, `selection_mode`, `case_number`]. For example, ['wer', 'max', 50] means 50 testing\n            waveforms with the largest WER will be selected.\n        \"\"\"\n        return None\n\n    def batch_to_cuda(\n        self, data: Dict[str, torch.Tensor] or torch.Tensor\n    ) -&gt; Dict[str, torch.Tensor] or torch.Tensor:\n        \"\"\"The recursive function that transfers the batch data to the specified device\n        in the current process.\n\n        Args:\n            data: Dict or torch.Tensor\n                The input batch data. It should be either a Tensor or a Dict of Tensors. For the Dict input, the\n                function itself will be called once by each Tensor element.\n\n        Returns: Dict or torch.Tensor\n            If the input is a Dict, the returned output will also be a Dict of Tensors transferred to the target device;\n            If the input is a Tensor, the returned output will be its copy on the target device.\n        \"\"\"\n        # if the data is in the form of Dict, recursively process each key-value pair\n        if isinstance(data, Dict):\n            return {key: self.batch_to_cuda(value) for key, value in data.items()}\n        # if the data is in the form of tensor, put it on GPUs by .cuda()\n        elif isinstance(data, torch.Tensor):\n            return data.cuda(device=self.device, non_blocking=self.non_blocking)\n        # do nothing for other types of data\n        else:\n            return data\n\n    def forward(self, batch_data: Dict, epoch: int = None, **kwargs):\n        \"\"\"\n        The general model forward function shared by all the _Model_ subclasses. This forward function has 3 steps:\n            1. preprocess and transfer the batch data to GPUs\n            2. obtain the model prediction results\n            3. calculate the loss function and evaluate the prediction results\n\n        For each step above, we provide interface functions for you to override and make your own implementation.\n\n        Args:\n            batch_data: Dict\n                The input batch data received from the `train` or `valid` dataloader object in the experimental\n                pipeline.\n            epoch: int = None\n                The number of the current epoch. Used for real-time model visualization and model prediction.\n            **kwargs:\n                The additional arguments for real-time model visualization. If given, the code will go through the model\n                visualization branch.\n\n        Returns:\n            In the training branch, the loss functions and evaluation metrics will be returned each of which is in the\n            form of a Dict.\n            In the validation branch, only the evaluation metrics will be returned.\n            In the visualization branch, the model snapshots on the given validation instance will be returned.\n\n        \"\"\"\n        # --- 1. Batch Data Preprocessing and GPU transferring --- #\n        # --- data preparation below is shared by all the three branches: training, validation, and visualization --- #\n        # preprocess the batch data if needed\n        batch_data = self.batch_preprocess_fn(batch_data)\n\n        # put the batch data onto GPUs\n        batch_data = self.batch_to_cuda(batch_data)\n\n        # --- 2.1. Model Visualization Branch --- #\n        # if there are additional arguments other than batch_data and epoch, the visualization branch is activated\n        if len(kwargs) != 0:\n            return self.visualize(epoch=epoch, **batch_data, **kwargs)\n\n        # --- 2.2. Model Forward Calculation --- #\n        # --- model forward is shared by both the training and validation branches --- #\n        # context function used when doing the loss backward for efficient gradient accumulation in the DDP mode\n        forward_context = nullcontext if self.training else torch.inference_mode\n        with forward_context():\n            try:\n                # Feed the input batch into the model and get the outputs, copy.deepcopy() here is for the data safety\n                model_outputs = self.module_forward(\n                    epoch=epoch, **copy.deepcopy(batch_data)\n                )\n            except Exception as e:\n                if not self.distributed:\n                    raise e\n                else:\n                    skip_flag_list = torch.LongTensor(\n                        [False for _ in range(torch.distributed.get_world_size())]\n                    ).cuda(self.device)\n                    skip_flag = torch.LongTensor([True]).cuda(self.device)\n                    # as long as one node meets an error, all nodes will skip the current step at the same time\n                    torch.distributed.all_gather_into_tensor(skip_flag_list, skip_flag)\n                    if skip_flag_list.sum() &gt;= 1:\n                        raise e\n            else:\n                if self.distributed:\n                    skip_flag_list = torch.LongTensor(\n                        [False for _ in range(torch.distributed.get_world_size())]\n                    ).cuda(self.device)\n                    skip_flag = torch.LongTensor([False]).cuda(self.device)\n                    # as long as one node meets an error, all nodes will skip the current step at the same time\n                    torch.distributed.all_gather_into_tensor(skip_flag_list, skip_flag)\n                    if skip_flag_list.sum() &gt;= 1:\n                        raise RuntimeError(\n                            \"Other ranks meet errors during model forwarding, \"\n                            \"so this rank will also skip the current step!\"\n                        )\n\n        # copy.deepcopy() cannot receive the non-leaf nodes in the computation graph (model_outputs). Since\n        # model_outputs cannot be detached from the graph (gradients necessary), copy.deepcopy() is not used below.\n        def combine_input_output(_batch_data: Dict, _model_outputs: Dict):\n            combination, batch_keys = dict(), list(_batch_data.keys())\n            # if the input batch data is in the form of Dict, it means there are multiple dataloaders\n            if isinstance(_batch_data[batch_keys[0]], Dict):\n                for key in batch_keys:\n                    combination[key] = dict(**_batch_data[key], **_model_outputs[key])\n            # if the input batch data is in the form of Tensor, it means there is only one dataloader.\n            else:\n                combination.update(_batch_data)\n                combination.update(_model_outputs)\n            return combination\n\n        # --- 3.1. Model Training Branch --- #\n        if self.training:\n            # In the training stage, both the trainable losses and non-trainable metrics will be returned\n            losses, metrics = self.criterion_forward(\n                **combine_input_output(batch_data, model_outputs)\n            )\n            metrics.update(self.get_recordable_para())\n\n            # post-checking for training losses, they must be trainable tensors\n            assert sum(\n                [\n                    isinstance(loss, torch.Tensor) and loss.requires_grad\n                    for loss in losses.values()\n                ]\n            ) == len(losses), \"Training losses must be trainable tensors!\"\n            # post-checking for validation metrics, they must be either non-trainable tensors or other datatypes\n            assert sum(\n                [\n                    not isinstance(metric, torch.Tensor) or not metric.requires_grad\n                    for metric in metrics.values()\n                ]\n            ) == len(\n                metrics\n            ), \"Validation metrics must be either non-trainable tensors or other datatypes!\"\n\n            # the non-trainable metrics will be averaged across all the processes in the distributed mode\n            if self.distributed:\n                metrics = self.aver_metrics_across_procs(metrics, batch_data)\n            return losses, metrics\n\n        # --- 3.2. Model Validation Branch --- #\n        else:\n            # In the validation stage, only the non-trainable metrics will be returned\n            with torch.inference_mode():\n                metrics = self.criterion_forward(\n                    **combine_input_output(batch_data, model_outputs)\n                )\n            metrics.update(self.get_recordable_para())\n\n            # post-checking for validation metrics, they must be either non-trainable tensors or other datatypes\n            assert sum(\n                [\n                    not isinstance(metric, torch.Tensor) or not metric.requires_grad\n                    for metric in metrics.values()\n                ]\n            ) == len(\n                metrics\n            ), \"Validation metrics must be either non-trainable tensors or other datatypes!\"\n\n            # the non-trainable metrics will be averaged across all the processes in the distributed mode\n            if self.distributed:\n                metrics = self.aver_metrics_across_procs(metrics, batch_data)\n            return metrics\n\n    def batch_preprocess_fn(self, batch_data: Dict) -&gt; Dict:\n        \"\"\"This hook function does the preprocessing for the input batch data before\n        using them in self.model_forward(). This function is not mandatory to be\n        overridden and the original implementation in the base Model class does the\n        tensor transformation for the string-like data in batch_data (i.e., text and\n        spk_ids).\n\n        Note: the key names in the returned Dict should match the argument names in self.model_forward().\n\n        Args:\n            batch_data: Dict\n                The raw data of the input batch to be preprocessed in this hook function.\n\n        Returns: Dict\n            The processed data of the input batch that is ready to be used in `self.model_forward()`.\n        \"\"\"\n\n        def process_strings(data_dict: Dict):\n            \"\"\"Turn the text and speaker strings into tensors and get their lengths.\"\"\"\n            # --- Process the Text String and its Length --- #\n            if \"text\" in data_dict.keys():\n                if isinstance(data_dict[\"text\"], List):\n                    data_dict[\"text\"], data_dict[\"text_len\"] = text2tensor_and_len(\n                        text_list=data_dict[\"text\"],\n                        text2tensor_func=self.tokenizer.text2tensor,\n                        ignore_idx=self.tokenizer.ignore_idx,\n                    )\n                else:\n                    assert isinstance(data_dict[\"text\"], torch.Tensor)\n\n            # --- Process the Speaker ID String --- #\n            if \"spk_ids\" in data_dict.keys():\n                if isinstance(data_dict[\"spk_ids\"], List):\n                    if hasattr(self, \"spk2idx\"):\n                        data_dict[\"spk_ids\"] = spk2tensor(\n                            spk_list=data_dict[\"spk_ids\"], spk2idx_dict=self.spk2idx\n                        )\n                elif not isinstance(data_dict[\"spk_ids\"], torch.Tensor):\n                    raise TypeError\n\n            return data_dict\n\n        # check whether the batch_data is made by multiple dataloaders\n        leaf_flags = [not isinstance(value, Dict) for value in batch_data.values()]\n        if sum(leaf_flags) == 0:\n            return {key: process_strings(value) for key, value in batch_data.items()}\n        elif sum(leaf_flags) == len(batch_data):\n            return process_strings(batch_data)\n        else:\n            raise RuntimeError(\"Wrong composition of batch_data!\")\n\n    def aver_metrics_across_procs(\n        self, metrics: Dict[str, torch.Tensor], batch_data: Dict\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"This function averages the evaluation metrics across all GPU processes in the\n        DDP mode for model distribution.\n\n        Args:\n            metrics: Dict[str, torch.Tensor]\n                The evaluation metrics to be averaged across all GPU processes.\n            batch_data: Dict\n                The input batch data used to calculate the batch size for averaging evaluation metrics.\n\n        Returns: Dict[str, torch.Tensor]\n            The evaluation metrics _Dict_ after averaging. The key names remain the same.\n        \"\"\"\n\n        def get_batch_size(input_dict: Dict):\n            _batch_size = None\n            for value in input_dict.values():\n                # len() considers all types of array: torch.Tensor, np.ndarray, List, ...\n                if _batch_size is None:\n                    _batch_size = len(value)\n                else:\n                    assert _batch_size == len(value)\n            return _batch_size\n\n        # check the batch size\n        multi_flag = sum(\n            [isinstance(value, Dict) for value in batch_data.values()]\n        ) == len(batch_data)\n        # we take the summation of all data-labels pairs in a single batch made by multiple dataloaders\n        if multi_flag:\n            batch_size = sum([get_batch_size(value) for value in batch_data.values()])\n        else:\n            batch_size = get_batch_size(batch_data)\n        batch_size = torch.tensor([batch_size], dtype=torch.long, device=self.device)\n\n        # sum up all the weighed metrics at rank no.0\n        for key in metrics.keys():\n            # each metric should be one-dimensional scalar\n            if metrics[key].dim() == 0:\n                metrics[key] = metrics[key][None]\n            elif metrics[key].dim() != 1:\n                raise RuntimeError(\n                    f\"Each metric value must be one-dimensional scalar, \"\n                    f\"but got metrics[{key}]={metrics[key]}!\"\n                )\n\n            # batch_size acts as the weight for each metric value in the current process\n            metrics[key] *= batch_size.type(metrics[key].dtype)\n            # sum up the weighted metric values at rank no.0\n            torch.distributed.reduce(\n                metrics[key], dst=0, op=torch.distributed.ReduceOp.SUM\n            )\n\n        # sum up the batch size across at rank no.0 to get the overall batch size\n        torch.distributed.reduce(batch_size, dst=0, op=torch.distributed.ReduceOp.SUM)\n        if torch.distributed.get_rank() == 0:\n            for key in metrics.keys():\n                # turn the object value to the overall batch-level\n                metrics[key] /= batch_size.type(metrics[key].dtype)\n\n        return metrics\n\n    @abstractmethod\n    def module_forward(self, epoch: int = None, **batch_data) -&gt; Dict:\n        \"\"\"\n        This function forwards the input batch data by all _Module_ members.\n        Note:\n            1. This interface function must be overridden for each Model subclass.\n            2. The argument names should match the key names in the returned Dict of `self.batch_preprocess_fn()`.\n            3. The key names in the returned Dict should match the argument names of `self.loss_calculation()` and\n            `self.metrics_calculation()`.\n\n        Args:\n            epoch:\n            **batch_data:\n                Processed data of the input batch received from `self.batch_preprocess_fn()`.\n\n        Returns: Dict\n            Prediction results (logits) of the model on the input batch data.\n            Some intermediate results (e.g., attention matrices) can also be returned for later use.\n\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    @abstractmethod\n    def criterion_forward(\n        self, **kwargs\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n        \"\"\"This interface function is activated after `self.model_forward()`. It\n        receives the model prediction results from `self.model_forward()` and input\n        batch data from `self.batch_preprocess_fn()`.\n\n        Args:\n            **kwargs:\n                The combination of the returned arguments from `self.batch_preprocess_fn()` and `self.model_forward()`.\n\n        Returns: (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]\n            The returned values should be different for the training and validation branches.\n            1. For training, two Dict[str, torch.Tensor] should be returned where the first one contains all the\n            trainable training losses for optimization and the second one contains all the non-trainable evaluation\n            metrics used to record the training status.\n            2. For validation, only one Dict[str, torch.Tensor] should be returned which contains all the non-trainable\n            evaluation metrics used to record the validation status.\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    def get_recordable_para(self) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Recursively retrieves the recordable parameters from the module's sub-\n        modules.\n\n        Returns:\n            Dict[str, torch.Tensor]: A dictionary mapping the parameter names to their corresponding tensor values.\n        \"\"\"\n\n        def recur_get_module_recordable_para(curr_node, prefix_list: List[str] = None):\n            if prefix_list is None:\n                prefix_list = []\n            if isinstance(curr_node, Dict):\n                _output = dict()\n                for _key, _value in curr_node.items():\n                    _output.update(\n                        recur_get_module_recordable_para(_value, prefix_list + [_key])\n                    )\n                return _output\n            else:\n                if curr_node is None:\n                    return {}\n                elif isinstance(curr_node, torch.Tensor):\n                    return {\"_\".join(prefix_list): curr_node.clone().detach()}\n                else:\n                    raise RuntimeError\n\n        output = dict()\n        for key, value in self._modules.items():\n            if isinstance(value, Module):\n                output.update(\n                    recur_get_module_recordable_para(value.get_recordable_para(), [key])\n                )\n        return output\n\n    def matrix_snapshot(\n        self,\n        vis_logs: List,\n        hypo_attention: Dict,\n        subfolder_names: List[str] or str,\n        epoch: int,\n    ):\n        \"\"\"Used by the abstract function visualize() to make the snapshot materials for\n        attention matrices.\"\"\"\n        if isinstance(subfolder_names, str):\n            subfolder_names = [subfolder_names]\n        keys = list(hypo_attention.keys())\n\n        # process the input data by different data types\n        if isinstance(hypo_attention[keys[0]], Dict):\n            for key, value in hypo_attention.items():\n                self.matrix_snapshot(\n                    vis_logs=vis_logs,\n                    hypo_attention=value,\n                    subfolder_names=subfolder_names + [key],\n                    epoch=epoch,\n                )\n\n        # snapshot the information in the materials\n        elif isinstance(hypo_attention[keys[0]], np.ndarray):\n            vis_logs.append(\n                dict(\n                    plot_type=\"matrix\",\n                    materials=hypo_attention,\n                    epoch=epoch,\n                    sep_save=False,\n                    data_save=True,\n                    subfolder_names=subfolder_names,\n                )\n            )\n\n    def attention_reshape(self, hypo_attention: Dict, prefix_list: List = None) -&gt; Dict:\n        \"\"\"Used by the abstract function visualize() to reshape the attention matrices\n        before matrix_snapshot().\"\"\"\n        if prefix_list is None:\n            prefix_list = []\n\n        # process the input data by different data types\n        if isinstance(hypo_attention, Dict):\n            return {\n                key: self.attention_reshape(value, prefix_list + [key])\n                for key, value in hypo_attention.items()\n            }\n        elif isinstance(hypo_attention, List):\n            return {\n                str(index - len(hypo_attention)): self.attention_reshape(\n                    hypo_attention[index],\n                    prefix_list + [str(index - len(hypo_attention))],\n                )\n                for index in range(len(hypo_attention) - 1, -1, -1)\n            }\n        elif isinstance(hypo_attention, torch.Tensor):\n            hypo_attention = hypo_attention.squeeze()\n            if hypo_attention.is_cuda:\n                hypo_attention = hypo_attention.detach().cpu()\n\n            if hypo_attention.dim() == 2:\n                return {\".\".join(prefix_list + [str(0)]): hypo_attention.numpy()}\n            elif hypo_attention.dim() == 3:\n                return {\n                    \".\".join(prefix_list + [str(index)]): element.numpy()\n                    for index, element in enumerate(hypo_attention)\n                }\n            else:\n                raise RuntimeError\n\n    @abstractmethod\n    def visualize(self, epoch: int, sample_index: str, **valid_sample):\n        \"\"\"\n\n        Args:\n            epoch:\n            sample_index:\n            **valid_sample:\n\n        Returns:\n\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    def evaluate(self, test_batch: Dict, infer_conf: Dict):\n        \"\"\"\n        The shared evaluation function by all _Model_ subclasses. This evaluation function has 2 steps:\n            1. preprocess and transfer the batch data to GPUs\n            2. calculate the inference results\n\n        For each step above, we provide interface functions for you to override and make your own implementation.\n\n        Args:\n            test_batch: Dict\n                The input batch data received from the `test` dataloader object in the experimental pipeline.\n            infer_conf: Dict\n                The configuration used for model inference.\n\n        Returns:\n            A Dict of the inference results where each key-value item corresponds to one evaluation metric you want to\n            save to the disk.\n\n        \"\"\"\n        # preprocess the batch data if needed\n        test_batch = self.batch_preprocess_fn(test_batch)\n\n        # put the batch data onto GPUs\n        test_batch = self.batch_to_cuda(test_batch)\n\n        # get the inference results\n        evaluate_results = self.inference(infer_conf=infer_conf, **test_batch)\n        if (\n            hasattr(self, \"instance_report_cache\")\n            and self.instance_report_cache is not None\n        ):\n            evaluate_results[\"instance_reports.md\"] = self.instance_report_cache\n            self.instance_report_cache = None\n\n        # post-check the format of evaluate_results\n        if isinstance(evaluate_results, Dict):\n            for key, value in evaluate_results.items():\n                if \"format\" not in value.keys() or \"content\" not in value.keys():\n                    raise RuntimeError(\n                        \"Each element of the returned value of self.inference() must contain the keys \"\n                        \"named both 'format' and 'content'!\"\n                    )\n        else:\n            raise RuntimeError(\n                f\"The returned value of self.inference() must be a Dict, \"\n                f\"but got {type(evaluate_results)}!\"\n            )\n        return evaluate_results\n\n    @abstractmethod\n    def inference(\n        self, infer_conf: Dict, **kwargs\n    ) -&gt; Dict[str, Dict[str, str or List]]:\n        \"\"\"This function receives the test data and test configuration. The inference\n        results will be packaged into a Dict[str, Dict] which is passed to TestMonitor\n        for disk storage. The returned Dict should be in the form of ``` dict(\n        {file_name}=dict( format={file_format},\n\n                content={file_content}\n            )\n        )\n        ```\n        The first-level key is used to decide the name of the meta file as `idx2{file_name}`. Its value is also a Dict\n        and there must be two keys in this sub-Dict: 'format' and 'content'. The configuration of the sub-Dict is\n        different for different file formats:\n\n            1. For pure text metadata files, the value of 'format' must be 'txt' and the value of 'content' must be a\n            list of Python built-in data type (i.e.,. int, float, str, bool, ...).\n            Each line of the file `idx2{file_name}` will be made up of the index of a test data instance and its\n            metadata value in the `content` List which are separated by a blank.\n            For example,\n            `dict(cer=dict(format='txt', content=[0.1, 0.2, 0.3]))` will create a pure text file named 'idx2cer' which\n            looks like\n            ```\n            {test_index1} 0.1\n            {test_index2} 0.2\n            {test_index3} 0.3\n            ```\n            Note: if the first-level key ends with '.md', there will not be 'idx2' attached at the beginning of the\n            file name.\n\n            2. For audio files, the value of 'format' must be either 'wav' or 'flac' and the value of 'content' must be\n            a list of array-like data type (e.g. numpy.ndarry, torch.Tensor, ...).\n            Moreover, there must be an additional key named 'sample_rate' to indicate the sampling rate of the waveforms\n            to be saved in audio files.\n            There will be a folder named `{file_name}` that contains all the audio files and a pure text file named\n            `idx2{file_name}` that contains the absolute paths of all the saved audio files.\n            For example,\n            `dict(wav=dict(format='flac', content=[np_arr1, np_arr2, np_arr3]))` will create a folder named 'wav' and\n            a pure text file named 'idx2wav' in the same directory. The file 'idx2wav' looks like:\n            ```\n            {test_index1} /x/xx/wav/{test_index1}.flac\n            {test_index2} /x/xx/wav/{test_index2}.flac\n            {test_index3} /x/xx/wav/{test_index3}.flac\n            ```\n            where `/x/xx/` is your result path given in your `exp_cfg`.\n\n            3. For binary files, the value of 'format' in the sub-Dict must be 'npy' and the value of 'content' must be\n            a list of numpy.ndarry (torch.Tensor is not supported).\n            There will be a folder named `{file_name}` that contains all the .npy files and a pure text file\n            named `idx2{file_name}` that contains the absolute paths of all the saved binary files.\n            For example,\n            `dict(feat=dict(format='npy', content=[np_arr1, np_arr2, np_arr3]))`\n            will create a folder named 'feat' and a pure text file named 'idx2feat'. The 'idx2feat' file is like:\n            ```\n            {test_index1} /x/xx/feat/{test_index1}.npy\n            {test_index2} /x/xx/feat/{test_index2}.npy\n            {test_index3} /x/xx/feat/{test_index3}.npy\n            ```\n            where `/x/xx/` is your result path given in your `exp_cfg`.\n        \"\"\"\n        pass  # raise NotImplementedError\n\n    def register_instance_reports(\n        self, md_list_dict: Dict[str, List], extra_string_list: List[str] = None\n    ):\n        \"\"\"\n\n        Args:\n            md_list_dict:\n            extra_string_list:\n\n        Returns:\n\n        \"\"\"\n        # --- 1. Arguments Checking --- #\n        if extra_string_list is not None:\n            assert isinstance(extra_string_list, List)\n\n        ele_len = []\n        for value in md_list_dict.values():\n            assert isinstance(value, List)\n            if extra_string_list is not None:\n                assert len(value) == len(extra_string_list)\n            ele_len.append(len(value))\n\n        if len(set(ele_len)) == 1:\n            ele_len = ele_len[0]\n        else:\n            raise RuntimeError\n\n        # --- 2. Generate .md Instance Report for the current step --- #\n        instance_reports = []\n        for i in range(ele_len):\n            ele_dict = {\n                key: value[i] if isinstance(value[i], str) else str(value[i])\n                for key, value in md_list_dict.items()\n            }\n            _curr_report = \"\\n\\n\" + get_list_strings(ele_dict) + \"\\n\"\n\n            if extra_string_list is not None:\n                _curr_report += extra_string_list[i] + \"\\n\"\n            instance_reports.append(_curr_report)\n\n        self.instance_report_cache = dict(format=\"txt\", content=instance_reports)\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.__init__","title":"<code>__init__(device, module_conf, result_path, model_conf=None, criterion_conf=None, non_blocking=False, distributed=False)</code>","text":"<p>In this initialization function, there are two parts of initialization: model-specific customized initialization and model- independent general initialization.</p> <p>Model-specific customized initialization is done by two interface functions: module_init() and criterion_init(). module_init() initializes the neural network structure of the model while criterion_init() initializes the criteria used to optimize (loss functions) and evaluate (validation metrics) the model.</p> <p>After the customized initialization, there are 3 steps for general initialization:     1. Pretrained parameters will be loaded into your model if the key <code>pretrained_model</code> is given. Multiple     pretrained models can be specified and each of them can be loaded into different parts of your model. The     mismatch between the names of pretrained parameters and the parameters of your model is handled by the key     'mapping'. The value of the key <code>mapping</code> is a dictionary where each key-value item corresponds to a mapping     of parameter names. The key is the parameter name in the pretrained parameters while the value is the     parameter name of your model.</p> <pre><code>2. If `pretrained_model` is not given, the parameters of your model will be initialized by the function that\nmatches your input query 'init'. Please refer to the built-in dictionary `init_class_dict` for the available\ninitialization functions. If `init` is not given, the default initialization function\n`torch.nn.init.xavier_normal_` will be used to initialize your model.\n\n3. Finally, the specified parts of your model will be frozen if 'frozen_modules' is given. If there is only\none frozen module, you can directly give the string of its name to 'frozen_modules' like\n'frozen_modules: {module_name}'; if there are multiple modules you want to freeze, you can give their names\nin a list as\n```\nfrozen_modules:\n  - {module_name1}\n  - {module_name2}\n  - ...\n```\nMoreover, the frozen granularity depends on your input `frozen_modules`.\nFor example,\n    1. If you give 'frozen_modules: encoder_prenet', all parameters of the prenet of your encoder will be\n    frozen\n    2. If you give 'frozen_modules: encoder_prenet.conv', only the convolution layers of the prenet of your\n    encoder will be frozen\n    3. If you give 'frozen_modules: encoder_prenet.conv.0', only the first convolution layer of the prenet\n    of your encoder will be frozen\n    4. If you give 'frozen_modules: encoder_prenet.conv.0.bias', only the bias vector of the first\n    convolution layer of the prenet of your encoder will be frozen\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The computational device used for model calculation in the current GPU process.</p> required <code>model_conf</code> <code>Dict</code> <p>The model configuration used for general model initialization.</p> <code>None</code> <code>module_conf</code> <code>Dict</code> <p>The module configuration used for network structure initialization.</p> required <code>criterion_conf</code> <code>Dict</code> <p>The criterion configuration used for criterion (loss functions and evaluation metrics) initialization.</p> <code>None</code> Source code in <code>speechain/model/abs.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    module_conf: Dict,\n    result_path: str,\n    model_conf: Dict = None,\n    criterion_conf: Dict = None,\n    non_blocking: bool = False,\n    distributed: bool = False,\n):\n    \"\"\"In this initialization function, there are two parts of\n    initialization: model-specific customized initialization and model-\n    independent general initialization.\n\n    Model-specific customized initialization is done by two interface functions: module_init() and criterion_init().\n    module_init() initializes the neural network structure of the model while criterion_init() initializes the\n    criteria used to optimize (loss functions) and evaluate (validation metrics) the model.\n\n    After the customized initialization, there are 3 steps for general initialization:\n        1. Pretrained parameters will be loaded into your model if the key `pretrained_model` is given. Multiple\n        pretrained models can be specified and each of them can be loaded into different parts of your model. The\n        mismatch between the names of pretrained parameters and the parameters of your model is handled by the key\n        'mapping'. The value of the key `mapping` is a dictionary where each key-value item corresponds to a mapping\n        of parameter names. The key is the parameter name in the pretrained parameters while the value is the\n        parameter name of your model.\n\n        2. If `pretrained_model` is not given, the parameters of your model will be initialized by the function that\n        matches your input query 'init'. Please refer to the built-in dictionary `init_class_dict` for the available\n        initialization functions. If `init` is not given, the default initialization function\n        `torch.nn.init.xavier_normal_` will be used to initialize your model.\n\n        3. Finally, the specified parts of your model will be frozen if 'frozen_modules' is given. If there is only\n        one frozen module, you can directly give the string of its name to 'frozen_modules' like\n        'frozen_modules: {module_name}'; if there are multiple modules you want to freeze, you can give their names\n        in a list as\n        ```\n        frozen_modules:\n          - {module_name1}\n          - {module_name2}\n          - ...\n        ```\n        Moreover, the frozen granularity depends on your input `frozen_modules`.\n        For example,\n            1. If you give 'frozen_modules: encoder_prenet', all parameters of the prenet of your encoder will be\n            frozen\n            2. If you give 'frozen_modules: encoder_prenet.conv', only the convolution layers of the prenet of your\n            encoder will be frozen\n            3. If you give 'frozen_modules: encoder_prenet.conv.0', only the first convolution layer of the prenet\n            of your encoder will be frozen\n            4. If you give 'frozen_modules: encoder_prenet.conv.0.bias', only the bias vector of the first\n            convolution layer of the prenet of your encoder will be frozen\n\n    Args:\n        device (torch.device):\n            The computational device used for model calculation in the current GPU process.\n        model_conf (Dict):\n            The model configuration used for general model initialization.\n        module_conf (Dict):\n            The module configuration used for network structure initialization.\n        criterion_conf (Dict):\n            The criterion configuration used for criterion (loss functions and evaluation metrics) initialization.\n    \"\"\"\n    super(Model, self).__init__()\n\n    # input argument checking\n    assert module_conf is not None, \"module_conf cannot be None!\"\n    # model_conf is default to be an empty dictionary\n    model_conf = dict() if model_conf is None else model_conf\n    # criterion_conf is default to be an empty dictionary\n    criterion_conf = dict() if criterion_conf is None else criterion_conf\n    # customize_conf is default to be an empty dictionary\n    if \"customize_conf\" not in model_conf.keys():\n        model_conf[\"customize_conf\"] = dict()\n\n    # general argument registration\n    self.non_blocking = non_blocking\n    self.distributed = distributed\n    self.device = device\n\n    # snapshotting-related argument registration\n    self.result_path = result_path\n    if \"visual_infer_conf\" in model_conf.keys():\n        # configuration is given as a .yaml file\n        if isinstance(model_conf[\"visual_infer_conf\"], str):\n            self.visual_infer_conf = load_yaml(\n                open(parse_path_args(model_conf[\"visual_infer_conf\"]))\n            )\n        # configuration is explicitly given\n        elif isinstance(model_conf[\"visual_infer_conf\"], Dict):\n            self.visual_infer_conf = model_conf[\"visual_infer_conf\"]\n        else:\n            raise RuntimeError(\n                \"model_conf['visual_infer_conf'] must be given as either a string or a Dict.\"\n            )\n    else:\n        self.visual_infer_conf = dict()\n\n    # --- 1. Model Construction --- #\n    self.module_init(**module_conf, **model_conf[\"customize_conf\"])\n    self.criterion_init(**criterion_conf)\n    # initialize the bad case selection methods by the hook function\n    self.bad_cases_selection = self.bad_cases_selection_init_fn()\n\n    # --- 2.1. Pretrained Model Loading --- #\n    pretrained_model = (\n        model_conf[\"pretrained_model\"]\n        if \"pretrained_model\" in model_conf.keys()\n        else None\n    )\n    if pretrained_model is not None:\n        pretrained_model = (\n            pretrained_model\n            if isinstance(pretrained_model, list)\n            else [pretrained_model]\n        )\n\n        for ptm in pretrained_model:\n            # argument checking\n            if isinstance(ptm, str):\n                ptm = dict(path=parse_path_args(ptm))\n            elif isinstance(ptm, Dict):\n                assert \"path\" in ptm.keys(), (\n                    \"If model['model_conf']['pretrained_model'] is given as a Dict, \"\n                    \"please give a key named 'path' to specify where your pretrained model is placed.\"\n                )\n                if os.path.exists(ptm[\"path\"]):\n                    raise RuntimeError(\n                        f\"The specified path of your pretrained model {ptm['path']} doesn't exist! \"\n                        f\"Please check the input path.\"\n                    )\n            else:\n                raise TypeError(\n                    f\"The elements in model['model_conf']['pretrained_model'] must be either a string \"\n                    f\"or a Dict, but got {ptm}\"\n                )\n\n            _pt_model = torch.load(\n                parse_path_args(ptm[\"path\"]), map_location=self.device\n            )\n            mapping = ptm[\"mapping\"] if \"mapping\" in ptm.keys() else None\n            if mapping is None:\n                self.load_state_dict(\n                    _pt_model,\n                    strict=True if \"strict\" not in ptm.keys() else ptm[\"strict\"],\n                )\n            else:\n                assert isinstance(mapping, dict) and len(mapping) &gt;= 1, (\n                    f\"mapping must be given as a dict and cannot be empty! \"\n                    f\"Got type(mapping)={type(mapping)} and len(mapping)={len(mapping)}\"\n                )\n\n                _src_modules = OrderedDict()\n                # loop each name-parameter pair in the model\n                for name, para in _pt_model.items():\n                    # loop each source-target mapping pair\n                    for src, tgt in mapping.items():\n                        # attach '.' to the end is for making the name unique\n                        src, tgt = src + \".\", tgt + \".\"\n                        # change the parameter name in the middle\n                        if src in name:\n                            name = name.replace(src, tgt)\n                    # record the parameter no matter whether its name is modified or not\n                    _src_modules[name] = para\n                self.load_state_dict(\n                    _src_modules,\n                    strict=True if \"strict\" not in ptm.keys() else ptm[\"strict\"],\n                )\n\n    # --- 2.2. Model Parameter Initialization --- #\n    else:\n        # the default initialization method is xavier (i.e. xavier_normal)\n        init = model_conf[\"init\"] if \"init\" in model_conf.keys() else \"xavier\"\n        assert (\n            init in self.init_class_dict.keys()\n        ), f\"Only the initialization methods {self.init_class_dict.keys()} are supported, but got init={init}.\"\n\n        for name, para in self.named_parameters():\n            # initialize all the bias vectors to zero\n            if \".bias\" in name and para.dim() == 1:\n                torch.nn.init.zeros_(para)\n            # initialize all the weight vectors except for those of normalization layers (BatchNorm &amp; LayerNorm)\n            elif para.dim() &gt; 1:\n                self.init_class_dict[init](para)\n\n        # initialize the modules that have their own default init methods\n        for module in self.modules():\n            if isinstance(module, tuple(self.default_init_modules)):\n                module.reset_parameters()\n\n    # --- 3. Model Parameter Freezing --- #\n    frozen_modules = (\n        model_conf[\"frozen_modules\"]\n        if \"frozen_modules\" in model_conf.keys()\n        else None\n    )\n    if frozen_modules is not None:\n        if frozen_modules != \"all\":\n            frozen_modules = (\n                frozen_modules\n                if isinstance(frozen_modules, list)\n                else [frozen_modules]\n            )\n\n        for name, para in self.named_parameters():\n            frozen_flag = False\n            if frozen_modules != \"all\":\n                for module in frozen_modules:\n                    frozen_flag = name.startswith(module + \".\")\n            else:\n                frozen_flag = True\n\n            if frozen_flag:\n                para.requires_grad = False\n            else:\n                raise RuntimeError(\n                    f\"frozen_modules: Parameters of {name} are not found in the model!\"\n                )\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.attention_reshape","title":"<code>attention_reshape(hypo_attention, prefix_list=None)</code>","text":"<p>Used by the abstract function visualize() to reshape the attention matrices before matrix_snapshot().</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def attention_reshape(self, hypo_attention: Dict, prefix_list: List = None) -&gt; Dict:\n    \"\"\"Used by the abstract function visualize() to reshape the attention matrices\n    before matrix_snapshot().\"\"\"\n    if prefix_list is None:\n        prefix_list = []\n\n    # process the input data by different data types\n    if isinstance(hypo_attention, Dict):\n        return {\n            key: self.attention_reshape(value, prefix_list + [key])\n            for key, value in hypo_attention.items()\n        }\n    elif isinstance(hypo_attention, List):\n        return {\n            str(index - len(hypo_attention)): self.attention_reshape(\n                hypo_attention[index],\n                prefix_list + [str(index - len(hypo_attention))],\n            )\n            for index in range(len(hypo_attention) - 1, -1, -1)\n        }\n    elif isinstance(hypo_attention, torch.Tensor):\n        hypo_attention = hypo_attention.squeeze()\n        if hypo_attention.is_cuda:\n            hypo_attention = hypo_attention.detach().cpu()\n\n        if hypo_attention.dim() == 2:\n            return {\".\".join(prefix_list + [str(0)]): hypo_attention.numpy()}\n        elif hypo_attention.dim() == 3:\n            return {\n                \".\".join(prefix_list + [str(index)]): element.numpy()\n                for index, element in enumerate(hypo_attention)\n            }\n        else:\n            raise RuntimeError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.aver_metrics_across_procs","title":"<code>aver_metrics_across_procs(metrics, batch_data)</code>","text":"<p>This function averages the evaluation metrics across all GPU processes in the DDP mode for model distribution.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor] The evaluation metrics to be averaged across all GPU processes.</p> required <code>batch_data</code> <code>Dict</code> <p>Dict The input batch data used to calculate the batch size for averaging evaluation metrics.</p> required <p>Dict[str, torch.Tensor]</p> Type Description <code>Dict[str, Tensor]</code> <p>The evaluation metrics Dict after averaging. The key names remain the same.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def aver_metrics_across_procs(\n    self, metrics: Dict[str, torch.Tensor], batch_data: Dict\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"This function averages the evaluation metrics across all GPU processes in the\n    DDP mode for model distribution.\n\n    Args:\n        metrics: Dict[str, torch.Tensor]\n            The evaluation metrics to be averaged across all GPU processes.\n        batch_data: Dict\n            The input batch data used to calculate the batch size for averaging evaluation metrics.\n\n    Returns: Dict[str, torch.Tensor]\n        The evaluation metrics _Dict_ after averaging. The key names remain the same.\n    \"\"\"\n\n    def get_batch_size(input_dict: Dict):\n        _batch_size = None\n        for value in input_dict.values():\n            # len() considers all types of array: torch.Tensor, np.ndarray, List, ...\n            if _batch_size is None:\n                _batch_size = len(value)\n            else:\n                assert _batch_size == len(value)\n        return _batch_size\n\n    # check the batch size\n    multi_flag = sum(\n        [isinstance(value, Dict) for value in batch_data.values()]\n    ) == len(batch_data)\n    # we take the summation of all data-labels pairs in a single batch made by multiple dataloaders\n    if multi_flag:\n        batch_size = sum([get_batch_size(value) for value in batch_data.values()])\n    else:\n        batch_size = get_batch_size(batch_data)\n    batch_size = torch.tensor([batch_size], dtype=torch.long, device=self.device)\n\n    # sum up all the weighed metrics at rank no.0\n    for key in metrics.keys():\n        # each metric should be one-dimensional scalar\n        if metrics[key].dim() == 0:\n            metrics[key] = metrics[key][None]\n        elif metrics[key].dim() != 1:\n            raise RuntimeError(\n                f\"Each metric value must be one-dimensional scalar, \"\n                f\"but got metrics[{key}]={metrics[key]}!\"\n            )\n\n        # batch_size acts as the weight for each metric value in the current process\n        metrics[key] *= batch_size.type(metrics[key].dtype)\n        # sum up the weighted metric values at rank no.0\n        torch.distributed.reduce(\n            metrics[key], dst=0, op=torch.distributed.ReduceOp.SUM\n        )\n\n    # sum up the batch size across at rank no.0 to get the overall batch size\n    torch.distributed.reduce(batch_size, dst=0, op=torch.distributed.ReduceOp.SUM)\n    if torch.distributed.get_rank() == 0:\n        for key in metrics.keys():\n            # turn the object value to the overall batch-level\n            metrics[key] /= batch_size.type(metrics[key].dtype)\n\n    return metrics\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.bad_cases_selection_init_fn","title":"<code>bad_cases_selection_init_fn()</code>  <code>staticmethod</code>","text":"<p>This hook function returns the default bad case selection method of each Model object. This default value will be referred by the Runner to present the top-N bad cases.</p> <p>The original hook implementation in the base Model class returns None which means no default value.</p> <p>List[List[str or int]]</p> Type Description <code>List[List[str or int]] or None</code> <p>The returned default value should be a list of tri-list where each tri-list is in the form of</p> <code>List[List[str or int]] or None</code> <p>[<code>selection_metric</code>, <code>selection_mode</code>, <code>case_number</code>]. For example, ['wer', 'max', 50] means 50 testing</p> <code>List[List[str or int]] or None</code> <p>waveforms with the largest WER will be selected.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>@staticmethod\ndef bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n    \"\"\"This hook function returns the default bad case selection method of each\n    Model object. This default value will be referred by the _Runner_ to present the\n    top-N bad cases.\n\n    The original hook implementation in the base Model class returns None which means no default value.\n\n    Returns: List[List[str or int]]\n        The returned default value should be a list of tri-list where each tri-list is in the form of\n        [`selection_metric`, `selection_mode`, `case_number`]. For example, ['wer', 'max', 50] means 50 testing\n        waveforms with the largest WER will be selected.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.batch_preprocess_fn","title":"<code>batch_preprocess_fn(batch_data)</code>","text":"<p>This hook function does the preprocessing for the input batch data before using them in self.model_forward(). This function is not mandatory to be overridden and the original implementation in the base Model class does the tensor transformation for the string-like data in batch_data (i.e., text and spk_ids).</p> <p>Note: the key names in the returned Dict should match the argument names in self.model_forward().</p> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Dict</code> <p>Dict The raw data of the input batch to be preprocessed in this hook function.</p> required <p>Dict</p> Type Description <code>Dict</code> <p>The processed data of the input batch that is ready to be used in <code>self.model_forward()</code>.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def batch_preprocess_fn(self, batch_data: Dict) -&gt; Dict:\n    \"\"\"This hook function does the preprocessing for the input batch data before\n    using them in self.model_forward(). This function is not mandatory to be\n    overridden and the original implementation in the base Model class does the\n    tensor transformation for the string-like data in batch_data (i.e., text and\n    spk_ids).\n\n    Note: the key names in the returned Dict should match the argument names in self.model_forward().\n\n    Args:\n        batch_data: Dict\n            The raw data of the input batch to be preprocessed in this hook function.\n\n    Returns: Dict\n        The processed data of the input batch that is ready to be used in `self.model_forward()`.\n    \"\"\"\n\n    def process_strings(data_dict: Dict):\n        \"\"\"Turn the text and speaker strings into tensors and get their lengths.\"\"\"\n        # --- Process the Text String and its Length --- #\n        if \"text\" in data_dict.keys():\n            if isinstance(data_dict[\"text\"], List):\n                data_dict[\"text\"], data_dict[\"text_len\"] = text2tensor_and_len(\n                    text_list=data_dict[\"text\"],\n                    text2tensor_func=self.tokenizer.text2tensor,\n                    ignore_idx=self.tokenizer.ignore_idx,\n                )\n            else:\n                assert isinstance(data_dict[\"text\"], torch.Tensor)\n\n        # --- Process the Speaker ID String --- #\n        if \"spk_ids\" in data_dict.keys():\n            if isinstance(data_dict[\"spk_ids\"], List):\n                if hasattr(self, \"spk2idx\"):\n                    data_dict[\"spk_ids\"] = spk2tensor(\n                        spk_list=data_dict[\"spk_ids\"], spk2idx_dict=self.spk2idx\n                    )\n            elif not isinstance(data_dict[\"spk_ids\"], torch.Tensor):\n                raise TypeError\n\n        return data_dict\n\n    # check whether the batch_data is made by multiple dataloaders\n    leaf_flags = [not isinstance(value, Dict) for value in batch_data.values()]\n    if sum(leaf_flags) == 0:\n        return {key: process_strings(value) for key, value in batch_data.items()}\n    elif sum(leaf_flags) == len(batch_data):\n        return process_strings(batch_data)\n    else:\n        raise RuntimeError(\"Wrong composition of batch_data!\")\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.batch_to_cuda","title":"<code>batch_to_cuda(data)</code>","text":"<p>The recursive function that transfers the batch data to the specified device in the current process.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Tensor] or Tensor</code> <p>Dict or torch.Tensor The input batch data. It should be either a Tensor or a Dict of Tensors. For the Dict input, the function itself will be called once by each Tensor element.</p> required <p>Dict or torch.Tensor</p> Type Description <code>Dict[str, Tensor] or Tensor</code> <p>If the input is a Dict, the returned output will also be a Dict of Tensors transferred to the target device;</p> <code>Dict[str, Tensor] or Tensor</code> <p>If the input is a Tensor, the returned output will be its copy on the target device.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def batch_to_cuda(\n    self, data: Dict[str, torch.Tensor] or torch.Tensor\n) -&gt; Dict[str, torch.Tensor] or torch.Tensor:\n    \"\"\"The recursive function that transfers the batch data to the specified device\n    in the current process.\n\n    Args:\n        data: Dict or torch.Tensor\n            The input batch data. It should be either a Tensor or a Dict of Tensors. For the Dict input, the\n            function itself will be called once by each Tensor element.\n\n    Returns: Dict or torch.Tensor\n        If the input is a Dict, the returned output will also be a Dict of Tensors transferred to the target device;\n        If the input is a Tensor, the returned output will be its copy on the target device.\n    \"\"\"\n    # if the data is in the form of Dict, recursively process each key-value pair\n    if isinstance(data, Dict):\n        return {key: self.batch_to_cuda(value) for key, value in data.items()}\n    # if the data is in the form of tensor, put it on GPUs by .cuda()\n    elif isinstance(data, torch.Tensor):\n        return data.cuda(device=self.device, non_blocking=self.non_blocking)\n    # do nothing for other types of data\n    else:\n        return data\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.criterion_forward","title":"<code>criterion_forward(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>This interface function is activated after <code>self.model_forward()</code>. It receives the model prediction results from <code>self.model_forward()</code> and input batch data from <code>self.batch_preprocess_fn()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The combination of the returned arguments from <code>self.batch_preprocess_fn()</code> and <code>self.model_forward()</code>.</p> <code>{}</code> <p>(Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]</p> Type Description <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <p>The returned values should be different for the training and validation branches.</p> <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <ol> <li>For training, two Dict[str, torch.Tensor] should be returned where the first one contains all the</li> </ol> <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <p>trainable training losses for optimization and the second one contains all the non-trainable evaluation</p> <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <p>metrics used to record the training status.</p> <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <ol> <li>For validation, only one Dict[str, torch.Tensor] should be returned which contains all the non-trainable</li> </ol> <code>(Dict[str, Tensor], Dict[str, Tensor]) or Dict[str, Tensor]</code> <p>evaluation metrics used to record the validation status.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef criterion_forward(\n    self, **kwargs\n) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n    \"\"\"This interface function is activated after `self.model_forward()`. It\n    receives the model prediction results from `self.model_forward()` and input\n    batch data from `self.batch_preprocess_fn()`.\n\n    Args:\n        **kwargs:\n            The combination of the returned arguments from `self.batch_preprocess_fn()` and `self.model_forward()`.\n\n    Returns: (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]\n        The returned values should be different for the training and validation branches.\n        1. For training, two Dict[str, torch.Tensor] should be returned where the first one contains all the\n        trainable training losses for optimization and the second one contains all the non-trainable evaluation\n        metrics used to record the training status.\n        2. For validation, only one Dict[str, torch.Tensor] should be returned which contains all the non-trainable\n        evaluation metrics used to record the validation status.\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.criterion_init","title":"<code>criterion_init(**criterion_conf)</code>  <code>abstractmethod</code>","text":"<p>The interface function that initializes the Criterion members of the model. These Criterion members can be divided into two parts: the loss functions used for training and the evaluation metrics used for validation.</p> <p>Parameters:</p> Name Type Description Default <code>**criterion_conf</code> <p>The arguments in your given <code>criterion_conf</code>.</p> <code>{}</code> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef criterion_init(self, **criterion_conf) -&gt; None:\n    \"\"\"\n    The interface function that initializes the Criterion members of the model. These Criterion members can be\n    divided into two parts: the loss functions used for training and the evaluation metrics used for validation.\n\n    Args:\n        **criterion_conf:\n            The arguments in your given `criterion_conf`.\n\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.evaluate","title":"<code>evaluate(test_batch, infer_conf)</code>","text":"<p>The shared evaluation function by all Model subclasses. This evaluation function has 2 steps:     1. preprocess and transfer the batch data to GPUs     2. calculate the inference results</p> <p>For each step above, we provide interface functions for you to override and make your own implementation.</p> <p>Parameters:</p> Name Type Description Default <code>test_batch</code> <code>Dict</code> <p>Dict The input batch data received from the <code>test</code> dataloader object in the experimental pipeline.</p> required <code>infer_conf</code> <code>Dict</code> <p>Dict The configuration used for model inference.</p> required <p>Returns:</p> Type Description <p>A Dict of the inference results where each key-value item corresponds to one evaluation metric you want to</p> <p>save to the disk.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def evaluate(self, test_batch: Dict, infer_conf: Dict):\n    \"\"\"\n    The shared evaluation function by all _Model_ subclasses. This evaluation function has 2 steps:\n        1. preprocess and transfer the batch data to GPUs\n        2. calculate the inference results\n\n    For each step above, we provide interface functions for you to override and make your own implementation.\n\n    Args:\n        test_batch: Dict\n            The input batch data received from the `test` dataloader object in the experimental pipeline.\n        infer_conf: Dict\n            The configuration used for model inference.\n\n    Returns:\n        A Dict of the inference results where each key-value item corresponds to one evaluation metric you want to\n        save to the disk.\n\n    \"\"\"\n    # preprocess the batch data if needed\n    test_batch = self.batch_preprocess_fn(test_batch)\n\n    # put the batch data onto GPUs\n    test_batch = self.batch_to_cuda(test_batch)\n\n    # get the inference results\n    evaluate_results = self.inference(infer_conf=infer_conf, **test_batch)\n    if (\n        hasattr(self, \"instance_report_cache\")\n        and self.instance_report_cache is not None\n    ):\n        evaluate_results[\"instance_reports.md\"] = self.instance_report_cache\n        self.instance_report_cache = None\n\n    # post-check the format of evaluate_results\n    if isinstance(evaluate_results, Dict):\n        for key, value in evaluate_results.items():\n            if \"format\" not in value.keys() or \"content\" not in value.keys():\n                raise RuntimeError(\n                    \"Each element of the returned value of self.inference() must contain the keys \"\n                    \"named both 'format' and 'content'!\"\n                )\n    else:\n        raise RuntimeError(\n            f\"The returned value of self.inference() must be a Dict, \"\n            f\"but got {type(evaluate_results)}!\"\n        )\n    return evaluate_results\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.forward","title":"<code>forward(batch_data, epoch=None, **kwargs)</code>","text":"<p>The general model forward function shared by all the Model subclasses. This forward function has 3 steps:     1. preprocess and transfer the batch data to GPUs     2. obtain the model prediction results     3. calculate the loss function and evaluate the prediction results</p> <p>For each step above, we provide interface functions for you to override and make your own implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Dict</code> <p>Dict The input batch data received from the <code>train</code> or <code>valid</code> dataloader object in the experimental pipeline.</p> required <code>epoch</code> <code>int</code> <p>int = None The number of the current epoch. Used for real-time model visualization and model prediction.</p> <code>None</code> <code>**kwargs</code> <p>The additional arguments for real-time model visualization. If given, the code will go through the model visualization branch.</p> <code>{}</code> <p>Returns:</p> Type Description <p>In the training branch, the loss functions and evaluation metrics will be returned each of which is in the</p> <p>form of a Dict.</p> <p>In the validation branch, only the evaluation metrics will be returned.</p> <p>In the visualization branch, the model snapshots on the given validation instance will be returned.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def forward(self, batch_data: Dict, epoch: int = None, **kwargs):\n    \"\"\"\n    The general model forward function shared by all the _Model_ subclasses. This forward function has 3 steps:\n        1. preprocess and transfer the batch data to GPUs\n        2. obtain the model prediction results\n        3. calculate the loss function and evaluate the prediction results\n\n    For each step above, we provide interface functions for you to override and make your own implementation.\n\n    Args:\n        batch_data: Dict\n            The input batch data received from the `train` or `valid` dataloader object in the experimental\n            pipeline.\n        epoch: int = None\n            The number of the current epoch. Used for real-time model visualization and model prediction.\n        **kwargs:\n            The additional arguments for real-time model visualization. If given, the code will go through the model\n            visualization branch.\n\n    Returns:\n        In the training branch, the loss functions and evaluation metrics will be returned each of which is in the\n        form of a Dict.\n        In the validation branch, only the evaluation metrics will be returned.\n        In the visualization branch, the model snapshots on the given validation instance will be returned.\n\n    \"\"\"\n    # --- 1. Batch Data Preprocessing and GPU transferring --- #\n    # --- data preparation below is shared by all the three branches: training, validation, and visualization --- #\n    # preprocess the batch data if needed\n    batch_data = self.batch_preprocess_fn(batch_data)\n\n    # put the batch data onto GPUs\n    batch_data = self.batch_to_cuda(batch_data)\n\n    # --- 2.1. Model Visualization Branch --- #\n    # if there are additional arguments other than batch_data and epoch, the visualization branch is activated\n    if len(kwargs) != 0:\n        return self.visualize(epoch=epoch, **batch_data, **kwargs)\n\n    # --- 2.2. Model Forward Calculation --- #\n    # --- model forward is shared by both the training and validation branches --- #\n    # context function used when doing the loss backward for efficient gradient accumulation in the DDP mode\n    forward_context = nullcontext if self.training else torch.inference_mode\n    with forward_context():\n        try:\n            # Feed the input batch into the model and get the outputs, copy.deepcopy() here is for the data safety\n            model_outputs = self.module_forward(\n                epoch=epoch, **copy.deepcopy(batch_data)\n            )\n        except Exception as e:\n            if not self.distributed:\n                raise e\n            else:\n                skip_flag_list = torch.LongTensor(\n                    [False for _ in range(torch.distributed.get_world_size())]\n                ).cuda(self.device)\n                skip_flag = torch.LongTensor([True]).cuda(self.device)\n                # as long as one node meets an error, all nodes will skip the current step at the same time\n                torch.distributed.all_gather_into_tensor(skip_flag_list, skip_flag)\n                if skip_flag_list.sum() &gt;= 1:\n                    raise e\n        else:\n            if self.distributed:\n                skip_flag_list = torch.LongTensor(\n                    [False for _ in range(torch.distributed.get_world_size())]\n                ).cuda(self.device)\n                skip_flag = torch.LongTensor([False]).cuda(self.device)\n                # as long as one node meets an error, all nodes will skip the current step at the same time\n                torch.distributed.all_gather_into_tensor(skip_flag_list, skip_flag)\n                if skip_flag_list.sum() &gt;= 1:\n                    raise RuntimeError(\n                        \"Other ranks meet errors during model forwarding, \"\n                        \"so this rank will also skip the current step!\"\n                    )\n\n    # copy.deepcopy() cannot receive the non-leaf nodes in the computation graph (model_outputs). Since\n    # model_outputs cannot be detached from the graph (gradients necessary), copy.deepcopy() is not used below.\n    def combine_input_output(_batch_data: Dict, _model_outputs: Dict):\n        combination, batch_keys = dict(), list(_batch_data.keys())\n        # if the input batch data is in the form of Dict, it means there are multiple dataloaders\n        if isinstance(_batch_data[batch_keys[0]], Dict):\n            for key in batch_keys:\n                combination[key] = dict(**_batch_data[key], **_model_outputs[key])\n        # if the input batch data is in the form of Tensor, it means there is only one dataloader.\n        else:\n            combination.update(_batch_data)\n            combination.update(_model_outputs)\n        return combination\n\n    # --- 3.1. Model Training Branch --- #\n    if self.training:\n        # In the training stage, both the trainable losses and non-trainable metrics will be returned\n        losses, metrics = self.criterion_forward(\n            **combine_input_output(batch_data, model_outputs)\n        )\n        metrics.update(self.get_recordable_para())\n\n        # post-checking for training losses, they must be trainable tensors\n        assert sum(\n            [\n                isinstance(loss, torch.Tensor) and loss.requires_grad\n                for loss in losses.values()\n            ]\n        ) == len(losses), \"Training losses must be trainable tensors!\"\n        # post-checking for validation metrics, they must be either non-trainable tensors or other datatypes\n        assert sum(\n            [\n                not isinstance(metric, torch.Tensor) or not metric.requires_grad\n                for metric in metrics.values()\n            ]\n        ) == len(\n            metrics\n        ), \"Validation metrics must be either non-trainable tensors or other datatypes!\"\n\n        # the non-trainable metrics will be averaged across all the processes in the distributed mode\n        if self.distributed:\n            metrics = self.aver_metrics_across_procs(metrics, batch_data)\n        return losses, metrics\n\n    # --- 3.2. Model Validation Branch --- #\n    else:\n        # In the validation stage, only the non-trainable metrics will be returned\n        with torch.inference_mode():\n            metrics = self.criterion_forward(\n                **combine_input_output(batch_data, model_outputs)\n            )\n        metrics.update(self.get_recordable_para())\n\n        # post-checking for validation metrics, they must be either non-trainable tensors or other datatypes\n        assert sum(\n            [\n                not isinstance(metric, torch.Tensor) or not metric.requires_grad\n                for metric in metrics.values()\n            ]\n        ) == len(\n            metrics\n        ), \"Validation metrics must be either non-trainable tensors or other datatypes!\"\n\n        # the non-trainable metrics will be averaged across all the processes in the distributed mode\n        if self.distributed:\n            metrics = self.aver_metrics_across_procs(metrics, batch_data)\n        return metrics\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.get_recordable_para","title":"<code>get_recordable_para()</code>","text":"<p>Recursively retrieves the recordable parameters from the module's sub- modules.</p> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor]: A dictionary mapping the parameter names to their corresponding tensor values.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def get_recordable_para(self) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Recursively retrieves the recordable parameters from the module's sub-\n    modules.\n\n    Returns:\n        Dict[str, torch.Tensor]: A dictionary mapping the parameter names to their corresponding tensor values.\n    \"\"\"\n\n    def recur_get_module_recordable_para(curr_node, prefix_list: List[str] = None):\n        if prefix_list is None:\n            prefix_list = []\n        if isinstance(curr_node, Dict):\n            _output = dict()\n            for _key, _value in curr_node.items():\n                _output.update(\n                    recur_get_module_recordable_para(_value, prefix_list + [_key])\n                )\n            return _output\n        else:\n            if curr_node is None:\n                return {}\n            elif isinstance(curr_node, torch.Tensor):\n                return {\"_\".join(prefix_list): curr_node.clone().detach()}\n            else:\n                raise RuntimeError\n\n    output = dict()\n    for key, value in self._modules.items():\n        if isinstance(value, Module):\n            output.update(\n                recur_get_module_recordable_para(value.get_recordable_para(), [key])\n            )\n    return output\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.inference","title":"<code>inference(infer_conf, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>This function receives the test data and test configuration. The inference results will be packaged into a Dict[str, Dict] which is passed to TestMonitor for disk storage. The returned Dict should be in the form of ``` dict( {file_name}=dict( format={file_format},</p> <pre><code>    content={file_content}\n)\n</code></pre> <p>) <code>`` The first-level key is used to decide the name of the meta file as</code>idx2{file_name}`. Its value is also a Dict and there must be two keys in this sub-Dict: 'format' and 'content'. The configuration of the sub-Dict is different for different file formats:</p> <pre><code>1. For pure text metadata files, the value of 'format' must be 'txt' and the value of 'content' must be a\nlist of Python built-in data type (i.e.,. int, float, str, bool, ...).\nEach line of the file `idx2{file_name}` will be made up of the index of a test data instance and its\nmetadata value in the `content` List which are separated by a blank.\nFor example,\n`dict(cer=dict(format='txt', content=[0.1, 0.2, 0.3]))` will create a pure text file named 'idx2cer' which\nlooks like\n```\n{test_index1} 0.1\n{test_index2} 0.2\n{test_index3} 0.3\n```\nNote: if the first-level key ends with '.md', there will not be 'idx2' attached at the beginning of the\nfile name.\n\n2. For audio files, the value of 'format' must be either 'wav' or 'flac' and the value of 'content' must be\na list of array-like data type (e.g. numpy.ndarry, torch.Tensor, ...).\nMoreover, there must be an additional key named 'sample_rate' to indicate the sampling rate of the waveforms\nto be saved in audio files.\nThere will be a folder named `{file_name}` that contains all the audio files and a pure text file named\n`idx2{file_name}` that contains the absolute paths of all the saved audio files.\nFor example,\n`dict(wav=dict(format='flac', content=[np_arr1, np_arr2, np_arr3]))` will create a folder named 'wav' and\na pure text file named 'idx2wav' in the same directory. The file 'idx2wav' looks like:\n```\n{test_index1} /x/xx/wav/{test_index1}.flac\n{test_index2} /x/xx/wav/{test_index2}.flac\n{test_index3} /x/xx/wav/{test_index3}.flac\n```\nwhere `/x/xx/` is your result path given in your `exp_cfg`.\n\n3. For binary files, the value of 'format' in the sub-Dict must be 'npy' and the value of 'content' must be\na list of numpy.ndarry (torch.Tensor is not supported).\nThere will be a folder named `{file_name}` that contains all the .npy files and a pure text file\nnamed `idx2{file_name}` that contains the absolute paths of all the saved binary files.\nFor example,\n`dict(feat=dict(format='npy', content=[np_arr1, np_arr2, np_arr3]))`\nwill create a folder named 'feat' and a pure text file named 'idx2feat'. The 'idx2feat' file is like:\n```\n{test_index1} /x/xx/feat/{test_index1}.npy\n{test_index2} /x/xx/feat/{test_index2}.npy\n{test_index3} /x/xx/feat/{test_index3}.npy\n```\nwhere `/x/xx/` is your result path given in your `exp_cfg`.\n</code></pre> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef inference(\n    self, infer_conf: Dict, **kwargs\n) -&gt; Dict[str, Dict[str, str or List]]:\n    \"\"\"This function receives the test data and test configuration. The inference\n    results will be packaged into a Dict[str, Dict] which is passed to TestMonitor\n    for disk storage. The returned Dict should be in the form of ``` dict(\n    {file_name}=dict( format={file_format},\n\n            content={file_content}\n        )\n    )\n    ```\n    The first-level key is used to decide the name of the meta file as `idx2{file_name}`. Its value is also a Dict\n    and there must be two keys in this sub-Dict: 'format' and 'content'. The configuration of the sub-Dict is\n    different for different file formats:\n\n        1. For pure text metadata files, the value of 'format' must be 'txt' and the value of 'content' must be a\n        list of Python built-in data type (i.e.,. int, float, str, bool, ...).\n        Each line of the file `idx2{file_name}` will be made up of the index of a test data instance and its\n        metadata value in the `content` List which are separated by a blank.\n        For example,\n        `dict(cer=dict(format='txt', content=[0.1, 0.2, 0.3]))` will create a pure text file named 'idx2cer' which\n        looks like\n        ```\n        {test_index1} 0.1\n        {test_index2} 0.2\n        {test_index3} 0.3\n        ```\n        Note: if the first-level key ends with '.md', there will not be 'idx2' attached at the beginning of the\n        file name.\n\n        2. For audio files, the value of 'format' must be either 'wav' or 'flac' and the value of 'content' must be\n        a list of array-like data type (e.g. numpy.ndarry, torch.Tensor, ...).\n        Moreover, there must be an additional key named 'sample_rate' to indicate the sampling rate of the waveforms\n        to be saved in audio files.\n        There will be a folder named `{file_name}` that contains all the audio files and a pure text file named\n        `idx2{file_name}` that contains the absolute paths of all the saved audio files.\n        For example,\n        `dict(wav=dict(format='flac', content=[np_arr1, np_arr2, np_arr3]))` will create a folder named 'wav' and\n        a pure text file named 'idx2wav' in the same directory. The file 'idx2wav' looks like:\n        ```\n        {test_index1} /x/xx/wav/{test_index1}.flac\n        {test_index2} /x/xx/wav/{test_index2}.flac\n        {test_index3} /x/xx/wav/{test_index3}.flac\n        ```\n        where `/x/xx/` is your result path given in your `exp_cfg`.\n\n        3. For binary files, the value of 'format' in the sub-Dict must be 'npy' and the value of 'content' must be\n        a list of numpy.ndarry (torch.Tensor is not supported).\n        There will be a folder named `{file_name}` that contains all the .npy files and a pure text file\n        named `idx2{file_name}` that contains the absolute paths of all the saved binary files.\n        For example,\n        `dict(feat=dict(format='npy', content=[np_arr1, np_arr2, np_arr3]))`\n        will create a folder named 'feat' and a pure text file named 'idx2feat'. The 'idx2feat' file is like:\n        ```\n        {test_index1} /x/xx/feat/{test_index1}.npy\n        {test_index2} /x/xx/feat/{test_index2}.npy\n        {test_index3} /x/xx/feat/{test_index3}.npy\n        ```\n        where `/x/xx/` is your result path given in your `exp_cfg`.\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.matrix_snapshot","title":"<code>matrix_snapshot(vis_logs, hypo_attention, subfolder_names, epoch)</code>","text":"<p>Used by the abstract function visualize() to make the snapshot materials for attention matrices.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def matrix_snapshot(\n    self,\n    vis_logs: List,\n    hypo_attention: Dict,\n    subfolder_names: List[str] or str,\n    epoch: int,\n):\n    \"\"\"Used by the abstract function visualize() to make the snapshot materials for\n    attention matrices.\"\"\"\n    if isinstance(subfolder_names, str):\n        subfolder_names = [subfolder_names]\n    keys = list(hypo_attention.keys())\n\n    # process the input data by different data types\n    if isinstance(hypo_attention[keys[0]], Dict):\n        for key, value in hypo_attention.items():\n            self.matrix_snapshot(\n                vis_logs=vis_logs,\n                hypo_attention=value,\n                subfolder_names=subfolder_names + [key],\n                epoch=epoch,\n            )\n\n    # snapshot the information in the materials\n    elif isinstance(hypo_attention[keys[0]], np.ndarray):\n        vis_logs.append(\n            dict(\n                plot_type=\"matrix\",\n                materials=hypo_attention,\n                epoch=epoch,\n                sep_save=False,\n                data_save=True,\n                subfolder_names=subfolder_names,\n            )\n        )\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.module_forward","title":"<code>module_forward(epoch=None, **batch_data)</code>  <code>abstractmethod</code>","text":"<p>This function forwards the input batch data by all Module members. Note:     1. This interface function must be overridden for each Model subclass.     2. The argument names should match the key names in the returned Dict of <code>self.batch_preprocess_fn()</code>.     3. The key names in the returned Dict should match the argument names of <code>self.loss_calculation()</code> and     <code>self.metrics_calculation()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <code>None</code> <code>**batch_data</code> <p>Processed data of the input batch received from <code>self.batch_preprocess_fn()</code>.</p> <code>{}</code> <p>Dict</p> Type Description <code>Dict</code> <p>Prediction results (logits) of the model on the input batch data.</p> <code>Dict</code> <p>Some intermediate results (e.g., attention matrices) can also be returned for later use.</p> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef module_forward(self, epoch: int = None, **batch_data) -&gt; Dict:\n    \"\"\"\n    This function forwards the input batch data by all _Module_ members.\n    Note:\n        1. This interface function must be overridden for each Model subclass.\n        2. The argument names should match the key names in the returned Dict of `self.batch_preprocess_fn()`.\n        3. The key names in the returned Dict should match the argument names of `self.loss_calculation()` and\n        `self.metrics_calculation()`.\n\n    Args:\n        epoch:\n        **batch_data:\n            Processed data of the input batch received from `self.batch_preprocess_fn()`.\n\n    Returns: Dict\n        Prediction results (logits) of the model on the input batch data.\n        Some intermediate results (e.g., attention matrices) can also be returned for later use.\n\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.module_init","title":"<code>module_init(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>The interface function that initializes the Module members of the model. These Module members make up the neural network structure of the model. Some models have their customized part that also needs to be initialization in this function, e.g. the tokenizer of ASR and TTS models.</p> <p>Note: This interface function must be overridden for each Model subclass.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The combination of the arguments in your given <code>module_conf</code> and <code>model_conf['customize_conf']</code>.</p> <code>{}</code> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef module_init(self, **kwargs) -&gt; None:\n    \"\"\"The interface function that initializes the Module members of the model.\n    These Module members make up the neural network structure of the model. Some\n    models have their customized part that also needs to be initialization in this\n    function, e.g. the tokenizer of ASR and TTS models.\n\n    Note: This interface function must be overridden for each Model subclass.\n\n    Args:\n        **kwargs:\n            The combination of the arguments in your given `module_conf` and `model_conf['customize_conf']`.\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.register_instance_reports","title":"<code>register_instance_reports(md_list_dict, extra_string_list=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>md_list_dict</code> <code>Dict[str, List]</code> required <code>extra_string_list</code> <code>List[str]</code> <code>None</code> <p>Returns:</p> Source code in <code>speechain/model/abs.py</code> <pre><code>def register_instance_reports(\n    self, md_list_dict: Dict[str, List], extra_string_list: List[str] = None\n):\n    \"\"\"\n\n    Args:\n        md_list_dict:\n        extra_string_list:\n\n    Returns:\n\n    \"\"\"\n    # --- 1. Arguments Checking --- #\n    if extra_string_list is not None:\n        assert isinstance(extra_string_list, List)\n\n    ele_len = []\n    for value in md_list_dict.values():\n        assert isinstance(value, List)\n        if extra_string_list is not None:\n            assert len(value) == len(extra_string_list)\n        ele_len.append(len(value))\n\n    if len(set(ele_len)) == 1:\n        ele_len = ele_len[0]\n    else:\n        raise RuntimeError\n\n    # --- 2. Generate .md Instance Report for the current step --- #\n    instance_reports = []\n    for i in range(ele_len):\n        ele_dict = {\n            key: value[i] if isinstance(value[i], str) else str(value[i])\n            for key, value in md_list_dict.items()\n        }\n        _curr_report = \"\\n\\n\" + get_list_strings(ele_dict) + \"\\n\"\n\n        if extra_string_list is not None:\n            _curr_report += extra_string_list[i] + \"\\n\"\n        instance_reports.append(_curr_report)\n\n    self.instance_report_cache = dict(format=\"txt\", content=instance_reports)\n</code></pre>"},{"location":"reference/model/abs/#model.abs.Model.visualize","title":"<code>visualize(epoch, sample_index, **valid_sample)</code>  <code>abstractmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> required <code>sample_index</code> <code>str</code> required <code>**valid_sample</code> <code>{}</code> <p>Returns:</p> Source code in <code>speechain/model/abs.py</code> <pre><code>@abstractmethod\ndef visualize(self, epoch: int, sample_index: str, **valid_sample):\n    \"\"\"\n\n    Args:\n        epoch:\n        sample_index:\n        **valid_sample:\n\n    Returns:\n\n    \"\"\"\n    pass  # raise NotImplementedError\n</code></pre>"},{"location":"reference/model/ar_asr/","title":"ar_asr","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/model/ar_asr/#model.ar_asr.ARASR","title":"<code>ARASR</code>","text":"<p>               Bases: <code>Model</code></p> <p>Auto-Regressive Attention-based Automatic Speech Recognition (AR-ASR) implementation.</p> <p>The neural network structure of an <code>ASR</code> Model object is made up of 3 Module members:</p> <ol> <li> <p>an <code>ASREncoder</code> made up of:</p> <ol> <li><code>frontend</code> converts the  raw waveforms into acoustic features on-the-fly.</li> <li><code>normalize</code> normalizes the extracted acoustic features to normal distribution for faster convergence.</li> <li><code>specaug</code> randomly warps and masks the normalized acoustic features.</li> <li><code>prenet</code> preprocesses the augmented acoustic features before passing them to the encoder.</li> <li><code>encoder</code> extracts the encoder hidden representations of the preprocessed acoustic features and passes them     to <code>ARASRDecoder</code>.</li> </ol> </li> <li> <p>an <code>ARASRDecoder</code> made up of:</p> <ol> <li><code>embedding</code> embeds each tokens in the input sentence into token embedding vectors.</li> <li><code>decoder</code> extracts the decoder hidden representations based on the token embedding vectors and encoder     hidden representations.</li> <li><code>postnet</code> predicts the probability of the next tokens by the decoder hidden representations.</li> </ol> </li> <li> <p>(optional) a CTC layer made up of a 'TokenPostnet'</p> </li> </ol> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>class ARASR(Model):\n    \"\"\"Auto-Regressive Attention-based Automatic Speech Recognition (AR-ASR)\n    implementation.\n\n    The neural network structure of an `ASR` Model object is made up of 3 Module members:\n\n    1. an `ASREncoder` made up of:\n        1. `frontend` converts the  raw waveforms into acoustic features on-the-fly.\n        2. `normalize` normalizes the extracted acoustic features to normal distribution for faster convergence.\n        3. `specaug` randomly warps and masks the normalized acoustic features.\n        4. `prenet` preprocesses the augmented acoustic features before passing them to the encoder.\n        5. `encoder` extracts the encoder hidden representations of the preprocessed acoustic features and passes them\n            to `ARASRDecoder`.\n\n    2. an `ARASRDecoder` made up of:\n        1. `embedding` embeds each tokens in the input sentence into token embedding vectors.\n        2. `decoder` extracts the decoder hidden representations based on the token embedding vectors and encoder\n            hidden representations.\n        3. `postnet` predicts the probability of the next tokens by the decoder hidden representations.\n\n    3. (optional) a CTC layer made up of a 'TokenPostnet'\n    \"\"\"\n\n    def module_init(\n        self,\n        token_type: str,\n        token_path: str,\n        enc_prenet: Dict,\n        encoder: Dict,\n        dec_emb: Dict,\n        decoder: Dict,\n        frontend: Dict = None,\n        normalize: Dict or bool = None,\n        specaug: Dict or bool = None,\n        ilm_weight: float = 0.0,\n        ilm_sub_weight: float = 0.0,\n        ctc_weight: float = 0.0,\n        sample_rate: int = 16000,\n        audio_format: str = \"wav\",\n        return_att_type: List[str] or str = None,\n        return_att_head_num: int = 2,\n        return_att_layer_num: int = 2,\n        lm_model_cfg: Dict or str = None,\n        lm_model_path: str = None,\n    ):\n        \"\"\"\n        This initialization function contains 4 steps:\n        1. `Tokenizer` initialization.\n        2. `ASREncoder` initialization.\n        3. `ARASRDecoder` initialization.\n        4. (optional) 'CTC' layer initialization\n\n        The input arguments of this function are two-fold:\n        1. the ones from `customize_conf` of `model` in `train_cfg`\n        2. the ones from `module_conf` of `model` in `train_cfg`\n\n        Args:\n            # --- module_conf arguments --- #\n            frontend: (optional)\n                The configuration of the acoustic feature extraction frontend in the `ASREncoder` member.\n                This argument must be given since our toolkit doesn't support time-domain ASR.\n                For more details about how to give `frontend`, please refer to speechain.module.encoder.asr.ASREncoder.\n            normalize: (optional)\n                The configuration of the normalization layer in the `ASREncoder` member.\n                This argument can also be given as a bool value.\n                True means the default configuration and False means no normalization.\n                For more details about how to give `normalize`, please refer to\n                    speechain.module.norm.feat_norm.FeatureNormalization.\n            specaug: (optional)\n                The configuration of the SpecAugment layer in the `ASREncoder` member.\n                This argument can also be given as a bool value.\n                True means the default configuration and False means no SpecAugment.\n                For more details about how to give `specaug`, please refer to\n                    speechain.module.augment.specaug.SpecAugment.\n            enc_prenet: (mandatory)\n                The configuration of the prenet in the `ASREncoder` member.\n                The encoder prenet embeds the input acoustic features into hidden embeddings before feeding them into\n                the encoder.\n                For more details about how to give `enc_prent`, please refer to speechain.module.encoder.asr.ASREncoder.\n            encoder: (mandatory)\n                The configuration of the encoder main body in the `ASREncoder` member.\n                The encoder embeds the hidden embeddings into the encoder representations at each time steps of the\n                input acoustic features.\n                For more details about how to give `encoder`, please refer to speechain.module.encoder.asr.ASREncoder.\n            dec_emb: (mandatory)\n                The configuration of the embedding layer in the `ARASRDecoder` member.\n                The decoder prenet embeds the input token ids into hidden embeddings before feeding them into\n                the decoder.\n                For more details about how to give `dec_emb`, please refer to speechain.module.encoder.asr.ASREncoder.\n            decoder: (mandatory)\n                The configuration of the decoder main body in the `ARASRDecoder` member.\n                The decoder predicts the probability of the next token at each time steps based on the token embeddings.\n                For more details about how to give `decoder`, please refer to speechain.module.decoder.ar_asr.ARASRDecoder.\n            # --- customize_conf arguments --- #\n            token_type: (mandatory)\n                The type of the built-in tokenizer.\n            token_path: (mandatory)\n                The path of the vocabulary for initializing the built-in tokenizer.\n            sample_rate: int = 16000 (optional)\n                The sampling rate of the input speech.\n                Currently, it's used for acoustic feature extraction frontend initialization and tensorboard register of\n                the input speech for model visualization.\n                In the future, this argument will also be used to on-the-fly downsample the input speech.\n            audio_format: (optional)\n                This argument is only used for input speech recording during model visualization.\n            return_att_type: List[str] or str = ['encdec', 'enc', 'dec']\n                The type of attentions you want to return for both attention guidance and attention visualization.\n                It can be given as a string (one type) or a list of strings (multiple types).\n                The type should be one of\n                    1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN\n                    2. 'enc': the encoder self-attention, only for Transformer\n                    3. 'dec': the decoder self-attention, only for Transformer\n            return_att_head_num: int = -1\n                The number of returned attention heads. If -1, all the heads in an attention layer will be returned.\n                RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.\n            return_att_layer_num: int = 1\n                The number of returned attention layers. If -1, all the attention layers will be returned.\n                RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.\n            lm_model_cfg: Dict or str\n                The configuration for the language model used for joint decoding.\n                Can be either a Dict or a string indicating where the .yaml model configuration file is placed.\n            lm_model_path: str\n                The string indicating where the .pth model parameter file is placed.\n\n        \"\"\"\n\n        # --- 1. Module-independent Initialization --- #\n        # initialize the tokenizer\n        if token_type.lower() == \"char\":\n            self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n        elif token_type.lower() == \"sentencepiece\":\n            self.tokenizer = SentencePieceTokenizer(\n                token_path, copy_path=self.result_path\n            )\n        else:\n            raise ValueError(\n                f\"Unknown token_type {token_type}. \"\n                f\"Currently, {self.__class__.__name__} supports one of ['char', 'sentencepiece'].\"\n            )\n\n        # initialize the sampling rate, mainly used for visualizing the input audio during training\n        self.sample_rate = sample_rate\n        self.audio_format = audio_format.lower()\n\n        # attention-related\n        if return_att_type is None:\n            self.return_att_type = [\"encdec\", \"enc\", \"dec\"]\n        else:\n            self.return_att_type = (\n                return_att_type\n                if isinstance(return_att_type, List)\n                else [return_att_type]\n            )\n        for i in range(len(self.return_att_type)):\n            if self.return_att_type[i].lower() in [\"enc\", \"dec\", \"encdec\"]:\n                self.return_att_type[i] = self.return_att_type[i].lower()\n            else:\n                raise ValueError(\n                    \"The elements of your input return_att_type must be one of ['enc', 'dec', 'encdec'], \"\n                    f\"but got {self.return_att_type[i]}!\"\n                )\n        self.return_att_head_num = return_att_head_num\n        self.return_att_layer_num = return_att_layer_num\n\n        # language model-related, used for lazy initialization during inference\n        self.lm_model_cfg = lm_model_cfg\n        self.lm_model_path = lm_model_path\n\n        # --- 2. Module Initialization --- #\n        # --- 2.1 Encoder construction --- #\n        # the sampling rate will be first initialized\n        if \"sr\" not in frontend[\"conf\"].keys():\n            frontend[\"conf\"][\"sr\"] = self.sample_rate\n        # update the sampling rate into the ASR Model object\n        self.sample_rate = frontend[\"conf\"][\"sr\"]\n        self.encoder = ASREncoder(\n            frontend=frontend,\n            normalize=normalize,\n            specaug=specaug,\n            prenet=enc_prenet,\n            encoder=encoder,\n            distributed=self.distributed,\n        )\n\n        # --- 2.2 CTC layer construction (optional) --- #\n        self.ctc_weight = ctc_weight\n        assert ctc_weight &gt;= 0, \"ctc_weight cannot be lower than 0!\"\n        if ctc_weight &gt; 0:\n            self.ctc_layer = TokenPostnet(\n                input_size=self.encoder.output_size,\n                vocab_size=self.tokenizer.vocab_size,\n            )\n\n        # --- 2.3 Decoder construction --- #\n        self.ilm_weight = ilm_weight\n        assert ilm_weight &gt;= 0, \"ilm_weight cannot be lower than 0!\"\n        self.ilm_sub_weight = ilm_sub_weight\n        assert ilm_sub_weight &gt;= 0, \"ilm_sub_weight cannot be lower than 0!\"\n        # the vocabulary size is given by the built-in tokenizer instead of the input configuration\n        if \"vocab_size\" in dec_emb[\"conf\"].keys():\n            if dec_emb[\"conf\"][\"vocab_size\"] != self.tokenizer.vocab_size:\n                warnings.warn(\n                    f\"Your input vocabulary size is different from the one obtained from the built-in \"\n                    f\"tokenizer ({self.tokenizer.vocab_size}). The latter one will be used to initialize the \"\n                    f\"decoder for correctness.\"\n                )\n            dec_emb[\"conf\"].pop(\"vocab_size\")\n        self.decoder = ARASRDecoder(\n            vocab_size=self.tokenizer.vocab_size, embedding=dec_emb, decoder=decoder\n        )\n\n    def criterion_init(\n        self,\n        ce_loss: Dict[str, Any] = None,\n        ilm_loss: Dict[str, Any] = None,\n        ctc_loss: Dict[str, Any] or bool = None,\n        att_guid_loss: Dict[str, Any] or bool = None,\n    ):\n        \"\"\"\n        This function initializes all the necessary _Criterion_ members:\n            1. `speechain.criterion.cross_entropy.CrossEntropy` for training loss calculation.\n            2. `speechain.criterion.ctc.CTCLoss` for training loss calculation.\n            3. `speechain.criterion.accuracy.Accuracy` for teacher-forcing validation accuracy calculation.\n            4. `speechain.criterion.error_rate.ErrorRate` for evaluation CER &amp; WER calculation.\n\n        Args:\n            ce_loss: Dict[str, Any]\n                The arguments for CrossEntropy(). If not given, the default setting of CrossEntropy() will be used.\n                Please refer to speechain.criterion.cross_entropy.CrossEntropy for more details.\n            ilm_loss:\n            ctc_loss: Dict[str, Any] or bool\n                The arguments for CTCLoss(). If not given, self.ctc_loss won't be initialized.\n                This argument can also be set to a bool value 'True'. If True, the default setting of CTCLoss()\n                will be used.\n                Please refer to speechain.criterion.ctc.CTCLoss for more details.\n            att_guid_loss: Dict[str, Any] or bool\n                The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized.\n                This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance()\n                will be used.\n                Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.\n\n        \"\"\"\n\n        # initialize cross-entropy loss for the encoder-decoder\n        if ce_loss is None:\n            ce_loss = {}\n        self.ce_loss = CrossEntropy(**ce_loss)\n\n        # initialize cross-entropy loss for the internal LM\n        if self.ilm_weight &gt; 0:\n            # ilm_loss is default to be normalized by sentence lengths\n            if ilm_loss is None:\n                ilm_loss = dict(length_normalized=True)\n            self.ilm_loss = CrossEntropy(**ilm_loss)\n\n        # initialize ctc loss\n        if self.ctc_weight &gt; 0:\n            # if ctc_loss is given as True or None, the default arguments of CTCLoss will be used\n            if not isinstance(ctc_loss, Dict):\n                ctc_loss = {}\n\n            if self.device != \"cpu\" and self.tokenizer.ignore_idx != 0:\n                raise RuntimeError(\n                    f\"For speeding up CTC calculation by CuDNN, \"\n                    f\"please set the blank id to 0 (got {self.tokenizer.ignore_idx}).\"\n                )\n\n            ctc_loss[\"blank\"] = self.tokenizer.ignore_idx\n            self.ctc_loss = CTCLoss(**ctc_loss)\n\n        # initialize attention guidance loss\n        if att_guid_loss is not None:\n            # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n            if not isinstance(att_guid_loss, Dict):\n                assert (\n                    isinstance(att_guid_loss, bool) and att_guid_loss\n                ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n                att_guid_loss = {}\n\n            assert (\n                \"encdec\" in self.return_att_type\n            ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n            self.att_guid_loss = AttentionGuidance(**att_guid_loss)\n\n        # initialize teacher-forcing accuracy for validation\n        self.accuracy = Accuracy()\n\n        # initialize text perplexity calculator for internal LM if needed\n        self.perplexity = Perplexity()\n\n        # initialize error rate (CER &amp; WER) for evaluation\n        self.error_rate = ErrorRate(tokenizer=self.tokenizer)\n\n    @staticmethod\n    def bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n        return [\n            [\"wer\", \"max\", 30],\n            [\"cer\", \"max\", 30],\n            [\"feat_token_len_ratio\", \"min\", 30],\n            [\"feat_token_len_ratio\", \"max\", 30],\n            [\"text_confid\", \"min\", 30],\n            [\"text_confid\", \"max\", 30],\n        ]\n\n    def module_forward(\n        self,\n        epoch: int = None,\n        feat: torch.Tensor = None,\n        text: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        domain: str = None,\n        return_att: bool = False,\n        **kwargs,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input speech data. feat_dim = 1 in the case of raw speech waveforms.\n            feat_len: (batch,)\n                The lengths of input speech data\n            text: (batch, text_maxlen)\n                The input text data with &lt;sos/eos&gt; at the beginning and end\n            text_len: (batch,)\n                The lengths of input text data\n            epoch: int\n                The number of the current training epoch.\n                Mainly used for mean&amp;std calculation in the feature normalization\n            domain: str = None\n            return_att: bool\n                Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n            kwargs:\n                Temporary register used to store the redundant arguments.\n\n        \"\"\"\n        # para checking\n        assert feat is not None and feat_len is not None\n        assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n            0\n        ), \"The amounts of utterances and sentences are not equal to each other.\"\n        assert feat_len.size(0) == feat.size(\n            0\n        ), \"The amounts of utterances and their lengths are not equal to each other.\"\n        assert text_len.size(0) == text.size(\n            0\n        ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n        # remove the &lt;sos/eos&gt; at the end of each sentence\n        for i in range(text_len.size(0)):\n            text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n        text, text_len = text[:, :-1], text_len - 1\n\n        # --- 1. Encoder: Input Feature to Encoder Hidden Representation --- #\n        enc_returns = self.encoder(\n            feat=feat, feat_len=feat_len, epoch=epoch, domain=domain\n        )\n        # Transformer-based encoder additionally returns the encoder self-attention\n        if len(enc_returns) == 4:\n            enc_feat, enc_feat_mask, enc_attmat, enc_hidden = enc_returns\n        # RNN-based encoder doesn't return any attention\n        elif len(enc_returns) == 3:\n            (enc_feat, enc_feat_mask, enc_hidden), enc_attmat = enc_returns, None\n        else:\n            raise RuntimeError\n\n        # --- 2. Decoder: Encoder Hidden Representation to Decoder Hidden Representation --- #\n        dec_returns = self.decoder(\n            enc_feat=enc_feat, enc_feat_mask=enc_feat_mask, text=text, text_len=text_len\n        )\n        # Transformer-based decoder additionally returns the decoder self-attention\n        if len(dec_returns) == 4:\n            dec_feat, dec_attmat, encdec_attmat, dec_hidden = dec_returns\n        # RNN-based decoder only returns the encoder-decoder attention\n        elif len(dec_returns) == 3:\n            (dec_feat, encdec_attmat, dec_hidden), dec_attmat = dec_returns, None\n        else:\n            raise RuntimeError\n\n        # initialize the asr output to be the decoder predictions\n        outputs = dict(logits=dec_feat)\n\n        # --- 3.(optional) Decoder: Internal LM Estimation by zeroing Encoder Hidden Representation --- #\n        if self.ilm_weight &gt; 0 or self.ilm_sub_weight &gt; 0:\n            ilm_returns = self.decoder(\n                enc_feat=torch.zeros(\n                    enc_feat.size(0), 1, enc_feat.size(2), device=enc_feat.device\n                ),\n                enc_feat_mask=torch.ones(\n                    enc_feat_mask.size(0),\n                    enc_feat_mask.size(1),\n                    1,\n                    dtype=torch.bool,\n                    device=enc_feat_mask.device,\n                ),\n                text=text,\n                text_len=text_len,\n            )\n            # Transformer-based decoder additionally returns the decoder self-attention\n            if len(ilm_returns) == 4:\n                ilm_feat, ilm_dec_attmat, ilm_encdec_attmat, ilm_hidden = ilm_returns\n            # RNN-based decoder only returns the encoder-decoder attention\n            elif len(ilm_returns) == 3:\n                (ilm_feat, ilm_encdec_attmat, ilm_hidden), ilm_dec_attmat = (\n                    ilm_returns,\n                    None,\n                )\n            else:\n                raise RuntimeError\n\n            if self.ilm_weight &gt; 0:\n                outputs.update(ilm_logits=ilm_feat)\n            elif self.ilm_sub_weight &gt; 0:\n                outputs[\"logits\"] -= self.ilm_sub_weight * ilm_feat\n\n        # --- 4.(optional) Encoder Hidden Representation to CTC Prediction --- #\n        if hasattr(self, \"ctc_layer\"):\n            ctc_logits = self.ctc_layer(enc_feat)\n            outputs.update(\n                ctc_logits=ctc_logits,\n                enc_feat_len=torch.sum(enc_feat_mask.squeeze(dim=1), dim=-1),\n            )\n\n        def shrink_attention(input_att_list):\n            # pick up the target attention layers\n            if (\n                self.return_att_layer_num != -1\n                and len(input_att_list) &gt; self.return_att_layer_num\n            ):\n                input_att_list = input_att_list[-self.return_att_layer_num :]\n            # pick up the target attention heads\n            if (\n                self.return_att_head_num != -1\n                and input_att_list[0].size(1) &gt; self.return_att_head_num\n            ):\n                input_att_list = [\n                    att[:, : self.return_att_head_num] for att in input_att_list\n                ]\n            return input_att_list\n\n        # return the attention results if specified\n        if return_att or hasattr(self, \"att_guid_loss\"):\n            # encoder-decoder attention\n            if \"encdec\" in self.return_att_type:\n                # register the encoder-decoder attention\n                outputs.update(att=dict(encdec=shrink_attention(encdec_attmat)))\n            # encoder self-attention\n            if enc_attmat is not None and \"enc\" in self.return_att_type:\n                outputs[\"att\"].update(enc=shrink_attention(enc_attmat))\n            # decoder self-attention\n            if dec_attmat is not None and \"dec\" in self.return_att_type:\n                outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n        return outputs\n\n    def get_ctc_forward_results(\n        self,\n        ctc_logits: torch.Tensor,\n        enc_feat_len: torch.Tensor,\n        text: torch.Tensor,\n        ctc_loss_fn: callable,\n    ):\n\n        ctc_text_confid = ctc_logits.log_softmax(dim=-1).max(dim=-1)[0].sum(dim=-1)\n        ctc_recover_text = [\n            self.tokenizer.tensor2text(ctc_text)\n            for ctc_text in ctc_loss_fn.recover(ctc_logits, enc_feat_len)\n        ]\n        real_text = [self.tokenizer.tensor2text(t) for t in text]\n        ctc_cer_wer = [\n            self.error_rate(hypo_text=ctc_recover_text[i], real_text=real_text[i])\n            for i in range(len(real_text))\n        ]\n\n        return dict(\n            ctc_text_confid=ctc_text_confid.clone().detach(),\n            ctc_cer=np.mean([ctc_cer_wer[i][0] for i in range(len(ctc_cer_wer))]),\n            ctc_wer=np.mean([ctc_cer_wer[i][1] for i in range(len(ctc_cer_wer))]),\n            ctc_text=ctc_recover_text,\n        )\n\n    def criterion_forward(\n        self,\n        logits: torch.Tensor,\n        text: torch.Tensor,\n        text_len: torch.Tensor,\n        ilm_logits: torch.Tensor = None,\n        ctc_logits: torch.Tensor = None,\n        enc_feat_len: torch.Tensor = None,\n        att: Dict[str, List[torch.Tensor]] = None,\n        ce_loss_fn: CrossEntropy = None,\n        ilm_loss_fn: CrossEntropy = None,\n        ctc_loss_fn: CTCLoss = None,\n        att_guid_loss_fn: AttentionGuidance = None,\n        forward_ctc: bool = False,\n        **kwargs,\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n\n        accuracy = self.accuracy(logits=logits, text=text, text_len=text_len)\n        metrics = dict(accuracy=accuracy.detach())\n\n        # the external cross-entropy loss function has the higher priority\n        if ce_loss_fn is None:\n            ce_loss_fn = self.ce_loss\n        ce_loss = ce_loss_fn(logits=logits, text=text, text_len=text_len)\n        metrics.update(ce_loss=ce_loss.clone().detach())\n\n        # if ctc_loss is specified, calculate the weighted sum of ctc_loss and ce_loss as loss in the metrics Dict\n        if ctc_loss_fn is not None or hasattr(self, \"ctc_loss\"):\n            # the external ctc loss function has the higher priority\n            if ctc_loss_fn is None:\n                ctc_loss_fn = self.ctc_loss\n\n            ctc_loss = ctc_loss_fn(ctc_logits, enc_feat_len, text, text_len)\n            loss = (1 - self.ctc_weight) * ce_loss + self.ctc_weight * ctc_loss\n            metrics.update(ctc_loss=ctc_loss.clone().detach())\n\n            if forward_ctc:\n                metrics.update(\n                    self.get_ctc_forward_results(\n                        ctc_logits=ctc_logits,\n                        enc_feat_len=enc_feat_len,\n                        text=text,\n                        ctc_loss_fn=ctc_loss_fn,\n                    )\n                )\n\n        # if ctc_loss is not specified, only record ce_loss as loss in the returned Dicts\n        else:\n            loss = ce_loss\n\n        # if ilm_loss is specified, add ilm_loss to loss in the metrics Dict\n        if ilm_loss_fn is not None or hasattr(self, \"ilm_loss\"):\n            # the external ctc loss function has the higher priority\n            if ilm_loss_fn is None:\n                ilm_loss_fn = self.ilm_loss\n\n            ilm_loss = ilm_loss_fn(logits=ilm_logits, text=text, text_len=text_len)\n            metrics.update(ilm_loss=ilm_loss.clone().detach())\n            loss += self.ilm_weight * ilm_loss\n\n            # calculate perplexity\n            ilm_ppl = self.perplexity(logits=ilm_logits, text=text, text_len=text_len)\n            metrics.update(ilm_text_ppl=ilm_ppl.detach())\n\n        # if att_guid_loss is given, record att_guid_loss in the metrics Dict\n        if att_guid_loss_fn is not None or hasattr(self, \"att_guid_loss\"):\n            # the external attention guidance loss function has the higher priority\n            if att_guid_loss_fn is None:\n                att_guid_loss_fn = self.att_guid_loss\n\n            # layer_num * (batch, head_num, ...) -&gt; (batch, layer_num * head_num, ...)\n            att_tensor = torch.cat(att[\"encdec\"], dim=1)\n            att_guid_loss = att_guid_loss_fn(att_tensor, text_len, enc_feat_len)\n            loss += att_guid_loss\n            metrics.update(att_guid_loss=att_guid_loss.clone().detach())\n\n        losses = dict(loss=loss)\n        # .clone() here prevents the loss from being modified by accum_grad\n        metrics.update(loss=loss.clone().detach())\n\n        if self.training:\n            return losses, metrics\n        else:\n            return metrics\n\n    def visualize(\n        self,\n        epoch: int,\n        sample_index: str,\n        snapshot_interval: int = 1,\n        epoch_records: Dict = None,\n        domain: str = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n    ):\n\n        # remove the padding zeros at the end of the input feat\n        for i in range(len(feat)):\n            feat[i] = feat[i][: feat_len[i]]\n\n        # visualization inference is default to be done by teacher-forcing\n        if len(self.visual_infer_conf) == 0:\n            self.visual_infer_conf = dict(teacher_forcing=True)\n        # obtain the inference results\n        infer_results = self.inference(\n            infer_conf=self.visual_infer_conf,\n            domain=domain,\n            return_att=True,\n            feat=feat,\n            feat_len=feat_len,\n            text=text,\n            text_len=text_len,\n        )\n\n        # --- snapshot the objective metrics --- #\n        vis_logs = []\n        # CER, WER, Confidence\n        materials = dict()\n        for metric in [\n            \"cer\",\n            \"wer\",\n            \"ctc_cer\",\n            \"ctc_wer\",\n            \"accuracy\",\n            \"text_confid\",\n            \"ilm_text_ppl\",\n        ]:\n            if metric not in infer_results.keys():\n                continue\n\n            # store each target metric into materials\n            if metric not in epoch_records[sample_index].keys():\n                epoch_records[sample_index][metric] = []\n            if not isinstance(infer_results[metric][\"content\"], List):\n                infer_results[metric][\"content\"] = [infer_results[metric][\"content\"]]\n            epoch_records[sample_index][metric].append(\n                infer_results[metric][\"content\"][0]\n            )\n            materials[metric] = epoch_records[sample_index][metric]\n        # save the visualization log\n        vis_logs.append(\n            dict(\n                plot_type=\"curve\",\n                materials=copy.deepcopy(materials),\n                epoch=epoch,\n                xlabel=\"epoch\",\n                x_stride=snapshot_interval,\n                sep_save=False,\n                subfolder_names=sample_index,\n            )\n        )\n\n        # --- snapshot the subjective metrics --- #\n        # record the input audio and real text at the first snapshotting step\n        if epoch // snapshot_interval == 1:\n            # snapshot input audio\n            vis_logs.append(\n                dict(\n                    plot_type=\"audio\",\n                    materials=dict(input_audio=copy.deepcopy(feat[0])),\n                    sample_rate=self.sample_rate,\n                    audio_format=self.audio_format,\n                    subfolder_names=sample_index,\n                )\n            )\n            # snapshot real text\n            vis_logs.append(\n                dict(\n                    materials=dict(\n                        real_text=[\n                            copy.deepcopy(self.tokenizer.tensor2text(text[0][1:-1]))\n                        ]\n                    ),\n                    plot_type=\"text\",\n                    subfolder_names=sample_index,\n                )\n            )\n\n        # teacher-forcing text and CTC-decoding text\n        for text_name in [\"text\", \"ctc_text\"]:\n            if text_name not in infer_results.keys():\n                continue\n            if text_name not in epoch_records[sample_index].keys():\n                epoch_records[sample_index][text_name] = []\n            epoch_records[sample_index][text_name].append(\n                infer_results[text_name][\"content\"][0]\n            )\n            # snapshot the information in the materials\n            vis_logs.append(\n                dict(\n                    materials=dict(\n                        hypo_text=copy.deepcopy(epoch_records[sample_index][text_name])\n                    ),\n                    plot_type=\"text\",\n                    epoch=epoch,\n                    x_stride=snapshot_interval,\n                    subfolder_names=sample_index,\n                )\n            )\n\n        # hypothesis attention matrix\n        infer_results[\"att\"] = self.attention_reshape(infer_results[\"att\"])\n        self.matrix_snapshot(\n            vis_logs=vis_logs,\n            hypo_attention=copy.deepcopy(infer_results[\"att\"]),\n            subfolder_names=sample_index,\n            epoch=epoch,\n        )\n        return vis_logs\n\n    def inference(\n        self,\n        infer_conf: Dict,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        domain: str = None,\n        return_att: bool = False,\n        decode_only: bool = False,\n        teacher_forcing: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        The inference function for ASR models. There are two steps in this function:\n            1. Decode the input speech into hypothesis transcript\n            2. Evaluate the hypothesis transcript by the ground-truth\n\n        This function can be called for model evaluation, on-the-fly model visualization, and even pseudo transcript\n        generation during training.\n\n        Args:\n            # --- Testing data arguments --- #\n            feat: torch.Tensor\n                The speech data to be inferred.\n            feat_len: torch.Tensor\n                The length of `feat`.\n            text: torch.Tensor\n                The ground-truth transcript for the input speech\n            text_len: torch.Tensor\n                The length of `text`.\n            # --- Explicit inference arguments --- #\n            domain: str = None\n                This argument indicates which domain the input speech belongs to.\n                It's used to indicate the `ASREncoder` member how to encode the input speech.\n            return_att: bool = False\n                Whether the attention matrix of the input speech is returned.\n            decode_only: bool = False\n                Whether skip the evaluation step and do the decoding step only.\n            teacher_forcing: bool = True\n                Whether you use the teacher-forcing technique to generate the hypothesis transcript.\n            # --- Implicit inference arguments given by infer_cfg from runner.py --- #\n            infer_conf: Dict\n                The inference configuration given from the `infer_cfg` in your `exp_cfg`.\n                For more details, please refer to speechain.infer_func.beam_search.beam_searching.\n\n        Returns: Dict\n            A Dict containing all the decoding and evaluation results.\n\n        \"\"\"\n        assert feat is not None and feat_len is not None\n\n        # --- 0. Hyperparameter &amp; Model Preparation Stage --- #\n        # in-place replace infer_conf to protect the original information\n        infer_conf = copy.deepcopy(infer_conf)\n        if \"decode_only\" in infer_conf.keys():\n            decode_only = infer_conf.pop(\"decode_only\")\n        if \"teacher_forcing\" in infer_conf.keys():\n            teacher_forcing = infer_conf.pop(\"teacher_forcing\")\n        hypo_text, hypo_text_len, feat_token_len_ratio, hypo_text_confid, hypo_att = (\n            None,\n            None,\n            None,\n            None,\n            None,\n        )\n\n        # only initialize the language model when lm_weight is given as a positive float number\n        if \"lm_weight\" in infer_conf.keys() and infer_conf[\"lm_weight\"] &gt; 0:\n            assert self.lm_model_cfg is not None or self.lm_model_path is not None, (\n                \"If you want to use ASR-LM joint decoding, \"\n                \"please give lm_model_cfg and lm_model_path in model['customize_conf']!\"\n            )\n            # lazily initialize the language model only at the first time\n            if not hasattr(self, \"lm\"):\n                # use the built-in lm configuration if lm_model_cfg is not given\n                if self.lm_model_cfg is None:\n                    self.lm_model_cfg = os.path.join(\n                        self.result_path, \"lm_model_cfg.yaml\"\n                    )\n\n                # get the Dict-like configuration for the language model\n                if isinstance(self.lm_model_cfg, str):\n                    self.lm_model_cfg = load_yaml(parse_path_args(self.lm_model_cfg))\n                if \"model\" in self.lm_model_cfg.keys():\n                    self.lm_model_cfg = self.lm_model_cfg[\"model\"]\n                if \"module_conf\" in self.lm_model_cfg.keys():\n                    self.lm_model_cfg = self.lm_model_cfg[\"module_conf\"]\n\n                # update the built-in configuration yaml file\n                with open(\n                    os.path.join(self.result_path, \"lm_model_cfg.yaml\"),\n                    \"w\",\n                    encoding=\"utf-8\",\n                ) as f:\n                    yaml.dump(self.lm_model_cfg, f, sort_keys=False)\n                self.lm = LanguageModel(\n                    vocab_size=self.tokenizer.vocab_size, **self.lm_model_cfg\n                ).cuda(self.device)\n\n                # use the built-in lm model if lm_model_path is not given\n                if self.lm_model_path is None:\n                    self.lm_model_path = os.path.join(self.result_path, \"lm_model.pth\")\n\n                # load the parameters of the target lm\n                _lm_model_para = torch.load(\n                    parse_path_args(self.lm_model_path), map_location=self.device\n                )\n                lm_model_para = OrderedDict()\n                for key, para in _lm_model_para.items():\n                    if key.startswith(\"lm.\"):\n                        key = key.replace(\"lm.\", \"\", 1)\n                    lm_model_para[key] = para\n\n                # update the built-in lm parameters and load them into the lm for inference\n                torch.save(\n                    lm_model_para, os.path.join(self.result_path, \"lm_model.pth\")\n                )\n                self.lm.load_state_dict(lm_model_para)\n\n        outputs = dict()\n        # --- 1. The 1st Pass: ASR Decoding by Beam Searching --- #\n        if not teacher_forcing:\n            # copy the input data in advance for data safety\n            model_input = copy.deepcopy(dict(feat=feat, feat_len=feat_len))\n\n            # Encoding input speech\n            enc_feat, enc_feat_mask, _, _ = self.encoder(domain=domain, **model_input)\n\n            # generate the model hypothesis\n            infer_results = beam_searching(\n                enc_feat=enc_feat,\n                enc_feat_mask=enc_feat_mask,\n                asr_decode_fn=self.decoder,\n                ctc_decode_fn=self.ctc_layer if hasattr(self, \"ctc_layer\") else None,\n                lm_decode_fn=self.lm if hasattr(self, \"lm\") else None,\n                vocab_size=self.tokenizer.vocab_size,\n                sos_eos=self.tokenizer.sos_eos_idx,\n                padding_idx=self.tokenizer.ignore_idx,\n                **infer_conf,\n            )\n            hypo_text = infer_results[\"hypo_text\"]\n            hypo_text_len = infer_results[\"hypo_text_len\"]\n            feat_token_len_ratio = infer_results[\"feat_token_len_ratio\"]\n            hypo_text_confid = infer_results[\"hypo_text_confid\"]\n\n        # --- 2. The 2nd Pass: ASR Decoding by Teacher Forcing --- #\n        if teacher_forcing or return_att:\n            # copy the input data in advance for data safety\n            model_input = copy.deepcopy(\n                dict(\n                    feat=feat,\n                    feat_len=feat_len,\n                    text=text if teacher_forcing else hypo_text,\n                    text_len=text_len if teacher_forcing else hypo_text_len,\n                )\n            )\n            infer_results = self.module_forward(\n                return_att=return_att, domain=domain, **model_input\n            )\n            # return the attention matrices\n            if return_att:\n                hypo_att = infer_results[\"att\"]\n\n            # update the hypothesis text-related data in the teacher forcing mode\n            if teacher_forcing:\n                tf_results = self.criterion_forward(\n                    text=text, text_len=text_len, forward_ctc=True, **infer_results\n                )\n                for key, content in tf_results.items():\n                    outputs[key] = dict(\n                        format=\"txt\",\n                        content=(\n                            content if isinstance(content, List) else to_cpu(content)\n                        ),\n                    )\n\n                # the last token is meaningless because the text is padded with eos at the end\n                infer_results[\"logits\"] = torch.log_softmax(\n                    infer_results[\"logits\"][:, :-1], dim=-1\n                )\n                hypo_text_prob, hypo_text = torch.max(infer_results[\"logits\"], dim=-1)\n                # the original text contains both sos at the beginning and eos at the end\n                hypo_text_len = text_len - 2\n                feat_token_len_ratio = feat_len / (hypo_text_len + 1e-10)\n                # sum up the log-probability of all time steps to get the confidence\n                length_penalty = (\n                    infer_conf[\"length_penalty\"]\n                    if \"length_penalty\" in infer_conf.keys()\n                    else 1.0\n                )\n                hypo_text_confid = torch.sum(hypo_text_prob, dim=-1) / (\n                    hypo_text_len**length_penalty\n                )\n\n        # turn the data all the unsupervised metrics into the cpu version (List)\n        # consider one &lt;sos/eos&gt; at the end, so hypo_text_len is added to 1\n        hypo_text_len, feat_token_len_ratio, hypo_text_confid = (\n            to_cpu(hypo_text_len + 1),\n            to_cpu(feat_token_len_ratio),\n            to_cpu(hypo_text_confid),\n        )\n\n        # --- 3. Unsupervised Metrics Calculation (ground-truth text is not involved here) --- #\n        # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n        # hypo_text = [self.tokenizer.tensor2text(hypo[(hypo != self.tokenizer.ignore_idx) &amp;\n        #                                              (hypo != self.tokenizer.sos_eos_idx)]) for hypo in hypo_text]\n        hypo_text = [self.tokenizer.tensor2text(hypo) for hypo in hypo_text]\n\n        # in the decoding-only mode, only the hypothesis-related results will be returned\n        outputs.update(\n            text=dict(format=\"txt\", content=hypo_text),\n            text_len=dict(format=\"txt\", content=hypo_text_len),\n            feat_token_len_ratio=dict(format=\"txt\", content=feat_token_len_ratio),\n            text_confid=dict(format=\"txt\", content=hypo_text_confid),\n        )\n\n        # add the attention matrix into the output Dict, only used for model visualization during training\n        # because it will consume too much time for saving the attention matrices of all testing samples during testing\n        if return_att:\n            outputs.update(att=hypo_att)\n\n        # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n        text = [\n            self.tokenizer.tensor2text(\n                real[\n                    (real != self.tokenizer.ignore_idx)\n                    &amp; (real != self.tokenizer.sos_eos_idx)\n                ]\n            )\n            for real in text\n        ]\n        # evaluation reports for all the testing instances\n        (\n            instance_report_dict,\n            align_table_list,\n            cer_list,\n            wer_list,\n            insertion_list,\n            deletion_list,\n            substitution_list,\n        ) = ({}, [], [], [], [], [], [])\n        # loop each sentence\n        for i in range(len(text)):\n            # add the confidence into instance_reports.md\n            if \"Hypothesis Confidence\" not in instance_report_dict.keys():\n                instance_report_dict[\"Hypothesis Confidence\"] = []\n            instance_report_dict[\"Hypothesis Confidence\"].append(\n                f\"{hypo_text_confid[i]:.6f}\"\n            )\n\n            # add the frame-token length ratio into instance_reports.md\n            if \"Feature-Token Length Ratio\" not in instance_report_dict.keys():\n                instance_report_dict[\"Feature-Token Length Ratio\"] = []\n            instance_report_dict[\"Feature-Token Length Ratio\"].append(\n                f\"{feat_token_len_ratio[i]:.2f}\"\n            )\n\n            # --- 4. Supervised Metrics Calculation (Reference is involved here)  --- #\n            if not decode_only:\n                # obtain the cer and wer metrics\n                cer, wer = self.error_rate(hypo_text=hypo_text[i], real_text=text[i])\n                i_num, d_num, s_num, align_table = get_word_edit_alignment(\n                    hypo_text[i], text[i]\n                )\n\n                # record the string of hypothesis-reference alignment table\n                align_table_list.append(align_table)\n\n                # record the CER value of the current data instance\n                cer_list.append(cer[0])\n                if \"CER\" not in instance_report_dict.keys():\n                    instance_report_dict[\"CER\"] = []\n                instance_report_dict[\"CER\"].append(f\"{cer[0]:.2%}\")\n\n                # record the WER value of the current data instance\n                wer_list.append(wer[0])\n                if \"WER\" not in instance_report_dict.keys():\n                    instance_report_dict[\"WER\"] = []\n                instance_report_dict[\"WER\"].append(f\"{wer[0]:.2%}\")\n\n                # record the word insertion value of the current data instance\n                insertion_list.append(i_num)\n                if \"Word Insertion\" not in instance_report_dict.keys():\n                    instance_report_dict[\"Word Insertion\"] = []\n                instance_report_dict[\"Word Insertion\"].append(f\"{i_num}\")\n\n                # record the word deletion value of the current data instance\n                deletion_list.append(d_num)\n                if \"Word Deletion\" not in instance_report_dict.keys():\n                    instance_report_dict[\"Word Deletion\"] = []\n                instance_report_dict[\"Word Deletion\"].append(f\"{d_num}\")\n\n                # record the word substitution value of the current data instance\n                substitution_list.append(s_num)\n                if \"Word Substitution\" not in instance_report_dict.keys():\n                    instance_report_dict[\"Word Substitution\"] = []\n                instance_report_dict[\"Word Substitution\"].append(f\"{s_num}\")\n\n        # register the instance reports and the strings of alignment tables for generating instance_reports.md\n        self.register_instance_reports(\n            md_list_dict=instance_report_dict, extra_string_list=align_table_list\n        )\n\n        # not return the supervised metrics in the decoding-only mode\n        if not decode_only:\n            outputs.update(\n                cer=dict(format=\"txt\", content=cer_list),\n                wer=dict(format=\"txt\", content=wer_list),\n                insertion=dict(format=\"txt\", content=insertion_list),\n                deletion=dict(format=\"txt\", content=deletion_list),\n                substitution=dict(format=\"txt\", content=substitution_list),\n            )\n        return outputs\n</code></pre>"},{"location":"reference/model/ar_asr/#model.ar_asr.ARASR.criterion_init","title":"<code>criterion_init(ce_loss=None, ilm_loss=None, ctc_loss=None, att_guid_loss=None)</code>","text":"This function initializes all the necessary Criterion members <ol> <li><code>speechain.criterion.cross_entropy.CrossEntropy</code> for training loss calculation.</li> <li><code>speechain.criterion.ctc.CTCLoss</code> for training loss calculation.</li> <li><code>speechain.criterion.accuracy.Accuracy</code> for teacher-forcing validation accuracy calculation.</li> <li><code>speechain.criterion.error_rate.ErrorRate</code> for evaluation CER &amp; WER calculation.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>ce_loss</code> <code>Dict[str, Any]</code> <p>Dict[str, Any] The arguments for CrossEntropy(). If not given, the default setting of CrossEntropy() will be used. Please refer to speechain.criterion.cross_entropy.CrossEntropy for more details.</p> <code>None</code> <code>ilm_loss</code> <code>Dict[str, Any]</code> <code>None</code> <code>ctc_loss</code> <code>Dict[str, Any] or bool</code> <p>Dict[str, Any] or bool The arguments for CTCLoss(). If not given, self.ctc_loss won't be initialized. This argument can also be set to a bool value 'True'. If True, the default setting of CTCLoss() will be used. Please refer to speechain.criterion.ctc.CTCLoss for more details.</p> <code>None</code> <code>att_guid_loss</code> <code>Dict[str, Any] or bool</code> <p>Dict[str, Any] or bool The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized. This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance() will be used. Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.</p> <code>None</code> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>def criterion_init(\n    self,\n    ce_loss: Dict[str, Any] = None,\n    ilm_loss: Dict[str, Any] = None,\n    ctc_loss: Dict[str, Any] or bool = None,\n    att_guid_loss: Dict[str, Any] or bool = None,\n):\n    \"\"\"\n    This function initializes all the necessary _Criterion_ members:\n        1. `speechain.criterion.cross_entropy.CrossEntropy` for training loss calculation.\n        2. `speechain.criterion.ctc.CTCLoss` for training loss calculation.\n        3. `speechain.criterion.accuracy.Accuracy` for teacher-forcing validation accuracy calculation.\n        4. `speechain.criterion.error_rate.ErrorRate` for evaluation CER &amp; WER calculation.\n\n    Args:\n        ce_loss: Dict[str, Any]\n            The arguments for CrossEntropy(). If not given, the default setting of CrossEntropy() will be used.\n            Please refer to speechain.criterion.cross_entropy.CrossEntropy for more details.\n        ilm_loss:\n        ctc_loss: Dict[str, Any] or bool\n            The arguments for CTCLoss(). If not given, self.ctc_loss won't be initialized.\n            This argument can also be set to a bool value 'True'. If True, the default setting of CTCLoss()\n            will be used.\n            Please refer to speechain.criterion.ctc.CTCLoss for more details.\n        att_guid_loss: Dict[str, Any] or bool\n            The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized.\n            This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance()\n            will be used.\n            Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.\n\n    \"\"\"\n\n    # initialize cross-entropy loss for the encoder-decoder\n    if ce_loss is None:\n        ce_loss = {}\n    self.ce_loss = CrossEntropy(**ce_loss)\n\n    # initialize cross-entropy loss for the internal LM\n    if self.ilm_weight &gt; 0:\n        # ilm_loss is default to be normalized by sentence lengths\n        if ilm_loss is None:\n            ilm_loss = dict(length_normalized=True)\n        self.ilm_loss = CrossEntropy(**ilm_loss)\n\n    # initialize ctc loss\n    if self.ctc_weight &gt; 0:\n        # if ctc_loss is given as True or None, the default arguments of CTCLoss will be used\n        if not isinstance(ctc_loss, Dict):\n            ctc_loss = {}\n\n        if self.device != \"cpu\" and self.tokenizer.ignore_idx != 0:\n            raise RuntimeError(\n                f\"For speeding up CTC calculation by CuDNN, \"\n                f\"please set the blank id to 0 (got {self.tokenizer.ignore_idx}).\"\n            )\n\n        ctc_loss[\"blank\"] = self.tokenizer.ignore_idx\n        self.ctc_loss = CTCLoss(**ctc_loss)\n\n    # initialize attention guidance loss\n    if att_guid_loss is not None:\n        # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n        if not isinstance(att_guid_loss, Dict):\n            assert (\n                isinstance(att_guid_loss, bool) and att_guid_loss\n            ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n            att_guid_loss = {}\n\n        assert (\n            \"encdec\" in self.return_att_type\n        ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n        self.att_guid_loss = AttentionGuidance(**att_guid_loss)\n\n    # initialize teacher-forcing accuracy for validation\n    self.accuracy = Accuracy()\n\n    # initialize text perplexity calculator for internal LM if needed\n    self.perplexity = Perplexity()\n\n    # initialize error rate (CER &amp; WER) for evaluation\n    self.error_rate = ErrorRate(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/model/ar_asr/#model.ar_asr.ARASR.inference","title":"<code>inference(infer_conf, feat=None, feat_len=None, text=None, text_len=None, domain=None, return_att=False, decode_only=False, teacher_forcing=False)</code>","text":"<p>The inference function for ASR models. There are two steps in this function:     1. Decode the input speech into hypothesis transcript     2. Evaluate the hypothesis transcript by the ground-truth</p> <p>This function can be called for model evaluation, on-the-fly model visualization, and even pseudo transcript generation during training.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>torch.Tensor The speech data to be inferred.</p> <code>None</code> <code>feat_len</code> <code>Tensor</code> <p>torch.Tensor The length of <code>feat</code>.</p> <code>None</code> <code>text</code> <code>Tensor</code> <p>torch.Tensor The ground-truth transcript for the input speech</p> <code>None</code> <code>text_len</code> <code>Tensor</code> <p>torch.Tensor The length of <code>text</code>.</p> <code>None</code> <code>domain</code> <code>str</code> <p>str = None This argument indicates which domain the input speech belongs to. It's used to indicate the <code>ASREncoder</code> member how to encode the input speech.</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool = False Whether the attention matrix of the input speech is returned.</p> <code>False</code> <code>decode_only</code> <code>bool</code> <p>bool = False Whether skip the evaluation step and do the decoding step only.</p> <code>False</code> <code>teacher_forcing</code> <code>bool</code> <p>bool = True Whether you use the teacher-forcing technique to generate the hypothesis transcript.</p> <code>False</code> <code>infer_conf</code> <code>Dict</code> <p>Dict The inference configuration given from the <code>infer_cfg</code> in your <code>exp_cfg</code>. For more details, please refer to speechain.infer_func.beam_search.beam_searching.</p> required <p>Dict</p> Type Description <code>Dict[str, Any]</code> <p>A Dict containing all the decoding and evaluation results.</p> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>def inference(\n    self,\n    infer_conf: Dict,\n    feat: torch.Tensor = None,\n    feat_len: torch.Tensor = None,\n    text: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    domain: str = None,\n    return_att: bool = False,\n    decode_only: bool = False,\n    teacher_forcing: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    The inference function for ASR models. There are two steps in this function:\n        1. Decode the input speech into hypothesis transcript\n        2. Evaluate the hypothesis transcript by the ground-truth\n\n    This function can be called for model evaluation, on-the-fly model visualization, and even pseudo transcript\n    generation during training.\n\n    Args:\n        # --- Testing data arguments --- #\n        feat: torch.Tensor\n            The speech data to be inferred.\n        feat_len: torch.Tensor\n            The length of `feat`.\n        text: torch.Tensor\n            The ground-truth transcript for the input speech\n        text_len: torch.Tensor\n            The length of `text`.\n        # --- Explicit inference arguments --- #\n        domain: str = None\n            This argument indicates which domain the input speech belongs to.\n            It's used to indicate the `ASREncoder` member how to encode the input speech.\n        return_att: bool = False\n            Whether the attention matrix of the input speech is returned.\n        decode_only: bool = False\n            Whether skip the evaluation step and do the decoding step only.\n        teacher_forcing: bool = True\n            Whether you use the teacher-forcing technique to generate the hypothesis transcript.\n        # --- Implicit inference arguments given by infer_cfg from runner.py --- #\n        infer_conf: Dict\n            The inference configuration given from the `infer_cfg` in your `exp_cfg`.\n            For more details, please refer to speechain.infer_func.beam_search.beam_searching.\n\n    Returns: Dict\n        A Dict containing all the decoding and evaluation results.\n\n    \"\"\"\n    assert feat is not None and feat_len is not None\n\n    # --- 0. Hyperparameter &amp; Model Preparation Stage --- #\n    # in-place replace infer_conf to protect the original information\n    infer_conf = copy.deepcopy(infer_conf)\n    if \"decode_only\" in infer_conf.keys():\n        decode_only = infer_conf.pop(\"decode_only\")\n    if \"teacher_forcing\" in infer_conf.keys():\n        teacher_forcing = infer_conf.pop(\"teacher_forcing\")\n    hypo_text, hypo_text_len, feat_token_len_ratio, hypo_text_confid, hypo_att = (\n        None,\n        None,\n        None,\n        None,\n        None,\n    )\n\n    # only initialize the language model when lm_weight is given as a positive float number\n    if \"lm_weight\" in infer_conf.keys() and infer_conf[\"lm_weight\"] &gt; 0:\n        assert self.lm_model_cfg is not None or self.lm_model_path is not None, (\n            \"If you want to use ASR-LM joint decoding, \"\n            \"please give lm_model_cfg and lm_model_path in model['customize_conf']!\"\n        )\n        # lazily initialize the language model only at the first time\n        if not hasattr(self, \"lm\"):\n            # use the built-in lm configuration if lm_model_cfg is not given\n            if self.lm_model_cfg is None:\n                self.lm_model_cfg = os.path.join(\n                    self.result_path, \"lm_model_cfg.yaml\"\n                )\n\n            # get the Dict-like configuration for the language model\n            if isinstance(self.lm_model_cfg, str):\n                self.lm_model_cfg = load_yaml(parse_path_args(self.lm_model_cfg))\n            if \"model\" in self.lm_model_cfg.keys():\n                self.lm_model_cfg = self.lm_model_cfg[\"model\"]\n            if \"module_conf\" in self.lm_model_cfg.keys():\n                self.lm_model_cfg = self.lm_model_cfg[\"module_conf\"]\n\n            # update the built-in configuration yaml file\n            with open(\n                os.path.join(self.result_path, \"lm_model_cfg.yaml\"),\n                \"w\",\n                encoding=\"utf-8\",\n            ) as f:\n                yaml.dump(self.lm_model_cfg, f, sort_keys=False)\n            self.lm = LanguageModel(\n                vocab_size=self.tokenizer.vocab_size, **self.lm_model_cfg\n            ).cuda(self.device)\n\n            # use the built-in lm model if lm_model_path is not given\n            if self.lm_model_path is None:\n                self.lm_model_path = os.path.join(self.result_path, \"lm_model.pth\")\n\n            # load the parameters of the target lm\n            _lm_model_para = torch.load(\n                parse_path_args(self.lm_model_path), map_location=self.device\n            )\n            lm_model_para = OrderedDict()\n            for key, para in _lm_model_para.items():\n                if key.startswith(\"lm.\"):\n                    key = key.replace(\"lm.\", \"\", 1)\n                lm_model_para[key] = para\n\n            # update the built-in lm parameters and load them into the lm for inference\n            torch.save(\n                lm_model_para, os.path.join(self.result_path, \"lm_model.pth\")\n            )\n            self.lm.load_state_dict(lm_model_para)\n\n    outputs = dict()\n    # --- 1. The 1st Pass: ASR Decoding by Beam Searching --- #\n    if not teacher_forcing:\n        # copy the input data in advance for data safety\n        model_input = copy.deepcopy(dict(feat=feat, feat_len=feat_len))\n\n        # Encoding input speech\n        enc_feat, enc_feat_mask, _, _ = self.encoder(domain=domain, **model_input)\n\n        # generate the model hypothesis\n        infer_results = beam_searching(\n            enc_feat=enc_feat,\n            enc_feat_mask=enc_feat_mask,\n            asr_decode_fn=self.decoder,\n            ctc_decode_fn=self.ctc_layer if hasattr(self, \"ctc_layer\") else None,\n            lm_decode_fn=self.lm if hasattr(self, \"lm\") else None,\n            vocab_size=self.tokenizer.vocab_size,\n            sos_eos=self.tokenizer.sos_eos_idx,\n            padding_idx=self.tokenizer.ignore_idx,\n            **infer_conf,\n        )\n        hypo_text = infer_results[\"hypo_text\"]\n        hypo_text_len = infer_results[\"hypo_text_len\"]\n        feat_token_len_ratio = infer_results[\"feat_token_len_ratio\"]\n        hypo_text_confid = infer_results[\"hypo_text_confid\"]\n\n    # --- 2. The 2nd Pass: ASR Decoding by Teacher Forcing --- #\n    if teacher_forcing or return_att:\n        # copy the input data in advance for data safety\n        model_input = copy.deepcopy(\n            dict(\n                feat=feat,\n                feat_len=feat_len,\n                text=text if teacher_forcing else hypo_text,\n                text_len=text_len if teacher_forcing else hypo_text_len,\n            )\n        )\n        infer_results = self.module_forward(\n            return_att=return_att, domain=domain, **model_input\n        )\n        # return the attention matrices\n        if return_att:\n            hypo_att = infer_results[\"att\"]\n\n        # update the hypothesis text-related data in the teacher forcing mode\n        if teacher_forcing:\n            tf_results = self.criterion_forward(\n                text=text, text_len=text_len, forward_ctc=True, **infer_results\n            )\n            for key, content in tf_results.items():\n                outputs[key] = dict(\n                    format=\"txt\",\n                    content=(\n                        content if isinstance(content, List) else to_cpu(content)\n                    ),\n                )\n\n            # the last token is meaningless because the text is padded with eos at the end\n            infer_results[\"logits\"] = torch.log_softmax(\n                infer_results[\"logits\"][:, :-1], dim=-1\n            )\n            hypo_text_prob, hypo_text = torch.max(infer_results[\"logits\"], dim=-1)\n            # the original text contains both sos at the beginning and eos at the end\n            hypo_text_len = text_len - 2\n            feat_token_len_ratio = feat_len / (hypo_text_len + 1e-10)\n            # sum up the log-probability of all time steps to get the confidence\n            length_penalty = (\n                infer_conf[\"length_penalty\"]\n                if \"length_penalty\" in infer_conf.keys()\n                else 1.0\n            )\n            hypo_text_confid = torch.sum(hypo_text_prob, dim=-1) / (\n                hypo_text_len**length_penalty\n            )\n\n    # turn the data all the unsupervised metrics into the cpu version (List)\n    # consider one &lt;sos/eos&gt; at the end, so hypo_text_len is added to 1\n    hypo_text_len, feat_token_len_ratio, hypo_text_confid = (\n        to_cpu(hypo_text_len + 1),\n        to_cpu(feat_token_len_ratio),\n        to_cpu(hypo_text_confid),\n    )\n\n    # --- 3. Unsupervised Metrics Calculation (ground-truth text is not involved here) --- #\n    # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n    # hypo_text = [self.tokenizer.tensor2text(hypo[(hypo != self.tokenizer.ignore_idx) &amp;\n    #                                              (hypo != self.tokenizer.sos_eos_idx)]) for hypo in hypo_text]\n    hypo_text = [self.tokenizer.tensor2text(hypo) for hypo in hypo_text]\n\n    # in the decoding-only mode, only the hypothesis-related results will be returned\n    outputs.update(\n        text=dict(format=\"txt\", content=hypo_text),\n        text_len=dict(format=\"txt\", content=hypo_text_len),\n        feat_token_len_ratio=dict(format=\"txt\", content=feat_token_len_ratio),\n        text_confid=dict(format=\"txt\", content=hypo_text_confid),\n    )\n\n    # add the attention matrix into the output Dict, only used for model visualization during training\n    # because it will consume too much time for saving the attention matrices of all testing samples during testing\n    if return_att:\n        outputs.update(att=hypo_att)\n\n    # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n    text = [\n        self.tokenizer.tensor2text(\n            real[\n                (real != self.tokenizer.ignore_idx)\n                &amp; (real != self.tokenizer.sos_eos_idx)\n            ]\n        )\n        for real in text\n    ]\n    # evaluation reports for all the testing instances\n    (\n        instance_report_dict,\n        align_table_list,\n        cer_list,\n        wer_list,\n        insertion_list,\n        deletion_list,\n        substitution_list,\n    ) = ({}, [], [], [], [], [], [])\n    # loop each sentence\n    for i in range(len(text)):\n        # add the confidence into instance_reports.md\n        if \"Hypothesis Confidence\" not in instance_report_dict.keys():\n            instance_report_dict[\"Hypothesis Confidence\"] = []\n        instance_report_dict[\"Hypothesis Confidence\"].append(\n            f\"{hypo_text_confid[i]:.6f}\"\n        )\n\n        # add the frame-token length ratio into instance_reports.md\n        if \"Feature-Token Length Ratio\" not in instance_report_dict.keys():\n            instance_report_dict[\"Feature-Token Length Ratio\"] = []\n        instance_report_dict[\"Feature-Token Length Ratio\"].append(\n            f\"{feat_token_len_ratio[i]:.2f}\"\n        )\n\n        # --- 4. Supervised Metrics Calculation (Reference is involved here)  --- #\n        if not decode_only:\n            # obtain the cer and wer metrics\n            cer, wer = self.error_rate(hypo_text=hypo_text[i], real_text=text[i])\n            i_num, d_num, s_num, align_table = get_word_edit_alignment(\n                hypo_text[i], text[i]\n            )\n\n            # record the string of hypothesis-reference alignment table\n            align_table_list.append(align_table)\n\n            # record the CER value of the current data instance\n            cer_list.append(cer[0])\n            if \"CER\" not in instance_report_dict.keys():\n                instance_report_dict[\"CER\"] = []\n            instance_report_dict[\"CER\"].append(f\"{cer[0]:.2%}\")\n\n            # record the WER value of the current data instance\n            wer_list.append(wer[0])\n            if \"WER\" not in instance_report_dict.keys():\n                instance_report_dict[\"WER\"] = []\n            instance_report_dict[\"WER\"].append(f\"{wer[0]:.2%}\")\n\n            # record the word insertion value of the current data instance\n            insertion_list.append(i_num)\n            if \"Word Insertion\" not in instance_report_dict.keys():\n                instance_report_dict[\"Word Insertion\"] = []\n            instance_report_dict[\"Word Insertion\"].append(f\"{i_num}\")\n\n            # record the word deletion value of the current data instance\n            deletion_list.append(d_num)\n            if \"Word Deletion\" not in instance_report_dict.keys():\n                instance_report_dict[\"Word Deletion\"] = []\n            instance_report_dict[\"Word Deletion\"].append(f\"{d_num}\")\n\n            # record the word substitution value of the current data instance\n            substitution_list.append(s_num)\n            if \"Word Substitution\" not in instance_report_dict.keys():\n                instance_report_dict[\"Word Substitution\"] = []\n            instance_report_dict[\"Word Substitution\"].append(f\"{s_num}\")\n\n    # register the instance reports and the strings of alignment tables for generating instance_reports.md\n    self.register_instance_reports(\n        md_list_dict=instance_report_dict, extra_string_list=align_table_list\n    )\n\n    # not return the supervised metrics in the decoding-only mode\n    if not decode_only:\n        outputs.update(\n            cer=dict(format=\"txt\", content=cer_list),\n            wer=dict(format=\"txt\", content=wer_list),\n            insertion=dict(format=\"txt\", content=insertion_list),\n            deletion=dict(format=\"txt\", content=deletion_list),\n            substitution=dict(format=\"txt\", content=substitution_list),\n        )\n    return outputs\n</code></pre>"},{"location":"reference/model/ar_asr/#model.ar_asr.ARASR.module_forward","title":"<code>module_forward(epoch=None, feat=None, text=None, feat_len=None, text_len=None, domain=None, return_att=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input speech data. feat_dim = 1 in the case of raw speech waveforms.</p> <code>None</code> <code>feat_len</code> <code>Tensor</code> <p>(batch,) The lengths of input speech data</p> <code>None</code> <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The input text data with  at the beginning and end <code>None</code> <code>text_len</code> <code>Tensor</code> <p>(batch,) The lengths of input text data</p> <code>None</code> <code>epoch</code> <code>int</code> <p>int The number of the current training epoch. Mainly used for mean&amp;std calculation in the feature normalization</p> <code>None</code> <code>domain</code> <code>str</code> <p>str = None</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool Controls whether the attention matrices of each layer in the encoder and decoder will be returned.</p> <code>False</code> <code>kwargs</code> <p>Temporary register used to store the redundant arguments.</p> <code>{}</code> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>def module_forward(\n    self,\n    epoch: int = None,\n    feat: torch.Tensor = None,\n    text: torch.Tensor = None,\n    feat_len: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    domain: str = None,\n    return_att: bool = False,\n    **kwargs,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input speech data. feat_dim = 1 in the case of raw speech waveforms.\n        feat_len: (batch,)\n            The lengths of input speech data\n        text: (batch, text_maxlen)\n            The input text data with &lt;sos/eos&gt; at the beginning and end\n        text_len: (batch,)\n            The lengths of input text data\n        epoch: int\n            The number of the current training epoch.\n            Mainly used for mean&amp;std calculation in the feature normalization\n        domain: str = None\n        return_att: bool\n            Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n        kwargs:\n            Temporary register used to store the redundant arguments.\n\n    \"\"\"\n    # para checking\n    assert feat is not None and feat_len is not None\n    assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n        0\n    ), \"The amounts of utterances and sentences are not equal to each other.\"\n    assert feat_len.size(0) == feat.size(\n        0\n    ), \"The amounts of utterances and their lengths are not equal to each other.\"\n    assert text_len.size(0) == text.size(\n        0\n    ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n    # remove the &lt;sos/eos&gt; at the end of each sentence\n    for i in range(text_len.size(0)):\n        text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n    text, text_len = text[:, :-1], text_len - 1\n\n    # --- 1. Encoder: Input Feature to Encoder Hidden Representation --- #\n    enc_returns = self.encoder(\n        feat=feat, feat_len=feat_len, epoch=epoch, domain=domain\n    )\n    # Transformer-based encoder additionally returns the encoder self-attention\n    if len(enc_returns) == 4:\n        enc_feat, enc_feat_mask, enc_attmat, enc_hidden = enc_returns\n    # RNN-based encoder doesn't return any attention\n    elif len(enc_returns) == 3:\n        (enc_feat, enc_feat_mask, enc_hidden), enc_attmat = enc_returns, None\n    else:\n        raise RuntimeError\n\n    # --- 2. Decoder: Encoder Hidden Representation to Decoder Hidden Representation --- #\n    dec_returns = self.decoder(\n        enc_feat=enc_feat, enc_feat_mask=enc_feat_mask, text=text, text_len=text_len\n    )\n    # Transformer-based decoder additionally returns the decoder self-attention\n    if len(dec_returns) == 4:\n        dec_feat, dec_attmat, encdec_attmat, dec_hidden = dec_returns\n    # RNN-based decoder only returns the encoder-decoder attention\n    elif len(dec_returns) == 3:\n        (dec_feat, encdec_attmat, dec_hidden), dec_attmat = dec_returns, None\n    else:\n        raise RuntimeError\n\n    # initialize the asr output to be the decoder predictions\n    outputs = dict(logits=dec_feat)\n\n    # --- 3.(optional) Decoder: Internal LM Estimation by zeroing Encoder Hidden Representation --- #\n    if self.ilm_weight &gt; 0 or self.ilm_sub_weight &gt; 0:\n        ilm_returns = self.decoder(\n            enc_feat=torch.zeros(\n                enc_feat.size(0), 1, enc_feat.size(2), device=enc_feat.device\n            ),\n            enc_feat_mask=torch.ones(\n                enc_feat_mask.size(0),\n                enc_feat_mask.size(1),\n                1,\n                dtype=torch.bool,\n                device=enc_feat_mask.device,\n            ),\n            text=text,\n            text_len=text_len,\n        )\n        # Transformer-based decoder additionally returns the decoder self-attention\n        if len(ilm_returns) == 4:\n            ilm_feat, ilm_dec_attmat, ilm_encdec_attmat, ilm_hidden = ilm_returns\n        # RNN-based decoder only returns the encoder-decoder attention\n        elif len(ilm_returns) == 3:\n            (ilm_feat, ilm_encdec_attmat, ilm_hidden), ilm_dec_attmat = (\n                ilm_returns,\n                None,\n            )\n        else:\n            raise RuntimeError\n\n        if self.ilm_weight &gt; 0:\n            outputs.update(ilm_logits=ilm_feat)\n        elif self.ilm_sub_weight &gt; 0:\n            outputs[\"logits\"] -= self.ilm_sub_weight * ilm_feat\n\n    # --- 4.(optional) Encoder Hidden Representation to CTC Prediction --- #\n    if hasattr(self, \"ctc_layer\"):\n        ctc_logits = self.ctc_layer(enc_feat)\n        outputs.update(\n            ctc_logits=ctc_logits,\n            enc_feat_len=torch.sum(enc_feat_mask.squeeze(dim=1), dim=-1),\n        )\n\n    def shrink_attention(input_att_list):\n        # pick up the target attention layers\n        if (\n            self.return_att_layer_num != -1\n            and len(input_att_list) &gt; self.return_att_layer_num\n        ):\n            input_att_list = input_att_list[-self.return_att_layer_num :]\n        # pick up the target attention heads\n        if (\n            self.return_att_head_num != -1\n            and input_att_list[0].size(1) &gt; self.return_att_head_num\n        ):\n            input_att_list = [\n                att[:, : self.return_att_head_num] for att in input_att_list\n            ]\n        return input_att_list\n\n    # return the attention results if specified\n    if return_att or hasattr(self, \"att_guid_loss\"):\n        # encoder-decoder attention\n        if \"encdec\" in self.return_att_type:\n            # register the encoder-decoder attention\n            outputs.update(att=dict(encdec=shrink_attention(encdec_attmat)))\n        # encoder self-attention\n        if enc_attmat is not None and \"enc\" in self.return_att_type:\n            outputs[\"att\"].update(enc=shrink_attention(enc_attmat))\n        # decoder self-attention\n        if dec_attmat is not None and \"dec\" in self.return_att_type:\n            outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n    return outputs\n</code></pre>"},{"location":"reference/model/ar_asr/#model.ar_asr.ARASR.module_init","title":"<code>module_init(token_type, token_path, enc_prenet, encoder, dec_emb, decoder, frontend=None, normalize=None, specaug=None, ilm_weight=0.0, ilm_sub_weight=0.0, ctc_weight=0.0, sample_rate=16000, audio_format='wav', return_att_type=None, return_att_head_num=2, return_att_layer_num=2, lm_model_cfg=None, lm_model_path=None)</code>","text":"<p>This initialization function contains 4 steps: 1. <code>Tokenizer</code> initialization. 2. <code>ASREncoder</code> initialization. 3. <code>ARASRDecoder</code> initialization. 4. (optional) 'CTC' layer initialization</p> <p>The input arguments of this function are two-fold: 1. the ones from <code>customize_conf</code> of <code>model</code> in <code>train_cfg</code> 2. the ones from <code>module_conf</code> of <code>model</code> in <code>train_cfg</code></p> <p>Parameters:</p> Name Type Description Default <code>frontend</code> <code>Dict</code> <p>(optional) The configuration of the acoustic feature extraction frontend in the <code>ASREncoder</code> member. This argument must be given since our toolkit doesn't support time-domain ASR. For more details about how to give <code>frontend</code>, please refer to speechain.module.encoder.asr.ASREncoder.</p> <code>None</code> <code>normalize</code> <code>Dict or bool</code> <p>(optional) The configuration of the normalization layer in the <code>ASREncoder</code> member. This argument can also be given as a bool value. True means the default configuration and False means no normalization. For more details about how to give <code>normalize</code>, please refer to     speechain.module.norm.feat_norm.FeatureNormalization.</p> <code>None</code> <code>specaug</code> <code>Dict or bool</code> <p>(optional) The configuration of the SpecAugment layer in the <code>ASREncoder</code> member. This argument can also be given as a bool value. True means the default configuration and False means no SpecAugment. For more details about how to give <code>specaug</code>, please refer to     speechain.module.augment.specaug.SpecAugment.</p> <code>None</code> <code>enc_prenet</code> <code>Dict</code> <p>(mandatory) The configuration of the prenet in the <code>ASREncoder</code> member. The encoder prenet embeds the input acoustic features into hidden embeddings before feeding them into the encoder. For more details about how to give <code>enc_prent</code>, please refer to speechain.module.encoder.asr.ASREncoder.</p> required <code>encoder</code> <code>Dict</code> <p>(mandatory) The configuration of the encoder main body in the <code>ASREncoder</code> member. The encoder embeds the hidden embeddings into the encoder representations at each time steps of the input acoustic features. For more details about how to give <code>encoder</code>, please refer to speechain.module.encoder.asr.ASREncoder.</p> required <code>dec_emb</code> <code>Dict</code> <p>(mandatory) The configuration of the embedding layer in the <code>ARASRDecoder</code> member. The decoder prenet embeds the input token ids into hidden embeddings before feeding them into the decoder. For more details about how to give <code>dec_emb</code>, please refer to speechain.module.encoder.asr.ASREncoder.</p> required <code>decoder</code> <code>Dict</code> <p>(mandatory) The configuration of the decoder main body in the <code>ARASRDecoder</code> member. The decoder predicts the probability of the next token at each time steps based on the token embeddings. For more details about how to give <code>decoder</code>, please refer to speechain.module.decoder.ar_asr.ARASRDecoder.</p> required <code>token_type</code> <code>str</code> <p>(mandatory) The type of the built-in tokenizer.</p> required <code>token_path</code> <code>str</code> <p>(mandatory) The path of the vocabulary for initializing the built-in tokenizer.</p> required <code>sample_rate</code> <code>int</code> <p>int = 16000 (optional) The sampling rate of the input speech. Currently, it's used for acoustic feature extraction frontend initialization and tensorboard register of the input speech for model visualization. In the future, this argument will also be used to on-the-fly downsample the input speech.</p> <code>16000</code> <code>audio_format</code> <code>str</code> <p>(optional) This argument is only used for input speech recording during model visualization.</p> <code>'wav'</code> <code>return_att_type</code> <code>List[str] or str</code> <p>List[str] or str = ['encdec', 'enc', 'dec'] The type of attentions you want to return for both attention guidance and attention visualization. It can be given as a string (one type) or a list of strings (multiple types). The type should be one of     1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN     2. 'enc': the encoder self-attention, only for Transformer     3. 'dec': the decoder self-attention, only for Transformer</p> <code>None</code> <code>return_att_head_num</code> <code>int</code> <p>int = -1 The number of returned attention heads. If -1, all the heads in an attention layer will be returned. RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.</p> <code>2</code> <code>return_att_layer_num</code> <code>int</code> <p>int = 1 The number of returned attention layers. If -1, all the attention layers will be returned. RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.</p> <code>2</code> <code>lm_model_cfg</code> <code>Dict or str</code> <p>Dict or str The configuration for the language model used for joint decoding. Can be either a Dict or a string indicating where the .yaml model configuration file is placed.</p> <code>None</code> <code>lm_model_path</code> <code>str</code> <p>str The string indicating where the .pth model parameter file is placed.</p> <code>None</code> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>def module_init(\n    self,\n    token_type: str,\n    token_path: str,\n    enc_prenet: Dict,\n    encoder: Dict,\n    dec_emb: Dict,\n    decoder: Dict,\n    frontend: Dict = None,\n    normalize: Dict or bool = None,\n    specaug: Dict or bool = None,\n    ilm_weight: float = 0.0,\n    ilm_sub_weight: float = 0.0,\n    ctc_weight: float = 0.0,\n    sample_rate: int = 16000,\n    audio_format: str = \"wav\",\n    return_att_type: List[str] or str = None,\n    return_att_head_num: int = 2,\n    return_att_layer_num: int = 2,\n    lm_model_cfg: Dict or str = None,\n    lm_model_path: str = None,\n):\n    \"\"\"\n    This initialization function contains 4 steps:\n    1. `Tokenizer` initialization.\n    2. `ASREncoder` initialization.\n    3. `ARASRDecoder` initialization.\n    4. (optional) 'CTC' layer initialization\n\n    The input arguments of this function are two-fold:\n    1. the ones from `customize_conf` of `model` in `train_cfg`\n    2. the ones from `module_conf` of `model` in `train_cfg`\n\n    Args:\n        # --- module_conf arguments --- #\n        frontend: (optional)\n            The configuration of the acoustic feature extraction frontend in the `ASREncoder` member.\n            This argument must be given since our toolkit doesn't support time-domain ASR.\n            For more details about how to give `frontend`, please refer to speechain.module.encoder.asr.ASREncoder.\n        normalize: (optional)\n            The configuration of the normalization layer in the `ASREncoder` member.\n            This argument can also be given as a bool value.\n            True means the default configuration and False means no normalization.\n            For more details about how to give `normalize`, please refer to\n                speechain.module.norm.feat_norm.FeatureNormalization.\n        specaug: (optional)\n            The configuration of the SpecAugment layer in the `ASREncoder` member.\n            This argument can also be given as a bool value.\n            True means the default configuration and False means no SpecAugment.\n            For more details about how to give `specaug`, please refer to\n                speechain.module.augment.specaug.SpecAugment.\n        enc_prenet: (mandatory)\n            The configuration of the prenet in the `ASREncoder` member.\n            The encoder prenet embeds the input acoustic features into hidden embeddings before feeding them into\n            the encoder.\n            For more details about how to give `enc_prent`, please refer to speechain.module.encoder.asr.ASREncoder.\n        encoder: (mandatory)\n            The configuration of the encoder main body in the `ASREncoder` member.\n            The encoder embeds the hidden embeddings into the encoder representations at each time steps of the\n            input acoustic features.\n            For more details about how to give `encoder`, please refer to speechain.module.encoder.asr.ASREncoder.\n        dec_emb: (mandatory)\n            The configuration of the embedding layer in the `ARASRDecoder` member.\n            The decoder prenet embeds the input token ids into hidden embeddings before feeding them into\n            the decoder.\n            For more details about how to give `dec_emb`, please refer to speechain.module.encoder.asr.ASREncoder.\n        decoder: (mandatory)\n            The configuration of the decoder main body in the `ARASRDecoder` member.\n            The decoder predicts the probability of the next token at each time steps based on the token embeddings.\n            For more details about how to give `decoder`, please refer to speechain.module.decoder.ar_asr.ARASRDecoder.\n        # --- customize_conf arguments --- #\n        token_type: (mandatory)\n            The type of the built-in tokenizer.\n        token_path: (mandatory)\n            The path of the vocabulary for initializing the built-in tokenizer.\n        sample_rate: int = 16000 (optional)\n            The sampling rate of the input speech.\n            Currently, it's used for acoustic feature extraction frontend initialization and tensorboard register of\n            the input speech for model visualization.\n            In the future, this argument will also be used to on-the-fly downsample the input speech.\n        audio_format: (optional)\n            This argument is only used for input speech recording during model visualization.\n        return_att_type: List[str] or str = ['encdec', 'enc', 'dec']\n            The type of attentions you want to return for both attention guidance and attention visualization.\n            It can be given as a string (one type) or a list of strings (multiple types).\n            The type should be one of\n                1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN\n                2. 'enc': the encoder self-attention, only for Transformer\n                3. 'dec': the decoder self-attention, only for Transformer\n        return_att_head_num: int = -1\n            The number of returned attention heads. If -1, all the heads in an attention layer will be returned.\n            RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.\n        return_att_layer_num: int = 1\n            The number of returned attention layers. If -1, all the attention layers will be returned.\n            RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.\n        lm_model_cfg: Dict or str\n            The configuration for the language model used for joint decoding.\n            Can be either a Dict or a string indicating where the .yaml model configuration file is placed.\n        lm_model_path: str\n            The string indicating where the .pth model parameter file is placed.\n\n    \"\"\"\n\n    # --- 1. Module-independent Initialization --- #\n    # initialize the tokenizer\n    if token_type.lower() == \"char\":\n        self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n    elif token_type.lower() == \"sentencepiece\":\n        self.tokenizer = SentencePieceTokenizer(\n            token_path, copy_path=self.result_path\n        )\n    else:\n        raise ValueError(\n            f\"Unknown token_type {token_type}. \"\n            f\"Currently, {self.__class__.__name__} supports one of ['char', 'sentencepiece'].\"\n        )\n\n    # initialize the sampling rate, mainly used for visualizing the input audio during training\n    self.sample_rate = sample_rate\n    self.audio_format = audio_format.lower()\n\n    # attention-related\n    if return_att_type is None:\n        self.return_att_type = [\"encdec\", \"enc\", \"dec\"]\n    else:\n        self.return_att_type = (\n            return_att_type\n            if isinstance(return_att_type, List)\n            else [return_att_type]\n        )\n    for i in range(len(self.return_att_type)):\n        if self.return_att_type[i].lower() in [\"enc\", \"dec\", \"encdec\"]:\n            self.return_att_type[i] = self.return_att_type[i].lower()\n        else:\n            raise ValueError(\n                \"The elements of your input return_att_type must be one of ['enc', 'dec', 'encdec'], \"\n                f\"but got {self.return_att_type[i]}!\"\n            )\n    self.return_att_head_num = return_att_head_num\n    self.return_att_layer_num = return_att_layer_num\n\n    # language model-related, used for lazy initialization during inference\n    self.lm_model_cfg = lm_model_cfg\n    self.lm_model_path = lm_model_path\n\n    # --- 2. Module Initialization --- #\n    # --- 2.1 Encoder construction --- #\n    # the sampling rate will be first initialized\n    if \"sr\" not in frontend[\"conf\"].keys():\n        frontend[\"conf\"][\"sr\"] = self.sample_rate\n    # update the sampling rate into the ASR Model object\n    self.sample_rate = frontend[\"conf\"][\"sr\"]\n    self.encoder = ASREncoder(\n        frontend=frontend,\n        normalize=normalize,\n        specaug=specaug,\n        prenet=enc_prenet,\n        encoder=encoder,\n        distributed=self.distributed,\n    )\n\n    # --- 2.2 CTC layer construction (optional) --- #\n    self.ctc_weight = ctc_weight\n    assert ctc_weight &gt;= 0, \"ctc_weight cannot be lower than 0!\"\n    if ctc_weight &gt; 0:\n        self.ctc_layer = TokenPostnet(\n            input_size=self.encoder.output_size,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n    # --- 2.3 Decoder construction --- #\n    self.ilm_weight = ilm_weight\n    assert ilm_weight &gt;= 0, \"ilm_weight cannot be lower than 0!\"\n    self.ilm_sub_weight = ilm_sub_weight\n    assert ilm_sub_weight &gt;= 0, \"ilm_sub_weight cannot be lower than 0!\"\n    # the vocabulary size is given by the built-in tokenizer instead of the input configuration\n    if \"vocab_size\" in dec_emb[\"conf\"].keys():\n        if dec_emb[\"conf\"][\"vocab_size\"] != self.tokenizer.vocab_size:\n            warnings.warn(\n                f\"Your input vocabulary size is different from the one obtained from the built-in \"\n                f\"tokenizer ({self.tokenizer.vocab_size}). The latter one will be used to initialize the \"\n                f\"decoder for correctness.\"\n            )\n        dec_emb[\"conf\"].pop(\"vocab_size\")\n    self.decoder = ARASRDecoder(\n        vocab_size=self.tokenizer.vocab_size, embedding=dec_emb, decoder=decoder\n    )\n</code></pre>"},{"location":"reference/model/ar_asr/#model.ar_asr.MultiDataLoaderARASR","title":"<code>MultiDataLoaderARASR</code>","text":"<p>               Bases: <code>ARASR</code></p> <p>Auto-Regressive ASR model trained by multiple dataloaders.</p> Source code in <code>speechain/model/ar_asr.py</code> <pre><code>class MultiDataLoaderARASR(ARASR):\n    \"\"\"Auto-Regressive ASR model trained by multiple dataloaders.\"\"\"\n\n    def criterion_init(\n        self,\n        loss_weights: Dict[str, float] = None,\n        ce_loss: Dict = None,\n        ctc_loss: Dict = None,\n        att_guid_loss: Dict = None,\n        **kwargs,\n    ):\n\n        # register the weight for each loss if loss_weights is given\n        if loss_weights is not None:\n            self.loss_weights = dict()\n            for loss_name, weight in loss_weights.items():\n                assert (\n                    isinstance(weight, float) and 0 &lt; weight &lt; 1\n                ), f\"Your input weight should be a float number in (0, 1), but got loss_weights[{loss_name}]={weight}!\"\n                self.loss_weights[loss_name] = weight\n\n        def recur_init_loss_by_dict(loss_dict: Dict, loss_class):\n            leaf_num = sum(\n                [not isinstance(value, Dict) for value in loss_dict.values()]\n            )\n            # all the items in loss_dict are not Dict mean that the loss function is shared by all the dataloaders\n            if leaf_num == len(loss_dict):\n                return loss_class(**loss_dict)\n            # no item in loss_dict is Dict mean that each dataloader has its own loss function\n            else:\n                if hasattr(self, \"loss_weights\"):\n                    assert len(loss_dict) == len(\n                        self.loss_weights\n                    ), \"The key number in the xxx_loss should match the one in the loss_weights\"\n\n                nested_loss = dict()\n                for name, conf in loss_dict.items():\n                    if hasattr(self, \"loss_weights\"):\n                        assert (\n                            name in self.loss_weights.keys()\n                        ), f\"The key name {name} doesn't match anyone in the loss_weights!\"\n                    nested_loss[name] = loss_class(**conf) if conf is not None else None\n                return nested_loss\n\n        # cross-entropy will be initialized no matter whether ce_loss is given or not\n        self.ce_loss = (\n            recur_init_loss_by_dict(ce_loss, CrossEntropy)\n            if ce_loss is not None\n            else CrossEntropy()\n        )\n\n        # only initialize ctc loss if it is given\n        if self.ctc_weight &gt; 0:\n            if not isinstance(ctc_loss, Dict):\n                ctc_loss = {}\n\n            if ctc_loss is not None:\n                for key in ctc_loss.keys():\n                    if isinstance(ctc_loss[key], bool):\n                        ctc_loss[key] = {} if ctc_loss[key] else None\n                    if ctc_loss[key] is not None:\n                        if self.device != \"cpu\" and self.tokenizer.ignore_idx != 0:\n                            raise RuntimeError(\n                                f\"For speeding up CTC calculation by CuDNN, \"\n                                f\"please set the blank id to 0 (got {self.tokenizer.ignore_idx}).\"\n                            )\n                        ctc_loss[key][\"blank\"] = self.tokenizer.ignore_idx\n            self.ctc_loss = recur_init_loss_by_dict(ctc_loss, CTCLoss)\n\n        # only initialize attention-guidance loss if it is given\n        if att_guid_loss is not None:\n            assert (\n                \"encdec\" in self.return_att_type\n            ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n\n            # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n            if not isinstance(att_guid_loss, Dict):\n                assert (\n                    isinstance(att_guid_loss, bool) and att_guid_loss\n                ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n\n            if isinstance(att_guid_loss, Dict):\n                self.att_guid_loss = recur_init_loss_by_dict(\n                    att_guid_loss, AttentionGuidance\n                )\n            # att_guid_loss is True, intialize the default AttentionGuidance criterion\n            else:\n                self.att_guid_loss = AttentionGuidance()\n\n        # initialize teacher-forcing accuracy for validation\n        self.accuracy = Accuracy()\n\n        # initialize error rate (CER &amp; WER) for evaluation\n        self.error_rate = ErrorRate(tokenizer=self.tokenizer)\n\n    def module_forward(self, **batch_data) -&gt; Dict[str, Dict or torch.Tensor]:\n\n        # whether the input batch_data is generated by multiple dataloaders\n        multi_flag = sum(\n            [not isinstance(value, torch.Tensor) for value in batch_data.values()]\n        ) == len(batch_data)\n\n        # Single-dataloader scenario\n        # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n        if not multi_flag:\n            return super(MultiDataLoaderARASR, self).module_forward(**batch_data)\n        # Multi-dataloader scenario\n        # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n        # data-label pairs in a single batch, we need to go through forward function once for each pair.\n        else:\n            # pop the non-Dict arguments from the input batch data\n            general_args, data_keys = dict(), list(batch_data.keys())\n            for key in data_keys:\n                if not isinstance(batch_data[key], Dict):\n                    general_args[key] = batch_data.pop(key)\n\n            # otherwise, go through the normal training process once for all the sub-batches\n            # (each sub-batch corresponds to a dataloader)\n            return {\n                domain: super(MultiDataLoaderARASR, self).module_forward(\n                    domain=domain, **general_args, **domain_data\n                )\n                for domain, domain_data in batch_data.items()\n            }\n\n    def criterion_forward(\n        self, **data_output_dict\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]):\n\n        # whether the input data_output_dict is generated by multiple dataloaders\n        multi_flag = sum(\n            [isinstance(value, Dict) for value in data_output_dict.values()]\n        ) == len(data_output_dict)\n\n        # Single-dataloader scenario\n        # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n        if not multi_flag:\n            return super(MultiDataLoaderARASR, self).criterion_forward(\n                **data_output_dict\n            )\n        # Multi-dataloader scenario\n        # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n        # data-label pairs in a single batch, we need to go through forward function once for each pair.\n        else:\n            losses, metrics, domain_list = dict(), dict(), list(data_output_dict.keys())\n            for domain in domain_list:\n                # initialize the cross-entropy loss function\n                ce_loss_fn = (\n                    self.ce_loss[domain]\n                    if isinstance(self.ce_loss, Dict)\n                    else self.ce_loss\n                )\n                # initialize the ctc loss function only if ctc_loss is created\n                if hasattr(self, \"ctc_loss\"):\n                    ctc_loss_fn = (\n                        self.ctc_loss[domain]\n                        if isinstance(self.ctc_loss, Dict)\n                        else self.ctc_loss\n                    )\n                else:\n                    ctc_loss_fn = None\n                # initialize the attention-guidance loss function only if att_guid_loss is created\n                if hasattr(self, \"att_guid_loss\"):\n                    att_guid_loss_fn = (\n                        self.att_guid_loss[domain]\n                        if isinstance(self.att_guid_loss, Dict)\n                        else self.att_guid_loss\n                    )\n                else:\n                    att_guid_loss_fn = None\n\n                # call the criterion_forward() of the parent class by the initialized loss functions\n                _criteria = super(MultiDataLoaderARASR, self).criterion_forward(\n                    ce_loss_fn=ce_loss_fn,\n                    ctc_loss_fn=ctc_loss_fn,\n                    att_guid_loss_fn=att_guid_loss_fn,\n                    **data_output_dict[domain],\n                )\n\n                # update loss and metric Dicts during training\n                if self.training:\n                    # update the losses and metrics Dicts by the domain name at the beginning\n                    losses.update(\n                        **{\n                            f\"{domain}_{_key}\": _value\n                            for _key, _value in _criteria[0].items()\n                        }\n                    )\n                    metrics.update(\n                        **{\n                            f\"{domain}_{_key}\": _value\n                            for _key, _value in _criteria[1].items()\n                        }\n                    )\n                # only update metric Dict during validation\n                else:\n                    metrics.update(\n                        **{\n                            (\n                                _key if len(domain_list) == 1 else f\"{domain}_{_key}\"\n                            ): _value\n                            for _key, _value in _criteria.items()\n                        }\n                    )\n\n            # calculate the overall weighted loss during training\n            if self.training:\n                # normalize losses of all the domains by the given loss_weights\n                if hasattr(self, \"loss_weights\"):\n                    assert len(self.loss_weights) == len(\n                        domain_list\n                    ), \"There is a number mismatch of the domains between your data_cfg and train_cfg.\"\n                    assert sum(\n                        [domain in self.loss_weights.keys() for domain in domain_list]\n                    ) == len(\n                        domain_list\n                    ), \"There is a name mismatch of the domains between your data_cfg and train_cfg.\"\n                    losses.update(\n                        loss=sum(\n                            [\n                                losses[f\"{domain}_loss\"] * self.loss_weights[domain]\n                                for domain in domain_list\n                            ]\n                        )\n                        / sum([self.loss_weights[domain] for domain in domain_list])\n                    )\n                # average losses of all the domains if loss_weights is not given\n                else:\n                    losses.update(\n                        loss=sum([losses[f\"{domain}_loss\"] for domain in domain_list])\n                        / len(domain_list)\n                    )\n                metrics.update(loss=losses[\"loss\"].clone().detach())\n\n            if self.training:\n                return losses, metrics\n            else:\n                return metrics\n\n    def inference(self, infer_conf: Dict, **test_batch) -&gt; Dict[str, Any]:\n\n        multi_flag = sum(\n            [isinstance(value, Dict) for value in test_batch.values()]\n        ) == len(test_batch)\n        # no sub-Dict means one normal supervised dataloader, go through the inference function of ASR\n        if not multi_flag:\n            return super(MultiDataLoaderARASR, self).inference(\n                infer_conf=infer_conf, **test_batch\n            )\n\n        # sub-Dict means that the domain information is given for ASR inference\n        else:\n            assert (\n                len(test_batch) == 1\n            ), \"If you want to evaluate the ASR model by multiple domains, please evaluate them one by one.\"\n            for domain, domain_batch in test_batch.items():\n                return super(MultiDataLoaderARASR, self).inference(\n                    infer_conf=infer_conf, domain=domain, **domain_batch\n                )\n</code></pre>"},{"location":"reference/model/ar_tts/","title":"ar_tts","text":"<p>Author: Sashi Novitasari Affiliation: NAIST Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS","title":"<code>ARTTS</code>","text":"<p>               Bases: <code>Model</code></p> <p>Auto-Regressive Attention-based Text-To-Speech Synthesis Model.</p> <p>(single-speaker or multi-speaker)</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>class ARTTS(Model):\n    \"\"\"Auto-Regressive Attention-based Text-To-Speech Synthesis Model.\n\n    (single-speaker or multi-speaker)\n    \"\"\"\n\n    def module_init(\n        self,\n        token_type: str,\n        token_path: str,\n        enc_emb: Dict,\n        enc_prenet: Dict,\n        encoder: Dict,\n        dec_prenet: Dict,\n        decoder: Dict,\n        dec_postnet: Dict,\n        frontend: Dict = None,\n        normalize: Dict or bool = True,\n        spk_list: str = None,\n        spk_emb: Dict = None,\n        sample_rate: int = 22050,\n        audio_format: str = \"wav\",\n        reduction_factor: int = 1,\n        stop_pos_weight: float = 5.0,\n        stop_threshold: float = 0.5,\n        return_att_type: List[str] or str = None,\n        return_att_head_num: int = 2,\n        return_att_layer_num: int = 2,\n    ):\n        \"\"\"\n\n        Args:\n            # --- module_conf arguments --- #\n            frontend: Dict (mandatory)\n                The configuration of the acoustic feature extraction frontend in the `ARTTSDecoder` member.\n                This argument must be given since our toolkit doesn't support time-domain TTS.\n                For more details about how to give `frontend`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n            normalize: Dict\n                The configuration of the normalization layer in the `ARTTSDecoder` member.\n                This argument can also be given as a bool value.\n                True means the default configuration and False means no normalization.\n                For more details about how to give `normalize`, please refer to\n                    speechain.module.norm.feat_norm.FeatureNormalization.\n            enc_emb: Dict (mandatory)\n                The configuration of the embedding layer in the `TTSEncoder` member.\n                The encoder prenet embeds the input token id into token embeddings before feeding them into\n                the encoder.\n                For more details about how to give `enc_emb`, please refer to speechain.module.encoder.tts.TTSEncoder.\n            enc_prenet: Dict (mandatory)\n                The configuration of the prenet in the `TTSEncoder` member.\n                The encoder prenet embeds the input token embeddings into high-level embeddings before feeding them into\n                the encoder.\n                For more details about how to give `enc_prent`, please refer to speechain.module.encoder.tts.TTSEncoder.\n            encoder: Dict (mandatory)\n                The configuration of the encoder main body in the `TTSEncoder` member.\n                The encoder embeds the input embeddings into the encoder representations at each time steps of the\n                input acoustic features.\n                For more details about how to give `encoder`, please refer to speechain.module.encoder.tts.TTSEncoder.\n            spk_emb: Dict = None (conditionally mandatory)\n                The configuration for the `SPKEmbedPrenet` in the `ARTTSDecoder` member.\n                For more details about how to give `spk_emb`, please refer to\n                    speechain.module.prenet.spk_embed.SpeakerEmbedPrenet.\n            dec_prenet: Dict (mandatory)\n                The configuration of the prenet in the `ARTTSDecoder` member.\n                For more details about how to give `dec_prenet`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n            decoder: Dict (mandatory)\n                The configuration of the decoder main body in the `ARTTSDecoder` member.\n                For more details about how to give `decoder`, please refer to speechain.module.decoder.ar_tts.ARTTSDecoder.\n            dec_postnet: Dict (mandatory)\n                The configuration of the postnet in the `ARTTSDecoder` member.\n                For more details about how to give `dec_postnet`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n            # --- customize_conf arguments --- #\n            token_type: (mandatory)\n                The type of the built-in tokenizer.\n                Currently, we support 'char' for `CharTokenizer` and 'phn' for `PhonemeTokenizer`.\n            token_path: (mandatory)\n                The path of the vocabulary list `vocab` for initializing the built-in tokenizer.\n            spk_list: str = None (conditionally mandatory)\n                The path of the speaker list that contains all the speaker ids in your training set.\n                If you would like to train a close-set multi-speaker TTS, you need to give a spk_list.\n            sample_rate: int = 22050 (optional)\n                The sampling rate of the target speech.\n                Currently it's used for acoustic feature extraction frontend initialization and tensorboard register of\n                the input speech during model visualization.\n                In the future, this argument will also be used to dynamically downsample the input speech during training.\n            audio_format: str = 'wav' (optional)\n                This argument is only used for input speech recording during model visualization.\n            reduction_factor: int = 1 (mandatory)\n                The factor that controls how much the length of output speech feature is reduced.\n            stop_threshold: float = 0.5 (mandatory)\n                The threshold that controls whether the speech synthesis stops or not.\n            return_att_type: List[str] or str = 'encdec'\n                The type of attentions you want to return for both attention guidance and attention visualization.\n                It can be given as a string (one type) or a list of strings (multiple types).\n                The type should be one of\n                    1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN\n                    2. 'enc': the encoder self-attention, only for Transformer\n                    3. 'dec': the decoder self-attention, only for Transformer\n            return_att_head_num: int = -1\n                The number of returned attention heads. If -1, all the heads in an attention layer will be returned.\n                RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.\n            return_att_layer_num: int = 1\n                The number of returned attention layers. If -1, all the attention layers will be returned.\n                RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.\n\n        \"\"\"\n        # --- 1. Model-Customized Part Initialization --- #\n        # initialize the tokenizer\n        if token_type == \"char\":\n            self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n        elif token_type in [\"g2p\", \"mfa\"]:\n            self.tokenizer = GraphemeToPhonemeTokenizer(\n                token_path, copy_path=self.result_path\n            )\n        else:\n            raise ValueError(\n                f\"Unknown token type {token_type}. \"\n                f\"Currently, {self.__class__.__name__} supports one of ['char', 'g2p'].\"\n            )\n\n        # initialize the speaker list if given\n        if spk_list is not None:\n            spk_list = np.loadtxt(parse_path_args(spk_list), dtype=str)\n            # when the input file is idx2spk, only retain the column of speaker ids\n            if len(spk_list.shape) == 2:\n                assert spk_list.shape[1] == 2\n                spk_list = spk_list[:, 1]\n            # otherwise, the input file must be spk_list which is a single-column file and each row is a speaker id\n            elif len(spk_list.shape) != 1:\n                raise RuntimeError\n            # 1. remove redundant elements; 2. sort up the speaker ids in order\n            spk_list = sorted(set(spk_list))\n            # 3. get the corresponding indices (start from 1 since 0 is reserved for unknown speakers)\n            self.idx2spk = dict(zip(range(1, len(spk_list) + 1), spk_list))\n            # 4. exchange the positions of indices and speaker ids\n            self.spk2idx = dict(map(reversed, self.idx2spk.items()))\n\n        # initialize the sampling rate, mainly used for visualizing the input audio during training\n        self.sample_rate = sample_rate\n        self.audio_format = audio_format.lower()\n        self.reduction_factor = reduction_factor\n        self.stop_pos_weight = stop_pos_weight\n        self.stop_threshold = stop_threshold\n\n        if return_att_type is None:\n            self.return_att_type = [\"encdec\", \"enc\", \"dec\"]\n        else:\n            self.return_att_type = (\n                return_att_type\n                if isinstance(return_att_type, List)\n                else [return_att_type]\n            )\n        for i in range(len(self.return_att_type)):\n            if self.return_att_type[i].lower() in [\"enc\", \"dec\", \"encdec\"]:\n                self.return_att_type[i] = self.return_att_type[i].lower()\n            else:\n                raise ValueError(\n                    \"The elements of your input return_att_type must be one of ['enc', 'dec', 'encdec'], \"\n                    f\"but got {self.return_att_type[i]}!\"\n                )\n        self.return_att_head_num = return_att_head_num\n        self.return_att_layer_num = return_att_layer_num\n\n        # --- 2. Module Part Construction --- #\n        # --- 2.1. Encoder construction --- #\n        # the vocabulary size is given by the built-in tokenizer instead of the input configuration\n        if \"vocab_size\" in enc_emb[\"conf\"].keys():\n            if enc_emb[\"conf\"][\"vocab_size\"] != self.tokenizer.vocab_size:\n                warnings.warn(\n                    f\"Your input vocabulary size is different from the one obtained from the built-in \"\n                    f\"tokenizer ({self.tokenizer.vocab_size}). The latter one will be used to initialize the \"\n                    f\"encoder for correctness.\"\n                )\n            enc_emb[\"conf\"].pop(\"vocab_size\")\n        self.encoder = TTSEncoder(\n            vocab_size=self.tokenizer.vocab_size,\n            embedding=enc_emb,\n            prenet=enc_prenet,\n            encoder=encoder,\n        )\n\n        # --- 2.2. Decoder construction --- #\n        # check the sampling rate of the decoder frontend\n        if \"sr\" not in frontend[\"conf\"].keys():\n            frontend[\"conf\"][\"sr\"] = self.sample_rate\n        # update the sampling rate into the TTS Model object\n        self.sample_rate = frontend[\"conf\"][\"sr\"]\n\n        # check the speaker embedding configuration\n        if spk_emb is not None:\n            # speaker number for the close-set multi-speaker TTS\n            if hasattr(self, \"spk2idx\"):\n                if (\n                    \"spk_num\" in spk_emb.keys()\n                    and spk_emb[\"spk_num\"] != len(self.spk2idx) + 1\n                ):\n                    warnings.warn(\n                        \"Your input spk_num is different from the number of speakers in your given spk_list. \"\n                        f\"Currently, the spk_num is set to {len(self.spk2idx) + 1}.\"\n                    )\n                # all seen speakers plus an unknown speaker (ID: 0)\n                spk_emb[\"spk_num\"], spk_emb[\"use_lookup\"] = len(self.spk2idx) + 1, True\n            elif \"use_lookup\" in spk_emb.keys() and spk_emb[\"use_lookup\"]:\n                raise RuntimeError(\n                    \"Please give spk_list in model['customize_conf'] if you want to use speaker lookup \"\n                    \"table for close-set multi-speaker TTS.\"\n                )\n\n        self.decoder = ARTTSDecoder(\n            spk_emb=spk_emb,\n            frontend=frontend,\n            normalize=normalize,\n            prenet=dec_prenet,\n            decoder=decoder,\n            postnet=dec_postnet,\n            distributed=self.distributed,\n            reduction_factor=self.reduction_factor,\n        )\n\n    @staticmethod\n    def bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n        return [\n            [\"feat_token_len_ratio\", \"max\", 30],\n            [\"feat_token_len_ratio\", \"min\", 30],\n            [\"feat_len\", \"max\", 30],\n            [\"feat_len\", \"min\", 30],\n        ]\n\n    def criterion_init(\n        self, feat_loss: Dict = None, att_guid_loss: Dict or bool = None\n    ):\n        \"\"\"\n        This function initializes all the necessary Criterion members for an autoregressive TTS:\n            1. `speechain.criterion.least_error.LeastError` for acoustic feature prediction loss calculation.\n            2. `speechain.criterion.bce_logits.BCELogits` for stop flag prediction loss calculation.\n            3. `speechain.criterion.accuracy.Accuracy` for teacher-forcing stop flag prediction accuracy calculation.\n            4. `speechain.criterion.fbeta_score.FBetaScore` for teacher-forcing stop flag prediction f-score calculation.\n\n        Args:\n            feat_loss: Dict[str, Any]\n                The arguments for LeastError(). If not given, the default setting of LeastError() will be used.\n                Please refer to speechain.criterion.least_error.LeastError for more details.\n            att_guid_loss: Dict[str, Any] or bool\n                The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized.\n                This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance()\n                will be used.\n                Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.\n\n        \"\"\"\n        # --- Criterion Part Initialization --- #\n        # feature prediction loss\n        if feat_loss is None:\n            feat_loss = {}\n        self.feat_loss = LeastError(**feat_loss)\n\n        # synthesis stop loss\n        self.stop_loss = BCELogits(pos_weight=self.stop_pos_weight)\n\n        if att_guid_loss is not None:\n            # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n            if not isinstance(att_guid_loss, Dict):\n                assert (\n                    isinstance(att_guid_loss, bool) and att_guid_loss\n                ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n                att_guid_loss = {}\n\n            assert (\n                \"encdec\" in self.return_att_type\n            ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n            self.att_guid_loss = AttentionGuidance(**att_guid_loss)\n\n        # validation metrics\n        self.stop_accuracy = Accuracy()\n        self.stop_fbeta = FBetaScore(beta=2)\n\n    def module_forward(\n        self,\n        epoch: int = None,\n        feat: torch.Tensor = None,\n        text: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        return_att: bool = False,\n        **kwargs,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input speech data (grouped or downsampled and edge-padded).\n            feat_len: (batch,)\n                The lengths of input speech data\n            text: (batch, text_maxlen)\n                The input text data with &lt;sos/eos&gt; at the beginning and end\n            text_len: (batch,)\n                The lengths of input text data\n            spk_feat: (batch, 1, speaker embedding dim)\n                Pre-extracted speaker embedding. (None means single-speaker TTS)\n            spk_ids: (batch,)\n                The speaker ids of each speech data. In the form of integer values.\n            epoch: int\n                The number of the current training epoch.\n                Mainly used for mean&amp;std calculation in the feature normalization\n            return_att: bool\n                Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n            kwargs:\n                Temporary register used to store the redundant arguments.\n\n        Returns:\n            A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses\n\n        \"\"\"\n        # para checking\n        assert text is not None and text_len is not None\n        assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n            0\n        ), \"The amounts of utterances and sentences are not equal to each other.\"\n        assert feat_len.size(0) == feat.size(\n            0\n        ), \"The amounts of utterances and their lengths are not equal to each other.\"\n        assert text_len.size(0) == text.size(\n            0\n        ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n        # Encoding, we don't remove the &lt;sos/eos&gt; at the beginning and end of the sentence\n        enc_returns = self.encoder(text=text, text_len=text_len)\n        # Transformer-based encoder additionally returns the encoder self-attention\n        if len(enc_returns) == 4:\n            enc_text, enc_text_mask, enc_attmat, enc_hidden = enc_returns\n        # RNN-based encoder doesn't return any attention\n        elif len(enc_returns) == 3:\n            (enc_text, enc_text_mask, enc_hidden), enc_attmat = enc_returns, None\n        else:\n            raise RuntimeError\n\n        # Decoding\n        dec_returns = self.decoder(\n            enc_text=enc_text,\n            enc_text_mask=enc_text_mask,\n            feat=feat,\n            feat_len=feat_len,\n            spk_feat=spk_feat,\n            spk_ids=spk_ids,\n            epoch=epoch,\n        )\n        # Transformer-based decoder additionally returns the decoder self-attention\n        if len(dec_returns) == 8:\n            (\n                pred_stop,\n                pred_feat_before,\n                pred_feat_after,\n                tgt_feat,\n                tgt_feat_len,\n                dec_attmat,\n                encdec_attmat,\n                dec_hidden,\n            ) = dec_returns\n        # RNN-based decoder only returns the encoder-decoder attention\n        elif len(dec_returns) == 7:\n            (\n                pred_stop,\n                pred_feat_before,\n                pred_feat_after,\n                tgt_feat,\n                tgt_feat_len,\n                encdec_attmat,\n                dec_hidden,\n            ), dec_attmat = (dec_returns, None)\n        else:\n            raise RuntimeError\n\n        # initialize the TTS output to be the decoder predictions\n        outputs = dict(\n            pred_feat_before=pred_feat_before,\n            pred_feat_after=pred_feat_after,\n            pred_stop=pred_stop,\n            tgt_feat=tgt_feat,\n            tgt_feat_len=tgt_feat_len,\n        )\n\n        def shrink_attention(input_att_list):\n            # pick up the target attention layers\n            if (\n                self.return_att_layer_num != -1\n                and len(input_att_list) &gt; self.return_att_layer_num\n            ):\n                input_att_list = input_att_list[-self.return_att_layer_num :]\n            # pick up the target attention heads\n            if (\n                self.return_att_head_num != -1\n                and input_att_list[0].size(1) &gt; self.return_att_head_num\n            ):\n                input_att_list = [\n                    att[:, : self.return_att_head_num] for att in input_att_list\n                ]\n            return input_att_list\n\n        # return the attention results if specified\n        if return_att or hasattr(self, \"att_guid_loss\"):\n            # encoder-decoder attention\n            if \"encdec\" in self.return_att_type:\n                # register the encoder-decoder attention\n                outputs.update(att=dict(encdec=shrink_attention(encdec_attmat)))\n            # encoder self-attention\n            if enc_attmat is not None and \"enc\" in self.return_att_type:\n                outputs[\"att\"].update(enc=shrink_attention(enc_attmat))\n            # decoder self-attention\n            if dec_attmat is not None and \"dec\" in self.return_att_type:\n                outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n        return outputs\n\n    def criterion_forward(\n        self,\n        pred_stop: torch.Tensor,\n        pred_feat_before: torch.Tensor,\n        pred_feat_after: torch.Tensor,\n        tgt_feat: torch.Tensor,\n        tgt_feat_len: torch.Tensor,\n        text_len: torch.Tensor,\n        att: Dict[str, List[torch.Tensor]] = None,\n        feat_loss_fn: LeastError = None,\n        stop_loss_fn: BCELogits = None,\n        att_guid_loss_fn: AttentionGuidance = None,\n        **kwargs,\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            pred_stop: (batch, seq_len, 1)\n                predicted stop probability\n            pred_feat_before: (batch, seq_len, feat_dim * reduction_factor)\n                predicted acoustic feature before postnet residual addition\n            pred_feat_after: (batch, seq_len, feat_dim * reduction_factor)\n                predicted acoustic feature after postnet residual addition\n            tgt_feat: (batch, seq_len, feat_dim)\n                processed acoustic features, length-reduced and edge-padded\n            tgt_feat_len: (batch,)\n            text_len: (batch,)\n            att:\n            feat_loss_fn:\n            stop_loss_fn:\n            att_guid_loss_fn:\n            **kwargs:\n                Unnecessary arguments for criterion calculation.\n\n        \"\"\"\n        # --- Losses Calculation --- #\n        # the external feature loss function has the higher priority\n        if feat_loss_fn is None:\n            feat_loss_fn = self.feat_loss\n        # acoustic feature prediction loss\n        feat_loss_before = feat_loss_fn(\n            pred=pred_feat_before, tgt=tgt_feat, tgt_len=tgt_feat_len\n        )\n        feat_loss_after = feat_loss_fn(\n            pred=pred_feat_after, tgt=tgt_feat, tgt_len=tgt_feat_len\n        )\n\n        # feature prediction stop loss\n        pred_stop = pred_stop.squeeze(-1)\n        tgt_stop = 1.0 - make_mask_from_len(\n            tgt_feat_len - 1,\n            max_len=tgt_feat_len.max().item(),\n            mask_type=torch.float,\n            return_3d=False,\n        )\n        if pred_stop.is_cuda:\n            tgt_stop = tgt_stop.cuda(pred_stop.device)\n        # the external feature loss function has the higher priority\n        if stop_loss_fn is None:\n            stop_loss_fn = self.stop_loss\n        # end-flag prediction\n        stop_loss = stop_loss_fn(pred=pred_stop, tgt=tgt_stop, tgt_len=tgt_feat_len)\n\n        # combine all losses into the final one\n        loss = feat_loss_before + feat_loss_after + stop_loss\n\n        # attention guidance loss\n        if att_guid_loss_fn is not None or hasattr(self, \"att_guid_loss\"):\n            # the external attention guidance loss function has the higher priority\n            if att_guid_loss_fn is None:\n                att_guid_loss_fn = self.att_guid_loss\n\n            # layer_num * (batch, head_num, ...) -&gt; (batch, layer_num * head_num, ...)\n            att_tensor = torch.cat(att[\"encdec\"], dim=1)\n            att_guid_loss = att_guid_loss_fn(att_tensor, tgt_feat_len, text_len)\n            loss += att_guid_loss\n        else:\n            att_guid_loss = None\n\n        # --- Metrics Calculation --- #\n        logits_threshold = -math.log(1 / self.stop_threshold - 1)\n        pred_stop_hard = pred_stop &gt; logits_threshold\n        stop_accuracy = self.stop_accuracy(pred_stop_hard, tgt_stop, tgt_feat_len)\n        stop_fbeta = self.stop_fbeta(pred_stop_hard, tgt_stop, tgt_feat_len)\n\n        losses = dict(loss=loss)\n        # .clone() here prevents the trainable variables from value modification\n        metrics = dict(\n            loss=loss.clone().detach(),\n            feat_loss_before=feat_loss_before.clone().detach(),\n            feat_loss_after=feat_loss_after.clone().detach(),\n            stop_loss=stop_loss.clone().detach(),\n            stop_accuracy=stop_accuracy.detach(),\n        )\n        metrics[f\"stop_f{int(self.stop_fbeta.beta)}\"] = stop_fbeta.detach()\n        if att_guid_loss is not None:\n            metrics[\"att_guid_loss\"] = att_guid_loss.clone().detach()\n\n        if self.training:\n            return losses, metrics\n        else:\n            return metrics\n\n    def visualize(\n        self,\n        epoch: int,\n        sample_index: str,\n        snapshot_interval: int = 1,\n        epoch_records: Dict = None,\n        domain: str = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n    ):\n\n        # visualization inference is default to be done by teacher-forcing\n        if len(self.visual_infer_conf) == 0:\n            self.visual_infer_conf = dict(\n                teacher_forcing=True, return_gl_wav=False, return_feat=True\n            )\n\n        # obtain the inference results\n        infer_results = self.inference(\n            infer_conf=self.visual_infer_conf,\n            return_att=True,\n            feat=feat,\n            feat_len=feat_len,\n            text=text,\n            text_len=text_len,\n            spk_ids=spk_ids,\n            spk_feat=spk_feat,\n        )\n\n        # --- snapshot the objective metrics --- #\n        vis_logs = []\n        # numerical metrics recording\n        materials = dict()\n        for metric in [\"loss\", \"stop_accuracy\", \"stop_f2\"]:\n            # store each target metric into materials\n            if metric not in epoch_records[sample_index].keys():\n                epoch_records[sample_index][metric] = []\n            epoch_records[sample_index][metric].append(\n                infer_results[metric][\"content\"][0]\n            )\n            materials[metric] = epoch_records[sample_index][metric]\n        # save the visualization log\n        vis_logs.append(\n            dict(\n                plot_type=\"curve\",\n                materials=copy.deepcopy(materials),\n                epoch=epoch,\n                xlabel=\"epoch\",\n                x_stride=snapshot_interval,\n                sep_save=False,\n                subfolder_names=sample_index,\n            )\n        )\n\n        # --- snapshot the subjective metrics --- #\n        # record the input audio and real text at the first snapshotting step\n        if epoch // snapshot_interval == 1:\n            # if the audio source is raw/wav\n            if feat.size(-1) == 1:\n                vis_logs.append(\n                    dict(\n                        plot_type=\"audio\",\n                        materials=dict(real_wav=copy.deepcopy(feat[0])),\n                        sample_rate=self.sample_rate,\n                        audio_format=self.audio_format,\n                        subfolder_names=sample_index,\n                    )\n                )\n            # if the audio source is audio feature (mel spectrogram etc)\n            else:\n                vis_logs.append(\n                    dict(\n                        plot_type=\"matrix\",\n                        materials=dict(real_feat=copy.deepcopy(feat[0])),\n                        epoch=epoch,\n                        sep_save=True,\n                        sum_save=False,\n                        data_save=True,\n                        flip_y=True,\n                        subfolder_names=sample_index,\n                    )\n                )\n\n            # snapshot input text\n            vis_logs.append(\n                dict(\n                    materials=dict(\n                        real_text=[\n                            copy.deepcopy(self.tokenizer.tensor2text(text[0][1:-1]))\n                        ]\n                    ),\n                    plot_type=\"text\",\n                    subfolder_names=sample_index,\n                )\n            )\n\n        # snapshot the generated hypothesis acoustic features into a heatmap\n        vis_logs.append(\n            dict(\n                plot_type=\"matrix\",\n                materials=dict(\n                    hypo_feat=infer_results[\"feat\"][\"content\"][0].transpose()\n                ),\n                epoch=epoch,\n                sep_save=False,\n                sum_save=True,\n                data_save=True,\n                flip_y=True,\n                subfolder_names=[sample_index, \"hypo_feat\"],\n            )\n        )\n\n        # hypothesis attention matrix\n        infer_results[\"att\"] = self.attention_reshape(infer_results[\"att\"])\n        self.matrix_snapshot(\n            vis_logs=vis_logs,\n            hypo_attention=copy.deepcopy(infer_results[\"att\"]),\n            subfolder_names=sample_index,\n            epoch=epoch,\n        )\n        return vis_logs\n\n    def inference(\n        self,\n        infer_conf: Dict,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n        spk_feat_ids: List[str] = None,\n        domain: str = None,\n        return_att: bool = False,\n        return_gl_wav: bool = True,\n        return_feat: bool = False,\n        use_dropout: bool = False,\n        use_before: bool = False,\n        teacher_forcing: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n\n        Args:\n            # --- Testing data arguments --- #\n            feat: (batch_size, feat_maxlen, feat_dim)\n                The ground-truth utterance for the input text\n                Used for teacher-forcing decoding and objective evaluation\n            feat_len: (batch_size,)\n                The length of `feat`.\n            text: (batch_size, text_maxlen)\n                The text data to be inferred.\n            text_len: (batch_size,)\n                The length of `text`.\n            spk_ids: (batch_size,)\n                The ID of the reference speaker.\n            spk_feat: (batch_size, spk_feat_dim)\n                The speaker embedding of the reference speaker.\n            spk_feat_ids: List[str] = None\n                The IDs for the input spk_feat. Mainly used to record the reference speaker embedding during inference.\n            # --- General inference arguments --- #\n            domain: str = None\n                This argument indicates which domain the input speech belongs to.\n                It's used to indicate the `TTSDecoder` member how to encode the input speech.\n            return_att: bool = False\n                Whether the attention matrix of the input speech is returned.\n            return_gl_wav: bool = True\n                Whether to convert the generated acoustic features back to GL waveforms.\n            use_dropout: bool = False\n                Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.\n            use_before: bool = False\n                Whether return the acoustic feature not processed by the postnet.\n            teacher_forcing: bool = False\n                Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.\n            # --- TTS decoding arguments --- #\n            infer_conf:\n                The inference configuration given from the `infer_cfg` in your `exp_cfg`.\n                For more details, please refer to speechain.infer_func.tts_decoding.auto_regression.\n\n        \"\"\"\n        assert text is not None and text_len is not None\n\n        # --- 0. Hyperparameter &amp; Model Preparation Stage --- #\n        # in-place replace infer_conf with its copy to protect the original information\n        infer_conf = copy.deepcopy(infer_conf)\n        # The following argumentsin infer_conf has the higher priority and will not be passed to auto_regression()\n        if \"teacher_forcing\" in infer_conf.keys():\n            teacher_forcing = infer_conf.pop(\"teacher_forcing\")\n        if \"use_dropout\" in infer_conf.keys():\n            use_dropout = infer_conf.pop(\"use_dropout\")\n\n        # 'stop_threshold', and 'use_before' are kept as the arguments of auto_regression()\n        # stop_threshold in infer_conf has the higher priority than the built-in one of the model\n        if \"stop_threshold\" not in infer_conf.keys():\n            infer_conf[\"stop_threshold\"] = self.stop_threshold\n        # use_before in infer_conf has the higher priority than the default values\n        if \"use_before\" in infer_conf.keys():\n            use_before = infer_conf[\"use_before\"]\n        else:\n            infer_conf[\"use_before\"] = use_before\n\n        # return_gl_wav in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"return_gl_wav\" in infer_conf.keys():\n            return_gl_wav = infer_conf.pop(\"return_gl_wav\")\n        # return_feat in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"return_feat\" in infer_conf.keys():\n            return_feat = infer_conf.pop(\"return_feat\")\n        assert (\n            return_gl_wav or return_feat\n        ), \"return_gl_wav and return_feat cannot be False at the same time.\"\n\n        # return_sr in infer_conf has the higher priority and will not be passed to self.module_forward()\n        return_sr = None\n        if \"return_sr\" in infer_conf.keys():\n            return_sr = infer_conf.pop(\"return_sr\")\n            assert return_sr &lt; self.sample_rate, (\n                f\"You should input 'return_sr' lower than the one of the model {self.sample_rate}, \"\n                f\"but got return_sr={return_sr}!\"\n            )\n            if not hasattr(self, \"resampler\"):\n                self.resampler = torchaudio.transforms.Resample(\n                    orig_freq=self.sample_rate, new_freq=return_sr\n                )\n                if text.is_cuda:\n                    self.resampler = self.resampler.cuda(text.device)\n\n        hypo_feat, hypo_feat_len, feat_token_len_ratio, hypo_att = (\n            None,\n            None,\n            None,\n            None,\n        )\n\n        # turn the dropout layer in the decoder on for introducing variability to the synthetic utterances\n        if use_dropout:\n            self.decoder.turn_on_dropout()\n\n        # Multi-speaker TTS scenario\n        if hasattr(self.decoder, \"spk_emb\"):\n            batch_size = text.size(0)\n            # close-set multi-speaker TTS\n            if self.decoder.spk_emb.use_lookup:\n                assert hasattr(self, \"idx2spk\")\n                # randomly pick up training speakers as the reference speakers\n                if spk_ids is None:\n                    spk_ids = torch.randint(\n                        low=1,\n                        high=len(self.idx2spk) + 1,\n                        size=(batch_size,),\n                        device=text.device,\n                    )\n            # open-set multi-speaker TTS\n            elif self.decoder.spk_emb.use_pretrain:\n                # use random vectors as the reference speaker embedding if spk_feat is not given\n                if spk_feat is None:\n                    # make sure that the range of random speaker feature is [-1, 1)\n                    spk_feat = (\n                        torch.rand(\n                            (batch_size, self.decoder.spk_emb.spk_emb_dim),\n                            device=text.device,\n                        )\n                        * 2\n                        - 1\n                    )\n                    spk_feat_ids = [\"rand_spk\" for _ in range(batch_size)]\n\n        # --- 1. Acoustic Feature Generation Stage --- #\n        outputs = dict()\n        # --- 1.1. The 1st Pass: TTS Auto-Regressive Decoding --- #\n        if not teacher_forcing:\n            # copy the input data in advance for data safety\n            model_input = copy.deepcopy(dict(text=text, text_len=text_len))\n\n            # Encoding input text\n            enc_text, enc_text_mask, _, _ = self.encoder(**model_input)\n\n            # Generate the synthetic acoustic features auto-regressively\n            infer_results = auto_regression(\n                enc_text=enc_text,\n                enc_text_mask=enc_text_mask,\n                spk_ids=spk_ids,\n                spk_feat=spk_feat,\n                reduction_factor=self.reduction_factor,\n                feat_dim=self.decoder.output_size,\n                decode_one_step=self.decoder,\n                **infer_conf,\n            )\n            hypo_feat = infer_results[\"hypo_feat\"]\n            hypo_feat_len = infer_results[\"hypo_feat_len\"]\n            feat_token_len_ratio = infer_results[\"feat_token_len_ratio\"]\n\n        # --- 1.2. The 2nd Pass: TTS Teacher-Forcing Decoding --- #\n        if teacher_forcing or return_att:\n            infer_results = self.module_forward(\n                feat=feat if teacher_forcing else hypo_feat,\n                feat_len=feat_len if teacher_forcing else hypo_feat_len,\n                text=text,\n                text_len=text_len,\n                spk_feat=spk_feat,\n                spk_ids=spk_ids,\n                return_att=return_att,\n            )\n            # return the attention matrices\n            if return_att:\n                hypo_att = infer_results[\"att\"]\n\n            # update the hypothesis feature-related data in the teacher forcing mode\n            if teacher_forcing:\n                criterion_results = self.criterion_forward(\n                    text_len=text_len, **infer_results\n                )\n                outputs.update(\n                    {\n                        cri_name: dict(format=\"txt\", content=to_cpu(tensor_result))\n                        for cri_name, tensor_result in criterion_results.items()\n                    }\n                )\n                hypo_feat = infer_results[\n                    \"pred_feat_before\" if use_before else \"pred_feat_after\"\n                ]\n                hypo_feat_len = infer_results[\"tgt_feat_len\"]\n                # hypo_feat &amp; hypo_feat_len recovery by reduction_factor\n                if self.reduction_factor &gt; 1:\n                    batch_size, feat_dim = hypo_feat.size(0), hypo_feat.size(-1)\n                    hypo_feat = hypo_feat.reshape(\n                        batch_size,\n                        hypo_feat.size(1) * self.reduction_factor,\n                        feat_dim // self.reduction_factor,\n                    )\n                    hypo_feat_len *= self.reduction_factor\n                # remove the sos at the beginning and eos at the end\n                feat_token_len_ratio = hypo_feat_len / (text_len - 2 + 1e-10)\n\n        # --- 1.3. The 3rd Pass: denormalize the acoustic feature and transformation to waveforms --- #\n        if hasattr(self.decoder, \"normalize\"):\n            hypo_feat = self.decoder.normalize.recover(hypo_feat, group_ids=spk_ids)\n\n        # turn the tensor-like spk_ids (preprocessed by self.spk2idx) into a list\n        if isinstance(spk_ids, torch.Tensor):\n            spk_ids = [\n                self.idx2spk[s_id.item()] if s_id != 0 else \"aver_spk\"\n                for s_id in spk_ids\n            ]\n\n        # convert the acoustic features back to GL waveforms if specified\n        if return_gl_wav:\n            hypo_wav, hypo_wav_len = self.decoder.frontend.recover(\n                hypo_feat, hypo_feat_len\n            )\n            # remove the redundant silence parts at the end of the synthetic waveforms\n            hypo_wav = [\n                (\n                    hypo_wav[i][: hypo_wav_len[i]]\n                    if return_sr is None\n                    else self.resampler(hypo_wav[i][: hypo_wav_len[i]])\n                )\n                for i in range(len(hypo_wav))\n            ]\n            hypo_wav_len = [wav.size(0) for wav in hypo_wav]\n            outputs.update(\n                gl_wav=dict(\n                    format=\"wav\",\n                    sample_rate=self.sample_rate if return_sr is None else return_sr,\n                    group_ids=spk_ids,\n                    content=to_cpu(hypo_wav, tgt=\"numpy\"),\n                ),\n                gl_wav_len=dict(format=\"txt\", content=to_cpu(hypo_wav_len)),\n            )\n\n        # --- 2. Post-processing for the Generated Acoustic Features --- #\n        # return the acoustic features if specified\n        if return_feat:\n            # remove the redundant silence parts at the end of the synthetic frames\n            hypo_feat = [\n                hypo_feat[i][: hypo_feat_len[i]] for i in range(len(hypo_feat))\n            ]\n            outputs.update(\n                # the sampling rate of the acoustic features remain the one of the TTS model\n                feat=dict(\n                    format=\"npz\",\n                    sample_rate=self.sample_rate,\n                    group_ids=spk_ids,\n                    content=to_cpu(hypo_feat, tgt=\"numpy\"),\n                ),\n                feat_len=dict(format=\"txt\", content=to_cpu(hypo_feat_len)),\n            )\n        outputs.update(\n            feat_token_len_ratio=dict(\n                format=\"txt\", content=to_cpu(feat_token_len_ratio)\n            )\n        )\n\n        # record the speaker ID used as the reference\n        if spk_ids is not None:\n            outputs.update(ref_spk=dict(format=\"txt\", content=spk_ids))\n        # record the speaker embedding ID used as the reference\n        if spk_feat_ids is not None:\n            outputs.update(ref_spk_feat=dict(format=\"txt\", content=spk_feat_ids))\n\n        # evaluation reports for all the testing instances\n        instance_report_dict = {}\n        # loop each utterance\n        for i in range(len(text)):\n            if \"Feature-Token Length Ratio\" not in instance_report_dict.keys():\n                instance_report_dict[\"Feature-Token Length Ratio\"] = []\n            instance_report_dict[\"Feature-Token Length Ratio\"].append(\n                f\"{feat_token_len_ratio[i]:.2f}\"\n            )\n\n            if \"Feature Length\" not in instance_report_dict.keys():\n                instance_report_dict[\"Feature Length\"] = []\n            instance_report_dict[\"Feature Length\"].append(f\"{hypo_feat_len[i]:d}\")\n        # register the instance reports for generating instance_reports.md\n        self.register_instance_reports(md_list_dict=instance_report_dict)\n\n        # add the attention matrix into the output Dict, only used for model visualization during training\n        # because it will consume too much time for saving the attention matrices of all testing samples during testing\n        if return_att:\n            outputs.update(att=hypo_att)\n        return outputs\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS.criterion_forward","title":"<code>criterion_forward(pred_stop, pred_feat_before, pred_feat_after, tgt_feat, tgt_feat_len, text_len, att=None, feat_loss_fn=None, stop_loss_fn=None, att_guid_loss_fn=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred_stop</code> <code>Tensor</code> <p>(batch, seq_len, 1) predicted stop probability</p> required <code>pred_feat_before</code> <code>Tensor</code> <p>(batch, seq_len, feat_dim * reduction_factor) predicted acoustic feature before postnet residual addition</p> required <code>pred_feat_after</code> <code>Tensor</code> <p>(batch, seq_len, feat_dim * reduction_factor) predicted acoustic feature after postnet residual addition</p> required <code>tgt_feat</code> <code>Tensor</code> <p>(batch, seq_len, feat_dim) processed acoustic features, length-reduced and edge-padded</p> required <code>tgt_feat_len</code> <code>Tensor</code> <p>(batch,)</p> required <code>text_len</code> <code>Tensor</code> <p>(batch,)</p> required <code>att</code> <code>Dict[str, List[Tensor]]</code> <code>None</code> <code>feat_loss_fn</code> <code>LeastError</code> <code>None</code> <code>stop_loss_fn</code> <code>BCELogits</code> <code>None</code> <code>att_guid_loss_fn</code> <code>AttentionGuidance</code> <code>None</code> <code>**kwargs</code> <p>Unnecessary arguments for criterion calculation.</p> <code>{}</code> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def criterion_forward(\n    self,\n    pred_stop: torch.Tensor,\n    pred_feat_before: torch.Tensor,\n    pred_feat_after: torch.Tensor,\n    tgt_feat: torch.Tensor,\n    tgt_feat_len: torch.Tensor,\n    text_len: torch.Tensor,\n    att: Dict[str, List[torch.Tensor]] = None,\n    feat_loss_fn: LeastError = None,\n    stop_loss_fn: BCELogits = None,\n    att_guid_loss_fn: AttentionGuidance = None,\n    **kwargs,\n) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        pred_stop: (batch, seq_len, 1)\n            predicted stop probability\n        pred_feat_before: (batch, seq_len, feat_dim * reduction_factor)\n            predicted acoustic feature before postnet residual addition\n        pred_feat_after: (batch, seq_len, feat_dim * reduction_factor)\n            predicted acoustic feature after postnet residual addition\n        tgt_feat: (batch, seq_len, feat_dim)\n            processed acoustic features, length-reduced and edge-padded\n        tgt_feat_len: (batch,)\n        text_len: (batch,)\n        att:\n        feat_loss_fn:\n        stop_loss_fn:\n        att_guid_loss_fn:\n        **kwargs:\n            Unnecessary arguments for criterion calculation.\n\n    \"\"\"\n    # --- Losses Calculation --- #\n    # the external feature loss function has the higher priority\n    if feat_loss_fn is None:\n        feat_loss_fn = self.feat_loss\n    # acoustic feature prediction loss\n    feat_loss_before = feat_loss_fn(\n        pred=pred_feat_before, tgt=tgt_feat, tgt_len=tgt_feat_len\n    )\n    feat_loss_after = feat_loss_fn(\n        pred=pred_feat_after, tgt=tgt_feat, tgt_len=tgt_feat_len\n    )\n\n    # feature prediction stop loss\n    pred_stop = pred_stop.squeeze(-1)\n    tgt_stop = 1.0 - make_mask_from_len(\n        tgt_feat_len - 1,\n        max_len=tgt_feat_len.max().item(),\n        mask_type=torch.float,\n        return_3d=False,\n    )\n    if pred_stop.is_cuda:\n        tgt_stop = tgt_stop.cuda(pred_stop.device)\n    # the external feature loss function has the higher priority\n    if stop_loss_fn is None:\n        stop_loss_fn = self.stop_loss\n    # end-flag prediction\n    stop_loss = stop_loss_fn(pred=pred_stop, tgt=tgt_stop, tgt_len=tgt_feat_len)\n\n    # combine all losses into the final one\n    loss = feat_loss_before + feat_loss_after + stop_loss\n\n    # attention guidance loss\n    if att_guid_loss_fn is not None or hasattr(self, \"att_guid_loss\"):\n        # the external attention guidance loss function has the higher priority\n        if att_guid_loss_fn is None:\n            att_guid_loss_fn = self.att_guid_loss\n\n        # layer_num * (batch, head_num, ...) -&gt; (batch, layer_num * head_num, ...)\n        att_tensor = torch.cat(att[\"encdec\"], dim=1)\n        att_guid_loss = att_guid_loss_fn(att_tensor, tgt_feat_len, text_len)\n        loss += att_guid_loss\n    else:\n        att_guid_loss = None\n\n    # --- Metrics Calculation --- #\n    logits_threshold = -math.log(1 / self.stop_threshold - 1)\n    pred_stop_hard = pred_stop &gt; logits_threshold\n    stop_accuracy = self.stop_accuracy(pred_stop_hard, tgt_stop, tgt_feat_len)\n    stop_fbeta = self.stop_fbeta(pred_stop_hard, tgt_stop, tgt_feat_len)\n\n    losses = dict(loss=loss)\n    # .clone() here prevents the trainable variables from value modification\n    metrics = dict(\n        loss=loss.clone().detach(),\n        feat_loss_before=feat_loss_before.clone().detach(),\n        feat_loss_after=feat_loss_after.clone().detach(),\n        stop_loss=stop_loss.clone().detach(),\n        stop_accuracy=stop_accuracy.detach(),\n    )\n    metrics[f\"stop_f{int(self.stop_fbeta.beta)}\"] = stop_fbeta.detach()\n    if att_guid_loss is not None:\n        metrics[\"att_guid_loss\"] = att_guid_loss.clone().detach()\n\n    if self.training:\n        return losses, metrics\n    else:\n        return metrics\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS.criterion_init","title":"<code>criterion_init(feat_loss=None, att_guid_loss=None)</code>","text":"This function initializes all the necessary Criterion members for an autoregressive TTS <ol> <li><code>speechain.criterion.least_error.LeastError</code> for acoustic feature prediction loss calculation.</li> <li><code>speechain.criterion.bce_logits.BCELogits</code> for stop flag prediction loss calculation.</li> <li><code>speechain.criterion.accuracy.Accuracy</code> for teacher-forcing stop flag prediction accuracy calculation.</li> <li><code>speechain.criterion.fbeta_score.FBetaScore</code> for teacher-forcing stop flag prediction f-score calculation.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>feat_loss</code> <code>Dict</code> <p>Dict[str, Any] The arguments for LeastError(). If not given, the default setting of LeastError() will be used. Please refer to speechain.criterion.least_error.LeastError for more details.</p> <code>None</code> <code>att_guid_loss</code> <code>Dict or bool</code> <p>Dict[str, Any] or bool The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized. This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance() will be used. Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.</p> <code>None</code> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def criterion_init(\n    self, feat_loss: Dict = None, att_guid_loss: Dict or bool = None\n):\n    \"\"\"\n    This function initializes all the necessary Criterion members for an autoregressive TTS:\n        1. `speechain.criterion.least_error.LeastError` for acoustic feature prediction loss calculation.\n        2. `speechain.criterion.bce_logits.BCELogits` for stop flag prediction loss calculation.\n        3. `speechain.criterion.accuracy.Accuracy` for teacher-forcing stop flag prediction accuracy calculation.\n        4. `speechain.criterion.fbeta_score.FBetaScore` for teacher-forcing stop flag prediction f-score calculation.\n\n    Args:\n        feat_loss: Dict[str, Any]\n            The arguments for LeastError(). If not given, the default setting of LeastError() will be used.\n            Please refer to speechain.criterion.least_error.LeastError for more details.\n        att_guid_loss: Dict[str, Any] or bool\n            The arguments for AttentionGuidance(). If not given, self.att_guid_loss won't be initialized.\n            This argument can also be set to a bool value 'True'. If True, the default setting of AttentionGuidance()\n            will be used.\n            Please refer to speechain.criterion.att_guid.AttentionGuidance for more details.\n\n    \"\"\"\n    # --- Criterion Part Initialization --- #\n    # feature prediction loss\n    if feat_loss is None:\n        feat_loss = {}\n    self.feat_loss = LeastError(**feat_loss)\n\n    # synthesis stop loss\n    self.stop_loss = BCELogits(pos_weight=self.stop_pos_weight)\n\n    if att_guid_loss is not None:\n        # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n        if not isinstance(att_guid_loss, Dict):\n            assert (\n                isinstance(att_guid_loss, bool) and att_guid_loss\n            ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n            att_guid_loss = {}\n\n        assert (\n            \"encdec\" in self.return_att_type\n        ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n        self.att_guid_loss = AttentionGuidance(**att_guid_loss)\n\n    # validation metrics\n    self.stop_accuracy = Accuracy()\n    self.stop_fbeta = FBetaScore(beta=2)\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS.inference","title":"<code>inference(infer_conf, text=None, text_len=None, feat=None, feat_len=None, spk_ids=None, spk_feat=None, spk_feat_ids=None, domain=None, return_att=False, return_gl_wav=True, return_feat=False, use_dropout=False, use_before=False, teacher_forcing=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch_size, feat_maxlen, feat_dim) The ground-truth utterance for the input text Used for teacher-forcing decoding and objective evaluation</p> <code>None</code> <code>feat_len</code> <code>Tensor</code> <p>(batch_size,) The length of <code>feat</code>.</p> <code>None</code> <code>text</code> <code>Tensor</code> <p>(batch_size, text_maxlen) The text data to be inferred.</p> <code>None</code> <code>text_len</code> <code>Tensor</code> <p>(batch_size,) The length of <code>text</code>.</p> <code>None</code> <code>spk_ids</code> <code>Tensor</code> <p>(batch_size,) The ID of the reference speaker.</p> <code>None</code> <code>spk_feat</code> <code>Tensor</code> <p>(batch_size, spk_feat_dim) The speaker embedding of the reference speaker.</p> <code>None</code> <code>spk_feat_ids</code> <code>List[str]</code> <p>List[str] = None The IDs for the input spk_feat. Mainly used to record the reference speaker embedding during inference.</p> <code>None</code> <code>domain</code> <code>str</code> <p>str = None This argument indicates which domain the input speech belongs to. It's used to indicate the <code>TTSDecoder</code> member how to encode the input speech.</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool = False Whether the attention matrix of the input speech is returned.</p> <code>False</code> <code>return_gl_wav</code> <code>bool</code> <p>bool = True Whether to convert the generated acoustic features back to GL waveforms.</p> <code>True</code> <code>use_dropout</code> <code>bool</code> <p>bool = False Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.</p> <code>False</code> <code>use_before</code> <code>bool</code> <p>bool = False Whether return the acoustic feature not processed by the postnet.</p> <code>False</code> <code>teacher_forcing</code> <code>bool</code> <p>bool = False Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.</p> <code>False</code> <code>infer_conf</code> <code>Dict</code> <p>The inference configuration given from the <code>infer_cfg</code> in your <code>exp_cfg</code>. For more details, please refer to speechain.infer_func.tts_decoding.auto_regression.</p> required Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def inference(\n    self,\n    infer_conf: Dict,\n    text: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    feat: torch.Tensor = None,\n    feat_len: torch.Tensor = None,\n    spk_ids: torch.Tensor = None,\n    spk_feat: torch.Tensor = None,\n    spk_feat_ids: List[str] = None,\n    domain: str = None,\n    return_att: bool = False,\n    return_gl_wav: bool = True,\n    return_feat: bool = False,\n    use_dropout: bool = False,\n    use_before: bool = False,\n    teacher_forcing: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n\n    Args:\n        # --- Testing data arguments --- #\n        feat: (batch_size, feat_maxlen, feat_dim)\n            The ground-truth utterance for the input text\n            Used for teacher-forcing decoding and objective evaluation\n        feat_len: (batch_size,)\n            The length of `feat`.\n        text: (batch_size, text_maxlen)\n            The text data to be inferred.\n        text_len: (batch_size,)\n            The length of `text`.\n        spk_ids: (batch_size,)\n            The ID of the reference speaker.\n        spk_feat: (batch_size, spk_feat_dim)\n            The speaker embedding of the reference speaker.\n        spk_feat_ids: List[str] = None\n            The IDs for the input spk_feat. Mainly used to record the reference speaker embedding during inference.\n        # --- General inference arguments --- #\n        domain: str = None\n            This argument indicates which domain the input speech belongs to.\n            It's used to indicate the `TTSDecoder` member how to encode the input speech.\n        return_att: bool = False\n            Whether the attention matrix of the input speech is returned.\n        return_gl_wav: bool = True\n            Whether to convert the generated acoustic features back to GL waveforms.\n        use_dropout: bool = False\n            Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.\n        use_before: bool = False\n            Whether return the acoustic feature not processed by the postnet.\n        teacher_forcing: bool = False\n            Whether turn on the dropout layers in the prenet of the TTS decoder when decoding.\n        # --- TTS decoding arguments --- #\n        infer_conf:\n            The inference configuration given from the `infer_cfg` in your `exp_cfg`.\n            For more details, please refer to speechain.infer_func.tts_decoding.auto_regression.\n\n    \"\"\"\n    assert text is not None and text_len is not None\n\n    # --- 0. Hyperparameter &amp; Model Preparation Stage --- #\n    # in-place replace infer_conf with its copy to protect the original information\n    infer_conf = copy.deepcopy(infer_conf)\n    # The following argumentsin infer_conf has the higher priority and will not be passed to auto_regression()\n    if \"teacher_forcing\" in infer_conf.keys():\n        teacher_forcing = infer_conf.pop(\"teacher_forcing\")\n    if \"use_dropout\" in infer_conf.keys():\n        use_dropout = infer_conf.pop(\"use_dropout\")\n\n    # 'stop_threshold', and 'use_before' are kept as the arguments of auto_regression()\n    # stop_threshold in infer_conf has the higher priority than the built-in one of the model\n    if \"stop_threshold\" not in infer_conf.keys():\n        infer_conf[\"stop_threshold\"] = self.stop_threshold\n    # use_before in infer_conf has the higher priority than the default values\n    if \"use_before\" in infer_conf.keys():\n        use_before = infer_conf[\"use_before\"]\n    else:\n        infer_conf[\"use_before\"] = use_before\n\n    # return_gl_wav in infer_conf has the higher priority and will not be passed to self.module_forward()\n    if \"return_gl_wav\" in infer_conf.keys():\n        return_gl_wav = infer_conf.pop(\"return_gl_wav\")\n    # return_feat in infer_conf has the higher priority and will not be passed to self.module_forward()\n    if \"return_feat\" in infer_conf.keys():\n        return_feat = infer_conf.pop(\"return_feat\")\n    assert (\n        return_gl_wav or return_feat\n    ), \"return_gl_wav and return_feat cannot be False at the same time.\"\n\n    # return_sr in infer_conf has the higher priority and will not be passed to self.module_forward()\n    return_sr = None\n    if \"return_sr\" in infer_conf.keys():\n        return_sr = infer_conf.pop(\"return_sr\")\n        assert return_sr &lt; self.sample_rate, (\n            f\"You should input 'return_sr' lower than the one of the model {self.sample_rate}, \"\n            f\"but got return_sr={return_sr}!\"\n        )\n        if not hasattr(self, \"resampler\"):\n            self.resampler = torchaudio.transforms.Resample(\n                orig_freq=self.sample_rate, new_freq=return_sr\n            )\n            if text.is_cuda:\n                self.resampler = self.resampler.cuda(text.device)\n\n    hypo_feat, hypo_feat_len, feat_token_len_ratio, hypo_att = (\n        None,\n        None,\n        None,\n        None,\n    )\n\n    # turn the dropout layer in the decoder on for introducing variability to the synthetic utterances\n    if use_dropout:\n        self.decoder.turn_on_dropout()\n\n    # Multi-speaker TTS scenario\n    if hasattr(self.decoder, \"spk_emb\"):\n        batch_size = text.size(0)\n        # close-set multi-speaker TTS\n        if self.decoder.spk_emb.use_lookup:\n            assert hasattr(self, \"idx2spk\")\n            # randomly pick up training speakers as the reference speakers\n            if spk_ids is None:\n                spk_ids = torch.randint(\n                    low=1,\n                    high=len(self.idx2spk) + 1,\n                    size=(batch_size,),\n                    device=text.device,\n                )\n        # open-set multi-speaker TTS\n        elif self.decoder.spk_emb.use_pretrain:\n            # use random vectors as the reference speaker embedding if spk_feat is not given\n            if spk_feat is None:\n                # make sure that the range of random speaker feature is [-1, 1)\n                spk_feat = (\n                    torch.rand(\n                        (batch_size, self.decoder.spk_emb.spk_emb_dim),\n                        device=text.device,\n                    )\n                    * 2\n                    - 1\n                )\n                spk_feat_ids = [\"rand_spk\" for _ in range(batch_size)]\n\n    # --- 1. Acoustic Feature Generation Stage --- #\n    outputs = dict()\n    # --- 1.1. The 1st Pass: TTS Auto-Regressive Decoding --- #\n    if not teacher_forcing:\n        # copy the input data in advance for data safety\n        model_input = copy.deepcopy(dict(text=text, text_len=text_len))\n\n        # Encoding input text\n        enc_text, enc_text_mask, _, _ = self.encoder(**model_input)\n\n        # Generate the synthetic acoustic features auto-regressively\n        infer_results = auto_regression(\n            enc_text=enc_text,\n            enc_text_mask=enc_text_mask,\n            spk_ids=spk_ids,\n            spk_feat=spk_feat,\n            reduction_factor=self.reduction_factor,\n            feat_dim=self.decoder.output_size,\n            decode_one_step=self.decoder,\n            **infer_conf,\n        )\n        hypo_feat = infer_results[\"hypo_feat\"]\n        hypo_feat_len = infer_results[\"hypo_feat_len\"]\n        feat_token_len_ratio = infer_results[\"feat_token_len_ratio\"]\n\n    # --- 1.2. The 2nd Pass: TTS Teacher-Forcing Decoding --- #\n    if teacher_forcing or return_att:\n        infer_results = self.module_forward(\n            feat=feat if teacher_forcing else hypo_feat,\n            feat_len=feat_len if teacher_forcing else hypo_feat_len,\n            text=text,\n            text_len=text_len,\n            spk_feat=spk_feat,\n            spk_ids=spk_ids,\n            return_att=return_att,\n        )\n        # return the attention matrices\n        if return_att:\n            hypo_att = infer_results[\"att\"]\n\n        # update the hypothesis feature-related data in the teacher forcing mode\n        if teacher_forcing:\n            criterion_results = self.criterion_forward(\n                text_len=text_len, **infer_results\n            )\n            outputs.update(\n                {\n                    cri_name: dict(format=\"txt\", content=to_cpu(tensor_result))\n                    for cri_name, tensor_result in criterion_results.items()\n                }\n            )\n            hypo_feat = infer_results[\n                \"pred_feat_before\" if use_before else \"pred_feat_after\"\n            ]\n            hypo_feat_len = infer_results[\"tgt_feat_len\"]\n            # hypo_feat &amp; hypo_feat_len recovery by reduction_factor\n            if self.reduction_factor &gt; 1:\n                batch_size, feat_dim = hypo_feat.size(0), hypo_feat.size(-1)\n                hypo_feat = hypo_feat.reshape(\n                    batch_size,\n                    hypo_feat.size(1) * self.reduction_factor,\n                    feat_dim // self.reduction_factor,\n                )\n                hypo_feat_len *= self.reduction_factor\n            # remove the sos at the beginning and eos at the end\n            feat_token_len_ratio = hypo_feat_len / (text_len - 2 + 1e-10)\n\n    # --- 1.3. The 3rd Pass: denormalize the acoustic feature and transformation to waveforms --- #\n    if hasattr(self.decoder, \"normalize\"):\n        hypo_feat = self.decoder.normalize.recover(hypo_feat, group_ids=spk_ids)\n\n    # turn the tensor-like spk_ids (preprocessed by self.spk2idx) into a list\n    if isinstance(spk_ids, torch.Tensor):\n        spk_ids = [\n            self.idx2spk[s_id.item()] if s_id != 0 else \"aver_spk\"\n            for s_id in spk_ids\n        ]\n\n    # convert the acoustic features back to GL waveforms if specified\n    if return_gl_wav:\n        hypo_wav, hypo_wav_len = self.decoder.frontend.recover(\n            hypo_feat, hypo_feat_len\n        )\n        # remove the redundant silence parts at the end of the synthetic waveforms\n        hypo_wav = [\n            (\n                hypo_wav[i][: hypo_wav_len[i]]\n                if return_sr is None\n                else self.resampler(hypo_wav[i][: hypo_wav_len[i]])\n            )\n            for i in range(len(hypo_wav))\n        ]\n        hypo_wav_len = [wav.size(0) for wav in hypo_wav]\n        outputs.update(\n            gl_wav=dict(\n                format=\"wav\",\n                sample_rate=self.sample_rate if return_sr is None else return_sr,\n                group_ids=spk_ids,\n                content=to_cpu(hypo_wav, tgt=\"numpy\"),\n            ),\n            gl_wav_len=dict(format=\"txt\", content=to_cpu(hypo_wav_len)),\n        )\n\n    # --- 2. Post-processing for the Generated Acoustic Features --- #\n    # return the acoustic features if specified\n    if return_feat:\n        # remove the redundant silence parts at the end of the synthetic frames\n        hypo_feat = [\n            hypo_feat[i][: hypo_feat_len[i]] for i in range(len(hypo_feat))\n        ]\n        outputs.update(\n            # the sampling rate of the acoustic features remain the one of the TTS model\n            feat=dict(\n                format=\"npz\",\n                sample_rate=self.sample_rate,\n                group_ids=spk_ids,\n                content=to_cpu(hypo_feat, tgt=\"numpy\"),\n            ),\n            feat_len=dict(format=\"txt\", content=to_cpu(hypo_feat_len)),\n        )\n    outputs.update(\n        feat_token_len_ratio=dict(\n            format=\"txt\", content=to_cpu(feat_token_len_ratio)\n        )\n    )\n\n    # record the speaker ID used as the reference\n    if spk_ids is not None:\n        outputs.update(ref_spk=dict(format=\"txt\", content=spk_ids))\n    # record the speaker embedding ID used as the reference\n    if spk_feat_ids is not None:\n        outputs.update(ref_spk_feat=dict(format=\"txt\", content=spk_feat_ids))\n\n    # evaluation reports for all the testing instances\n    instance_report_dict = {}\n    # loop each utterance\n    for i in range(len(text)):\n        if \"Feature-Token Length Ratio\" not in instance_report_dict.keys():\n            instance_report_dict[\"Feature-Token Length Ratio\"] = []\n        instance_report_dict[\"Feature-Token Length Ratio\"].append(\n            f\"{feat_token_len_ratio[i]:.2f}\"\n        )\n\n        if \"Feature Length\" not in instance_report_dict.keys():\n            instance_report_dict[\"Feature Length\"] = []\n        instance_report_dict[\"Feature Length\"].append(f\"{hypo_feat_len[i]:d}\")\n    # register the instance reports for generating instance_reports.md\n    self.register_instance_reports(md_list_dict=instance_report_dict)\n\n    # add the attention matrix into the output Dict, only used for model visualization during training\n    # because it will consume too much time for saving the attention matrices of all testing samples during testing\n    if return_att:\n        outputs.update(att=hypo_att)\n    return outputs\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS.module_forward","title":"<code>module_forward(epoch=None, feat=None, text=None, feat_len=None, text_len=None, spk_feat=None, spk_ids=None, return_att=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input speech data (grouped or downsampled and edge-padded).</p> <code>None</code> <code>feat_len</code> <code>Tensor</code> <p>(batch,) The lengths of input speech data</p> <code>None</code> <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The input text data with  at the beginning and end <code>None</code> <code>text_len</code> <code>Tensor</code> <p>(batch,) The lengths of input text data</p> <code>None</code> <code>spk_feat</code> <code>Tensor</code> <p>(batch, 1, speaker embedding dim) Pre-extracted speaker embedding. (None means single-speaker TTS)</p> <code>None</code> <code>spk_ids</code> <code>Tensor</code> <p>(batch,) The speaker ids of each speech data. In the form of integer values.</p> <code>None</code> <code>epoch</code> <code>int</code> <p>int The number of the current training epoch. Mainly used for mean&amp;std calculation in the feature normalization</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool Controls whether the attention matrices of each layer in the encoder and decoder will be returned.</p> <code>False</code> <code>kwargs</code> <p>Temporary register used to store the redundant arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def module_forward(\n    self,\n    epoch: int = None,\n    feat: torch.Tensor = None,\n    text: torch.Tensor = None,\n    feat_len: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    spk_feat: torch.Tensor = None,\n    spk_ids: torch.Tensor = None,\n    return_att: bool = False,\n    **kwargs,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input speech data (grouped or downsampled and edge-padded).\n        feat_len: (batch,)\n            The lengths of input speech data\n        text: (batch, text_maxlen)\n            The input text data with &lt;sos/eos&gt; at the beginning and end\n        text_len: (batch,)\n            The lengths of input text data\n        spk_feat: (batch, 1, speaker embedding dim)\n            Pre-extracted speaker embedding. (None means single-speaker TTS)\n        spk_ids: (batch,)\n            The speaker ids of each speech data. In the form of integer values.\n        epoch: int\n            The number of the current training epoch.\n            Mainly used for mean&amp;std calculation in the feature normalization\n        return_att: bool\n            Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n        kwargs:\n            Temporary register used to store the redundant arguments.\n\n    Returns:\n        A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses\n\n    \"\"\"\n    # para checking\n    assert text is not None and text_len is not None\n    assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n        0\n    ), \"The amounts of utterances and sentences are not equal to each other.\"\n    assert feat_len.size(0) == feat.size(\n        0\n    ), \"The amounts of utterances and their lengths are not equal to each other.\"\n    assert text_len.size(0) == text.size(\n        0\n    ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n    # Encoding, we don't remove the &lt;sos/eos&gt; at the beginning and end of the sentence\n    enc_returns = self.encoder(text=text, text_len=text_len)\n    # Transformer-based encoder additionally returns the encoder self-attention\n    if len(enc_returns) == 4:\n        enc_text, enc_text_mask, enc_attmat, enc_hidden = enc_returns\n    # RNN-based encoder doesn't return any attention\n    elif len(enc_returns) == 3:\n        (enc_text, enc_text_mask, enc_hidden), enc_attmat = enc_returns, None\n    else:\n        raise RuntimeError\n\n    # Decoding\n    dec_returns = self.decoder(\n        enc_text=enc_text,\n        enc_text_mask=enc_text_mask,\n        feat=feat,\n        feat_len=feat_len,\n        spk_feat=spk_feat,\n        spk_ids=spk_ids,\n        epoch=epoch,\n    )\n    # Transformer-based decoder additionally returns the decoder self-attention\n    if len(dec_returns) == 8:\n        (\n            pred_stop,\n            pred_feat_before,\n            pred_feat_after,\n            tgt_feat,\n            tgt_feat_len,\n            dec_attmat,\n            encdec_attmat,\n            dec_hidden,\n        ) = dec_returns\n    # RNN-based decoder only returns the encoder-decoder attention\n    elif len(dec_returns) == 7:\n        (\n            pred_stop,\n            pred_feat_before,\n            pred_feat_after,\n            tgt_feat,\n            tgt_feat_len,\n            encdec_attmat,\n            dec_hidden,\n        ), dec_attmat = (dec_returns, None)\n    else:\n        raise RuntimeError\n\n    # initialize the TTS output to be the decoder predictions\n    outputs = dict(\n        pred_feat_before=pred_feat_before,\n        pred_feat_after=pred_feat_after,\n        pred_stop=pred_stop,\n        tgt_feat=tgt_feat,\n        tgt_feat_len=tgt_feat_len,\n    )\n\n    def shrink_attention(input_att_list):\n        # pick up the target attention layers\n        if (\n            self.return_att_layer_num != -1\n            and len(input_att_list) &gt; self.return_att_layer_num\n        ):\n            input_att_list = input_att_list[-self.return_att_layer_num :]\n        # pick up the target attention heads\n        if (\n            self.return_att_head_num != -1\n            and input_att_list[0].size(1) &gt; self.return_att_head_num\n        ):\n            input_att_list = [\n                att[:, : self.return_att_head_num] for att in input_att_list\n            ]\n        return input_att_list\n\n    # return the attention results if specified\n    if return_att or hasattr(self, \"att_guid_loss\"):\n        # encoder-decoder attention\n        if \"encdec\" in self.return_att_type:\n            # register the encoder-decoder attention\n            outputs.update(att=dict(encdec=shrink_attention(encdec_attmat)))\n        # encoder self-attention\n        if enc_attmat is not None and \"enc\" in self.return_att_type:\n            outputs[\"att\"].update(enc=shrink_attention(enc_attmat))\n        # decoder self-attention\n        if dec_attmat is not None and \"dec\" in self.return_att_type:\n            outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n    return outputs\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.ARTTS.module_init","title":"<code>module_init(token_type, token_path, enc_emb, enc_prenet, encoder, dec_prenet, decoder, dec_postnet, frontend=None, normalize=True, spk_list=None, spk_emb=None, sample_rate=22050, audio_format='wav', reduction_factor=1, stop_pos_weight=5.0, stop_threshold=0.5, return_att_type=None, return_att_head_num=2, return_att_layer_num=2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>frontend</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the acoustic feature extraction frontend in the <code>ARTTSDecoder</code> member. This argument must be given since our toolkit doesn't support time-domain TTS. For more details about how to give <code>frontend</code>, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.</p> <code>None</code> <code>normalize</code> <code>Dict or bool</code> <p>Dict The configuration of the normalization layer in the <code>ARTTSDecoder</code> member. This argument can also be given as a bool value. True means the default configuration and False means no normalization. For more details about how to give <code>normalize</code>, please refer to     speechain.module.norm.feat_norm.FeatureNormalization.</p> <code>True</code> <code>enc_emb</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the embedding layer in the <code>TTSEncoder</code> member. The encoder prenet embeds the input token id into token embeddings before feeding them into the encoder. For more details about how to give <code>enc_emb</code>, please refer to speechain.module.encoder.tts.TTSEncoder.</p> required <code>enc_prenet</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the prenet in the <code>TTSEncoder</code> member. The encoder prenet embeds the input token embeddings into high-level embeddings before feeding them into the encoder. For more details about how to give <code>enc_prent</code>, please refer to speechain.module.encoder.tts.TTSEncoder.</p> required <code>encoder</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the encoder main body in the <code>TTSEncoder</code> member. The encoder embeds the input embeddings into the encoder representations at each time steps of the input acoustic features. For more details about how to give <code>encoder</code>, please refer to speechain.module.encoder.tts.TTSEncoder.</p> required <code>spk_emb</code> <code>Dict</code> <p>Dict = None (conditionally mandatory) The configuration for the <code>SPKEmbedPrenet</code> in the <code>ARTTSDecoder</code> member. For more details about how to give <code>spk_emb</code>, please refer to     speechain.module.prenet.spk_embed.SpeakerEmbedPrenet.</p> <code>None</code> <code>dec_prenet</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the prenet in the <code>ARTTSDecoder</code> member. For more details about how to give <code>dec_prenet</code>, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.</p> required <code>decoder</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the decoder main body in the <code>ARTTSDecoder</code> member. For more details about how to give <code>decoder</code>, please refer to speechain.module.decoder.ar_tts.ARTTSDecoder.</p> required <code>dec_postnet</code> <code>Dict</code> <p>Dict (mandatory) The configuration of the postnet in the <code>ARTTSDecoder</code> member. For more details about how to give <code>dec_postnet</code>, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.</p> required <code>token_type</code> <code>str</code> <p>(mandatory) The type of the built-in tokenizer. Currently, we support 'char' for <code>CharTokenizer</code> and 'phn' for <code>PhonemeTokenizer</code>.</p> required <code>token_path</code> <code>str</code> <p>(mandatory) The path of the vocabulary list <code>vocab</code> for initializing the built-in tokenizer.</p> required <code>spk_list</code> <code>str</code> <p>str = None (conditionally mandatory) The path of the speaker list that contains all the speaker ids in your training set. If you would like to train a close-set multi-speaker TTS, you need to give a spk_list.</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>int = 22050 (optional) The sampling rate of the target speech. Currently it's used for acoustic feature extraction frontend initialization and tensorboard register of the input speech during model visualization. In the future, this argument will also be used to dynamically downsample the input speech during training.</p> <code>22050</code> <code>audio_format</code> <code>str</code> <p>str = 'wav' (optional) This argument is only used for input speech recording during model visualization.</p> <code>'wav'</code> <code>reduction_factor</code> <code>int</code> <p>int = 1 (mandatory) The factor that controls how much the length of output speech feature is reduced.</p> <code>1</code> <code>stop_threshold</code> <code>float</code> <p>float = 0.5 (mandatory) The threshold that controls whether the speech synthesis stops or not.</p> <code>0.5</code> <code>return_att_type</code> <code>List[str] or str</code> <p>List[str] or str = 'encdec' The type of attentions you want to return for both attention guidance and attention visualization. It can be given as a string (one type) or a list of strings (multiple types). The type should be one of     1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN     2. 'enc': the encoder self-attention, only for Transformer     3. 'dec': the decoder self-attention, only for Transformer</p> <code>None</code> <code>return_att_head_num</code> <code>int</code> <p>int = -1 The number of returned attention heads. If -1, all the heads in an attention layer will be returned. RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.</p> <code>2</code> <code>return_att_layer_num</code> <code>int</code> <p>int = 1 The number of returned attention layers. If -1, all the attention layers will be returned. RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.</p> <code>2</code> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def module_init(\n    self,\n    token_type: str,\n    token_path: str,\n    enc_emb: Dict,\n    enc_prenet: Dict,\n    encoder: Dict,\n    dec_prenet: Dict,\n    decoder: Dict,\n    dec_postnet: Dict,\n    frontend: Dict = None,\n    normalize: Dict or bool = True,\n    spk_list: str = None,\n    spk_emb: Dict = None,\n    sample_rate: int = 22050,\n    audio_format: str = \"wav\",\n    reduction_factor: int = 1,\n    stop_pos_weight: float = 5.0,\n    stop_threshold: float = 0.5,\n    return_att_type: List[str] or str = None,\n    return_att_head_num: int = 2,\n    return_att_layer_num: int = 2,\n):\n    \"\"\"\n\n    Args:\n        # --- module_conf arguments --- #\n        frontend: Dict (mandatory)\n            The configuration of the acoustic feature extraction frontend in the `ARTTSDecoder` member.\n            This argument must be given since our toolkit doesn't support time-domain TTS.\n            For more details about how to give `frontend`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n        normalize: Dict\n            The configuration of the normalization layer in the `ARTTSDecoder` member.\n            This argument can also be given as a bool value.\n            True means the default configuration and False means no normalization.\n            For more details about how to give `normalize`, please refer to\n                speechain.module.norm.feat_norm.FeatureNormalization.\n        enc_emb: Dict (mandatory)\n            The configuration of the embedding layer in the `TTSEncoder` member.\n            The encoder prenet embeds the input token id into token embeddings before feeding them into\n            the encoder.\n            For more details about how to give `enc_emb`, please refer to speechain.module.encoder.tts.TTSEncoder.\n        enc_prenet: Dict (mandatory)\n            The configuration of the prenet in the `TTSEncoder` member.\n            The encoder prenet embeds the input token embeddings into high-level embeddings before feeding them into\n            the encoder.\n            For more details about how to give `enc_prent`, please refer to speechain.module.encoder.tts.TTSEncoder.\n        encoder: Dict (mandatory)\n            The configuration of the encoder main body in the `TTSEncoder` member.\n            The encoder embeds the input embeddings into the encoder representations at each time steps of the\n            input acoustic features.\n            For more details about how to give `encoder`, please refer to speechain.module.encoder.tts.TTSEncoder.\n        spk_emb: Dict = None (conditionally mandatory)\n            The configuration for the `SPKEmbedPrenet` in the `ARTTSDecoder` member.\n            For more details about how to give `spk_emb`, please refer to\n                speechain.module.prenet.spk_embed.SpeakerEmbedPrenet.\n        dec_prenet: Dict (mandatory)\n            The configuration of the prenet in the `ARTTSDecoder` member.\n            For more details about how to give `dec_prenet`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n        decoder: Dict (mandatory)\n            The configuration of the decoder main body in the `ARTTSDecoder` member.\n            For more details about how to give `decoder`, please refer to speechain.module.decoder.ar_tts.ARTTSDecoder.\n        dec_postnet: Dict (mandatory)\n            The configuration of the postnet in the `ARTTSDecoder` member.\n            For more details about how to give `dec_postnet`, please refer to speechain.module.encoder.ar_tts.ARTTSDecoder.\n        # --- customize_conf arguments --- #\n        token_type: (mandatory)\n            The type of the built-in tokenizer.\n            Currently, we support 'char' for `CharTokenizer` and 'phn' for `PhonemeTokenizer`.\n        token_path: (mandatory)\n            The path of the vocabulary list `vocab` for initializing the built-in tokenizer.\n        spk_list: str = None (conditionally mandatory)\n            The path of the speaker list that contains all the speaker ids in your training set.\n            If you would like to train a close-set multi-speaker TTS, you need to give a spk_list.\n        sample_rate: int = 22050 (optional)\n            The sampling rate of the target speech.\n            Currently it's used for acoustic feature extraction frontend initialization and tensorboard register of\n            the input speech during model visualization.\n            In the future, this argument will also be used to dynamically downsample the input speech during training.\n        audio_format: str = 'wav' (optional)\n            This argument is only used for input speech recording during model visualization.\n        reduction_factor: int = 1 (mandatory)\n            The factor that controls how much the length of output speech feature is reduced.\n        stop_threshold: float = 0.5 (mandatory)\n            The threshold that controls whether the speech synthesis stops or not.\n        return_att_type: List[str] or str = 'encdec'\n            The type of attentions you want to return for both attention guidance and attention visualization.\n            It can be given as a string (one type) or a list of strings (multiple types).\n            The type should be one of\n                1. 'encdec': the encoder-decoder attention, shared by both Transformer and RNN\n                2. 'enc': the encoder self-attention, only for Transformer\n                3. 'dec': the decoder self-attention, only for Transformer\n        return_att_head_num: int = -1\n            The number of returned attention heads. If -1, all the heads in an attention layer will be returned.\n            RNN can be considered to one-head attention, so return_att_head_num &gt; 1 is equivalent to 1 for RNN.\n        return_att_layer_num: int = 1\n            The number of returned attention layers. If -1, all the attention layers will be returned.\n            RNN can be considered to one-layer attention, so return_att_layer_num &gt; 1 is equivalent to 1 for RNN.\n\n    \"\"\"\n    # --- 1. Model-Customized Part Initialization --- #\n    # initialize the tokenizer\n    if token_type == \"char\":\n        self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n    elif token_type in [\"g2p\", \"mfa\"]:\n        self.tokenizer = GraphemeToPhonemeTokenizer(\n            token_path, copy_path=self.result_path\n        )\n    else:\n        raise ValueError(\n            f\"Unknown token type {token_type}. \"\n            f\"Currently, {self.__class__.__name__} supports one of ['char', 'g2p'].\"\n        )\n\n    # initialize the speaker list if given\n    if spk_list is not None:\n        spk_list = np.loadtxt(parse_path_args(spk_list), dtype=str)\n        # when the input file is idx2spk, only retain the column of speaker ids\n        if len(spk_list.shape) == 2:\n            assert spk_list.shape[1] == 2\n            spk_list = spk_list[:, 1]\n        # otherwise, the input file must be spk_list which is a single-column file and each row is a speaker id\n        elif len(spk_list.shape) != 1:\n            raise RuntimeError\n        # 1. remove redundant elements; 2. sort up the speaker ids in order\n        spk_list = sorted(set(spk_list))\n        # 3. get the corresponding indices (start from 1 since 0 is reserved for unknown speakers)\n        self.idx2spk = dict(zip(range(1, len(spk_list) + 1), spk_list))\n        # 4. exchange the positions of indices and speaker ids\n        self.spk2idx = dict(map(reversed, self.idx2spk.items()))\n\n    # initialize the sampling rate, mainly used for visualizing the input audio during training\n    self.sample_rate = sample_rate\n    self.audio_format = audio_format.lower()\n    self.reduction_factor = reduction_factor\n    self.stop_pos_weight = stop_pos_weight\n    self.stop_threshold = stop_threshold\n\n    if return_att_type is None:\n        self.return_att_type = [\"encdec\", \"enc\", \"dec\"]\n    else:\n        self.return_att_type = (\n            return_att_type\n            if isinstance(return_att_type, List)\n            else [return_att_type]\n        )\n    for i in range(len(self.return_att_type)):\n        if self.return_att_type[i].lower() in [\"enc\", \"dec\", \"encdec\"]:\n            self.return_att_type[i] = self.return_att_type[i].lower()\n        else:\n            raise ValueError(\n                \"The elements of your input return_att_type must be one of ['enc', 'dec', 'encdec'], \"\n                f\"but got {self.return_att_type[i]}!\"\n            )\n    self.return_att_head_num = return_att_head_num\n    self.return_att_layer_num = return_att_layer_num\n\n    # --- 2. Module Part Construction --- #\n    # --- 2.1. Encoder construction --- #\n    # the vocabulary size is given by the built-in tokenizer instead of the input configuration\n    if \"vocab_size\" in enc_emb[\"conf\"].keys():\n        if enc_emb[\"conf\"][\"vocab_size\"] != self.tokenizer.vocab_size:\n            warnings.warn(\n                f\"Your input vocabulary size is different from the one obtained from the built-in \"\n                f\"tokenizer ({self.tokenizer.vocab_size}). The latter one will be used to initialize the \"\n                f\"encoder for correctness.\"\n            )\n        enc_emb[\"conf\"].pop(\"vocab_size\")\n    self.encoder = TTSEncoder(\n        vocab_size=self.tokenizer.vocab_size,\n        embedding=enc_emb,\n        prenet=enc_prenet,\n        encoder=encoder,\n    )\n\n    # --- 2.2. Decoder construction --- #\n    # check the sampling rate of the decoder frontend\n    if \"sr\" not in frontend[\"conf\"].keys():\n        frontend[\"conf\"][\"sr\"] = self.sample_rate\n    # update the sampling rate into the TTS Model object\n    self.sample_rate = frontend[\"conf\"][\"sr\"]\n\n    # check the speaker embedding configuration\n    if spk_emb is not None:\n        # speaker number for the close-set multi-speaker TTS\n        if hasattr(self, \"spk2idx\"):\n            if (\n                \"spk_num\" in spk_emb.keys()\n                and spk_emb[\"spk_num\"] != len(self.spk2idx) + 1\n            ):\n                warnings.warn(\n                    \"Your input spk_num is different from the number of speakers in your given spk_list. \"\n                    f\"Currently, the spk_num is set to {len(self.spk2idx) + 1}.\"\n                )\n            # all seen speakers plus an unknown speaker (ID: 0)\n            spk_emb[\"spk_num\"], spk_emb[\"use_lookup\"] = len(self.spk2idx) + 1, True\n        elif \"use_lookup\" in spk_emb.keys() and spk_emb[\"use_lookup\"]:\n            raise RuntimeError(\n                \"Please give spk_list in model['customize_conf'] if you want to use speaker lookup \"\n                \"table for close-set multi-speaker TTS.\"\n            )\n\n    self.decoder = ARTTSDecoder(\n        spk_emb=spk_emb,\n        frontend=frontend,\n        normalize=normalize,\n        prenet=dec_prenet,\n        decoder=decoder,\n        postnet=dec_postnet,\n        distributed=self.distributed,\n        reduction_factor=self.reduction_factor,\n    )\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.MultiDomainARTTS","title":"<code>MultiDomainARTTS</code>","text":"<p>               Bases: <code>ARTTS</code></p> <p>Auto-Regressive TTS model trained by multiple dataloaders on different domains.</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>class MultiDomainARTTS(ARTTS):\n    \"\"\"Auto-Regressive TTS model trained by multiple dataloaders on different\n    domains.\"\"\"\n\n    def criterion_init(\n        self,\n        loss_weights: Dict[str, float] = None,\n        feat_loss: Dict = None,\n        stop_loss: Dict = None,\n        att_guid_loss: Dict = None,\n        **kwargs,\n    ):\n\n        # register the weight for each loss if loss_weights is given\n        if loss_weights is not None:\n            self.loss_weights = dict()\n            for loss_name, weight in loss_weights.items():\n                assert (\n                    isinstance(weight, float) and 0 &lt; weight &lt; 1\n                ), f\"Your input weight should be a float number in (0, 1), but got loss_weights[{loss_name}]={weight}!\"\n                self.loss_weights[loss_name] = weight\n\n        def recur_init_loss_by_dict(loss_dict: Dict, loss_class):\n            leaf_num = sum(\n                [not isinstance(value, Dict) for value in loss_dict.values()]\n            )\n            # all the items in loss_dict are not Dict mean that the loss function is shared by all the dataloaders\n            if leaf_num == len(loss_dict):\n                return loss_class(**loss_dict)\n            # no item in loss_dict is Dict mean that each dataloader has its own loss function\n            elif leaf_num == 0:\n                if hasattr(self, \"loss_weights\"):\n                    assert len(loss_dict) == len(\n                        self.loss_weights\n                    ), \"The key number in the xxx_loss should match the one in the loss_weights\"\n\n                nested_loss = dict()\n                for name, conf in loss_dict.items():\n                    if hasattr(self, \"loss_weights\"):\n                        assert (\n                            name in self.loss_weights.keys()\n                        ), f\"The key name {name} doesn't match anyone in the loss_weights!\"\n                    nested_loss[name] = loss_class(**conf)\n                return nested_loss\n            else:\n                raise RuntimeError(\n                    \"Your loss configuration must be either Dict[str, Any] or Dict[str, Dict[str, Any]]\"\n                )\n\n        # feature loss will be initialized no matter whether feat_loss is given or not\n        self.feat_loss = (\n            recur_init_loss_by_dict(feat_loss, LeastError)\n            if feat_loss is not None\n            else LeastError()\n        )\n\n        # stop loss will be initialized no matter whether stop_loss is given or not\n        self.stop_loss = (\n            recur_init_loss_by_dict(stop_loss, BCELogits)\n            if stop_loss is not None\n            else BCELogits()\n        )\n\n        # only initialize attention-guidance loss if it is given\n        if att_guid_loss is not None:\n            assert (\n                \"encdec\" in self.return_att_type\n            ), \"If you want to enable attention guidance for ASR training, please include 'encdec' in return_att_type.\"\n\n            # if att_guid_loss is given as True, the default arguments of AttentionGuidance will be used\n            if not isinstance(att_guid_loss, Dict):\n                assert (\n                    isinstance(att_guid_loss, bool) and att_guid_loss\n                ), \"If you want to use the default setting of AttentionGuidance, please give att_guid_loss as True.\"\n\n            if isinstance(att_guid_loss, Dict):\n                self.att_guid_loss = recur_init_loss_by_dict(\n                    att_guid_loss, AttentionGuidance\n                )\n            # att_guid_loss is True, intialize the default AttentionGuidance criterion\n            else:\n                self.att_guid_loss = AttentionGuidance()\n\n        # validation metrics\n        self.stop_accuracy = Accuracy()\n        self.stop_fbeta = FBetaScore(beta=2)\n\n    def module_forward(self, **batch_data) -&gt; Dict[str, Dict or torch.Tensor]:\n        \"\"\"\n\n        Args:\n            **batch_data:\n\n        Returns:\n\n        \"\"\"\n        # whether the input batch_data is generated by multiple dataloaders\n        multi_flag = sum(\n            [not isinstance(value, torch.Tensor) for value in batch_data.values()]\n        ) == len(batch_data)\n\n        # Single-dataloader scenario\n        # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n        if not multi_flag:\n            return super(MultiDomainARTTS, self).module_forward(**batch_data)\n        # Multi-dataloader scenario\n        # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n        # data-label pairs in a single batch, we need to go through forward function once for each pair.\n        else:\n            # pop the non-Dict arguments from the input batch data\n            general_args, data_keys = dict(), list(batch_data.keys())\n            for key in data_keys:\n                if not isinstance(batch_data[key], Dict):\n                    general_args[key] = batch_data.pop(key)\n\n            # otherwise, go through the normal training process once for all the sub-batches\n            # (each sub-batch corresponds to a dataloader)\n            return {\n                domain: super(MultiDomainARTTS, self).module_forward(\n                    domain=domain, **general_args, **domain_data\n                )\n                for domain, domain_data in batch_data.items()\n            }\n\n    def criterion_forward(\n        self, **data_output_dict\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]):\n        \"\"\"\n\n        Args:\n            **data_output_dict:\n\n        Returns:\n\n        \"\"\"\n        # whether the input data_output_dict is generated by multiple dataloaders\n        multi_flag = sum(\n            [isinstance(value, Dict) for value in data_output_dict.values()]\n        ) == len(data_output_dict)\n\n        # Single-dataloader scenario\n        # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n        if not multi_flag:\n            return super(MultiDomainARTTS, self).criterion_forward(**data_output_dict)\n        # Multi-dataloader scenario\n        # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n        # data-label pairs in a single batch, we need to go through forward function once for each pair.\n        else:\n            losses, metrics, domain_list = dict(), dict(), list(data_output_dict.keys())\n            for domain in domain_list:\n                # initialize the feature loss function\n                feat_loss_fn = (\n                    self.feat_loss[domain]\n                    if isinstance(self.feat_loss, Dict)\n                    else self.feat_loss\n                )\n                # initialize the stop loss function\n                stop_loss_fn = (\n                    self.stop_loss[domain]\n                    if isinstance(self.stop_loss, Dict)\n                    else self.stop_loss\n                )\n                # initialize the attention-guidance loss function only if att_guid_loss is created\n                if hasattr(self, \"att_guid_loss\"):\n                    att_guid_loss_fn = (\n                        self.att_guid_loss[domain]\n                        if isinstance(self.att_guid_loss, Dict)\n                        else self.att_guid_loss\n                    )\n                else:\n                    att_guid_loss_fn = None\n\n                # call the criterion_forward() of the parent class by the initialized loss functions\n                _criteria = super(MultiDomainARTTS, self).criterion_forward(\n                    feat_loss_fn=feat_loss_fn,\n                    stop_loss_fn=stop_loss_fn,\n                    att_guid_loss_fn=att_guid_loss_fn,\n                    **data_output_dict[domain],\n                )\n\n                # update loss and metric Dicts during training\n                if self.training:\n                    # update the losses and metrics Dicts by the domain name at the beginning\n                    losses.update(\n                        **{\n                            f\"{domain}_{_key}\": _value\n                            for _key, _value in _criteria[0].items()\n                        }\n                    )\n                    metrics.update(\n                        **{\n                            f\"{domain}_{_key}\": _value\n                            for _key, _value in _criteria[1].items()\n                        }\n                    )\n                # only update metric Dict during validation\n                else:\n                    metrics.update(\n                        **{\n                            (\n                                _key if len(domain_list) == 1 else f\"{domain}_{_key}\"\n                            ): _value\n                            for _key, _value in _criteria.items()\n                        }\n                    )\n\n            # calculate the overall weighted loss during training\n            if self.training:\n                # normalize losses of all the domains by the given loss_weights\n                if hasattr(self, \"loss_weights\"):\n                    assert len(self.loss_weights) == len(\n                        domain_list\n                    ), \"There is a number mismatch of the domains between your data_cfg and train_cfg.\"\n                    assert sum(\n                        [domain in self.loss_weights.keys() for domain in domain_list]\n                    ) == len(\n                        domain_list\n                    ), \"There is a name mismatch of the domains between your data_cfg and train_cfg.\"\n                    losses.update(\n                        loss=sum(\n                            [\n                                losses[f\"{domain}_loss\"] * self.loss_weights[domain]\n                                for domain in domain_list\n                            ]\n                        )\n                        / sum([self.loss_weights[domain] for domain in domain_list])\n                    )\n                # average losses of all the domains if loss_weights is not given\n                else:\n                    losses.update(\n                        loss=sum([losses[f\"{domain}_loss\"] for domain in domain_list])\n                        / len(domain_list)\n                    )\n                metrics.update(loss=losses[\"loss\"].clone().detach())\n\n            if self.training:\n                return losses, metrics\n            else:\n                return metrics\n\n    def inference(self, infer_conf: Dict, **test_batch) -&gt; Dict[str, Any]:\n        \"\"\"\n\n        Args:\n            infer_conf:\n            **test_batch:\n\n        Returns:\n\n        \"\"\"\n        multi_flag = sum(\n            [isinstance(value, Dict) for value in test_batch.values()]\n        ) == len(test_batch)\n        # no sub-Dict means one normal supervised dataloader, go through the inference function of ASR\n        if not multi_flag:\n            return super(MultiDomainARTTS, self).inference(\n                infer_conf=infer_conf, **test_batch\n            )\n\n        # sub-Dict means that the domain information is given for ASR inference\n        else:\n            assert (\n                len(test_batch) == 1\n            ), \"If you want to evaluate the ASR model by multiple domains, please evaluate them one by one.\"\n            for domain, domain_batch in test_batch.items():\n                return super(MultiDomainARTTS, self).inference(\n                    infer_conf=infer_conf, domain=domain, **domain_batch\n                )\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.MultiDomainARTTS.criterion_forward","title":"<code>criterion_forward(**data_output_dict)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**data_output_dict</code> <code>{}</code> <p>Returns:</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def criterion_forward(\n    self, **data_output_dict\n) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]):\n    \"\"\"\n\n    Args:\n        **data_output_dict:\n\n    Returns:\n\n    \"\"\"\n    # whether the input data_output_dict is generated by multiple dataloaders\n    multi_flag = sum(\n        [isinstance(value, Dict) for value in data_output_dict.values()]\n    ) == len(data_output_dict)\n\n    # Single-dataloader scenario\n    # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n    if not multi_flag:\n        return super(MultiDomainARTTS, self).criterion_forward(**data_output_dict)\n    # Multi-dataloader scenario\n    # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n    # data-label pairs in a single batch, we need to go through forward function once for each pair.\n    else:\n        losses, metrics, domain_list = dict(), dict(), list(data_output_dict.keys())\n        for domain in domain_list:\n            # initialize the feature loss function\n            feat_loss_fn = (\n                self.feat_loss[domain]\n                if isinstance(self.feat_loss, Dict)\n                else self.feat_loss\n            )\n            # initialize the stop loss function\n            stop_loss_fn = (\n                self.stop_loss[domain]\n                if isinstance(self.stop_loss, Dict)\n                else self.stop_loss\n            )\n            # initialize the attention-guidance loss function only if att_guid_loss is created\n            if hasattr(self, \"att_guid_loss\"):\n                att_guid_loss_fn = (\n                    self.att_guid_loss[domain]\n                    if isinstance(self.att_guid_loss, Dict)\n                    else self.att_guid_loss\n                )\n            else:\n                att_guid_loss_fn = None\n\n            # call the criterion_forward() of the parent class by the initialized loss functions\n            _criteria = super(MultiDomainARTTS, self).criterion_forward(\n                feat_loss_fn=feat_loss_fn,\n                stop_loss_fn=stop_loss_fn,\n                att_guid_loss_fn=att_guid_loss_fn,\n                **data_output_dict[domain],\n            )\n\n            # update loss and metric Dicts during training\n            if self.training:\n                # update the losses and metrics Dicts by the domain name at the beginning\n                losses.update(\n                    **{\n                        f\"{domain}_{_key}\": _value\n                        for _key, _value in _criteria[0].items()\n                    }\n                )\n                metrics.update(\n                    **{\n                        f\"{domain}_{_key}\": _value\n                        for _key, _value in _criteria[1].items()\n                    }\n                )\n            # only update metric Dict during validation\n            else:\n                metrics.update(\n                    **{\n                        (\n                            _key if len(domain_list) == 1 else f\"{domain}_{_key}\"\n                        ): _value\n                        for _key, _value in _criteria.items()\n                    }\n                )\n\n        # calculate the overall weighted loss during training\n        if self.training:\n            # normalize losses of all the domains by the given loss_weights\n            if hasattr(self, \"loss_weights\"):\n                assert len(self.loss_weights) == len(\n                    domain_list\n                ), \"There is a number mismatch of the domains between your data_cfg and train_cfg.\"\n                assert sum(\n                    [domain in self.loss_weights.keys() for domain in domain_list]\n                ) == len(\n                    domain_list\n                ), \"There is a name mismatch of the domains between your data_cfg and train_cfg.\"\n                losses.update(\n                    loss=sum(\n                        [\n                            losses[f\"{domain}_loss\"] * self.loss_weights[domain]\n                            for domain in domain_list\n                        ]\n                    )\n                    / sum([self.loss_weights[domain] for domain in domain_list])\n                )\n            # average losses of all the domains if loss_weights is not given\n            else:\n                losses.update(\n                    loss=sum([losses[f\"{domain}_loss\"] for domain in domain_list])\n                    / len(domain_list)\n                )\n            metrics.update(loss=losses[\"loss\"].clone().detach())\n\n        if self.training:\n            return losses, metrics\n        else:\n            return metrics\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.MultiDomainARTTS.inference","title":"<code>inference(infer_conf, **test_batch)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>infer_conf</code> <code>Dict</code> required <code>**test_batch</code> <code>{}</code> <p>Returns:</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def inference(self, infer_conf: Dict, **test_batch) -&gt; Dict[str, Any]:\n    \"\"\"\n\n    Args:\n        infer_conf:\n        **test_batch:\n\n    Returns:\n\n    \"\"\"\n    multi_flag = sum(\n        [isinstance(value, Dict) for value in test_batch.values()]\n    ) == len(test_batch)\n    # no sub-Dict means one normal supervised dataloader, go through the inference function of ASR\n    if not multi_flag:\n        return super(MultiDomainARTTS, self).inference(\n            infer_conf=infer_conf, **test_batch\n        )\n\n    # sub-Dict means that the domain information is given for ASR inference\n    else:\n        assert (\n            len(test_batch) == 1\n        ), \"If you want to evaluate the ASR model by multiple domains, please evaluate them one by one.\"\n        for domain, domain_batch in test_batch.items():\n            return super(MultiDomainARTTS, self).inference(\n                infer_conf=infer_conf, domain=domain, **domain_batch\n            )\n</code></pre>"},{"location":"reference/model/ar_tts/#model.ar_tts.MultiDomainARTTS.module_forward","title":"<code>module_forward(**batch_data)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**batch_data</code> <code>{}</code> <p>Returns:</p> Source code in <code>speechain/model/ar_tts.py</code> <pre><code>def module_forward(self, **batch_data) -&gt; Dict[str, Dict or torch.Tensor]:\n    \"\"\"\n\n    Args:\n        **batch_data:\n\n    Returns:\n\n    \"\"\"\n    # whether the input batch_data is generated by multiple dataloaders\n    multi_flag = sum(\n        [not isinstance(value, torch.Tensor) for value in batch_data.values()]\n    ) == len(batch_data)\n\n    # Single-dataloader scenario\n    # probably for the validation stage of in-domain semi-supervised ASR where we only have one data-label pair\n    if not multi_flag:\n        return super(MultiDomainARTTS, self).module_forward(**batch_data)\n    # Multi-dataloader scenario\n    # For semi-supervised training or validation of out-domain semi-supervised ASR where we may have multiple\n    # data-label pairs in a single batch, we need to go through forward function once for each pair.\n    else:\n        # pop the non-Dict arguments from the input batch data\n        general_args, data_keys = dict(), list(batch_data.keys())\n        for key in data_keys:\n            if not isinstance(batch_data[key], Dict):\n                general_args[key] = batch_data.pop(key)\n\n        # otherwise, go through the normal training process once for all the sub-batches\n        # (each sub-batch corresponds to a dataloader)\n        return {\n            domain: super(MultiDomainARTTS, self).module_forward(\n                domain=domain, **general_args, **domain_data\n            )\n            for domain, domain_data in batch_data.items()\n        }\n</code></pre>"},{"location":"reference/model/lm/","title":"lm","text":""},{"location":"reference/model/lm/#model.lm.LM","title":"<code>LM</code>","text":"<p>               Bases: <code>Model</code></p> <p>Auto-Regressive Attention-based Language Model.</p> Source code in <code>speechain/model/lm.py</code> <pre><code>class LM(Model):\n    \"\"\"Auto-Regressive Attention-based Language Model.\"\"\"\n\n    def module_init(\n        self,\n        token_type: str,\n        token_path: str,\n        emb: Dict,\n        encoder: Dict,\n        return_att_head_num: int = 2,\n        return_att_layer_num: int = 2,\n    ):\n        \"\"\"\n\n        Args:\n            token_type:\n            token_vocab:\n            emb:\n            encoder:\n            return_att_head_num:\n            return_att_layer_num:\n\n        \"\"\"\n        # --- 1. Module-independent Initialization --- #\n        # initialize the tokenizer\n        if token_type.lower() == \"char\":\n            self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n        elif token_type.lower() == \"sentencepiece\":\n            self.tokenizer = SentencePieceTokenizer(\n                token_path, copy_path=self.result_path\n            )\n        else:\n            raise ValueError(\n                f\"Unknown token_type {token_type}. \"\n                f\"Currently, {self.__class__.__name__} supports one of ['char', 'sentencepiece'].\"\n            )\n\n        self.return_att_head_num = return_att_head_num\n        self.return_att_layer_num = return_att_layer_num\n\n        # --- 2. Module Initialization --- #\n        self.lm = LanguageModel(\n            vocab_size=self.tokenizer.vocab_size, emb=emb, encoder=encoder\n        )\n\n    def criterion_init(self, **criterion_conf):\n        \"\"\"\n\n        Args:\n            **criterion_conf:\n\n        \"\"\"\n        # initialize cross-entropy loss\n        self.ce_loss = CrossEntropy(**criterion_conf)\n\n        # initialize teacher-forcing accuracy for validation\n        self.accuracy = Accuracy()\n\n    @staticmethod\n    def bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n        return [\n            [\"text_ppl\", \"max\", 30],\n            [\"text_ppl\", \"min\", 30],\n            [\"text_confid\", \"max\", 30],\n            [\"text_confid\", \"min\", 30],\n        ]\n\n    def module_forward(\n        self,\n        text: torch.Tensor,\n        text_len: torch.Tensor,\n        epoch: int = None,\n        domain: str = None,\n        return_att: bool = False,\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"\n\n        Args:\n            text: (batch, text_maxlen)\n                The input text data with &lt;sos/eos&gt; at the beginning and end\n            text_len: (batch,)\n                The lengths of input text data\n            epoch: int\n                The number of the current training epoch.\n                Mainly used for mean&amp;std calculation in the feature normalization\n            domain: str = None\n            return_att: bool\n                Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n            kwargs:\n                Temporary register used to store the redundant arguments.\n\n        \"\"\"\n        assert text_len.size(0) == text.size(\n            0\n        ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n        # remove the &lt;sos/eos&gt; at the end of each sentence\n        for i in range(text_len.size(0)):\n            text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n        text, text_len = text[:, :-1], text_len - 1\n\n        # Next-Token prediction\n        logits, _, enc_attmat = self.lm(text, text_len)\n\n        # initialize the asr output to be the decoder predictions\n        outputs = dict(logits=logits)\n\n        def shrink_attention(input_att_list):\n            # pick up the target attention layers\n            if (\n                self.return_att_layer_num != -1\n                and len(input_att_list) &gt; self.return_att_layer_num\n            ):\n                input_att_list = input_att_list[-self.return_att_layer_num :]\n            # pick up the target attention heads\n            if (\n                self.return_att_head_num != -1\n                and input_att_list[0].size(1) &gt; self.return_att_head_num\n            ):\n                input_att_list = [\n                    att[:, : self.return_att_head_num] for att in input_att_list\n                ]\n            return input_att_list\n\n        # return the attention results if specified\n        if return_att:\n            if enc_attmat is not None:\n                outputs.update(att=shrink_attention(enc_attmat))\n        return outputs\n\n    def criterion_forward(\n        self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            logits:\n            text:\n            text_len:\n\n        Returns:\n\n        \"\"\"\n        accuracy = self.accuracy(logits=logits, text=text, text_len=text_len)\n\n        # mask generation for the input text\n        text_mask = make_mask_from_len(text_len - 1, return_3d=False)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        # perplexity calculation\n        log_prob = torch.log_softmax(logits, dim=-1)\n        text_prob = log_prob.gather(-1, text[:, 1:].view(text.size(0), -1, 1)).squeeze(\n            dim=-1\n        )\n        text_prob = text_prob.masked_fill(~text_mask, 0.0)\n        text_ppl = torch.exp(\n            torch.sum(text_prob, dim=-1) * (-1 / (text_len - 1))\n        ).mean()\n\n        metrics = dict(accuracy=accuracy.detach(), text_ppl=text_ppl.clone().detach())\n\n        loss = self.ce_loss(logits=logits, text=text, text_len=text_len)\n        losses = dict(loss=loss)\n        # .clone() here prevents the loss from being modified by accum_grad\n        metrics.update(loss=loss.clone().detach())\n\n        if self.training:\n            return losses, metrics\n        else:\n            return metrics\n\n    def visualize(\n        self,\n        epoch: int,\n        sample_index: str,\n        snapshot_interval: int = 1,\n        epoch_records: Dict = None,\n        domain: str = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n    ):\n\n        # visualization inference is default to be done by teacher-forcing\n        if len(self.visual_infer_conf) == 0:\n            self.visual_infer_conf = dict()\n\n        # obtain the inference results\n        infer_results = self.inference(\n            infer_conf=self.visual_infer_conf,\n            return_att=True,\n            text=text,\n            text_len=text_len,\n        )\n\n        # --- snapshot the objective metrics --- #\n        vis_logs = []\n        # numerical metrics recording\n        materials = dict()\n        for metric in [\"text_confid\", \"text_ppl\"]:\n            # store each target metric into materials\n            if metric not in epoch_records[sample_index].keys():\n                epoch_records[sample_index][metric] = []\n            epoch_records[sample_index][metric].append(\n                infer_results[metric][\"content\"][0]\n            )\n            materials[metric] = epoch_records[sample_index][metric]\n        # save the visualization log\n        vis_logs.append(\n            dict(\n                plot_type=\"curve\",\n                materials=copy.deepcopy(materials),\n                epoch=epoch,\n                xlabel=\"epoch\",\n                x_stride=snapshot_interval,\n                sep_save=False,\n                subfolder_names=sample_index,\n            )\n        )\n\n        # --- snapshot the subjective metrics --- #\n        # record the input audio and real text at the first snapshotting step\n        if epoch // snapshot_interval == 1:\n            # snapshot input text\n            vis_logs.append(\n                dict(\n                    materials=dict(\n                        real_text=[\n                            copy.deepcopy(self.tokenizer.tensor2text(text[0][1:-1]))\n                        ]\n                    ),\n                    plot_type=\"text\",\n                    subfolder_names=sample_index,\n                )\n            )\n\n        # hypothesis attention matrix\n        infer_results[\"att\"] = self.attention_reshape(infer_results[\"att\"])\n        self.matrix_snapshot(\n            vis_logs=vis_logs,\n            hypo_attention=copy.deepcopy(infer_results[\"att\"]),\n            subfolder_names=sample_index,\n            epoch=epoch,\n        )\n        return vis_logs\n\n    def inference(\n        self,\n        infer_conf: Dict,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        domain: str = None,\n        return_att: bool = False,\n    ) -&gt; Dict[str, Dict[str, str or List]]:\n        \"\"\"\n\n        Args:\n            infer_conf:\n            text:\n            text_len:\n            domain:\n            return_att:\n\n        Returns:\n\n        \"\"\"\n        assert text is not None and text_len is not None\n\n        # copy the input data in advance for data safety\n        model_input = copy.deepcopy(dict(text=text, text_len=text_len))\n\n        # LM Decoding by Teacher Forcing\n        infer_results = self.module_forward(return_att=return_att, **model_input)\n        outputs = dict()\n\n        # add the attention matrix into the output Dict, only used for model visualization during training\n        # because it will consume too much time for saving the attention matrices of all testing samples during testing\n        if return_att:\n            outputs.update(att=infer_results[\"att\"])\n\n        # --- Perplexity Calculation --- #\n        # the last token (EOS) should be included for perplexity\n        log_prob = torch.log_softmax(infer_results[\"logits\"], dim=-1)\n        hypo_text_prob = log_prob.gather(\n            -1, text[:, 1:].view(text.size(0), -1, 1)\n        ).squeeze(dim=-1)\n        hypo_text_ppl = torch.exp(\n            torch.sum(hypo_text_prob, dim=-1) * (-1 / (text_len - 1))\n        )\n\n        # --- Confidence Calculation --- #\n        # the last token is meaningless because the text is padded with eos at the end\n        log_prob = log_prob[:, :-1]\n        hypo_text_prob, hypo_text = torch.max(log_prob, dim=-1)\n        # the original text contains both sos at the beginning and eos at the end\n        # sum up the log-probability of all time steps to get the confidence\n        length_penalty = (\n            infer_conf[\"length_penalty\"]\n            if \"length_penalty\" in infer_conf.keys()\n            else 1.0\n        )\n        hypo_text_confid = torch.sum(hypo_text_prob, dim=-1) / (\n            (text_len - 2) ** length_penalty\n        )\n\n        # turn the data all the unsupervised metrics into the cpu version (List)\n        hypo_text_confid, hypo_text_ppl = to_cpu(hypo_text_confid), to_cpu(\n            hypo_text_ppl\n        )\n\n        # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n        hypo_text = [\n            self.tokenizer.tensor2text(\n                hypo[\n                    (hypo != self.tokenizer.ignore_idx)\n                    &amp; (hypo != self.tokenizer.sos_eos_idx)\n                ]\n            )\n            for hypo in hypo_text\n        ]\n\n        # in the decoding-only mode, only the hypothesis-related results will be returned\n        outputs.update(\n            text=dict(format=\"txt\", content=hypo_text),\n            text_confid=dict(format=\"txt\", content=hypo_text_confid),\n            text_ppl=dict(format=\"txt\", content=hypo_text_ppl),\n        )\n\n        # evaluation reports for all the testing instances\n        instance_report_dict = {}\n        # loop each utterance\n        for i in range(len(text)):\n            if \"Text Confidence\" not in instance_report_dict.keys():\n                instance_report_dict[\"Text Confidence\"] = []\n            instance_report_dict[\"Text Confidence\"].append(f\"{hypo_text_confid[i]:.6f}\")\n\n            if \"Text Perplexity\" not in instance_report_dict.keys():\n                instance_report_dict[\"Text Perplexity\"] = []\n            instance_report_dict[\"Text Perplexity\"].append(f\"{hypo_text_ppl[i]:.4f}\")\n        # register the instance reports for generating instance_reports.md\n        self.register_instance_reports(md_list_dict=instance_report_dict)\n\n        return outputs\n</code></pre>"},{"location":"reference/model/lm/#model.lm.LM.criterion_forward","title":"<code>criterion_forward(logits, text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> required <code>text</code> <code>Tensor</code> required <code>text_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/model/lm.py</code> <pre><code>def criterion_forward(\n    self, logits: torch.Tensor, text: torch.Tensor, text_len: torch.Tensor\n) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        logits:\n        text:\n        text_len:\n\n    Returns:\n\n    \"\"\"\n    accuracy = self.accuracy(logits=logits, text=text, text_len=text_len)\n\n    # mask generation for the input text\n    text_mask = make_mask_from_len(text_len - 1, return_3d=False)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    # perplexity calculation\n    log_prob = torch.log_softmax(logits, dim=-1)\n    text_prob = log_prob.gather(-1, text[:, 1:].view(text.size(0), -1, 1)).squeeze(\n        dim=-1\n    )\n    text_prob = text_prob.masked_fill(~text_mask, 0.0)\n    text_ppl = torch.exp(\n        torch.sum(text_prob, dim=-1) * (-1 / (text_len - 1))\n    ).mean()\n\n    metrics = dict(accuracy=accuracy.detach(), text_ppl=text_ppl.clone().detach())\n\n    loss = self.ce_loss(logits=logits, text=text, text_len=text_len)\n    losses = dict(loss=loss)\n    # .clone() here prevents the loss from being modified by accum_grad\n    metrics.update(loss=loss.clone().detach())\n\n    if self.training:\n        return losses, metrics\n    else:\n        return metrics\n</code></pre>"},{"location":"reference/model/lm/#model.lm.LM.criterion_init","title":"<code>criterion_init(**criterion_conf)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**criterion_conf</code> <code>{}</code> Source code in <code>speechain/model/lm.py</code> <pre><code>def criterion_init(self, **criterion_conf):\n    \"\"\"\n\n    Args:\n        **criterion_conf:\n\n    \"\"\"\n    # initialize cross-entropy loss\n    self.ce_loss = CrossEntropy(**criterion_conf)\n\n    # initialize teacher-forcing accuracy for validation\n    self.accuracy = Accuracy()\n</code></pre>"},{"location":"reference/model/lm/#model.lm.LM.inference","title":"<code>inference(infer_conf, text=None, text_len=None, domain=None, return_att=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>infer_conf</code> <code>Dict</code> required <code>text</code> <code>Tensor</code> <code>None</code> <code>text_len</code> <code>Tensor</code> <code>None</code> <code>domain</code> <code>str</code> <code>None</code> <code>return_att</code> <code>bool</code> <code>False</code> <p>Returns:</p> Source code in <code>speechain/model/lm.py</code> <pre><code>def inference(\n    self,\n    infer_conf: Dict,\n    text: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    domain: str = None,\n    return_att: bool = False,\n) -&gt; Dict[str, Dict[str, str or List]]:\n    \"\"\"\n\n    Args:\n        infer_conf:\n        text:\n        text_len:\n        domain:\n        return_att:\n\n    Returns:\n\n    \"\"\"\n    assert text is not None and text_len is not None\n\n    # copy the input data in advance for data safety\n    model_input = copy.deepcopy(dict(text=text, text_len=text_len))\n\n    # LM Decoding by Teacher Forcing\n    infer_results = self.module_forward(return_att=return_att, **model_input)\n    outputs = dict()\n\n    # add the attention matrix into the output Dict, only used for model visualization during training\n    # because it will consume too much time for saving the attention matrices of all testing samples during testing\n    if return_att:\n        outputs.update(att=infer_results[\"att\"])\n\n    # --- Perplexity Calculation --- #\n    # the last token (EOS) should be included for perplexity\n    log_prob = torch.log_softmax(infer_results[\"logits\"], dim=-1)\n    hypo_text_prob = log_prob.gather(\n        -1, text[:, 1:].view(text.size(0), -1, 1)\n    ).squeeze(dim=-1)\n    hypo_text_ppl = torch.exp(\n        torch.sum(hypo_text_prob, dim=-1) * (-1 / (text_len - 1))\n    )\n\n    # --- Confidence Calculation --- #\n    # the last token is meaningless because the text is padded with eos at the end\n    log_prob = log_prob[:, :-1]\n    hypo_text_prob, hypo_text = torch.max(log_prob, dim=-1)\n    # the original text contains both sos at the beginning and eos at the end\n    # sum up the log-probability of all time steps to get the confidence\n    length_penalty = (\n        infer_conf[\"length_penalty\"]\n        if \"length_penalty\" in infer_conf.keys()\n        else 1.0\n    )\n    hypo_text_confid = torch.sum(hypo_text_prob, dim=-1) / (\n        (text_len - 2) ** length_penalty\n    )\n\n    # turn the data all the unsupervised metrics into the cpu version (List)\n    hypo_text_confid, hypo_text_ppl = to_cpu(hypo_text_confid), to_cpu(\n        hypo_text_ppl\n    )\n\n    # recover the text tensors back to text strings (removing the padding and sos/eos tokens)\n    hypo_text = [\n        self.tokenizer.tensor2text(\n            hypo[\n                (hypo != self.tokenizer.ignore_idx)\n                &amp; (hypo != self.tokenizer.sos_eos_idx)\n            ]\n        )\n        for hypo in hypo_text\n    ]\n\n    # in the decoding-only mode, only the hypothesis-related results will be returned\n    outputs.update(\n        text=dict(format=\"txt\", content=hypo_text),\n        text_confid=dict(format=\"txt\", content=hypo_text_confid),\n        text_ppl=dict(format=\"txt\", content=hypo_text_ppl),\n    )\n\n    # evaluation reports for all the testing instances\n    instance_report_dict = {}\n    # loop each utterance\n    for i in range(len(text)):\n        if \"Text Confidence\" not in instance_report_dict.keys():\n            instance_report_dict[\"Text Confidence\"] = []\n        instance_report_dict[\"Text Confidence\"].append(f\"{hypo_text_confid[i]:.6f}\")\n\n        if \"Text Perplexity\" not in instance_report_dict.keys():\n            instance_report_dict[\"Text Perplexity\"] = []\n        instance_report_dict[\"Text Perplexity\"].append(f\"{hypo_text_ppl[i]:.4f}\")\n    # register the instance reports for generating instance_reports.md\n    self.register_instance_reports(md_list_dict=instance_report_dict)\n\n    return outputs\n</code></pre>"},{"location":"reference/model/lm/#model.lm.LM.module_forward","title":"<code>module_forward(text, text_len, epoch=None, domain=None, return_att=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The input text data with  at the beginning and end required <code>text_len</code> <code>Tensor</code> <p>(batch,) The lengths of input text data</p> required <code>epoch</code> <code>int</code> <p>int The number of the current training epoch. Mainly used for mean&amp;std calculation in the feature normalization</p> <code>None</code> <code>domain</code> <code>str</code> <p>str = None</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool Controls whether the attention matrices of each layer in the encoder and decoder will be returned.</p> <code>False</code> <code>kwargs</code> <p>Temporary register used to store the redundant arguments.</p> <code>{}</code> Source code in <code>speechain/model/lm.py</code> <pre><code>def module_forward(\n    self,\n    text: torch.Tensor,\n    text_len: torch.Tensor,\n    epoch: int = None,\n    domain: str = None,\n    return_att: bool = False,\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n\n    Args:\n        text: (batch, text_maxlen)\n            The input text data with &lt;sos/eos&gt; at the beginning and end\n        text_len: (batch,)\n            The lengths of input text data\n        epoch: int\n            The number of the current training epoch.\n            Mainly used for mean&amp;std calculation in the feature normalization\n        domain: str = None\n        return_att: bool\n            Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n        kwargs:\n            Temporary register used to store the redundant arguments.\n\n    \"\"\"\n    assert text_len.size(0) == text.size(\n        0\n    ), \"The amounts of sentences and their lengths are not equal to each other.\"\n\n    # remove the &lt;sos/eos&gt; at the end of each sentence\n    for i in range(text_len.size(0)):\n        text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n    text, text_len = text[:, :-1], text_len - 1\n\n    # Next-Token prediction\n    logits, _, enc_attmat = self.lm(text, text_len)\n\n    # initialize the asr output to be the decoder predictions\n    outputs = dict(logits=logits)\n\n    def shrink_attention(input_att_list):\n        # pick up the target attention layers\n        if (\n            self.return_att_layer_num != -1\n            and len(input_att_list) &gt; self.return_att_layer_num\n        ):\n            input_att_list = input_att_list[-self.return_att_layer_num :]\n        # pick up the target attention heads\n        if (\n            self.return_att_head_num != -1\n            and input_att_list[0].size(1) &gt; self.return_att_head_num\n        ):\n            input_att_list = [\n                att[:, : self.return_att_head_num] for att in input_att_list\n            ]\n        return input_att_list\n\n    # return the attention results if specified\n    if return_att:\n        if enc_attmat is not None:\n            outputs.update(att=shrink_attention(enc_attmat))\n    return outputs\n</code></pre>"},{"location":"reference/model/lm/#model.lm.LM.module_init","title":"<code>module_init(token_type, token_path, emb, encoder, return_att_head_num=2, return_att_layer_num=2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>token_type</code> <code>str</code> required <code>token_vocab</code> required <code>emb</code> <code>Dict</code> required <code>encoder</code> <code>Dict</code> required <code>return_att_head_num</code> <code>int</code> <code>2</code> <code>return_att_layer_num</code> <code>int</code> <code>2</code> Source code in <code>speechain/model/lm.py</code> <pre><code>def module_init(\n    self,\n    token_type: str,\n    token_path: str,\n    emb: Dict,\n    encoder: Dict,\n    return_att_head_num: int = 2,\n    return_att_layer_num: int = 2,\n):\n    \"\"\"\n\n    Args:\n        token_type:\n        token_vocab:\n        emb:\n        encoder:\n        return_att_head_num:\n        return_att_layer_num:\n\n    \"\"\"\n    # --- 1. Module-independent Initialization --- #\n    # initialize the tokenizer\n    if token_type.lower() == \"char\":\n        self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n    elif token_type.lower() == \"sentencepiece\":\n        self.tokenizer = SentencePieceTokenizer(\n            token_path, copy_path=self.result_path\n        )\n    else:\n        raise ValueError(\n            f\"Unknown token_type {token_type}. \"\n            f\"Currently, {self.__class__.__name__} supports one of ['char', 'sentencepiece'].\"\n        )\n\n    self.return_att_head_num = return_att_head_num\n    self.return_att_layer_num = return_att_layer_num\n\n    # --- 2. Module Initialization --- #\n    self.lm = LanguageModel(\n        vocab_size=self.tokenizer.vocab_size, emb=emb, encoder=encoder\n    )\n</code></pre>"},{"location":"reference/model/nar_tts/","title":"nar_tts","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2023.02</p>"},{"location":"reference/model/nar_tts/#model.nar_tts.FastSpeech2","title":"<code>FastSpeech2</code>","text":"<p>               Bases: <code>Model</code></p> <p>NonAuto-Regressive FastSpeech2 Text-To-Speech Synthesis Model.</p> <p>(single-speaker &amp; multi-speaker)</p> Source code in <code>speechain/model/nar_tts.py</code> <pre><code>class FastSpeech2(Model):\n    \"\"\"NonAuto-Regressive FastSpeech2 Text-To-Speech Synthesis Model.\n\n    (single-speaker &amp; multi-speaker)\n    \"\"\"\n\n    def module_init(\n        self,\n        token_type: str,\n        token_path: str,\n        enc_emb: Dict,\n        encoder: Dict,\n        pitch_predictor: Dict,\n        energy_predictor: Dict,\n        duration_predictor: Dict,\n        feat_frontend: Dict,\n        decoder: Dict,\n        enc_prenet: Dict = None,\n        dec_postnet: Dict = None,\n        feat_normalize: Dict or bool = True,\n        pitch_normalize: Dict or bool = True,\n        energy_normalize: Dict or bool = True,\n        spk_list: str = None,\n        spk_emb: Dict = None,\n        sample_rate: int = 22050,\n        audio_format: str = \"wav\",\n        reduction_factor: int = 1,\n        gate_pos_weight: float = 1.0,\n        return_att_type: List[str] or str = None,\n        return_att_head_num: int = 2,\n        return_att_layer_num: int = 2,\n    ):\n\n        # --- 1. Model-Customized Part Initialization --- #\n        # initialize the tokenizer\n        if token_type == \"char\":\n            self.tokenizer = CharTokenizer(token_path, copy_path=self.result_path)\n        elif token_type == \"mfa\":\n            self.tokenizer = GraphemeToPhonemeTokenizer(\n                token_path, copy_path=self.result_path\n            )\n        else:\n            raise ValueError(\n                f\"Unknown token type {token_type}. \"\n                f\"Currently, {self.__class__.__name__} supports one of ['char', 'mfa'].\"\n            )\n\n        # initialize the speaker list if given\n        if spk_list is not None:\n            if not isinstance(spk_list, list):\n                spk_list = [spk_list]\n            spk_list = np.concatenate(\n                [np.loadtxt(parse_path_args(s_l), dtype=str) for s_l in spk_list],\n                axis=0,\n            )\n            # when the input file is idx2spk, only retain the column of speaker ids\n            if len(spk_list.shape) == 2:\n                assert spk_list.shape[1] == 2\n                spk_list = spk_list[:, 1]\n            # otherwise, the input file must be spk_list which is a single-column file and each row is a speaker id\n            elif len(spk_list.shape) != 1:\n                raise RuntimeError\n            # 1. remove redundant elements; 2. sort up the speaker ids in order\n            spk_list = sorted(set(spk_list))\n            # 3. get the corresponding indices (start from 1 since 0 is reserved for unknown speakers)\n            self.idx2spk = dict(zip(range(1, len(spk_list) + 1), spk_list))\n            # 4. exchange the positions of indices and speaker ids\n            self.spk2idx = dict(map(reversed, self.idx2spk.items()))\n\n        # initialize the sampling rate, mainly used for visualizing the input audio during training\n        self.sample_rate = sample_rate\n        self.audio_format = audio_format.lower()\n        self.reduction_factor = reduction_factor\n        self.gate_pos_weight = gate_pos_weight\n\n        if return_att_type is None:\n            self.return_att_type = [\"enc\", \"dec\"]\n        else:\n            self.return_att_type = (\n                return_att_type\n                if isinstance(return_att_type, List)\n                else [return_att_type]\n            )\n        for i in range(len(self.return_att_type)):\n            if self.return_att_type[i].lower() in [\"enc\", \"dec\"]:\n                self.return_att_type[i] = self.return_att_type[i].lower()\n            else:\n                raise ValueError(\n                    \"The elements of your input return_att_type must be one of ['enc', 'dec'], \"\n                    f\"but got {self.return_att_type[i]}!\"\n                )\n        self.return_att_head_num = return_att_head_num\n        self.return_att_layer_num = return_att_layer_num\n\n        # --- 2. Module Part Construction --- #\n        # --- 2.1. Encoder construction --- #\n        # the vocabulary size is given by the built-in tokenizer instead of the input configuration\n        if \"vocab_size\" in enc_emb[\"conf\"].keys():\n            if enc_emb[\"conf\"][\"vocab_size\"] != self.tokenizer.vocab_size:\n                warnings.warn(\n                    f\"Your input vocabulary size is different from the one obtained from the built-in \"\n                    f\"tokenizer ({self.tokenizer.vocab_size}). The latter one will be used to initialize the \"\n                    f\"encoder for correctness.\"\n                )\n            enc_emb[\"conf\"].pop(\"vocab_size\")\n        self.encoder = TTSEncoder(\n            vocab_size=self.tokenizer.vocab_size,\n            embedding=enc_emb,\n            prenet=enc_prenet,\n            encoder=encoder,\n        )\n\n        # --- 2.2. Decoder construction --- #\n        # check the sampling rate of the decoder frontend\n        if \"sr\" not in feat_frontend[\"conf\"].keys():\n            # update the sampling rate of the frontend by the built-in one in the model\n            feat_frontend[\"conf\"][\"sr\"] = self.sample_rate\n        elif feat_frontend[\"conf\"][\"sr\"] != self.sample_rate:\n            raise RuntimeError(\n                f\"The sampling rate given in your feat_frontend['conf'] ({feat_frontend['conf']['sr']}) \"\n                f\"is different from your given sample_rate ({self.sample_rate})!\"\n            )\n\n        # check the speaker embedding configuration\n        if spk_emb is not None:\n            # speaker number for the close-set multi-speaker TTS\n            if hasattr(self, \"spk2idx\"):\n                if (\n                    \"spk_num\" in spk_emb.keys()\n                    and spk_emb[\"spk_num\"] != len(self.spk2idx) + 1\n                ):\n                    warnings.warn(\n                        \"Your input spk_num is different from the number of speakers in your given spk_list. \"\n                        f\"Currently, the spk_num is set to {len(self.spk2idx) + 1}.\"\n                    )\n                # all seen speakers plus an unknown speaker (ID: 0)\n                spk_emb[\"spk_num\"] = len(self.spk2idx) + 1\n                if (\n                    \"spk_emb_dim_lookup\" not in spk_emb.keys()\n                    or spk_emb[\"spk_emb_dim_lookup\"] is None\n                ):\n                    raise RuntimeError(\n                        \"If you want to use speaker look-up table for multi-speaker TTS, \"\n                        \"please give spk_emb_dim_lookup in model['module_conf']['spk_emb'].\"\n                    )\n\n            elif (\n                \"spk_emb_dim_lookup\" in spk_emb.keys()\n                and spk_emb[\"spk_emb_dim_lookup\"] is not None\n            ):\n                raise RuntimeError(\n                    \"Please give spk_list in model['customize_conf'] if you want to use speaker lookup \"\n                    \"table for close-set multi-speaker TTS.\"\n                )\n\n        self.decoder = FastSpeech2Decoder(\n            input_size=self.encoder.output_size,\n            spk_emb=spk_emb,\n            feat_frontend=feat_frontend,\n            feat_normalize=feat_normalize,\n            pitch_normalize=pitch_normalize,\n            energy_normalize=energy_normalize,\n            pitch_predictor=pitch_predictor,\n            energy_predictor=energy_predictor,\n            duration_predictor=duration_predictor,\n            decoder=decoder,\n            postnet=dec_postnet,\n            distributed=self.distributed,\n            reduction_factor=self.reduction_factor,\n        )\n\n    @staticmethod\n    def bad_cases_selection_init_fn() -&gt; List[List[str or int]] or None:\n        return [\n            [\"feat_token_len_ratio\", \"max\", 30],\n            [\"feat_token_len_ratio\", \"min\", 30],\n            [\"feat_len\", \"max\", 30],\n            [\"feat_len\", \"min\", 30],\n            [\"duration_f1\", \"min\", 30],\n        ]\n\n    def criterion_init(\n        self,\n        feat_loss: Dict = None,\n        pitch_loss: Dict = None,\n        energy_loss: Dict = None,\n        duration_loss: Dict = None,\n    ):\n\n        # acoustic feature loss, default to be MAE\n        if feat_loss is None:\n            feat_loss = dict(loss_type=\"L1\")\n        self.feat_loss = LeastError(**feat_loss)\n\n        # pitch loss, default to be MSE\n        if pitch_loss is None:\n            pitch_loss = dict(loss_type=\"L2\")\n        self.pitch_loss = LeastError(**pitch_loss)\n\n        # energy loss, default to be MSE\n        if energy_loss is None:\n            energy_loss = dict(loss_type=\"L2\")\n        self.energy_loss = LeastError(**energy_loss)\n\n        # phoneme duration loss, default to be MSE\n        if duration_loss is None:\n            duration_loss = dict(loss_type=\"L2\")\n        self.duration_loss = LeastError(**duration_loss)\n        self.duration_gate_loss = BCELogits(pos_weight=self.gate_pos_weight)\n        self.duration_f1 = FBetaScore()\n\n    def module_forward(\n        self,\n        epoch: int = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        duration: torch.Tensor = None,\n        duration_len: torch.Tensor = None,\n        pitch: torch.Tensor = None,\n        pitch_len: torch.Tensor = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        energy: torch.Tensor = None,\n        energy_len: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        return_att: bool = False,\n        min_frame_num: int = 0,\n        max_frame_num: int = None,\n        duration_alpha: torch.Tensor = None,\n        energy_alpha: torch.Tensor = None,\n        pitch_alpha: torch.Tensor = None,\n        **kwargs,\n    ) -&gt; Dict:\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input speech data (grouped or downsampled and edge-padded).\n            feat_len: (batch,)\n                The lengths of input speech data\n            text: (batch, text_maxlen)\n                The input text data with &lt;sos/eos&gt; at the beginning and end\n            text_len: (batch,)\n                The lengths of input text data\n            duration: (batch, text_maxlen)\n                The duration data for each token in text.\n            duration_len: (batch,)\n                The lengths of input duration data\n            pitch: (batch, text_maxlen)\n                The pitch data for each token in text.\n            pitch_len: (batch,)\n                The lengths of input pitch data\n            energy: (batch, text_maxlen)\n                The energy data for each token in text.\n            energy_len: (batch,)\n                The lengths of input energy data\n            spk_feat: (batch, 1, speaker embedding dim)\n                Pre-extracted speaker embedding. (None means single-speaker TTS)\n            spk_ids: (batch,)\n                The speaker ids of each speech data. In the form of integer values.\n            epoch: int\n                The number of the current training epoch.\n                Mainly used for mean&amp;std calculation in the feature normalization\n            return_att: bool\n                Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n            # Arguments for controllable TTS received from self.inference()\n            duration_alpha:\n            energy_alpha:\n            pitch_alpha:\n            kwargs:\n                Temporary register used to store the redundant arguments.\n\n        Returns:\n            A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses\n\n        \"\"\"\n        # text checking\n        assert text is not None and text_len is not None\n        assert text_len.size(0) == text.size(\n            0\n        ), \"The amounts of sentences and their lengths are not equal to each other.\"\n        # feat checking\n        if feat is not None and feat_len is not None:\n            assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n                0\n            ), \"The amounts of feat and text are not equal to each other.\"\n            assert feat_len.size(0) == feat.size(\n                0\n            ), \"The amounts of feat and their lengths are not equal to each other.\"\n        elif (feat is None) ^ (feat_len is None):\n            raise RuntimeError(\n                f\"In {self.__class__.__name__}, \"\n                f\"feat and feat_len must be None or not None at the same time! \"\n                f\"But got feat={feat} and feat_len={feat_len}.\"\n            )\n        # pitch checking\n        if pitch is not None and pitch_len is not None:\n            assert pitch.size(0) == feat.size(0) and pitch_len.size(0) == feat_len.size(\n                0\n            ), \"The amounts of pitch and feat are not equal to each other.\"\n            assert pitch_len.size(0) == pitch.size(\n                0\n            ), \"The amounts of pitch and their lengths are not equal to each other.\"\n        elif (pitch is None) ^ (pitch_len is None):\n            raise RuntimeError(\n                f\"In {self.__class__.__name__}, \"\n                f\"pitch and pitch_len must be None or not None at the same time! \"\n                f\"But got pitch={pitch} and pitch_len={pitch_len}.\"\n            )\n        # energy checking\n        if energy is not None and energy_len is not None:\n            assert energy.size(0) == feat.size(0) and energy_len.size(\n                0\n            ) == feat_len.size(\n                0\n            ), \"The amounts of energy and feat are not equal to each other.\"\n            assert energy_len.size(0) == energy.size(\n                0\n            ), \"The amounts of energy and their lengths are not equal to each other.\"\n        elif (energy is None) ^ (energy_len is None):\n            raise RuntimeError(\n                f\"In {self.__class__.__name__}, \"\n                f\"energy and energy_len must be None or not None at the same time! \"\n                f\"But got energy={energy} and energy_len={energy_len}.\"\n            )\n\n        # text preprocessing before duration checking\n        # remove the &lt;sos/eos&gt; at the beginning and the end of each sentence\n        for i in range(text_len.size(0)):\n            text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n        text, text_len = text[:, 1:-1], text_len - 2\n\n        # duration checking\n        if duration is not None and duration_len is not None:\n            assert duration.size(0) == text.size(\n                0\n            ), \"The amounts of durations and text are not equal to each other.\"\n            assert duration_len.size(0) == text_len.size(\n                0\n            ), \"The amounts of durations and text lengths are not equal to each other.\"\n            # check the length of duration and text\n            assert False not in [\n                len(text[i]) == len(duration[i]) for i in range(len(text))\n            ], \"The lengths of individual duration and text data don't match with each other.\"\n        elif (duration is None) ^ (duration_len is None):\n            raise RuntimeError(\n                f\"In {self.__class__.__name__}, \"\n                f\"duration and duration_len must be None or not None at the same time! \"\n                f\"But got duration={duration} and duration_len={duration_len}.\"\n            )\n\n        # Encoding the text data\n        enc_text, enc_text_mask, enc_attmat, enc_hidden = self.encoder(\n            text=text, text_len=text_len\n        )\n\n        # Decoding\n        (\n            pred_feat_before,\n            pred_feat_after,\n            pred_feat_len,\n            tgt_feat,\n            tgt_feat_len,\n            pred_pitch,\n            tgt_pitch,\n            tgt_pitch_len,\n            pred_energy,\n            tgt_energy,\n            tgt_energy_len,\n            pred_duration,\n            pred_duration_gate,\n            tgt_duration,\n            tgt_duration_len,\n            dec_attmat,\n            dec_hidden,\n        ) = self.decoder(\n            enc_text=enc_text,\n            enc_text_mask=enc_text_mask,\n            duration=duration,\n            duration_len=duration_len,\n            feat=feat,\n            feat_len=feat_len,\n            pitch=pitch,\n            pitch_len=pitch_len,\n            energy=energy,\n            energy_len=energy_len,\n            spk_feat=spk_feat,\n            spk_ids=spk_ids,\n            epoch=epoch,\n            min_frame_num=min_frame_num,\n            max_frame_num=max_frame_num,\n            duration_alpha=duration_alpha,\n            energy_alpha=energy_alpha,\n            pitch_alpha=pitch_alpha,\n        )\n\n        # initialize the TTS output to be the decoder predictions\n        outputs = dict(\n            pred_feat_before=pred_feat_before,\n            pred_feat_after=pred_feat_after,\n            pred_feat_len=pred_feat_len,\n            tgt_feat=tgt_feat,\n            tgt_feat_len=tgt_feat_len,\n            pred_pitch=pred_pitch,\n            tgt_pitch=tgt_pitch,\n            tgt_pitch_len=tgt_pitch_len,\n            pred_energy=pred_energy,\n            tgt_energy=tgt_energy,\n            tgt_energy_len=tgt_energy_len,\n            pred_duration=pred_duration,\n            pred_duration_gate=pred_duration_gate,\n            tgt_duration=tgt_duration,\n            tgt_duration_len=tgt_duration_len,\n        )\n\n        def shrink_attention(input_att_list):\n            # pick up the target attention layers\n            if (\n                self.return_att_layer_num != -1\n                and len(input_att_list) &gt; self.return_att_layer_num\n            ):\n                input_att_list = input_att_list[-self.return_att_layer_num :]\n            # pick up the target attention heads\n            if (\n                self.return_att_head_num != -1\n                and input_att_list[0].size(1) &gt; self.return_att_head_num\n            ):\n                input_att_list = [\n                    att[:, : self.return_att_head_num] for att in input_att_list\n                ]\n            return input_att_list\n\n        # return the attention results if specified\n        if return_att:\n            # encoder self-attention\n            if enc_attmat is not None and \"enc\" in self.return_att_type:\n                outputs.update(att=dict(enc=shrink_attention(enc_attmat)))\n            # decoder self-attention\n            if dec_attmat is not None and \"dec\" in self.return_att_type:\n                outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n        return outputs\n\n    def criterion_forward(\n        self,\n        pred_feat_before: torch.Tensor,\n        pred_feat_after: torch.Tensor,\n        tgt_feat: torch.Tensor,\n        tgt_feat_len: torch.Tensor,\n        pred_pitch: torch.Tensor,\n        tgt_pitch: torch.Tensor,\n        tgt_pitch_len: torch.Tensor,\n        pred_energy: torch.Tensor,\n        tgt_energy: torch.Tensor,\n        tgt_energy_len: torch.Tensor,\n        pred_duration: torch.Tensor,\n        pred_duration_gate: torch.Tensor,\n        tgt_duration: torch.Tensor,\n        tgt_duration_len: torch.Tensor,\n        feat_loss_fn: LeastError = None,\n        pitch_loss_fn: LeastError = None,\n        energy_loss_fn: LeastError = None,\n        duration_loss_fn: LeastError = None,\n        duration_gate_loss_fn: BCELogits = None,\n        **kwargs,\n    ) -&gt; (Dict[str, torch.Tensor], Dict[str, torch.Tensor]) or Dict[str, torch.Tensor]:\n\n        # --- Losses Calculation --- #\n        # the external feature loss function has the higher priority\n        if feat_loss_fn is None:\n            feat_loss_fn = self.feat_loss\n        # acoustic feature prediction loss\n        feat_loss_before = feat_loss_fn(\n            pred=pred_feat_before, tgt=tgt_feat, tgt_len=tgt_feat_len\n        )\n        feat_loss_after = feat_loss_fn(\n            pred=pred_feat_after, tgt=tgt_feat, tgt_len=tgt_feat_len\n        )\n\n        # the external pitch loss function has the higher priority\n        if pitch_loss_fn is None:\n            pitch_loss_fn = self.pitch_loss\n        # pitch prediction loss\n        pitch_loss = pitch_loss_fn(\n            pred=pred_pitch, tgt=tgt_pitch, tgt_len=tgt_pitch_len\n        )\n\n        # the external energy loss function has the higher priority\n        if energy_loss_fn is None:\n            energy_loss_fn = self.energy_loss\n        # energy prediction loss\n        energy_loss = energy_loss_fn(\n            pred=pred_energy, tgt=tgt_energy, tgt_len=tgt_energy_len\n        )\n\n        # the external duration loss function has the higher priority\n        if duration_loss_fn is None:\n            duration_loss_fn = self.duration_loss\n        # duration prediction loss, convert the target duration into the log domain\n        # note: predicted duration is already in the log domain\n        duration_loss = duration_loss_fn(\n            pred=pred_duration,\n            tgt=torch.log(tgt_duration.float() + 1),\n            tgt_len=tgt_duration_len,\n        )\n\n        # duration F1 score\n        tgt_duration_gate = tgt_duration == 0\n        duration_f1 = self.duration_f1(\n            pred=self.decoder.proc_duration(torch.exp(pred_duration) - 1) == 0,\n            tgt=tgt_duration_gate,\n            tgt_len=tgt_duration_len,\n        )\n\n        # calculate duration gate loss if pred_duration_gate is given\n        if pred_duration_gate is not None:\n            if duration_gate_loss_fn is None:\n                duration_gate_loss_fn = self.duration_gate_loss\n            duration_gate_loss = duration_gate_loss_fn(\n                pred=pred_duration_gate, tgt=tgt_duration_gate, tgt_len=tgt_duration_len\n            )\n        else:\n            duration_gate_loss = None\n\n        # .clone() here prevents the trainable variables from value modification\n        metrics = dict(\n            feat_loss_before=feat_loss_before.clone().detach(),\n            feat_loss_after=feat_loss_after.clone().detach(),\n            pitch_loss=pitch_loss.clone().detach(),\n            energy_loss=energy_loss.clone().detach(),\n            duration_loss=duration_loss.clone().detach(),\n            duration_f1=duration_f1.clone().detach(),\n        )\n\n        # combine all losses into the final one\n        loss = (\n            feat_loss_before\n            + feat_loss_after\n            + pitch_loss\n            + energy_loss\n            + duration_loss\n        )\n        if duration_gate_loss is not None:\n            loss += duration_gate_loss\n            metrics.update(duration_gate_loss=duration_gate_loss.clone().detach())\n        losses = dict(loss=loss)\n        metrics.update(loss=loss.clone().detach())\n\n        if self.training:\n            return losses, metrics\n        else:\n            return metrics\n\n    def visualize(\n        self,\n        epoch: int,\n        sample_index: str,\n        snapshot_interval: int = 1,\n        epoch_records: Dict = None,\n        domain: str = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        pitch: torch.Tensor = None,\n        pitch_len: torch.Tensor = None,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        duration: torch.Tensor = None,\n        duration_len: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n    ):\n\n        if len(self.visual_infer_conf) == 0:\n            self.visual_infer_conf = dict(\n                teacher_forcing=False, return_wav=False, return_feat=True\n            )\n\n        # obtain the inference results\n        infer_results = self.inference(\n            infer_conf=self.visual_infer_conf,\n            return_att=True,\n            text=text,\n            text_len=text_len,\n            duration=duration,\n            duration_len=duration_len,\n            feat=feat,\n            feat_len=feat_len,\n            pitch=pitch,\n            pitch_len=pitch_len,\n            spk_ids=spk_ids,\n            spk_feat=spk_feat,\n        )\n\n        # --- snapshot the objective metrics --- #\n        vis_logs = []\n\n        # --- snapshot the subjective metrics --- #\n        # record the input audio and real text at the first snapshotting step\n        if epoch // snapshot_interval == 1:\n            # # if the audio source is raw/wav\n            # if feat.size(-1) == 1:\n            #     vis_logs.append(\n            #         dict(\n            #             plot_type='audio', materials=dict(real_wav=copy.deepcopy(feat[0])),\n            #             sample_rate=self.sample_rate, audio_format=self.audio_format, subfolder_names=sample_index\n            #         )\n            #     )\n            # # if the audio source is audio feature (mel spectrogram etc)\n            # else:\n            #     vis_logs.append(\n            #         dict(\n            #             plot_type='matrix',\n            #             materials=dict(real_feat=copy.deepcopy(feat[0])),\n            #             epoch=epoch, sep_save=True, sum_save=False, data_save=True, flip_y=True,\n            #             subfolder_names=sample_index\n            #         )\n            #     )\n\n            # snapshot input text\n            vis_logs.append(\n                dict(\n                    materials=dict(\n                        real_text=[\n                            copy.deepcopy(self.tokenizer.tensor2text(text[0][1:-1]))\n                        ]\n                    ),\n                    plot_type=\"text\",\n                    subfolder_names=sample_index,\n                )\n            )\n\n        # snapshot the generated hypothesis acoustic features into a heatmap\n        vis_logs.append(\n            dict(\n                plot_type=\"matrix\",\n                materials=dict(\n                    hypo_feat=infer_results[\"feat\"][\"content\"][0].transpose()\n                ),\n                epoch=epoch,\n                sep_save=False,\n                sum_save=True,\n                data_save=True,\n                flip_y=True,\n                subfolder_names=[sample_index, \"hypo_feat\"],\n            )\n        )\n\n        # snapshot the predicted duration into a string\n        if \"duration\" not in epoch_records[sample_index].keys():\n            epoch_records[sample_index][\"duration\"] = []\n        epoch_records[sample_index][\"duration\"].append(\n            str(infer_results[\"duration\"][\"content\"][0])\n        )\n        # snapshot the information in the materials\n        vis_logs.append(\n            dict(\n                materials=dict(\n                    hypo_duration=copy.deepcopy(epoch_records[sample_index][\"duration\"])\n                ),\n                plot_type=\"text\",\n                epoch=epoch,\n                x_stride=snapshot_interval,\n                subfolder_names=sample_index,\n            )\n        )\n\n        # hypothesis attention matrix\n        infer_results[\"att\"] = self.attention_reshape(infer_results[\"att\"])\n        self.matrix_snapshot(\n            vis_logs=vis_logs,\n            hypo_attention=copy.deepcopy(infer_results[\"att\"]),\n            subfolder_names=sample_index,\n            epoch=epoch,\n        )\n        return vis_logs\n\n    @staticmethod\n    def generate_ctrl_alpha(\n        text: torch.Tensor,\n        alpha: float,\n        alpha_min: float,\n        alpha_max: float,\n        ctrl_duration: bool,\n        ctrl_energy: bool,\n        ctrl_pitch: bool,\n        ctrl_level: str,\n    ):\n        # initialize the alpha of duration for controllable TTS\n        duration_alpha = None\n        if ctrl_duration:\n            # random alpha\n            if alpha == 1.0:\n                # minus 2 to remove the influence of sos and eos in text\n                duration_alpha = torch.rand(\n                    size=(\n                        (text.size(0), 1)\n                        if ctrl_level == \"utterance\"\n                        else (text.size(0), text.size(1) - 2)\n                    )\n                )\n                duration_alpha = (alpha_max - alpha_min) * duration_alpha + alpha_min\n            # fixed alpha\n            else:\n                duration_alpha = torch.tensor(\n                    [alpha for _ in range(text.size(0))]\n                ).unsqueeze(-1)\n            # ensure the device consistency\n            if text.is_cuda:\n                duration_alpha = duration_alpha.cuda(text.device)\n\n        # initialize the alpha of energy for controllable TTS\n        energy_alpha = None\n        if ctrl_energy:\n            # random alpha\n            if alpha == 1.0:\n                # minus 2 to remove the influence of sos and eos in text\n                energy_alpha = torch.rand(\n                    size=(\n                        (text.size(0), 1)\n                        if ctrl_level == \"utterance\"\n                        else (text.size(0), text.size(1) - 2)\n                    )\n                )\n                energy_alpha = (alpha_max - alpha_min) * energy_alpha + alpha_min\n            # fixed alpha\n            else:\n                energy_alpha = torch.tensor(\n                    [alpha for _ in range(text.size(0))]\n                ).unsqueeze(-1)\n            # ensure the device consistency\n            if text.is_cuda:\n                energy_alpha = energy_alpha.cuda(text.device)\n\n        # initialize the alpha of pitch for controllable TTS\n        pitch_alpha = None\n        if ctrl_pitch:\n            # random alpha\n            if alpha == 1.0:\n                # minus 2 to remove the influence of sos and eos in text\n                pitch_alpha = torch.rand(\n                    size=(\n                        (text.size(0), 1)\n                        if ctrl_level == \"utterance\"\n                        else (text.size(0), text.size(1) - 2)\n                    )\n                )\n                pitch_alpha = (alpha_max - alpha_min) * pitch_alpha + alpha_min\n            # fixed alpha\n            else:\n                pitch_alpha = torch.tensor(\n                    [alpha for _ in range(text.size(0))]\n                ).unsqueeze(-1)\n            # ensure the device consistency\n            if text.is_cuda:\n                pitch_alpha = pitch_alpha.cuda(text.device)\n\n        return duration_alpha, pitch_alpha, energy_alpha\n\n    def inference(\n        self,\n        infer_conf: Dict,\n        text: torch.Tensor = None,\n        text_len: torch.Tensor = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        pitch: torch.Tensor = None,\n        pitch_len: torch.Tensor = None,\n        duration: torch.Tensor = None,\n        duration_len: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n        spk_feat_ids: List[str] = None,\n        domain: str = None,\n        return_att: bool = False,\n        return_feat: bool = False,\n        return_wav: bool = True,\n        use_before: bool = False,\n        vocoder: str = \"hifigan\",\n        teacher_forcing: bool = False,\n    ) -&gt; Dict[str, Dict[str, str or List]]:\n\n        assert text is not None and text_len is not None\n\n        # --- 0. Hyperparameter &amp; Model Preparation Stage --- #\n        # in-place replace infer_conf with its copy to protect the original information\n        infer_conf = copy.deepcopy(infer_conf)\n        # teacher_forcing in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"teacher_forcing\" in infer_conf.keys():\n            teacher_forcing = infer_conf.pop(\"teacher_forcing\")\n        if teacher_forcing:\n            assert (\n                feat is not None\n                and feat_len is not None\n                and duration is not None\n                and duration_len is not None\n            ), (\n                f\"If you want to decode {self.__class__.__name__} by the teacher-forcing technique, \"\n                f\"please give 'feat' and 'duration' in your data_cfg['test']!\"\n            )\n\n        # use_before in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"use_before\" in infer_conf.keys():\n            use_before = infer_conf.pop(\"use_before\")\n        # vocoder in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"vocoder\" in infer_conf.keys():\n            vocoder = infer_conf.pop(\"vocoder\")\n        # initialize the vocoder lazily\n        if not hasattr(self, \"vocode_func\"):\n            assert vocoder in [\n                \"gl\",\n                \"hifigan\",\n            ], f\"Currently, we only support 'gl' and 'hifigan' as vocoder, but got vocoder={vocoder}!\"\n            self.vocoder = vocoder\n            if self.vocoder == \"gl\":\n                self.vocode_func = self.decoder.feat_frontend.recover\n            else:\n                self.vocode_func = get_speechbrain_hifigan(\n                    device=self.device,\n                    sample_rate=self.sample_rate,\n                    use_multi_speaker=hasattr(self.decoder, \"spk_emb\"),\n                )\n\n        # return_wav in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"return_wav\" in infer_conf.keys():\n            return_wav = infer_conf.pop(\"return_wav\")\n        # if teacher_forcing is set, return_wav is default to False\n        elif teacher_forcing:\n            return_wav = False\n\n        # return_feat in infer_conf has the higher priority and will not be passed to self.module_forward()\n        if \"return_feat\" in infer_conf.keys():\n            return_feat = infer_conf.pop(\"return_feat\")\n\n        if not teacher_forcing:\n            assert (\n                return_wav or return_feat\n            ), \"return_wav and return_feat cannot be False at the same time.\"\n\n        # return_sr in infer_conf has the higher priority and will not be passed to self.module_forward()\n        return_sr = None\n        if \"return_sr\" in infer_conf.keys():\n            return_sr = infer_conf.pop(\"return_sr\")\n            assert return_sr &lt; self.sample_rate, (\n                f\"You should input 'return_sr' lower than the one of the model {self.sample_rate}, \"\n                f\"but got return_sr={return_sr}!\"\n            )\n            if not hasattr(self, \"resampler\"):\n                self.resampler = torchaudio.transforms.Resample(\n                    orig_freq=self.sample_rate, new_freq=return_sr\n                )\n                if text.is_cuda:\n                    self.resampler = self.resampler.cuda(text.device)\n\n        min_frame_num = (\n            infer_conf.pop(\"min_frame_num\")\n            if \"min_frame_num\" in infer_conf.keys()\n            else 0\n        )\n        max_frame_num = (\n            infer_conf.pop(\"max_frame_num\")\n            if \"max_frame_num\" in infer_conf.keys()\n            else 50\n        )\n\n        # arguments for controllable TTS if teacher-forcing is not used\n        if not teacher_forcing:\n            alpha = infer_conf.pop(\"alpha\") if \"alpha\" in infer_conf.keys() else 1.0\n            alpha_min = (\n                infer_conf.pop(\"alpha_min\") if \"alpha_min\" in infer_conf.keys() else 1.0\n            )\n            alpha_max = (\n                infer_conf.pop(\"alpha_max\") if \"alpha_max\" in infer_conf.keys() else 1.0\n            )\n            assert (alpha == 1.0) or (alpha_min == 1.0 and alpha_max == 1.0), (\n                \"(1) set alpha to a non-one float number; \"\n                \"(2) set alpha_min and/or alpha_max to non-one float numbers.\\n\"\n                \"You can only do one of them if you want to use controllable FastSpeech2, \"\n                f\"but got alpha={alpha}, alpha_min={alpha_min}, alpha_max={alpha_max}!\"\n            )\n            assert (\n                alpha_min &lt;= alpha_max\n            ), f\"alpha_min cannot be larger than alpha_max! Got alpha_min={alpha_min} and alpha_max={alpha_max}!\"\n\n            ctrl_duration = (\n                infer_conf.pop(\"ctrl_duration\")\n                if \"ctrl_duration\" in infer_conf.keys()\n                else False\n            )\n            ctrl_energy = (\n                infer_conf.pop(\"ctrl_energy\")\n                if \"ctrl_energy\" in infer_conf.keys()\n                else False\n            )\n            ctrl_pitch = (\n                infer_conf.pop(\"ctrl_pitch\")\n                if \"ctrl_pitch\" in infer_conf.keys()\n                else False\n            )\n            if (alpha != 1.0) ^ (alpha_min != 1.0 or alpha_max != 1.0):\n                assert ctrl_duration or ctrl_energy or ctrl_pitch, (\n                    \"If you want to use controllable FastSpeech2, \"\n                    \"please set at least one of the arguments 'ctrl_duration', 'ctrl_energy', 'ctrl_pitch' to True!\"\n                )\n\n            ctrl_level = (\n                infer_conf.pop(\"ctrl_level\")\n                if \"ctrl_level\" in infer_conf.keys()\n                else \"utterance\"\n            )\n            assert ctrl_level in [\n                \"utterance\",\n                \"token\",\n            ], f\"The argument ctrl_level should be either 'utterance' or 'token', but got ctrl_level={ctrl_level}!\"\n            if ctrl_level == \"token\" and alpha != 1.0:\n                raise ValueError(\n                    \"If you want to control TTS in the level of tokens, \"\n                    \"please use the arguments 'alpha_min' and 'alpha_max' instead of 'alpha'.\"\n                )\n\n            duration_alpha, pitch_alpha, energy_alpha = self.generate_ctrl_alpha(\n                text=text,\n                alpha=alpha,\n                alpha_min=alpha_min,\n                alpha_max=alpha_max,\n                ctrl_duration=ctrl_duration,\n                ctrl_pitch=ctrl_pitch,\n                ctrl_energy=ctrl_energy,\n                ctrl_level=ctrl_level,\n            )\n        # no controllable TTS is done if teacher-forcing is used\n        else:\n            duration_alpha, energy_alpha, pitch_alpha = None, None, None\n\n        if len(infer_conf) != 0:\n            raise RuntimeError(\n                f\"There are some unknown keys in infer_conf: {list(infer_conf.keys())}\"\n            )\n\n        # initialize the hypothesis variables\n        hypo_feat, hypo_feat_len, hypo_duration, feat_token_len_ratio, hypo_att = (\n            None,\n            None,\n            None,\n            None,\n            None,\n        )\n\n        # Multi-speaker TTS scenario\n        if hasattr(self.decoder, \"spk_emb\"):\n            batch_size = text.size(0)\n            # close-set multi-speaker TTS\n            if hasattr(self, \"idx2spk\"):\n                # randomly pick up training speakers as the reference speakers\n                if spk_ids is None:\n                    if not hasattr(self, \"spk_freq_dict\"):\n                        self.spk_freq_dict = {\n                            s_id: 0 for s_id in range(1, len(self.idx2spk) + 1)\n                        }\n                    spk_ids, self.spk_freq_dict = get_min_indices_by_freq(\n                        self.spk_freq_dict,\n                        chosen_idx_num=batch_size,\n                        freq_weights=to_cpu(text_len),\n                    )\n                    spk_ids = torch.LongTensor(spk_ids).to(text.device)\n\n            # open-set multi-speaker TTS\n            else:\n                # use random vectors as the reference speaker embedding if spk_feat is not given\n                if spk_feat is None:\n                    # the random spk_feat obey normal distribution\n                    spk_feat = torch.randn(\n                        (batch_size, self.decoder.spk_emb.spk_emb_dim),\n                        device=text.device,\n                    )\n                    spk_feat_ids = [\"rand_spk\" for _ in range(batch_size)]\n\n        # copy the input data in advance for data safety\n        model_input, outputs = copy.deepcopy(dict(text=text, text_len=text_len)), dict()\n        # remove the sos at the beginning and eos at the end after copying\n        text_len -= 2\n\n        # Self Decoding or Teacher Forcing\n        infer_results = self.module_forward(\n            duration=duration if teacher_forcing else None,\n            duration_len=duration_len if teacher_forcing else None,\n            feat=feat if teacher_forcing else None,\n            feat_len=feat_len if teacher_forcing else None,\n            pitch=pitch if teacher_forcing else None,\n            pitch_len=pitch_len if teacher_forcing else None,\n            spk_feat=spk_feat,\n            spk_ids=spk_ids,\n            return_att=return_att,\n            min_frame_num=min_frame_num,\n            max_frame_num=max_frame_num,\n            duration_alpha=duration_alpha,\n            energy_alpha=energy_alpha,\n            pitch_alpha=pitch_alpha,\n            **model_input,\n        )\n        # return the attention matrices\n        if return_att:\n            hypo_att = infer_results[\"att\"]\n\n        # return the teacher-forcing criterion\n        if teacher_forcing:\n            criterion_results = self.criterion_forward(**infer_results)\n            outputs.update(\n                {\n                    cri_name: dict(format=\"txt\", content=to_cpu(tensor_result))\n                    for cri_name, tensor_result in criterion_results.items()\n                }\n            )\n        else:\n            # pred_duration is the duration in log scale\n            hypo_duration = infer_results[\"tgt_duration\"]\n            # convert the duration into integers for the hard regulation\n            hypo_duration = [\n                hypo_duration[i][: text_len[i]].type(torch.int)\n                for i in range(len(hypo_duration))\n            ]\n            outputs.update(duration=dict(format=\"txt\", content=to_cpu(hypo_duration)))\n\n        hypo_feat = infer_results[\n            \"pred_feat_before\" if use_before else \"pred_feat_after\"\n        ]\n        hypo_feat_len = infer_results[\n            \"tgt_feat_len\" if teacher_forcing else \"pred_feat_len\"\n        ]\n        # hypo_feat &amp; hypo_feat_len recovery by reduction_factor\n        if self.reduction_factor &gt; 1:\n            batch_size, feat_dim = hypo_feat.size(0), hypo_feat.size(-1)\n            hypo_feat = hypo_feat.reshape(\n                batch_size,\n                hypo_feat.size(1) * self.reduction_factor,\n                feat_dim // self.reduction_factor,\n            )\n            hypo_feat_len *= self.reduction_factor\n\n        # denormalize the acoustic feature if needed\n        if hasattr(self.decoder, \"feat_normalize\"):\n            hypo_feat = self.decoder.feat_normalize.recover(\n                hypo_feat, group_ids=spk_ids\n            )\n\n        # turn the tensor-like spk_ids (preprocessed by self.spk2idx) into a list\n        if isinstance(spk_ids, torch.Tensor):\n            spk_ids = [\n                self.idx2spk[s_id.item()] if s_id != 0 else \"aver_spk\"\n                for s_id in spk_ids\n            ]\n\n        # calculate the Frame-to-Token ratio\n        feat_token_len_ratio = hypo_feat_len / (text_len + 1e-10)\n\n        # convert the acoustic features back to GL waveforms if specified\n        if return_wav:\n            try:\n                hypo_wav, hypo_wav_len = self.vocode_func(hypo_feat, hypo_feat_len)\n            # do not save waveforms if there is a RuntimeError\n            except RuntimeError:\n                pass\n            # save waveforms if no error happen\n            else:\n                # remove the redundant silence parts at the end of the synthetic waveforms\n                hypo_wav = [\n                    (\n                        hypo_wav[i][: hypo_wav_len[i]]\n                        if return_sr is None\n                        else self.resampler(hypo_wav[i][: hypo_wav_len[i]].squeeze(-1))\n                    )\n                    for i in range(len(hypo_wav))\n                ]\n                hypo_wav_len = [wav.size(0) for wav in hypo_wav]\n\n                # the sampling rate of the waveforms will be changed to return_sr\n                outputs[\"wav\"] = dict(\n                    format=\"wav\",\n                    sample_rate=self.sample_rate if return_sr is None else return_sr,\n                    group_ids=spk_ids,\n                    content=to_cpu(hypo_wav, tgt=\"numpy\"),\n                )\n                outputs[\"wav_len\"] = dict(format=\"txt\", content=to_cpu(hypo_wav_len))\n\n        # return the acoustic features if specified\n        if return_feat:\n            # remove the redundant silence parts at the end of the synthetic frames\n            hypo_feat = [\n                hypo_feat[i][: hypo_feat_len[i]] for i in range(len(hypo_feat))\n            ]\n            outputs.update(\n                # the sampling rate of the acoustic features remain the one of the TTS model\n                feat=dict(\n                    format=\"npz\",\n                    sample_rate=self.sample_rate,\n                    group_ids=spk_ids,\n                    content=to_cpu(hypo_feat, tgt=\"numpy\"),\n                ),\n                feat_len=dict(format=\"txt\", content=to_cpu(hypo_feat_len)),\n            )\n        outputs.update(\n            feat_token_len_ratio=dict(\n                format=\"txt\", content=to_cpu(feat_token_len_ratio)\n            )\n        )\n        # record the alpha values for controllable TTS\n        if duration_alpha is not None:\n            outputs.update(\n                duration_alpha=dict(\n                    format=\"txt\",\n                    content=[\n                        d_a[0] if len(d_a) == 1 else str([round(i, 2) for i in d_a])\n                        for d_a in to_cpu(duration_alpha)\n                    ],\n                )\n            )\n        if energy_alpha is not None:\n            outputs.update(\n                energy_alpha=dict(\n                    format=\"txt\",\n                    content=[\n                        e_a[0] if len(e_a) == 1 else str([round(i, 2) for i in e_a])\n                        for e_a in to_cpu(energy_alpha)\n                    ],\n                )\n            )\n        if pitch_alpha is not None:\n            outputs.update(\n                pitch_alpha=dict(\n                    format=\"txt\",\n                    content=[\n                        p_a[0] if len(p_a) == 1 else str([round(i, 2) for i in p_a])\n                        for p_a in to_cpu(pitch_alpha)\n                    ],\n                )\n            )\n\n        # record the speaker ID used as the reference\n        outputs.update(\n            ref_spk=dict(\n                format=\"txt\",\n                content=(\n                    spk_ids\n                    if spk_ids is not None\n                    else [\"single_speaker\" for _ in range(text.size(0))]\n                ),\n            )\n        )\n        # record the speaker embedding ID used as the reference\n        if spk_feat_ids is not None:\n            outputs.update(ref_spk_feat=dict(format=\"txt\", content=spk_feat_ids))\n\n        # evaluation reports for all the testing instances\n        instance_report_dict = {}\n        # loop each utterance\n        for i in range(len(text)):\n            if \"Feature-Token Length Ratio\" not in instance_report_dict.keys():\n                instance_report_dict[\"Feature-Token Length Ratio\"] = []\n            instance_report_dict[\"Feature-Token Length Ratio\"].append(\n                f\"{feat_token_len_ratio[i]:.2f}\"\n            )\n\n            if \"Feature Length\" not in instance_report_dict.keys():\n                instance_report_dict[\"Feature Length\"] = []\n            instance_report_dict[\"Feature Length\"].append(f\"{hypo_feat_len[i]:d}\")\n        # register the instance reports for generating instance_reports.md\n        self.register_instance_reports(md_list_dict=instance_report_dict)\n\n        # add the attention matrix into the output Dict, only used for model visualization during training\n        # because it will consume too much time for saving the attention matrices of all testing samples during testing\n        if return_att:\n            outputs.update(att=hypo_att)\n        return outputs\n</code></pre>"},{"location":"reference/model/nar_tts/#model.nar_tts.FastSpeech2.module_forward","title":"<code>module_forward(epoch=None, text=None, text_len=None, duration=None, duration_len=None, pitch=None, pitch_len=None, feat=None, feat_len=None, energy=None, energy_len=None, spk_feat=None, spk_ids=None, return_att=False, min_frame_num=0, max_frame_num=None, duration_alpha=None, energy_alpha=None, pitch_alpha=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input speech data (grouped or downsampled and edge-padded).</p> <code>None</code> <code>feat_len</code> <code>Tensor</code> <p>(batch,) The lengths of input speech data</p> <code>None</code> <code>text</code> <code>Tensor</code> <p>(batch, text_maxlen) The input text data with  at the beginning and end <code>None</code> <code>text_len</code> <code>Tensor</code> <p>(batch,) The lengths of input text data</p> <code>None</code> <code>duration</code> <code>Tensor</code> <p>(batch, text_maxlen) The duration data for each token in text.</p> <code>None</code> <code>duration_len</code> <code>Tensor</code> <p>(batch,) The lengths of input duration data</p> <code>None</code> <code>pitch</code> <code>Tensor</code> <p>(batch, text_maxlen) The pitch data for each token in text.</p> <code>None</code> <code>pitch_len</code> <code>Tensor</code> <p>(batch,) The lengths of input pitch data</p> <code>None</code> <code>energy</code> <code>Tensor</code> <p>(batch, text_maxlen) The energy data for each token in text.</p> <code>None</code> <code>energy_len</code> <code>Tensor</code> <p>(batch,) The lengths of input energy data</p> <code>None</code> <code>spk_feat</code> <code>Tensor</code> <p>(batch, 1, speaker embedding dim) Pre-extracted speaker embedding. (None means single-speaker TTS)</p> <code>None</code> <code>spk_ids</code> <code>Tensor</code> <p>(batch,) The speaker ids of each speech data. In the form of integer values.</p> <code>None</code> <code>epoch</code> <code>int</code> <p>int The number of the current training epoch. Mainly used for mean&amp;std calculation in the feature normalization</p> <code>None</code> <code>return_att</code> <code>bool</code> <p>bool Controls whether the attention matrices of each layer in the encoder and decoder will be returned.</p> <code>False</code> <code>duration_alpha</code> <code>Tensor</code> <code>None</code> <code>energy_alpha</code> <code>Tensor</code> <code>None</code> <code>pitch_alpha</code> <code>Tensor</code> <code>None</code> <code>kwargs</code> <p>Temporary register used to store the redundant arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses</p> Source code in <code>speechain/model/nar_tts.py</code> <pre><code>def module_forward(\n    self,\n    epoch: int = None,\n    text: torch.Tensor = None,\n    text_len: torch.Tensor = None,\n    duration: torch.Tensor = None,\n    duration_len: torch.Tensor = None,\n    pitch: torch.Tensor = None,\n    pitch_len: torch.Tensor = None,\n    feat: torch.Tensor = None,\n    feat_len: torch.Tensor = None,\n    energy: torch.Tensor = None,\n    energy_len: torch.Tensor = None,\n    spk_feat: torch.Tensor = None,\n    spk_ids: torch.Tensor = None,\n    return_att: bool = False,\n    min_frame_num: int = 0,\n    max_frame_num: int = None,\n    duration_alpha: torch.Tensor = None,\n    energy_alpha: torch.Tensor = None,\n    pitch_alpha: torch.Tensor = None,\n    **kwargs,\n) -&gt; Dict:\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input speech data (grouped or downsampled and edge-padded).\n        feat_len: (batch,)\n            The lengths of input speech data\n        text: (batch, text_maxlen)\n            The input text data with &lt;sos/eos&gt; at the beginning and end\n        text_len: (batch,)\n            The lengths of input text data\n        duration: (batch, text_maxlen)\n            The duration data for each token in text.\n        duration_len: (batch,)\n            The lengths of input duration data\n        pitch: (batch, text_maxlen)\n            The pitch data for each token in text.\n        pitch_len: (batch,)\n            The lengths of input pitch data\n        energy: (batch, text_maxlen)\n            The energy data for each token in text.\n        energy_len: (batch,)\n            The lengths of input energy data\n        spk_feat: (batch, 1, speaker embedding dim)\n            Pre-extracted speaker embedding. (None means single-speaker TTS)\n        spk_ids: (batch,)\n            The speaker ids of each speech data. In the form of integer values.\n        epoch: int\n            The number of the current training epoch.\n            Mainly used for mean&amp;std calculation in the feature normalization\n        return_att: bool\n            Controls whether the attention matrices of each layer in the encoder and decoder will be returned.\n        # Arguments for controllable TTS received from self.inference()\n        duration_alpha:\n        energy_alpha:\n        pitch_alpha:\n        kwargs:\n            Temporary register used to store the redundant arguments.\n\n    Returns:\n        A dictionary containing all the TTS model outputs (feature, eos bernouli prediction) necessary to calculate the losses\n\n    \"\"\"\n    # text checking\n    assert text is not None and text_len is not None\n    assert text_len.size(0) == text.size(\n        0\n    ), \"The amounts of sentences and their lengths are not equal to each other.\"\n    # feat checking\n    if feat is not None and feat_len is not None:\n        assert feat.size(0) == text.size(0) and feat_len.size(0) == text_len.size(\n            0\n        ), \"The amounts of feat and text are not equal to each other.\"\n        assert feat_len.size(0) == feat.size(\n            0\n        ), \"The amounts of feat and their lengths are not equal to each other.\"\n    elif (feat is None) ^ (feat_len is None):\n        raise RuntimeError(\n            f\"In {self.__class__.__name__}, \"\n            f\"feat and feat_len must be None or not None at the same time! \"\n            f\"But got feat={feat} and feat_len={feat_len}.\"\n        )\n    # pitch checking\n    if pitch is not None and pitch_len is not None:\n        assert pitch.size(0) == feat.size(0) and pitch_len.size(0) == feat_len.size(\n            0\n        ), \"The amounts of pitch and feat are not equal to each other.\"\n        assert pitch_len.size(0) == pitch.size(\n            0\n        ), \"The amounts of pitch and their lengths are not equal to each other.\"\n    elif (pitch is None) ^ (pitch_len is None):\n        raise RuntimeError(\n            f\"In {self.__class__.__name__}, \"\n            f\"pitch and pitch_len must be None or not None at the same time! \"\n            f\"But got pitch={pitch} and pitch_len={pitch_len}.\"\n        )\n    # energy checking\n    if energy is not None and energy_len is not None:\n        assert energy.size(0) == feat.size(0) and energy_len.size(\n            0\n        ) == feat_len.size(\n            0\n        ), \"The amounts of energy and feat are not equal to each other.\"\n        assert energy_len.size(0) == energy.size(\n            0\n        ), \"The amounts of energy and their lengths are not equal to each other.\"\n    elif (energy is None) ^ (energy_len is None):\n        raise RuntimeError(\n            f\"In {self.__class__.__name__}, \"\n            f\"energy and energy_len must be None or not None at the same time! \"\n            f\"But got energy={energy} and energy_len={energy_len}.\"\n        )\n\n    # text preprocessing before duration checking\n    # remove the &lt;sos/eos&gt; at the beginning and the end of each sentence\n    for i in range(text_len.size(0)):\n        text[i, text_len[i] - 1] = self.tokenizer.ignore_idx\n    text, text_len = text[:, 1:-1], text_len - 2\n\n    # duration checking\n    if duration is not None and duration_len is not None:\n        assert duration.size(0) == text.size(\n            0\n        ), \"The amounts of durations and text are not equal to each other.\"\n        assert duration_len.size(0) == text_len.size(\n            0\n        ), \"The amounts of durations and text lengths are not equal to each other.\"\n        # check the length of duration and text\n        assert False not in [\n            len(text[i]) == len(duration[i]) for i in range(len(text))\n        ], \"The lengths of individual duration and text data don't match with each other.\"\n    elif (duration is None) ^ (duration_len is None):\n        raise RuntimeError(\n            f\"In {self.__class__.__name__}, \"\n            f\"duration and duration_len must be None or not None at the same time! \"\n            f\"But got duration={duration} and duration_len={duration_len}.\"\n        )\n\n    # Encoding the text data\n    enc_text, enc_text_mask, enc_attmat, enc_hidden = self.encoder(\n        text=text, text_len=text_len\n    )\n\n    # Decoding\n    (\n        pred_feat_before,\n        pred_feat_after,\n        pred_feat_len,\n        tgt_feat,\n        tgt_feat_len,\n        pred_pitch,\n        tgt_pitch,\n        tgt_pitch_len,\n        pred_energy,\n        tgt_energy,\n        tgt_energy_len,\n        pred_duration,\n        pred_duration_gate,\n        tgt_duration,\n        tgt_duration_len,\n        dec_attmat,\n        dec_hidden,\n    ) = self.decoder(\n        enc_text=enc_text,\n        enc_text_mask=enc_text_mask,\n        duration=duration,\n        duration_len=duration_len,\n        feat=feat,\n        feat_len=feat_len,\n        pitch=pitch,\n        pitch_len=pitch_len,\n        energy=energy,\n        energy_len=energy_len,\n        spk_feat=spk_feat,\n        spk_ids=spk_ids,\n        epoch=epoch,\n        min_frame_num=min_frame_num,\n        max_frame_num=max_frame_num,\n        duration_alpha=duration_alpha,\n        energy_alpha=energy_alpha,\n        pitch_alpha=pitch_alpha,\n    )\n\n    # initialize the TTS output to be the decoder predictions\n    outputs = dict(\n        pred_feat_before=pred_feat_before,\n        pred_feat_after=pred_feat_after,\n        pred_feat_len=pred_feat_len,\n        tgt_feat=tgt_feat,\n        tgt_feat_len=tgt_feat_len,\n        pred_pitch=pred_pitch,\n        tgt_pitch=tgt_pitch,\n        tgt_pitch_len=tgt_pitch_len,\n        pred_energy=pred_energy,\n        tgt_energy=tgt_energy,\n        tgt_energy_len=tgt_energy_len,\n        pred_duration=pred_duration,\n        pred_duration_gate=pred_duration_gate,\n        tgt_duration=tgt_duration,\n        tgt_duration_len=tgt_duration_len,\n    )\n\n    def shrink_attention(input_att_list):\n        # pick up the target attention layers\n        if (\n            self.return_att_layer_num != -1\n            and len(input_att_list) &gt; self.return_att_layer_num\n        ):\n            input_att_list = input_att_list[-self.return_att_layer_num :]\n        # pick up the target attention heads\n        if (\n            self.return_att_head_num != -1\n            and input_att_list[0].size(1) &gt; self.return_att_head_num\n        ):\n            input_att_list = [\n                att[:, : self.return_att_head_num] for att in input_att_list\n            ]\n        return input_att_list\n\n    # return the attention results if specified\n    if return_att:\n        # encoder self-attention\n        if enc_attmat is not None and \"enc\" in self.return_att_type:\n            outputs.update(att=dict(enc=shrink_attention(enc_attmat)))\n        # decoder self-attention\n        if dec_attmat is not None and \"dec\" in self.return_att_type:\n            outputs[\"att\"].update(dec=shrink_attention(dec_attmat))\n    return outputs\n</code></pre>"},{"location":"reference/module/","title":"module","text":""},{"location":"reference/module/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/abs/#module.abs.Module","title":"<code>Module</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Module is the base class for all Module objects in this toolkit. For all the Model objects in this toolkit, their neural networks are constructed by many Module objects in a nested structure. Below is an example of the ASR model: <pre><code>ASR (Model)\n    ---&gt; ASREncoder (Module)\n        ---&gt; Speech2MelSpec (Module)\n            ---&gt; Speech2LinearSpec (Module)\n            ---&gt; LinearSpec2MelSpec (Module)\n        ---&gt; Conv2dPrenet (Module)\n            ---&gt; LinearPrenet (Module)\n        ---&gt; TransformerEncoder (Module)\n            ---&gt; PositionalEncoding (Module)\n            ---&gt; MultiHeadedAttention (Module)\n            ---&gt; PositionwiseFeedForward (Module)\n    ---&gt; ASRDecoder (Module)\n        ---&gt; EmbedPrenet (Module)\n        ---&gt; TransformerDecoder (Module)\n            ---&gt; PositionalEncoding (Module)\n            ---&gt; MultiHeadedAttention (Module)\n            ---&gt; PositionwiseFeedForward (Module)\n        ---&gt; TokenPostnet (Module)\n</code></pre></p> <p>This base class has two required abstract interface functions that must be overriden by all Module subclasses: module_init() and forward(). module_init() is for module initialization and forward() is for output calculation.</p> Source code in <code>speechain/module/abs.py</code> <pre><code>class Module(torch.nn.Module, ABC):\n    \"\"\"\n    Module is the base class for all Module objects in this toolkit. For all the Model objects in this toolkit, their\n    neural networks are constructed by many Module objects in a nested structure.\n    Below is an example of the ASR model:\n    ```\n    ASR (Model)\n        ---&gt; ASREncoder (Module)\n            ---&gt; Speech2MelSpec (Module)\n                ---&gt; Speech2LinearSpec (Module)\n                ---&gt; LinearSpec2MelSpec (Module)\n            ---&gt; Conv2dPrenet (Module)\n                ---&gt; LinearPrenet (Module)\n            ---&gt; TransformerEncoder (Module)\n                ---&gt; PositionalEncoding (Module)\n                ---&gt; MultiHeadedAttention (Module)\n                ---&gt; PositionwiseFeedForward (Module)\n        ---&gt; ASRDecoder (Module)\n            ---&gt; EmbedPrenet (Module)\n            ---&gt; TransformerDecoder (Module)\n                ---&gt; PositionalEncoding (Module)\n                ---&gt; MultiHeadedAttention (Module)\n                ---&gt; PositionwiseFeedForward (Module)\n            ---&gt; TokenPostnet (Module)\n    ```\n\n    This base class has two required abstract interface functions that must be overriden by all Module subclasses:\n    module_init() and forward(). module_init() is for module initialization and forward() is for output calculation.\n\n    \"\"\"\n\n    def __init__(\n        self, input_size: int = None, distributed: bool = False, **module_conf\n    ):\n        \"\"\"This initialization function is shared by all _Module_ subclasses.\n\n        There are two built-in variable members: `input_size` and `output_size`. `input_size` is the last dimension of\n        the input tensor while `output_size` is the last dimension of the output tensor.\n\n        These two member variables serve as the socket and plug that are used to communicate with the front and back\n        Module objects in a Model object.\n\n        You could utilize `self.input_size` in your `module_init()` implement to initialize your module and give the\n        output data dimension to `self.output_size`.\n\n        Note:\n        The usage of these two member variables is not mandatory, but it would be a convenient way for you to\n        initialize your module.\n\n        Args:\n            input_size: int = None\n                The last dimension of the tensor from the front _Module_ object.\n                If not given, this argument would be None.\n            distributed: bool = False\n                Whether the _Model_ object this _Module_ object is belong to is distributed to multiple GPUs.\n            **module_conf:\n                The arguments used by `module_init()` for your customized _Module_ initialization.\n        \"\"\"\n        super(Module, self).__init__()\n\n        # shared general members\n        self.input_size = input_size\n        self.output_size = None\n        self.distributed = distributed\n\n        # customized initialization\n        self.module_init(**module_conf)\n\n    @abstractmethod\n    def module_init(self, **module_conf):\n        \"\"\"Abstract interface function for customized initialization of each _Module_\n        subclass. This interface function is mandatory to be overridden by your\n        implementation.\n\n        Args:\n            **module_conf:\n                The arguments used for customized Module initialization.\n                For more details, please refer to the docstring of your target Module subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(self, **kwargs):\n        \"\"\"This abstract interface function is the customized implementation of\n        `torch.nn.Module.forward()` used during model forward calculation. This\n        interface function is mandatory to be overridden by your implementation.\n\n        Args:\n            **kwargs:\n                The input arguments for module forward calculation.\n                For more details, please refer to the docstring of `forward()` of your target _Module_ subclass.\n\n        Returns:\n            Module forward calculation results.\n            For more details, please refer to the docstring of `forward()` of your target _Module_ subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def recover(self, **kwargs):\n        \"\"\"This interface function is used to recover the module forward calculation\n        results back to the input data. It can be considered as the reverse process of\n        `forward()`. This interface function is not mandatory to be overridden.\n\n        Args:\n            **kwargs:\n                The input forward calculation results to be recovered.\n                For more details, please refer to the docstring of `recover()` of your target _Module_ subclass.\n\n        Returns:\n            The recovered data or closely-recovered data (sometimes `forward()` may not be totally recoverable).\n            For more details, please refer to the docstring of `recover()` of your target _Module_ subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset_parameters(self):\n        \"\"\"This abstract interface function is used to initialize the customized\n        parameters in the _Module_ subclass if had. Some _Module_ subclasses have their\n        customized parameters with specific initialization functions.\n\n        If your _Module_ implementation has some customized parameters and you want to initialize them by yourself,\n        please give the initialization logic in this interface function.\n\n        This interface function is not mandatory to be overridden.\n        Note: Don't forget to add `self.default_init_modules.append(YourModule)` in `model_init()` of your _Model_.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_recordable_para(self) -&gt; Dict or None:\n        \"\"\"This function returns the parameters of the module that you want to record as\n        part of step information.\n\n        If you want to record the value of the customized parameters of your module:\n\n        1. when it is a leaf (no _Module_ members) in the nested _Module_ tree of the model, please override this\n            function and return the parameter values in a _Dict_.\n            For an example, you can refer to [${SPEECHAIN_ROOT}/speechain/module/transformer/pos_enc.py]().\n\n        2. when it is a non-leaf (with _Module_ members) in the nested _Module_ tree of the model, please follow the pseudocode below:\n\n         &gt;&gt;&gt; class YourModule(Module):\n         ...   def get_recordable_para(self) -&gt; Dict or None:\n         ...      output = dict()\n         ...      # add the value of your target parameter into the output as key-value items\n         ...      output.update(super(YourModule, self).get_recordable_para())\n         ...      return output\n\n        Returns: Dict or None\n            For the leaf module, the default implementation returns None;\n            For the non-leaf module, the default implementation returns a Dict containing names and recordable\n            parameters of its member modules.\n        \"\"\"\n        # for the leaf module, the default implementation returns None\n        if sum([isinstance(module, Module) for module in self._modules.values()]) == 0:\n            return None\n        # for the non-leaf module, return a Dict containing names and recordable parameters of its member modules\n        else:\n            return {\n                name: module.get_recordable_para()\n                for name, module in self._modules.items()\n                if isinstance(module, Module)\n            }\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.__init__","title":"<code>__init__(input_size=None, distributed=False, **module_conf)</code>","text":"<p>This initialization function is shared by all Module subclasses.</p> <p>There are two built-in variable members: <code>input_size</code> and <code>output_size</code>. <code>input_size</code> is the last dimension of the input tensor while <code>output_size</code> is the last dimension of the output tensor.</p> <p>These two member variables serve as the socket and plug that are used to communicate with the front and back Module objects in a Model object.</p> <p>You could utilize <code>self.input_size</code> in your <code>module_init()</code> implement to initialize your module and give the output data dimension to <code>self.output_size</code>.</p> <p>Note: The usage of these two member variables is not mandatory, but it would be a convenient way for you to initialize your module.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>int = None The last dimension of the tensor from the front Module object. If not given, this argument would be None.</p> <code>None</code> <code>distributed</code> <code>bool</code> <p>bool = False Whether the Model object this Module object is belong to is distributed to multiple GPUs.</p> <code>False</code> <code>**module_conf</code> <p>The arguments used by <code>module_init()</code> for your customized Module initialization.</p> <code>{}</code> Source code in <code>speechain/module/abs.py</code> <pre><code>def __init__(\n    self, input_size: int = None, distributed: bool = False, **module_conf\n):\n    \"\"\"This initialization function is shared by all _Module_ subclasses.\n\n    There are two built-in variable members: `input_size` and `output_size`. `input_size` is the last dimension of\n    the input tensor while `output_size` is the last dimension of the output tensor.\n\n    These two member variables serve as the socket and plug that are used to communicate with the front and back\n    Module objects in a Model object.\n\n    You could utilize `self.input_size` in your `module_init()` implement to initialize your module and give the\n    output data dimension to `self.output_size`.\n\n    Note:\n    The usage of these two member variables is not mandatory, but it would be a convenient way for you to\n    initialize your module.\n\n    Args:\n        input_size: int = None\n            The last dimension of the tensor from the front _Module_ object.\n            If not given, this argument would be None.\n        distributed: bool = False\n            Whether the _Model_ object this _Module_ object is belong to is distributed to multiple GPUs.\n        **module_conf:\n            The arguments used by `module_init()` for your customized _Module_ initialization.\n    \"\"\"\n    super(Module, self).__init__()\n\n    # shared general members\n    self.input_size = input_size\n    self.output_size = None\n    self.distributed = distributed\n\n    # customized initialization\n    self.module_init(**module_conf)\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.forward","title":"<code>forward(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>This abstract interface function is the customized implementation of <code>torch.nn.Module.forward()</code> used during model forward calculation. This interface function is mandatory to be overridden by your implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The input arguments for module forward calculation. For more details, please refer to the docstring of <code>forward()</code> of your target Module subclass.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Module forward calculation results.</p> <p>For more details, please refer to the docstring of <code>forward()</code> of your target Module subclass.</p> Source code in <code>speechain/module/abs.py</code> <pre><code>@abstractmethod\ndef forward(self, **kwargs):\n    \"\"\"This abstract interface function is the customized implementation of\n    `torch.nn.Module.forward()` used during model forward calculation. This\n    interface function is mandatory to be overridden by your implementation.\n\n    Args:\n        **kwargs:\n            The input arguments for module forward calculation.\n            For more details, please refer to the docstring of `forward()` of your target _Module_ subclass.\n\n    Returns:\n        Module forward calculation results.\n        For more details, please refer to the docstring of `forward()` of your target _Module_ subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.get_recordable_para","title":"<code>get_recordable_para()</code>","text":"<p>This function returns the parameters of the module that you want to record as part of step information.</p> <p>If you want to record the value of the customized parameters of your module:</p> <ol> <li> <p>when it is a leaf (no Module members) in the nested Module tree of the model, please override this     function and return the parameter values in a Dict.     For an example, you can refer to ${SPEECHAIN_ROOT}/speechain/module/transformer/pos_enc.py.</p> </li> <li> <p>when it is a non-leaf (with Module members) in the nested Module tree of the model, please follow the pseudocode below:</p> </li> </ol> <p>class YourModule(Module):  ...   def get_recordable_para(self) -&gt; Dict or None:  ...      output = dict()  ...      # add the value of your target parameter into the output as key-value items  ...      output.update(super(YourModule, self).get_recordable_para())  ...      return output</p> <p>Dict or None</p> Type Description <code>Dict or None</code> <p>For the leaf module, the default implementation returns None;</p> <code>Dict or None</code> <p>For the non-leaf module, the default implementation returns a Dict containing names and recordable</p> <code>Dict or None</code> <p>parameters of its member modules.</p> Source code in <code>speechain/module/abs.py</code> <pre><code>def get_recordable_para(self) -&gt; Dict or None:\n    \"\"\"This function returns the parameters of the module that you want to record as\n    part of step information.\n\n    If you want to record the value of the customized parameters of your module:\n\n    1. when it is a leaf (no _Module_ members) in the nested _Module_ tree of the model, please override this\n        function and return the parameter values in a _Dict_.\n        For an example, you can refer to [${SPEECHAIN_ROOT}/speechain/module/transformer/pos_enc.py]().\n\n    2. when it is a non-leaf (with _Module_ members) in the nested _Module_ tree of the model, please follow the pseudocode below:\n\n     &gt;&gt;&gt; class YourModule(Module):\n     ...   def get_recordable_para(self) -&gt; Dict or None:\n     ...      output = dict()\n     ...      # add the value of your target parameter into the output as key-value items\n     ...      output.update(super(YourModule, self).get_recordable_para())\n     ...      return output\n\n    Returns: Dict or None\n        For the leaf module, the default implementation returns None;\n        For the non-leaf module, the default implementation returns a Dict containing names and recordable\n        parameters of its member modules.\n    \"\"\"\n    # for the leaf module, the default implementation returns None\n    if sum([isinstance(module, Module) for module in self._modules.values()]) == 0:\n        return None\n    # for the non-leaf module, return a Dict containing names and recordable parameters of its member modules\n    else:\n        return {\n            name: module.get_recordable_para()\n            for name, module in self._modules.items()\n            if isinstance(module, Module)\n        }\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.module_init","title":"<code>module_init(**module_conf)</code>  <code>abstractmethod</code>","text":"<p>Abstract interface function for customized initialization of each Module subclass. This interface function is mandatory to be overridden by your implementation.</p> <p>Parameters:</p> Name Type Description Default <code>**module_conf</code> <p>The arguments used for customized Module initialization. For more details, please refer to the docstring of your target Module subclass.</p> <code>{}</code> Source code in <code>speechain/module/abs.py</code> <pre><code>@abstractmethod\ndef module_init(self, **module_conf):\n    \"\"\"Abstract interface function for customized initialization of each _Module_\n    subclass. This interface function is mandatory to be overridden by your\n    implementation.\n\n    Args:\n        **module_conf:\n            The arguments used for customized Module initialization.\n            For more details, please refer to the docstring of your target Module subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.recover","title":"<code>recover(**kwargs)</code>","text":"<p>This interface function is used to recover the module forward calculation results back to the input data. It can be considered as the reverse process of <code>forward()</code>. This interface function is not mandatory to be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The input forward calculation results to be recovered. For more details, please refer to the docstring of <code>recover()</code> of your target Module subclass.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The recovered data or closely-recovered data (sometimes <code>forward()</code> may not be totally recoverable).</p> <p>For more details, please refer to the docstring of <code>recover()</code> of your target Module subclass.</p> Source code in <code>speechain/module/abs.py</code> <pre><code>def recover(self, **kwargs):\n    \"\"\"This interface function is used to recover the module forward calculation\n    results back to the input data. It can be considered as the reverse process of\n    `forward()`. This interface function is not mandatory to be overridden.\n\n    Args:\n        **kwargs:\n            The input forward calculation results to be recovered.\n            For more details, please refer to the docstring of `recover()` of your target _Module_ subclass.\n\n    Returns:\n        The recovered data or closely-recovered data (sometimes `forward()` may not be totally recoverable).\n        For more details, please refer to the docstring of `recover()` of your target _Module_ subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/module/abs/#module.abs.Module.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>This abstract interface function is used to initialize the customized parameters in the Module subclass if had. Some Module subclasses have their customized parameters with specific initialization functions.</p> <p>If your Module implementation has some customized parameters and you want to initialize them by yourself, please give the initialization logic in this interface function.</p> <p>This interface function is not mandatory to be overridden. Note: Don't forget to add <code>self.default_init_modules.append(YourModule)</code> in <code>model_init()</code> of your Model.</p> Source code in <code>speechain/module/abs.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"This abstract interface function is used to initialize the customized\n    parameters in the _Module_ subclass if had. Some _Module_ subclasses have their\n    customized parameters with specific initialization functions.\n\n    If your _Module_ implementation has some customized parameters and you want to initialize them by yourself,\n    please give the initialization logic in this interface function.\n\n    This interface function is not mandatory to be overridden.\n    Note: Don't forget to add `self.default_init_modules.append(YourModule)` in `model_init()` of your _Model_.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/module/augment/","title":"augment","text":""},{"location":"reference/module/augment/specaug/","title":"specaug","text":""},{"location":"reference/module/augment/specaug/#module.augment.specaug.SpecAugment","title":"<code>SpecAugment</code>","text":"<p>               Bases: <code>Module</code></p> <p>Batch-level SpecAugment.</p> <p>Implemented based on     'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition'     reference: https://arxiv.org/pdf/1904.08779.pdf This implementation is inspired by     https://github.com/espnet/espnet/blob/36e824be58ea6c6844e3d87b11e382f90ba4fb22/espnet2/layers/time_warp.py#L9     https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/augment.py#L116</p> Source code in <code>speechain/module/augment/specaug.py</code> <pre><code>class SpecAugment(Module):\n    \"\"\"Batch-level SpecAugment.\n\n    Implemented based on\n        'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition'\n        reference: https://arxiv.org/pdf/1904.08779.pdf\n    This implementation is inspired by\n        https://github.com/espnet/espnet/blob/36e824be58ea6c6844e3d87b11e382f90ba4fb22/espnet2/layers/time_warp.py#L9\n        https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/augment.py#L116\n    \"\"\"\n\n    def module_init(\n        self,\n        time_warp: bool = True,\n        time_warp_window: int = 5,\n        time_warp_mode: str = \"bicubic\",\n        freq_mask: bool = True,\n        freq_mask_width: Union[int, List[int]] = 30,\n        freq_mask_num: int = 2,\n        time_mask: bool = True,\n        time_mask_width: Union[int, float, List[int or float]] = 0.05,\n        time_mask_num: int = 2,\n        time_mask_ratio: float = 1.0,\n        feat_norm: bool = True,\n    ):\n\n        assert (\n            time_warp or freq_mask or time_mask\n        ), \"You must specify at least one type of augmentation in SpecAugment!\"\n        self.feat_dim = None\n        if self.input_size is not None:\n            self.feat_dim = self.input_size\n            self.output_size = self.input_size\n\n        # time warping arguments\n        self.time_warp = time_warp\n        self.time_warp_window = time_warp_window\n        self.time_warp_mode = time_warp_mode\n\n        # frequency masking arguments\n        self.freq_mask = freq_mask\n        if isinstance(freq_mask_width, int):\n            freq_mask_width = [0, freq_mask_width]\n        elif not isinstance(freq_mask_width, List):\n            raise ValueError\n        if self.feat_dim is not None:\n            assert freq_mask_width[1] &lt; self.feat_dim, (\n                \"The number of maximum frequency masking bins cannot be larger than the feature dimension! \"\n                f\"freq_mask_width[1]={freq_mask_width[1]} and self.feat_dim={self.feat_dim}.\"\n            )\n        self.freq_mask_width = freq_mask_width\n        self.freq_mask_num = freq_mask_num\n\n        # time masking arguments\n        self.time_mask = time_mask\n        if isinstance(time_mask_width, (int, float)):\n            time_mask_width = [0, time_mask_width]\n        elif not isinstance(time_mask_width, List):\n            raise ValueError\n        self.time_mask_width = time_mask_width\n        self.time_mask_num = time_mask_num\n\n        # used for deciding masking values\n        self.feat_norm = feat_norm\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"Both the time warping and time masking are done within the minimum length of\n        all the utterance in the input batch.\n\n        This practice is to make sure that the time warping and masking are done in the effective area of the input data\n        and the feature length information are still valid after augmentation.\n\n        Args:\n            feat:\n            feat_len:\n\n        Returns:\n        \"\"\"\n        batch_size, feat_dim = feat.size(0), feat.size(-1)\n        time_maxlen, time_minlen = feat_len.max().item(), feat_len.min().item()\n        # --- Time Warping --- #\n        if self.time_warp:\n            # create channel dimension: (batch_size, time_maxlen, feat_dim) -&gt; (batch_size, 1, time_maxlen, feat_dim)\n            if feat.dim() == 3:\n                feat = feat.unsqueeze(1)\n\n            # time_minlen must be larger than 2 times of the warping window length\n            # otherwise, the input is too short to be warped (do nothing to the feature)\n            if time_minlen &gt; 2 * self.time_warp_window + 1:\n                # center \u2208 {time_warp_window + 1, ..., time_minlen - time_warp_window - 1}\n                warp_center = torch.randint(\n                    low=self.time_warp_window + 1,\n                    high=time_minlen - self.time_warp_window,\n                    size=(1,),\n                )[0].item()\n                # position \u2208 {1, ..., time_minlen - 1} (consider the range of the center)\n                warp_pos = torch.randint(\n                    low=warp_center - self.time_warp_window,\n                    high=warp_center + self.time_warp_window,\n                    size=(1,),\n                )[0].item()\n                # interpolate the left and right parts of the selected center within time_minlen to protect feat_len\n                # align_corners=True to keep in line with the original paper\n                left_warp = torch.nn.functional.interpolate(\n                    feat[:, :, :warp_center],\n                    size=(warp_pos, feat_dim),\n                    mode=self.time_warp_mode,\n                    align_corners=True,\n                )\n                right_warp = torch.nn.functional.interpolate(\n                    feat[:, :, warp_center:time_minlen],\n                    size=(time_minlen - warp_pos, feat_dim),\n                    mode=self.time_warp_mode,\n                    align_corners=True,\n                )\n                feat[:, :, :warp_pos] = left_warp\n                feat[:, :, warp_pos:time_minlen] = right_warp\n\n            # remove the redundant channel dimension\n            feat = feat.view(batch_size, time_maxlen, feat_dim)\n\n        # --- Feature Masking (Frequency axis or Time axis) --- #\n        # overall mask\n        mask = None\n        # frequency mask generation\n        if self.freq_mask:\n            # lazily check the frequency masking width during training if self.feat_dim is not initialized\n            if self.feat_dim is None:\n                assert (\n                    self.feat_dim == feat_dim and self.freq_mask_width[1] &lt; feat_dim\n                ), (\n                    \"The number of maximum frequency masking bins cannot be larger than the feature dimension! \"\n                    f\"self.freq_mask_width[1]={self.freq_mask_width[1]} and feat_dim={feat_dim}.\"\n                )\n\n            # randomly select the masking length for each masking operation in each utterance\n            # (batch_size, freq_mask_num, 1), mask_len \u2208 {self.freq_mask_width[0], ..., self.freq_mask_width[1]}\n            mask_len = torch.randint(\n                self.freq_mask_width[0],\n                self.freq_mask_width[1] + 1,\n                size=(batch_size, self.freq_mask_num),\n                device=feat.device,\n            ).unsqueeze(2)\n            # randomly select the masking position for each masking operation in each utterance\n            # (batch_size, freq_mask_num, 1), mask_pos \u2208 {0, ..., feat_dim - mask_len.max - 1}\n            mask_pos = torch.randint(\n                0,\n                max(1, feat_dim - mask_len.max().item()),\n                size=(batch_size, self.freq_mask_num),\n                device=feat.device,\n            ).unsqueeze(2)\n            # (1, 1, feat_dim)\n            freq_axis = torch.arange(feat_dim, device=feat.device)[None, None, :]\n            # (batch_size, freq_mask_num, feat_dim) -&gt; (batch_size, 1, feat_dim)\n            feat_mask = (mask_pos &lt;= freq_axis) * (freq_axis &lt; (mask_pos + mask_len))\n            mask = feat_mask.any(dim=1, keepdim=True)\n\n        # time mask generation\n        if self.time_mask:\n            time_mask_lower, time_mask_upper = self.time_mask_width\n            if isinstance(time_mask_lower, float):\n                time_mask_lower = int(time_mask_lower * time_minlen)\n            if isinstance(time_mask_upper, float):\n                time_mask_upper = int(time_mask_upper * time_minlen)\n\n            # the maximum time masking width cannot be larger than ratio \u00d7 minimum time sequence length\n            time_mask_upper = min(time_mask_upper, time_minlen)\n\n            # randomly select the time masking length for each masking operation in each utterance\n            # (batch_size, 1, time_mask_num), mask_len \u2208 {time_mask_width[0], ..., time_mask_width[1]}\n            mask_len = torch.randint(\n                time_mask_lower,\n                time_mask_upper + 1,\n                size=(batch_size, self.time_mask_num),\n                device=feat.device,\n            ).unsqueeze(1)\n            # randomly select the time masking position for each masking operation in each utterance\n            # (batch_size, 1, time_mask_num), mask_pos \u2208 {0, ..., time_minlen - mask_len.max - 1}\n            mask_pos = torch.randint(\n                0,\n                max(1, time_minlen - mask_len.max().item()),\n                size=(batch_size, self.time_mask_num),\n                device=feat.device,\n            ).unsqueeze(1)\n            # (1, time_maxlen, 1)\n            time_axis = torch.arange(time_maxlen, device=feat.device)[None, :, None]\n            # (batch_size, time_maxlen, time_mask_num) -&gt; (batch_size, time_maxlen, 1)\n            time_mask = (mask_pos &lt;= time_axis) * (time_axis &lt; (mask_pos + mask_len))\n            time_mask = time_mask.any(dim=-1, keepdim=True)\n            # combine time mask with frequency mask if both are specified\n            # (batch_size, time_maxlen, 1) or (batch_size, 1, feat_dim) = (batch_size, time_maxlen, feat_dim)\n            mask = time_mask if mask is None else torch.logical_or(mask, time_mask)\n\n        # one-shot feature masking\n        if mask is not None:\n            mask_value = 0.0 if self.feat_norm else feat.mean()\n            feat = feat.masked_fill(mask, mask_value)\n\n        return feat, feat_len\n\n    def extra_repr(self) -&gt; str:\n        output = \"\"\n        if self.time_warp:\n            output += (\n                f\"time_warp_window={self.time_warp_window}, \"\n                f\"time_warp_mode={self.time_warp_mode}, \"\n            )\n\n        if self.freq_mask:\n            output += (\n                f\"\\nfreq_mask_width={self.freq_mask_width}, \"\n                f\"freq_mask_num={self.freq_mask_num}, \"\n            )\n\n        if self.time_mask:\n            output += (\n                f\"\\ntime_mask_width={self.time_mask_width}, \"\n                f\"time_mask_num={self.time_mask_num}\"\n            )\n\n        return output\n</code></pre>"},{"location":"reference/module/augment/specaug/#module.augment.specaug.SpecAugment.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Both the time warping and time masking are done within the minimum length of all the utterance in the input batch.</p> <p>This practice is to make sure that the time warping and masking are done in the effective area of the input data and the feature length information are still valid after augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>feat_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/augment/specaug.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"Both the time warping and time masking are done within the minimum length of\n    all the utterance in the input batch.\n\n    This practice is to make sure that the time warping and masking are done in the effective area of the input data\n    and the feature length information are still valid after augmentation.\n\n    Args:\n        feat:\n        feat_len:\n\n    Returns:\n    \"\"\"\n    batch_size, feat_dim = feat.size(0), feat.size(-1)\n    time_maxlen, time_minlen = feat_len.max().item(), feat_len.min().item()\n    # --- Time Warping --- #\n    if self.time_warp:\n        # create channel dimension: (batch_size, time_maxlen, feat_dim) -&gt; (batch_size, 1, time_maxlen, feat_dim)\n        if feat.dim() == 3:\n            feat = feat.unsqueeze(1)\n\n        # time_minlen must be larger than 2 times of the warping window length\n        # otherwise, the input is too short to be warped (do nothing to the feature)\n        if time_minlen &gt; 2 * self.time_warp_window + 1:\n            # center \u2208 {time_warp_window + 1, ..., time_minlen - time_warp_window - 1}\n            warp_center = torch.randint(\n                low=self.time_warp_window + 1,\n                high=time_minlen - self.time_warp_window,\n                size=(1,),\n            )[0].item()\n            # position \u2208 {1, ..., time_minlen - 1} (consider the range of the center)\n            warp_pos = torch.randint(\n                low=warp_center - self.time_warp_window,\n                high=warp_center + self.time_warp_window,\n                size=(1,),\n            )[0].item()\n            # interpolate the left and right parts of the selected center within time_minlen to protect feat_len\n            # align_corners=True to keep in line with the original paper\n            left_warp = torch.nn.functional.interpolate(\n                feat[:, :, :warp_center],\n                size=(warp_pos, feat_dim),\n                mode=self.time_warp_mode,\n                align_corners=True,\n            )\n            right_warp = torch.nn.functional.interpolate(\n                feat[:, :, warp_center:time_minlen],\n                size=(time_minlen - warp_pos, feat_dim),\n                mode=self.time_warp_mode,\n                align_corners=True,\n            )\n            feat[:, :, :warp_pos] = left_warp\n            feat[:, :, warp_pos:time_minlen] = right_warp\n\n        # remove the redundant channel dimension\n        feat = feat.view(batch_size, time_maxlen, feat_dim)\n\n    # --- Feature Masking (Frequency axis or Time axis) --- #\n    # overall mask\n    mask = None\n    # frequency mask generation\n    if self.freq_mask:\n        # lazily check the frequency masking width during training if self.feat_dim is not initialized\n        if self.feat_dim is None:\n            assert (\n                self.feat_dim == feat_dim and self.freq_mask_width[1] &lt; feat_dim\n            ), (\n                \"The number of maximum frequency masking bins cannot be larger than the feature dimension! \"\n                f\"self.freq_mask_width[1]={self.freq_mask_width[1]} and feat_dim={feat_dim}.\"\n            )\n\n        # randomly select the masking length for each masking operation in each utterance\n        # (batch_size, freq_mask_num, 1), mask_len \u2208 {self.freq_mask_width[0], ..., self.freq_mask_width[1]}\n        mask_len = torch.randint(\n            self.freq_mask_width[0],\n            self.freq_mask_width[1] + 1,\n            size=(batch_size, self.freq_mask_num),\n            device=feat.device,\n        ).unsqueeze(2)\n        # randomly select the masking position for each masking operation in each utterance\n        # (batch_size, freq_mask_num, 1), mask_pos \u2208 {0, ..., feat_dim - mask_len.max - 1}\n        mask_pos = torch.randint(\n            0,\n            max(1, feat_dim - mask_len.max().item()),\n            size=(batch_size, self.freq_mask_num),\n            device=feat.device,\n        ).unsqueeze(2)\n        # (1, 1, feat_dim)\n        freq_axis = torch.arange(feat_dim, device=feat.device)[None, None, :]\n        # (batch_size, freq_mask_num, feat_dim) -&gt; (batch_size, 1, feat_dim)\n        feat_mask = (mask_pos &lt;= freq_axis) * (freq_axis &lt; (mask_pos + mask_len))\n        mask = feat_mask.any(dim=1, keepdim=True)\n\n    # time mask generation\n    if self.time_mask:\n        time_mask_lower, time_mask_upper = self.time_mask_width\n        if isinstance(time_mask_lower, float):\n            time_mask_lower = int(time_mask_lower * time_minlen)\n        if isinstance(time_mask_upper, float):\n            time_mask_upper = int(time_mask_upper * time_minlen)\n\n        # the maximum time masking width cannot be larger than ratio \u00d7 minimum time sequence length\n        time_mask_upper = min(time_mask_upper, time_minlen)\n\n        # randomly select the time masking length for each masking operation in each utterance\n        # (batch_size, 1, time_mask_num), mask_len \u2208 {time_mask_width[0], ..., time_mask_width[1]}\n        mask_len = torch.randint(\n            time_mask_lower,\n            time_mask_upper + 1,\n            size=(batch_size, self.time_mask_num),\n            device=feat.device,\n        ).unsqueeze(1)\n        # randomly select the time masking position for each masking operation in each utterance\n        # (batch_size, 1, time_mask_num), mask_pos \u2208 {0, ..., time_minlen - mask_len.max - 1}\n        mask_pos = torch.randint(\n            0,\n            max(1, time_minlen - mask_len.max().item()),\n            size=(batch_size, self.time_mask_num),\n            device=feat.device,\n        ).unsqueeze(1)\n        # (1, time_maxlen, 1)\n        time_axis = torch.arange(time_maxlen, device=feat.device)[None, :, None]\n        # (batch_size, time_maxlen, time_mask_num) -&gt; (batch_size, time_maxlen, 1)\n        time_mask = (mask_pos &lt;= time_axis) * (time_axis &lt; (mask_pos + mask_len))\n        time_mask = time_mask.any(dim=-1, keepdim=True)\n        # combine time mask with frequency mask if both are specified\n        # (batch_size, time_maxlen, 1) or (batch_size, 1, feat_dim) = (batch_size, time_maxlen, feat_dim)\n        mask = time_mask if mask is None else torch.logical_or(mask, time_mask)\n\n    # one-shot feature masking\n    if mask is not None:\n        mask_value = 0.0 if self.feat_norm else feat.mean()\n        feat = feat.masked_fill(mask, mask_value)\n\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/conformer/","title":"conformer","text":""},{"location":"reference/module/conformer/attention/","title":"attention","text":""},{"location":"reference/module/conformer/attention/#module.conformer.attention.RelPosMultiHeadedAttention","title":"<code>RelPosMultiHeadedAttention</code>","text":"<p>               Bases: <code>MultiHeadedAttention</code></p> Source code in <code>speechain/module/conformer/attention.py</code> <pre><code>class RelPosMultiHeadedAttention(MultiHeadedAttention):\n    \"\"\"\"\"\"\n\n    def module_init(\n        self,\n        num_heads: int,\n        d_model: int,\n        dropout: float = 0.1,\n        scale_dp_by_head: bool = False,\n    ):\n        super(RelPosMultiHeadedAttention, self).module_init(\n            num_heads, d_model, dropout, scale_dp_by_head\n        )\n\n        self.pos_layer = nn.Linear(d_model, d_model, bias=False)\n\n        self.pos_bias_u = nn.Parameter(torch.Tensor(self.num_heads, self.head_size))\n        self.pos_bias_v = nn.Parameter(torch.Tensor(self.num_heads, self.head_size))\n\n    def rel_shift(self, matrix_bd: torch.Tensor):\n\n        # (batch_size, num_heads, seq_len, 1)\n        zero_pad = torch.zeros(\n            (*matrix_bd.size()[:3], 1), device=matrix_bd.device, dtype=matrix_bd.dtype\n        )\n        # (batch_size, num_heads, seq_len, 2 * seq_len)\n        matrix_bd_padded = torch.cat([zero_pad, matrix_bd], dim=-1)\n\n        # (batch_size, num_heads, 2 * seq_len, seq_len)\n        matrix_bd_padded = matrix_bd_padded.view(\n            *matrix_bd_padded.size()[:2],\n            matrix_bd_padded.size(3),\n            matrix_bd_padded.size(2)\n        )\n        # (batch_size, num_heads, seq_len)\n        matrix_bd = matrix_bd_padded[:, :, 1:].view_as(matrix_bd)[\n            :, :, :, : matrix_bd.size(-1) // 2 + 1\n        ]  # only keep the positions from 0 to time2\n\n        return matrix_bd\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        v: torch.Tensor,\n        q: torch.Tensor,\n        mask: torch.Tensor = None,\n        posenc: torch.Tensor = None,\n    ):\n\n        assert (\n            posenc is not None\n        ), \"posnec must be given for RelPosMultiHeadedAttention!\"\n\n        # (batch_size, num_heads, seq_len_q or seq_len_kv, head_size)\n        k, v, q = self.kvq_forward(k, v, q)\n\n        # (batch_size, num_heads, seq_len_q, head_size)\n        q_with_bias_u = q + self.pos_bias_u[None, :, None, :]\n        q_with_bias_v = q + self.pos_bias_v[None, :, None, :]\n\n        # (batch_size, num_heads, 2 * seq_len - 1, head_size)\n        posenc = (\n            self.pos_layer(posenc)\n            .view(posenc.size(0), -1, self.num_heads, self.head_size)\n            .transpose(1, 2)\n        )\n\n        # (batch_size, num_heads, seq_len_q, head_size) * (batch_size, num_heads, head_size, seq_len_kv) = (batch_size, num_heads, seq_len_q, seq_len_kv)\n        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(2, 3))\n\n        # (batch_size, num_heads, seq_len_q, head_size) * (batch_size, num_heads, head_size, 2 * seq_len_q - 1) = (batch_size, num_heads, seq_len, 2 * seq_len_q - 1)\n        matrix_bd = torch.matmul(q_with_bias_v, posenc.transpose(2, 3))\n        # (batch_size, num_heads, seq_len_q, seq_len_q)\n        matrix_bd = self.rel_shift(matrix_bd)\n\n        scores = (matrix_ac + matrix_bd) * self.scale\n        return self.attention_forward(v, scores, mask)\n</code></pre>"},{"location":"reference/module/conformer/encoder/","title":"encoder","text":""},{"location":"reference/module/conformer/encoder/#module.conformer.encoder.ConformerEncoder","title":"<code>ConformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/conformer/encoder.py</code> <pre><code>class ConformerEncoder(Module):\n    \"\"\"\"\"\"\n\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        posenc_maxlen: int = 5000,\n        posenc_dropout: float = 0.1,\n        emb_scale: bool = False,\n        d_model: int = 512,\n        num_heads: int = 4,\n        num_layers: int = 8,\n        att_dropout: float = 0.1,\n        depthwise_kernel_size: int = 31,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        fdfwd_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n        uni_direction: bool = False,\n    ):\n\n        # input_size and output_size initialization\n        if self.input_size is not None:\n            d_model = self.input_size\n        self.output_size = d_model\n\n        # para recording\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.layernorm_first = layernorm_first\n        self.uni_direction = uni_direction\n\n        self.posenc = RelPositionalEncoding(\n            posenc_type=posenc_type,\n            d_model=d_model,\n            emb_scale=emb_scale,\n            max_len=posenc_maxlen,\n            dropout=posenc_dropout,\n        )\n\n        # initialize transformer layers\n        self.cfm_layers = torch.nn.ModuleList(\n            [\n                ConformerEncoderLayer(\n                    d_model=d_model,\n                    num_heads=num_heads,\n                    att_dropout=att_dropout,\n                    depthwise_kernel_size=depthwise_kernel_size,\n                    fdfwd_dim=fdfwd_dim,\n                    fdfwd_type=fdfwd_type,\n                    fdfwd_activation=fdfwd_activation,\n                    fdfwd_args=fdfwd_args,\n                    fdfwd_dropout=fdfwd_dropout,\n                    res_dropout=res_dropout,\n                    layernorm_first=layernorm_first,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        # initialize layernorm layer if necessary\n        if self.layernorm_first:\n            self.layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n    @staticmethod\n    def subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n        \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n        Transformer helper function.\n\n        Args:\n            batch_size:\n            maxlen: int\n                size of mask (2nd and 3rd dim)\n\n        Returns:\n        \"\"\"\n        return ~torch.triu(\n            torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n        )\n\n    def forward(self, src: torch.Tensor, mask: torch.Tensor):\n        # add position encoding to word embeddings\n        src, posenc = self.posenc(src)\n\n        # generate the low-triangular mask for self-attention layers\n        if self.uni_direction:\n            batch_size, _, src_maxlen = mask.size()\n            mask = torch.logical_and(\n                mask.repeat(1, src_maxlen, 1),\n                self.subsequent_mask(batch_size, src_maxlen).to(mask.device),\n            )\n\n        # go through the Conformer layers\n        attmat, hidden = [], []\n        for l in range(len(self.cfm_layers)):\n            src, _tmp_attmat = self.cfm_layers[l](src, mask, posenc)\n            attmat.append(_tmp_attmat)\n            hidden.append(src.clone())\n\n        # go through the final layernorm layer if necessary\n        if self.layernorm_first:\n            src = self.layernorm(src)\n\n        return src, mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/conformer/encoder/#module.conformer.encoder.ConformerEncoder.subsequent_mask","title":"<code>subsequent_mask(batch_size, maxlen)</code>  <code>staticmethod</code>","text":"<p>Mask out subsequent positions (to prevent attending to future positions) Transformer helper function.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> required <code>maxlen</code> <code>int</code> <p>int size of mask (2nd and 3rd dim)</p> required <p>Returns:</p> Source code in <code>speechain/module/conformer/encoder.py</code> <pre><code>@staticmethod\ndef subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n    \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n    Transformer helper function.\n\n    Args:\n        batch_size:\n        maxlen: int\n            size of mask (2nd and 3rd dim)\n\n    Returns:\n    \"\"\"\n    return ~torch.triu(\n        torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n    )\n</code></pre>"},{"location":"reference/module/conformer/encoder/#module.conformer.encoder.ConformerEncoderLayer","title":"<code>ConformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/conformer/encoder.py</code> <pre><code>class ConformerEncoderLayer(Module):\n    def module_init(\n        self,\n        d_model: int = 512,\n        num_heads: int = 8,\n        att_dropout: float = 0.1,\n        depthwise_kernel_size: int = 31,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        fdfwd_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n    ):\n\n        # initialize feadforward layer in front of MHA and Conv modules\n        self.front_feed_forward = PositionwiseFeedForward(\n            d_model=d_model,\n            fdfwd_dim=fdfwd_dim,\n            fdfwd_type=fdfwd_type,\n            fdfwd_activation=fdfwd_activation,\n            fdfwd_args=fdfwd_args,\n            dropout=fdfwd_dropout,\n        )\n        self.front_fdfwd_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n        #\n        self.relpos_mha = RelPosMultiHeadedAttention(\n            d_model=d_model, num_heads=num_heads, dropout=att_dropout\n        )\n        self.mha_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n        #\n        self.conv_module = ConvolutionModule(\n            input_size=d_model, depthwise_kernel_size=depthwise_kernel_size\n        )\n        self.conv_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n        # initialize feadforward layer behind MHA and Conv modules\n        self.rear_feed_forward = PositionwiseFeedForward(\n            d_model=d_model,\n            fdfwd_dim=fdfwd_dim,\n            fdfwd_type=fdfwd_type,\n            fdfwd_activation=fdfwd_activation,\n            fdfwd_args=fdfwd_args,\n            dropout=fdfwd_dropout,\n        )\n        self.rear_fdfwd_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n        # initialize layernorm layers, each sublayer has an exclusive LayerNorm layer\n        self.layernorm_first = layernorm_first\n\n        # initialize residual dropout layer\n        self.dropout = nn.Dropout(res_dropout)\n\n    def forward(self, src: torch.Tensor, mask: torch.Tensor, posenc: torch.Tensor):\n        \"\"\"Forward pass for a single transformer encoder layer.\n\n        Args:\n            src: (batch, src_maxlen, d_model)\n                source input for the encoder\n            mask: (batch, 1, src_maxlen)\n                input mask\n\n        Returns:\n            The output of this Transformer encoder layer and the attention matrix\n        \"\"\"\n        \"Front Positional FeedForward Layer part\"\n        # go through the LayerNorm layer before the feedforward layer or not\n        src_norm = self.front_fdfwd_layernorm(src) if self.layernorm_first else src\n\n        # go through the feedforward layer and perform the residual connection\n        front_fdfwd_hidden = self.front_feed_forward(src_norm)\n        front_fdfwd_output = 0.5 * self.dropout(front_fdfwd_hidden) + src\n\n        # go through the LayerNorm layer after the feedforward layer or not\n        front_fdfwd_output = (\n            self.front_fdfwd_layernorm(front_fdfwd_output)\n            if not self.layernorm_first\n            else front_fdfwd_output\n        )\n        \"Relative Positional Multi-head Attention Layer part\"\n        # go through the LayerNorm layer before the multi-head attention layer or not\n        front_fdfwd_output_norm = (\n            self.mha_layernorm(front_fdfwd_output)\n            if self.layernorm_first\n            else front_fdfwd_output\n        )\n\n        # go through the multi-head attention layer and perform the residual connection\n        relpos_mha_hidden, attmat = self.relpos_mha(\n            front_fdfwd_output_norm,\n            front_fdfwd_output_norm,\n            front_fdfwd_output_norm,\n            mask,\n            posenc,\n        )\n        relpos_mha_output = self.dropout(relpos_mha_hidden) + front_fdfwd_output\n\n        # go through the LayerNorm layer after the multi-head attention layer or not\n        relpos_mha_output = (\n            self.mha_layernorm(relpos_mha_output)\n            if not self.layernorm_first\n            else relpos_mha_output\n        )\n        \"Convolutional Module part\"\n        # go through the LayerNorm layer before the feedforward layer or not\n        relpos_mha_output_norm = (\n            self.conv_layernorm(relpos_mha_output)\n            if self.layernorm_first\n            else relpos_mha_output\n        )\n\n        # go through the feedforward layer and perform the residual connection\n        conv_hidden = self.conv_module(relpos_mha_output_norm)\n        conv_output = self.dropout(conv_hidden) + relpos_mha_output\n\n        # go through the LayerNorm layer after the feedforward layer or not\n        conv_output = (\n            self.conv_layernorm(conv_output)\n            if not self.layernorm_first\n            else conv_output\n        )\n        \"Rear Positional FeedForward Layer part\"\n        # go through the LayerNorm layer before the feedforward layer or not\n        conv_output_norm = (\n            self.rear_fdfwd_layernorm(conv_output)\n            if self.layernorm_first\n            else conv_output\n        )\n\n        # go through the feedforward layer and perform the residual connection\n        rear_fdfwd_hidden = self.rear_feed_forward(conv_output_norm)\n        rear_fdfwd_output = 0.5 * self.dropout(rear_fdfwd_hidden) + conv_output\n\n        # go through the LayerNorm layer after the feedforward layer or not\n        rear_fdfwd_output = (\n            self.rear_fdfwd_layernorm(rear_fdfwd_output)\n            if not self.layernorm_first\n            else rear_fdfwd_output\n        )\n\n        return rear_fdfwd_output, attmat\n</code></pre>"},{"location":"reference/module/conformer/encoder/#module.conformer.encoder.ConformerEncoderLayer.forward","title":"<code>forward(src, mask, posenc)</code>","text":"<p>Forward pass for a single transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>(batch, src_maxlen, d_model) source input for the encoder</p> required <code>mask</code> <code>Tensor</code> <p>(batch, 1, src_maxlen) input mask</p> required <p>Returns:</p> Type Description <p>The output of this Transformer encoder layer and the attention matrix</p> Source code in <code>speechain/module/conformer/encoder.py</code> <pre><code>def forward(self, src: torch.Tensor, mask: torch.Tensor, posenc: torch.Tensor):\n    \"\"\"Forward pass for a single transformer encoder layer.\n\n    Args:\n        src: (batch, src_maxlen, d_model)\n            source input for the encoder\n        mask: (batch, 1, src_maxlen)\n            input mask\n\n    Returns:\n        The output of this Transformer encoder layer and the attention matrix\n    \"\"\"\n    \"Front Positional FeedForward Layer part\"\n    # go through the LayerNorm layer before the feedforward layer or not\n    src_norm = self.front_fdfwd_layernorm(src) if self.layernorm_first else src\n\n    # go through the feedforward layer and perform the residual connection\n    front_fdfwd_hidden = self.front_feed_forward(src_norm)\n    front_fdfwd_output = 0.5 * self.dropout(front_fdfwd_hidden) + src\n\n    # go through the LayerNorm layer after the feedforward layer or not\n    front_fdfwd_output = (\n        self.front_fdfwd_layernorm(front_fdfwd_output)\n        if not self.layernorm_first\n        else front_fdfwd_output\n    )\n    \"Relative Positional Multi-head Attention Layer part\"\n    # go through the LayerNorm layer before the multi-head attention layer or not\n    front_fdfwd_output_norm = (\n        self.mha_layernorm(front_fdfwd_output)\n        if self.layernorm_first\n        else front_fdfwd_output\n    )\n\n    # go through the multi-head attention layer and perform the residual connection\n    relpos_mha_hidden, attmat = self.relpos_mha(\n        front_fdfwd_output_norm,\n        front_fdfwd_output_norm,\n        front_fdfwd_output_norm,\n        mask,\n        posenc,\n    )\n    relpos_mha_output = self.dropout(relpos_mha_hidden) + front_fdfwd_output\n\n    # go through the LayerNorm layer after the multi-head attention layer or not\n    relpos_mha_output = (\n        self.mha_layernorm(relpos_mha_output)\n        if not self.layernorm_first\n        else relpos_mha_output\n    )\n    \"Convolutional Module part\"\n    # go through the LayerNorm layer before the feedforward layer or not\n    relpos_mha_output_norm = (\n        self.conv_layernorm(relpos_mha_output)\n        if self.layernorm_first\n        else relpos_mha_output\n    )\n\n    # go through the feedforward layer and perform the residual connection\n    conv_hidden = self.conv_module(relpos_mha_output_norm)\n    conv_output = self.dropout(conv_hidden) + relpos_mha_output\n\n    # go through the LayerNorm layer after the feedforward layer or not\n    conv_output = (\n        self.conv_layernorm(conv_output)\n        if not self.layernorm_first\n        else conv_output\n    )\n    \"Rear Positional FeedForward Layer part\"\n    # go through the LayerNorm layer before the feedforward layer or not\n    conv_output_norm = (\n        self.rear_fdfwd_layernorm(conv_output)\n        if self.layernorm_first\n        else conv_output\n    )\n\n    # go through the feedforward layer and perform the residual connection\n    rear_fdfwd_hidden = self.rear_feed_forward(conv_output_norm)\n    rear_fdfwd_output = 0.5 * self.dropout(rear_fdfwd_hidden) + conv_output\n\n    # go through the LayerNorm layer after the feedforward layer or not\n    rear_fdfwd_output = (\n        self.rear_fdfwd_layernorm(rear_fdfwd_output)\n        if not self.layernorm_first\n        else rear_fdfwd_output\n    )\n\n    return rear_fdfwd_output, attmat\n</code></pre>"},{"location":"reference/module/conformer/pos_enc/","title":"pos_enc","text":""},{"location":"reference/module/conformer/pos_enc/#module.conformer.pos_enc.RelPositionalEncoding","title":"<code>RelPositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/conformer/pos_enc.py</code> <pre><code>class RelPositionalEncoding(Module):\n    \"\"\"\"\"\"\n\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        d_model: int = 512,\n        emb_scale: bool = False,\n        max_len: int = 5000,\n        dropout: float = 0.0,\n    ):\n\n        assert posenc_type in [\n            \"mix\",\n            \"sep\",\n        ], f\"The type of PositionalEncoding layer must be either 'mix' or 'sep', but got type={posenc_type}!\"\n        assert (\n            d_model % 2 == 0\n        ), f\"Cannot apply sin/cos positional encoding to the vectors with odd dimensions (got d_model={d_model:d}).\"\n\n        self.posenc_type = posenc_type\n        self.d_model = d_model\n        self.emb_scale = emb_scale\n\n        # positional encoding matrix\n        self.max_len = max_len\n        self.update_posenc(max_len)\n\n        # positional encoding Dropout layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def update_posenc(self, seq_len: int = None):\n        \"\"\"\n\n        Args:\n            max_len:\n\n        \"\"\"\n        if seq_len is None:\n            seq_len = self.max_len\n\n        if seq_len &gt;= self.max_len:\n            # positional encoding calculation\n            position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n            div_term = torch.exp(\n                torch.arange(0, self.d_model, 2, dtype=torch.float)\n                * (math.log(10000.0) / self.d_model)\n            )\n            posenc_past, posenc_future = torch.zeros(\n                seq_len, self.d_model\n            ), torch.zeros(seq_len, self.d_model)\n\n            # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n            if self.posenc_type == \"mix\":\n                posenc_past[:, 0::2] = torch.sin(position / div_term)\n                posenc_past[:, 1::2] = torch.cos(position / div_term)\n\n                posenc_future[:, 0::2] = torch.sin(-1 * position / div_term)\n                posenc_future[:, 1::2] = torch.cos(-1 * position / div_term)\n            # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n            elif self.posenc_type == \"sep\":\n                div_term_ext = torch.exp(\n                    torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n                    * (math.log(10000.0) / self.d_model)\n                )\n                posenc_past[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n                posenc_past[:, int(self.d_model / 2) :] = torch.cos(\n                    position / div_term_ext\n                )\n\n                posenc_future[:, : int(self.d_model / 2)] = torch.sin(\n                    -1 * position / div_term\n                )\n                posenc_future[:, int(self.d_model / 2) :] = torch.cos(\n                    -1 * position / div_term_ext\n                )\n\n            # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n            # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n            self.register_buffer(\"posenc_past\", posenc_past.unsqueeze(0))\n            self.register_buffer(\"posenc_future\", posenc_future.unsqueeze(0))\n\n    def forward(self, emb_feat: torch.Tensor):\n\n        # update the buffered positional encodings by sequence length\n        seq_len = emb_feat.size(1)\n        self.update_posenc(seq_len)\n\n        with torch.no_grad():\n            posenc_past = torch.flip(self.posenc_past[:, :seq_len], (1,))\n            posenc_future = self.posenc_future[:, 1:seq_len]\n            # (1, 2 * seq_len - 1, embed_dim)\n            posenc = torch.cat([posenc_past, posenc_future], dim=1)\n\n        # (optional) scale the embedded feature up by sqrt(d_model)\n        if self.emb_scale:\n            emb_feat *= math.sqrt(self.d_model)\n\n        # apply dropout to both prenet embedded feature and positional encoding\n        return self.dropout(emb_feat), self.dropout(posenc)\n</code></pre>"},{"location":"reference/module/conformer/pos_enc/#module.conformer.pos_enc.RelPositionalEncoding.update_posenc","title":"<code>update_posenc(seq_len=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>max_len</code> required Source code in <code>speechain/module/conformer/pos_enc.py</code> <pre><code>def update_posenc(self, seq_len: int = None):\n    \"\"\"\n\n    Args:\n        max_len:\n\n    \"\"\"\n    if seq_len is None:\n        seq_len = self.max_len\n\n    if seq_len &gt;= self.max_len:\n        # positional encoding calculation\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float)\n            * (math.log(10000.0) / self.d_model)\n        )\n        posenc_past, posenc_future = torch.zeros(\n            seq_len, self.d_model\n        ), torch.zeros(seq_len, self.d_model)\n\n        # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n        if self.posenc_type == \"mix\":\n            posenc_past[:, 0::2] = torch.sin(position / div_term)\n            posenc_past[:, 1::2] = torch.cos(position / div_term)\n\n            posenc_future[:, 0::2] = torch.sin(-1 * position / div_term)\n            posenc_future[:, 1::2] = torch.cos(-1 * position / div_term)\n        # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n        elif self.posenc_type == \"sep\":\n            div_term_ext = torch.exp(\n                torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n                * (math.log(10000.0) / self.d_model)\n            )\n            posenc_past[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n            posenc_past[:, int(self.d_model / 2) :] = torch.cos(\n                position / div_term_ext\n            )\n\n            posenc_future[:, : int(self.d_model / 2)] = torch.sin(\n                -1 * position / div_term\n            )\n            posenc_future[:, int(self.d_model / 2) :] = torch.cos(\n                -1 * position / div_term_ext\n            )\n\n        # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n        # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n        self.register_buffer(\"posenc_past\", posenc_past.unsqueeze(0))\n        self.register_buffer(\"posenc_future\", posenc_future.unsqueeze(0))\n</code></pre>"},{"location":"reference/module/decoder/","title":"decoder","text":""},{"location":"reference/module/decoder/ar_asr/","title":"ar_asr","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/decoder/ar_asr/#module.decoder.ar_asr.ARASRDecoder","title":"<code>ARASRDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/decoder/ar_asr.py</code> <pre><code>class ARASRDecoder(Module):\n    \"\"\"\"\"\"\n\n    def module_init(self, embedding: Dict, decoder: Dict, vocab_size: int = None):\n        \"\"\"\n\n        Args:\n            embedding:\n            decoder:\n            vocab_size:\n\n        \"\"\"\n        # temporary register for connecting two sequential modules\n        _prev_output_size = None\n\n        # embedding layer of the E2E ASR decoder\n        embedding_class = import_class(\"speechain.module.\" + embedding[\"type\"])\n        embedding[\"conf\"] = (\n            dict() if \"conf\" not in embedding.keys() else embedding[\"conf\"]\n        )\n        self.embedding = embedding_class(vocab_size=vocab_size, **embedding[\"conf\"])\n        _prev_output_size = self.embedding.output_size\n\n        # main body of the E2E ASR decoder\n        decoder_class = import_class(\"speechain.module.\" + decoder[\"type\"])\n        decoder[\"conf\"] = dict() if \"conf\" not in decoder.keys() else decoder[\"conf\"]\n        self.decoder = decoder_class(input_size=_prev_output_size, **decoder[\"conf\"])\n        _prev_output_size = self.decoder.output_size\n\n        # token prediction layer for the E2E ASR decoder\n        self.postnet = TokenPostnet(input_size=_prev_output_size, vocab_size=vocab_size)\n\n    def forward(\n        self,\n        enc_feat: torch.Tensor,\n        enc_feat_mask: torch.Tensor,\n        text: torch.Tensor,\n        text_len: torch.Tensor,\n    ):\n        \"\"\"\n\n        # Args:\n            enc_feat:\n            enc_feat_mask:\n            text:\n            text_len:\n\n        Returns:\n\n        \"\"\"\n        # Text Embedding\n        emb_text = self.embedding(text)\n\n        # mask generation for the input text\n        text_mask = make_mask_from_len(text_len)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        dec_feat, self_attmat, encdec_attmat, hidden = self.decoder(\n            src=enc_feat, src_mask=enc_feat_mask, tgt=emb_text, tgt_mask=text_mask\n        )\n        return self.postnet(dec_feat), self_attmat, encdec_attmat, hidden\n</code></pre>"},{"location":"reference/module/decoder/ar_asr/#module.decoder.ar_asr.ARASRDecoder.forward","title":"<code>forward(enc_feat, enc_feat_mask, text, text_len)</code>","text":""},{"location":"reference/module/decoder/ar_asr/#module.decoder.ar_asr.ARASRDecoder.forward--args","title":"Args:","text":"<pre><code>enc_feat:\nenc_feat_mask:\ntext:\ntext_len:\n</code></pre> <p>Returns:</p> Source code in <code>speechain/module/decoder/ar_asr.py</code> <pre><code>def forward(\n    self,\n    enc_feat: torch.Tensor,\n    enc_feat_mask: torch.Tensor,\n    text: torch.Tensor,\n    text_len: torch.Tensor,\n):\n    \"\"\"\n\n    # Args:\n        enc_feat:\n        enc_feat_mask:\n        text:\n        text_len:\n\n    Returns:\n\n    \"\"\"\n    # Text Embedding\n    emb_text = self.embedding(text)\n\n    # mask generation for the input text\n    text_mask = make_mask_from_len(text_len)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    dec_feat, self_attmat, encdec_attmat, hidden = self.decoder(\n        src=enc_feat, src_mask=enc_feat_mask, tgt=emb_text, tgt_mask=text_mask\n    )\n    return self.postnet(dec_feat), self_attmat, encdec_attmat, hidden\n</code></pre>"},{"location":"reference/module/decoder/ar_asr/#module.decoder.ar_asr.ARASRDecoder.module_init","title":"<code>module_init(embedding, decoder, vocab_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>Dict</code> required <code>decoder</code> <code>Dict</code> required <code>vocab_size</code> <code>int</code> <code>None</code> Source code in <code>speechain/module/decoder/ar_asr.py</code> <pre><code>def module_init(self, embedding: Dict, decoder: Dict, vocab_size: int = None):\n    \"\"\"\n\n    Args:\n        embedding:\n        decoder:\n        vocab_size:\n\n    \"\"\"\n    # temporary register for connecting two sequential modules\n    _prev_output_size = None\n\n    # embedding layer of the E2E ASR decoder\n    embedding_class = import_class(\"speechain.module.\" + embedding[\"type\"])\n    embedding[\"conf\"] = (\n        dict() if \"conf\" not in embedding.keys() else embedding[\"conf\"]\n    )\n    self.embedding = embedding_class(vocab_size=vocab_size, **embedding[\"conf\"])\n    _prev_output_size = self.embedding.output_size\n\n    # main body of the E2E ASR decoder\n    decoder_class = import_class(\"speechain.module.\" + decoder[\"type\"])\n    decoder[\"conf\"] = dict() if \"conf\" not in decoder.keys() else decoder[\"conf\"]\n    self.decoder = decoder_class(input_size=_prev_output_size, **decoder[\"conf\"])\n    _prev_output_size = self.decoder.output_size\n\n    # token prediction layer for the E2E ASR decoder\n    self.postnet = TokenPostnet(input_size=_prev_output_size, vocab_size=vocab_size)\n</code></pre>"},{"location":"reference/module/decoder/ar_tts/","title":"ar_tts","text":"<p>Author: Sashi Novitasari Affiliation: NAIST (-2022) Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/decoder/ar_tts/#module.decoder.ar_tts.ARTTSDecoder","title":"<code>ARTTSDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder Module for Autoregressive TTS model.</p> Source code in <code>speechain/module/decoder/ar_tts.py</code> <pre><code>class ARTTSDecoder(Module):\n    \"\"\"Decoder Module for Autoregressive TTS model.\"\"\"\n\n    def module_init(\n        self,\n        decoder: Dict,\n        postnet: Dict,\n        frontend: Dict = None,\n        normalize: Dict or bool = True,\n        prenet: Dict = None,\n        spk_emb: Dict = None,\n        reduction_factor: int = 1,\n    ):\n\n        # temporary register for connecting two sequential modules\n        _prev_output_size = None\n\n        # --- Acoustic Feature Extraction Part --- #\n        # acoustic feature extraction frontend of the E2E TTS decoder\n        if frontend is not None:\n            frontend_class = import_class(\"speechain.module.\" + frontend[\"type\"])\n            frontend[\"conf\"] = (\n                dict() if \"conf\" not in frontend.keys() else frontend[\"conf\"]\n            )\n            self.frontend = frontend_class(**frontend[\"conf\"])\n            _prev_output_size = self.frontend.output_size\n\n        # feature normalization layer\n        if normalize is not None and normalize is not False:\n            normalize = dict() if normalize is True else normalize\n            self.normalize = FeatureNormalization(\n                input_size=_prev_output_size, distributed=self.distributed, **normalize\n            )\n\n        # reduction factor for acoustic feature sequence\n        self.reduction_factor = reduction_factor\n        _prev_output_size *= self.reduction_factor\n        feat_dim = _prev_output_size\n\n        # --- Main Body of TTS Decoder --- #\n        # feature embedding layer of the E2E TTS decoder\n        if prenet is not None:\n            prenet_class = import_class(\"speechain.module.\" + prenet[\"type\"])\n            prenet[\"conf\"] = dict() if \"conf\" not in prenet.keys() else prenet[\"conf\"]\n            self.prenet = prenet_class(input_size=_prev_output_size, **prenet[\"conf\"])\n            _prev_output_size = self.prenet.output_size\n\n        # initialize speaker embedding layer\n        if spk_emb is not None:\n            # check the model dimension\n            if \"d_model\" in spk_emb.keys() and spk_emb[\"d_model\"] != _prev_output_size:\n                warnings.warn(\n                    f\"Your input d_model ({spk_emb['d_model']}) is different from the one automatically \"\n                    f\"generated during model initialization ({_prev_output_size}).\"\n                    f\"Currently, the automatically generated one is used.\"\n                )\n\n            spk_emb[\"d_model\"] = _prev_output_size\n            self.spk_emb = SpeakerEmbedPrenet(**spk_emb)\n\n        # Initialize decoder\n        decoder_class = import_class(\"speechain.module.\" + decoder[\"type\"])\n        decoder[\"conf\"] = dict() if \"conf\" not in decoder.keys() else decoder[\"conf\"]\n        self.decoder = decoder_class(input_size=_prev_output_size, **decoder[\"conf\"])\n        _prev_output_size = self.decoder.output_size\n\n        # initialize prediction layers (feature prediction &amp; stop prediction)\n        self.feat_pred = torch.nn.Linear(\n            in_features=_prev_output_size, out_features=feat_dim\n        )\n        self.stop_pred = torch.nn.Linear(in_features=_prev_output_size, out_features=1)\n        self.output_size = feat_dim\n\n        # Initialize postnet of the decoder\n        postnet_class = import_class(\"speechain.module.\" + postnet[\"type\"])\n        postnet[\"conf\"] = dict() if \"conf\" not in postnet.keys() else postnet[\"conf\"]\n        self.postnet = postnet_class(input_size=feat_dim, **postnet[\"conf\"])\n\n    def forward(\n        self,\n        enc_text: torch.Tensor,\n        enc_text_mask: torch.Tensor,\n        feat: torch.Tensor,\n        feat_len: torch.Tensor,\n        spk_feat: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        epoch: int = None,\n        is_test: bool = False,\n    ):\n\n        # --- Acoustic Feature Extraction Part --- #\n        # in the training and validation stage, input data needs to be processed by the frontend\n        if not is_test:\n            # acoustic feature extraction for the raw waveform input\n            if feat.size(-1) == 1:\n                assert hasattr(self, \"frontend\"), (\n                    f\"Currently, {self.__class__.__name__} doesn't support time-domain TTS. \"\n                    f\"Please specify a feature extraction frontend.\"\n                )\n                # no amp operations for the frontend calculation to make sure the feature accuracy\n                with autocast(False):\n                    feat, feat_len = self.frontend(feat, feat_len)\n\n            # feature normalization\n            if hasattr(self, \"normalize\"):\n                feat, feat_len = self.normalize(\n                    feat, feat_len, group_ids=spk_ids, epoch=epoch\n                )\n\n            # acoustic feature length reduction\n            if self.reduction_factor &gt; 1:\n                _residual = feat.size(1) % self.reduction_factor\n                # clip the redundant part of acoustic features\n                if _residual != 0:\n                    feat = feat[:, :-_residual]\n\n                # group the features by the reduction factor\n                batch, feat_maxlen, feat_dim = feat.size()\n                feat = feat.reshape(\n                    batch,\n                    feat_maxlen // self.reduction_factor,\n                    feat_dim * self.reduction_factor,\n                )\n                feat_len = torch.div(\n                    feat_len, self.reduction_factor, rounding_mode=\"floor\"\n                ).type(torch.long)\n\n            # padding zeros at the beginning of acoustic feature sequence\n            padded_feat = torch.nn.functional.pad(feat, (0, 0, 1, 0), \"constant\", 0)\n            feat = padded_feat[:, :-1]\n            # target feature &amp; length, used for loss &amp; metric calculation\n            tgt_feat, tgt_feat_len = padded_feat[:, 1:], feat_len\n\n        # in the testing stage, input data has already been processed and structured, so no frontend processing here\n        else:\n            tgt_feat, tgt_feat_len = None, None\n\n        # --- Decoder Feature Transformation Part --- #\n        # feature Embedding\n        if hasattr(self, \"prenet\"):\n            feat, feat_len = self.prenet(feat, feat_len)\n\n        # mask generation for the input text\n        feat_mask = make_mask_from_len(feat_len)\n        if feat.is_cuda:\n            feat_mask = feat_mask.cuda(feat.device)\n\n        # Speaker Embedding\n        if hasattr(self, \"spk_emb\"):\n            # extract and process the speaker features (activation is not performed for random speaker feature)\n            spk_feat_lookup, spk_feat = self.spk_emb(spk_ids=spk_ids, spk_feat=spk_feat)\n            # combine the speaker features with the encoder outputs (and the decoder prenet outputs if specified)\n            enc_text, feat = self.spk_emb.combine_spk_feat(\n                spk_feat=spk_feat,\n                spk_feat_lookup=spk_feat_lookup,\n                enc_output=enc_text,\n                dec_input=feat,\n            )\n\n        # Decoding\n        dec_feat, self_attmat, encdec_attmat, hidden = self.decoder(\n            src=enc_text, src_mask=enc_text_mask, tgt=feat, tgt_mask=feat_mask\n        )\n        pred_stop = self.stop_pred(dec_feat)\n        pred_feat_before = self.feat_pred(dec_feat)\n        pred_feat_after = pred_feat_before + self.postnet(pred_feat_before, feat_len)\n\n        return (\n            pred_stop,\n            pred_feat_before,\n            pred_feat_after,\n            tgt_feat,\n            tgt_feat_len,\n            self_attmat,\n            encdec_attmat,\n            hidden,\n        )\n\n    def turn_on_dropout(self):\n        \"\"\"Turn on the dropout layers of the decoder prenet during inference.\n\n        Reference: the second paragraph from the bottom in Sec 2.2 of\n            'Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions'\n            https://arxiv.org/pdf/1712.05884.pdf\n        \"\"\"\n        assert hasattr(\n            self, \"prenet\"\n        ), \"If you want to apply dropout during TTS inference, your TTS model should have a decoder prenet.\"\n        if not self.prenet.training:\n            self.prenet.train()\n</code></pre>"},{"location":"reference/module/decoder/ar_tts/#module.decoder.ar_tts.ARTTSDecoder.turn_on_dropout","title":"<code>turn_on_dropout()</code>","text":"<p>Turn on the dropout layers of the decoder prenet during inference.</p> the second paragraph from the bottom in Sec 2.2 of <p>'Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions' https://arxiv.org/pdf/1712.05884.pdf</p> Source code in <code>speechain/module/decoder/ar_tts.py</code> <pre><code>def turn_on_dropout(self):\n    \"\"\"Turn on the dropout layers of the decoder prenet during inference.\n\n    Reference: the second paragraph from the bottom in Sec 2.2 of\n        'Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions'\n        https://arxiv.org/pdf/1712.05884.pdf\n    \"\"\"\n    assert hasattr(\n        self, \"prenet\"\n    ), \"If you want to apply dropout during TTS inference, your TTS model should have a decoder prenet.\"\n    if not self.prenet.training:\n        self.prenet.train()\n</code></pre>"},{"location":"reference/module/decoder/nar_tts/","title":"nar_tts","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2023.02</p>"},{"location":"reference/module/decoder/nar_tts/#module.decoder.nar_tts.FastSpeech2Decoder","title":"<code>FastSpeech2Decoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder Module for Non-Autoregressive FastSpeech2 model.</p> Source code in <code>speechain/module/decoder/nar_tts.py</code> <pre><code>class FastSpeech2Decoder(Module):\n    \"\"\"Decoder Module for Non-Autoregressive FastSpeech2 model.\"\"\"\n\n    def module_init(\n        self,\n        decoder: Dict,\n        postnet: Dict,\n        pitch_predictor: Dict,\n        energy_predictor: Dict,\n        duration_predictor: Dict,\n        spk_emb: Dict = None,\n        feat_frontend: Dict = None,\n        feat_normalize: Dict or bool = True,\n        pitch_normalize: Dict or bool = True,\n        energy_normalize: Dict or bool = True,\n        reduction_factor: int = 1,\n    ):\n\n        # reduction factor for acoustic feature sequence\n        self.reduction_factor = reduction_factor\n\n        # --- 1. Speaker Embedding Part --- #\n        if spk_emb is not None:\n            # check the model dimension\n            if \"d_model\" in spk_emb.keys() and spk_emb[\"d_model\"] != self.input_size:\n                warnings.warn(\n                    f\"Your input d_model ({spk_emb['d_model']}) is different from the one automatically \"\n                    f\"generated during model initialization ({self.input_size}).\"\n                    f\"Currently, the automatically generated one is used.\"\n                )\n\n            spk_emb[\"d_model\"] = self.input_size\n            self.spk_emb = SpeakerEmbedPrenet(**spk_emb)\n\n        # --- 2. Variance Predictors Part --- #\n        # duration predictor initialization\n        duration_predictor_class = import_class(\n            \"speechain.module.\" + duration_predictor[\"type\"]\n        )\n        duration_predictor[\"conf\"] = (\n            dict()\n            if \"conf\" not in duration_predictor.keys()\n            else duration_predictor[\"conf\"]\n        )\n        # the conv1d embedding layer of duration predictor is automatically turned off\n        duration_predictor[\"conf\"][\"use_conv_emb\"] = False\n        self.duration_predictor = duration_predictor_class(\n            input_size=self.input_size, **duration_predictor[\"conf\"]\n        )\n\n        # pitch predictor initialization\n        pitch_predictor_class = import_class(\n            \"speechain.module.\" + pitch_predictor[\"type\"]\n        )\n        pitch_predictor[\"conf\"] = (\n            dict() if \"conf\" not in pitch_predictor.keys() else pitch_predictor[\"conf\"]\n        )\n        # the conv1d embedding layer of pitch predictor is automatically turned on\n        pitch_predictor[\"conf\"][\"use_conv_emb\"] = True\n        self.pitch_predictor = pitch_predictor_class(\n            input_size=self.input_size, **pitch_predictor[\"conf\"]\n        )\n\n        # energy predictor initialization\n        energy_predictor_class = import_class(\n            \"speechain.module.\" + energy_predictor[\"type\"]\n        )\n        energy_predictor[\"conf\"] = (\n            dict()\n            if \"conf\" not in energy_predictor.keys()\n            else energy_predictor[\"conf\"]\n        )\n        # the conv1d embedding layer of energy predictor is automatically turned on\n        energy_predictor[\"conf\"][\"use_conv_emb\"] = True\n        self.energy_predictor = energy_predictor_class(\n            input_size=self.input_size, **energy_predictor[\"conf\"]\n        )\n\n        # --- 3. Acoustic Feature, Energy, &amp; Pitch Extraction Part --- #\n        # acoustic feature extraction frontend of the E2E TTS decoder\n        if feat_frontend is not None:\n            feat_frontend_class = import_class(\n                \"speechain.module.\" + feat_frontend[\"type\"]\n            )\n            feat_frontend[\"conf\"] = (\n                dict() if \"conf\" not in feat_frontend.keys() else feat_frontend[\"conf\"]\n            )\n            # feature frontend is automatically set to return frame-wise energy\n            feat_frontend[\"conf\"][\"return_energy\"] = True\n            self.feat_frontend = feat_frontend_class(**feat_frontend[\"conf\"])\n            feat_dim = self.feat_frontend.output_size * self.reduction_factor\n        else:\n            feat_dim = None\n\n        # feature normalization layer\n        if feat_normalize is not None and feat_normalize is not False:\n            feat_normalize = dict() if feat_normalize is True else feat_normalize\n            self.feat_normalize = FeatureNormalization(\n                input_size=feat_dim, distributed=self.distributed, **feat_normalize\n            )\n        # energy normalization layer\n        if energy_normalize is not None and energy_normalize is not False:\n            energy_normalize = dict() if energy_normalize is True else energy_normalize\n            self.energy_normalize = FeatureNormalization(\n                input_size=1, distributed=self.distributed, **energy_normalize\n            )\n\n        # feature normalization layer\n        if pitch_normalize is not None and pitch_normalize is not False:\n            pitch_normalize = dict() if pitch_normalize is True else pitch_normalize\n            self.pitch_normalize = FeatureNormalization(\n                input_size=1, distributed=self.distributed, **pitch_normalize\n            )\n\n        # --- 4. Mel-Spectrogram Decoder Part --- #\n        # Initialize decoder\n        decoder_class = import_class(\"speechain.module.\" + decoder[\"type\"])\n        decoder[\"conf\"] = dict() if \"conf\" not in decoder.keys() else decoder[\"conf\"]\n        self.decoder = decoder_class(input_size=self.input_size, **decoder[\"conf\"])\n\n        # initialize prediction layers (feature prediction &amp; stop prediction)\n        self.feat_pred = torch.nn.Linear(\n            in_features=self.decoder.output_size, out_features=feat_dim\n        )\n\n        # Initialize postnet of the decoder\n        postnet_class = import_class(\"speechain.module.\" + postnet[\"type\"])\n        postnet[\"conf\"] = dict() if \"conf\" not in postnet.keys() else postnet[\"conf\"]\n        self.postnet = postnet_class(input_size=feat_dim, **postnet[\"conf\"])\n\n    @staticmethod\n    def average_scalar_by_duration(\n        frame_scalar: torch.Tensor, duration: torch.Tensor, duration_len: torch.Tensor\n    ):\n        \"\"\"Compute the average scalar value for each token in a batch of variable length\n        frames.\n\n        Args:\n          frame_scalar:\n            A float tensor of shape (batch_size, max_frame_len) containing the scalar values of each frame in the batch.\n          duration:\n            An int tensor of shape (batch_size, max_token_len) containing the duration of each token in the batch.\n          duration_len:\n            An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.\n\n        Returns:\n          A tuple containing two tensors:\n          - token_scalar:\n                A float tensor of shape (batch_size, max_token_len) containing the average scalar value of each token.\n          - duration_len:\n                An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.\n\n        Note:\n          The max_frame_len and max_token_len are not necessarily the same.\n        \"\"\"\n        batch_size, max_frame_len = frame_scalar.size()\n        max_token_len = duration.size(1)\n        device = frame_scalar.device\n\n        # Create indices tensor for gather operation\n        end_frame_idxs = torch.cumsum(duration, dim=1).unsqueeze(-1)\n        start_frame_idxs = torch.nn.functional.pad(\n            end_frame_idxs[:, :-1], (0, 0, 1, 0), value=0\n        )\n        range_mat = torch.arange(max_frame_len, device=device)[None, None, :].expand(\n            batch_size, max_token_len, -1\n        )\n        idxs = torch.where(\n            (range_mat &gt;= start_frame_idxs) &amp; (range_mat &lt; end_frame_idxs),\n            range_mat,\n            -1,\n        )\n\n        # Compute the mean scalar value for each token\n        mask = idxs &gt;= 0\n        idxs = torch.clamp(idxs, min=0)\n\n        # Gather the frame scalar values using the index tensor\n        token_scalar = torch.gather(\n            frame_scalar.unsqueeze(1).expand(-1, max_token_len, -1), 2, idxs\n        )\n        token_scalar = torch.where(mask, token_scalar, 0)\n        token_scalar = token_scalar.sum(dim=2) / (mask.sum(dim=2) + 1e-10)\n\n        return token_scalar, duration_len\n\n    def proc_duration(\n        self,\n        duration: torch.Tensor,\n        min_frame_num: int = 0,\n        max_frame_num: int = None,\n        duration_alpha: torch.Tensor = None,\n    ):\n        # modify the duration by duration_alpha during evaluation\n        if not self.training and duration_alpha is not None:\n            duration = duration * duration_alpha\n\n        # convert the negative numbers to zeros\n        duration = torch.clamp(torch.round(duration), min=0)\n        duration_zero_mask = duration == 0\n\n        # round the phoneme duration to the nearest integer\n        duration = torch.clamp(\n            duration,\n            min=round(min_frame_num / self.reduction_factor),\n            max=(\n                None\n                if max_frame_num is None\n                else round(max_frame_num / self.reduction_factor)\n            ),\n        )\n        duration = torch.where(duration_zero_mask, 0, duration)\n        return duration\n\n    def forward(\n        self,\n        enc_text: torch.Tensor,\n        enc_text_mask: torch.Tensor,\n        duration: torch.Tensor = None,\n        duration_len: torch.Tensor = None,\n        pitch: torch.Tensor = None,\n        pitch_len: torch.Tensor = None,\n        feat: torch.Tensor = None,\n        feat_len: torch.Tensor = None,\n        energy: torch.Tensor = None,\n        energy_len: torch.Tensor = None,\n        spk_feat: torch.Tensor = None,\n        spk_ids: torch.Tensor = None,\n        epoch: int = None,\n        min_frame_num: int = None,\n        max_frame_num: int = None,\n        duration_alpha: torch.Tensor = None,\n        energy_alpha: torch.Tensor = None,\n        pitch_alpha: torch.Tensor = None,\n    ):\n\n        # --- 1. Speaker Embedding Combination --- #\n        if hasattr(self, \"spk_emb\"):\n            # extract and process the speaker features (activation is not performed for random speaker feature)\n            spk_feat_lookup, spk_feat = self.spk_emb(spk_ids=spk_ids, spk_feat=spk_feat)\n            # combine the speaker features with the encoder outputs (and the decoder prenet outputs if specified)\n            enc_text, _ = self.spk_emb.combine_spk_feat(\n                spk_feat=spk_feat, spk_feat_lookup=spk_feat_lookup, enc_output=enc_text\n            )\n\n        # --- 2. Acoustic Feature, Pitch, Energy Extraction --- #\n        # in the training and validation stage, input feature data needs to be processed by the feature frontend\n        if feat is not None:\n            # acoustic feature extraction for the raw waveform input\n            if feat.size(-1) == 1:\n                assert hasattr(self, \"feat_frontend\"), (\n                    f\"Currently, {self.__class__.__name__} doesn't support time-domain TTS. \"\n                    f\"Please specify a feature extraction frontend.\"\n                )\n                # no amp operations for the frontend calculation to make sure the feature extraction accuracy\n                with autocast(False):\n                    feat, feat_len, energy, energy_len = self.feat_frontend(\n                        feat, feat_len\n                    )\n\n            # feature normalization\n            if hasattr(self, \"feat_normalize\"):\n                feat, feat_len = self.feat_normalize(\n                    feat, feat_len, group_ids=spk_ids, epoch=epoch\n                )\n\n            # acoustic feature length reduction\n            if self.reduction_factor &gt; 1:\n                _residual = feat.size(1) % self.reduction_factor\n                # clip the redundant part of acoustic features\n                if _residual != 0:\n                    feat = feat[:, :-_residual]\n\n                # group the features by the reduction factor\n                batch, feat_maxlen, feat_dim = feat.size()\n                feat = feat.reshape(\n                    batch,\n                    feat_maxlen // self.reduction_factor,\n                    feat_dim * self.reduction_factor,\n                )\n                feat_len = torch.div(\n                    feat_len, self.reduction_factor, rounding_mode=\"floor\"\n                ).type(torch.long)\n\n        if pitch is not None:\n            # pitch normalization\n            if hasattr(self, \"pitch_normalize\"):\n                pitch, pitch_len = self.pitch_normalize(\n                    pitch, pitch_len, group_ids=spk_ids, epoch=epoch\n                )\n\n            # pitch length reduction\n            if self.reduction_factor &gt; 1:\n                _residual = pitch.size(1) % self.reduction_factor\n                # clip the redundant part of pitch\n                if _residual != 0:\n                    pitch = pitch[:, :-_residual]\n\n                # average the pitch by the reduction factor\n                batch, pitch_maxlen = pitch.size()\n                pitch = pitch.reshape(\n                    batch, pitch_maxlen // self.reduction_factor, self.reduction_factor\n                ).mean(dim=-1)\n                pitch_len = torch.div(\n                    pitch_len, self.reduction_factor, rounding_mode=\"floor\"\n                ).type(torch.long)\n\n        if energy is not None:\n            # energy normalization\n            if hasattr(self, \"energy_normalize\"):\n                energy, energy_len = self.energy_normalize(\n                    energy, energy_len, group_ids=spk_ids, epoch=epoch\n                )\n\n            # energy length reduction\n            if self.reduction_factor &gt; 1:\n                _residual = energy.size(1) % self.reduction_factor\n                # clip the redundant part of energy\n                if _residual != 0:\n                    energy = energy[:, :-_residual]\n\n                # average the energy by the reduction factor\n                batch, energy_maxlen = energy.size()\n                energy = energy.reshape(\n                    batch, energy_maxlen // self.reduction_factor, self.reduction_factor\n                ).mean(dim=-1)\n                energy_len = torch.div(\n                    energy_len, self.reduction_factor, rounding_mode=\"floor\"\n                ).type(torch.long)\n\n        # target labels checking for training stage\n        if self.training:\n            assert (\n                energy is not None\n                and energy_len is not None\n                and pitch is not None\n                and pitch_len is not None\n                and duration is not None\n                and duration_len is not None\n            ), \"During training, please give the ground-truth of energy, pitch, and duration.\"\n\n        # --- 3. Duration Prediction --- #\n        # note: pred_duration here is in the log domain!\n        pred_duration_outputs = self.duration_predictor(\n            enc_text, make_len_from_mask(enc_text_mask)\n        )\n        # without duration gate predictions\n        if len(pred_duration_outputs) == 2:\n            pred_duration, enc_text_len = pred_duration_outputs\n            pred_duration_gate = None\n        # with duration gate predictions\n        elif len(pred_duration_outputs) == 3:\n            pred_duration, enc_text_len, pred_duration_gate = pred_duration_outputs\n        else:\n            raise RuntimeError\n\n        # do teacher-forcing if ground-truth duration is given\n        if duration is not None:\n            # turn the duration from the second to the frame number\n            used_duration = self.proc_duration(\n                duration=duration\n                / duration.sum(dim=-1, keepdims=True)\n                * feat_len.unsqueeze(-1),\n                min_frame_num=min_frame_num,\n                max_frame_num=max_frame_num,\n                duration_alpha=duration_alpha,\n            )\n            used_duration_len = duration_len\n        # do self-decoding by the predicted duration\n        else:\n            # mask the predicted duration by gate predictions if available\n            if pred_duration_gate is not None:\n                pred_duration[pred_duration_gate &gt; 0] = 0.0\n            # use the exp-converted predicted duration\n            used_duration = self.proc_duration(\n                duration=torch.exp(pred_duration) - 1,\n                min_frame_num=min_frame_num,\n                max_frame_num=max_frame_num,\n                duration_alpha=duration_alpha,\n            )\n            used_duration_len = enc_text_len\n            used_duration.masked_fill_(\n                ~make_mask_from_len(enc_text_len, return_3d=False), 0.0\n            )\n\n        # --- 4. Pitch Prediction and Embedding --- #\n        pred_pitch, enc_text_len = self.pitch_predictor(enc_text, enc_text_len)\n        if pitch is not None:\n            # turn the frame-wise pitch values into frame-averaged pitch values\n            pitch, pitch_len = self.average_scalar_by_duration(\n                pitch, used_duration, used_duration_len\n            )\n        # during training, ground-truth pitch is used for embedding\n        # during validation &amp; evaluation, predicted pitch is used for embedding\n        used_pitch = pitch if self.training else pred_pitch\n        # modify the pitch by pitch_alpha during evaluation\n        if not self.training and pitch_alpha is not None:\n            used_pitch = used_pitch * pitch_alpha\n        emb_pitch = self.pitch_predictor.emb_pred_scalar(used_pitch)\n\n        # --- 5. Energy Prediction and Embedding --- #\n        pred_energy, enc_text_len = self.energy_predictor(enc_text, enc_text_len)\n        if energy is not None:\n            # turn the frame-wise energy values into frame-averaged energy values\n            energy, energy_len = self.average_scalar_by_duration(\n                energy, used_duration, used_duration_len\n            )\n        # during training, ground-truth energy is used for embedding\n        # during validation &amp; evaluation, predicted energy is used for embedding\n        used_energy = energy if self.training else pred_energy\n        # modify the energy by energy_alpha during evaluation\n        if not self.training and energy_alpha is not None:\n            used_energy = used_energy * energy_alpha\n        emb_energy = self.energy_predictor.emb_pred_scalar(used_energy)\n\n        # --- 6. Pitch &amp; Energy Embedding Combination --- #\n        enc_text = enc_text + emb_pitch + emb_energy\n\n        # --- 7. Length Regulation --- #\n        # loop each sentence\n        expand_enc_text_list = []\n        for i in range(len(enc_text_len)):\n            # calculate the number of frames needed for each token in the current sentence\n            frame_counts = used_duration[i].long()\n            # expand the phoneme embeddings by the number of frames for each token\n            expand_enc_text_list.append(\n                enc_text[i].repeat_interleave(frame_counts, dim=0)\n            )\n\n        # teacher-forcing by feat_len if it is given\n        if feat_len is not None:\n            expand_enc_text_len = feat_len\n        # self-decoding by expand_enc_text_list if feat_len is not given\n        else:\n            expand_enc_text_len = torch.LongTensor(\n                [len(i) for i in expand_enc_text_list]\n            ).to(enc_text.device)\n        expand_enc_text_maxlen = expand_enc_text_len.max().item()\n\n        # assemble all the expanded enc_text from the list into a single 3d tensor\n        for i in range(len(expand_enc_text_list)):\n            len_diff = expand_enc_text_maxlen - len(expand_enc_text_list[i])\n            if len_diff &gt; 0:\n                expand_enc_text_list[i] = torch.nn.functional.pad(\n                    expand_enc_text_list[i], (0, 0, 0, len_diff), \"constant\", 0\n                )\n            elif len_diff &lt; 0:\n                # note: len_diff is a negative integer\n                expand_enc_text_list[i] = expand_enc_text_list[i][\n                    :len_diff\n                ].contiguous()\n            expand_enc_text_list[i] = expand_enc_text_list[i].unsqueeze(0)\n        expand_enc_text = torch.cat(expand_enc_text_list)\n\n        # --- 6. Mel-Spectrogram Prediction --- #\n        dec_feat, dec_feat_mask, dec_attmat, dec_hidden = self.decoder(\n            expand_enc_text, make_mask_from_len(expand_enc_text_len)\n        )\n        pred_feat_before = self.feat_pred(dec_feat)\n        pred_feat_after = pred_feat_before + self.postnet(\n            pred_feat_before, make_len_from_mask(dec_feat_mask)\n        )\n\n        # the returned duration is used_duration (the actual one used for length regulation)\n        return (\n            pred_feat_before,\n            pred_feat_after,\n            make_len_from_mask(dec_feat_mask),\n            feat,\n            feat_len,\n            pred_pitch,\n            pitch,\n            pitch_len,\n            pred_energy,\n            energy,\n            energy_len,\n            pred_duration,\n            pred_duration_gate,\n            used_duration,\n            used_duration_len,\n            dec_attmat,\n            dec_hidden,\n        )\n</code></pre>"},{"location":"reference/module/decoder/nar_tts/#module.decoder.nar_tts.FastSpeech2Decoder.average_scalar_by_duration","title":"<code>average_scalar_by_duration(frame_scalar, duration, duration_len)</code>  <code>staticmethod</code>","text":"<p>Compute the average scalar value for each token in a batch of variable length frames.</p> <p>Parameters:</p> Name Type Description Default <code>frame_scalar</code> <code>Tensor</code> <p>A float tensor of shape (batch_size, max_frame_len) containing the scalar values of each frame in the batch.</p> required <code>duration</code> <code>Tensor</code> <p>An int tensor of shape (batch_size, max_token_len) containing the duration of each token in the batch.</p> required <code>duration_len</code> <code>Tensor</code> <p>An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.</p> required <p>Returns:</p> Type Description <p>A tuple containing two tensors:</p> <ul> <li>token_scalar:     A float tensor of shape (batch_size, max_token_len) containing the average scalar value of each token.</li> </ul> <ul> <li>duration_len:     An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.</li> </ul> Note <p>The max_frame_len and max_token_len are not necessarily the same.</p> Source code in <code>speechain/module/decoder/nar_tts.py</code> <pre><code>@staticmethod\ndef average_scalar_by_duration(\n    frame_scalar: torch.Tensor, duration: torch.Tensor, duration_len: torch.Tensor\n):\n    \"\"\"Compute the average scalar value for each token in a batch of variable length\n    frames.\n\n    Args:\n      frame_scalar:\n        A float tensor of shape (batch_size, max_frame_len) containing the scalar values of each frame in the batch.\n      duration:\n        An int tensor of shape (batch_size, max_token_len) containing the duration of each token in the batch.\n      duration_len:\n        An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.\n\n    Returns:\n      A tuple containing two tensors:\n      - token_scalar:\n            A float tensor of shape (batch_size, max_token_len) containing the average scalar value of each token.\n      - duration_len:\n            An int tensor of shape (batch_size,) containing the length of each sequence of tokens in the batch.\n\n    Note:\n      The max_frame_len and max_token_len are not necessarily the same.\n    \"\"\"\n    batch_size, max_frame_len = frame_scalar.size()\n    max_token_len = duration.size(1)\n    device = frame_scalar.device\n\n    # Create indices tensor for gather operation\n    end_frame_idxs = torch.cumsum(duration, dim=1).unsqueeze(-1)\n    start_frame_idxs = torch.nn.functional.pad(\n        end_frame_idxs[:, :-1], (0, 0, 1, 0), value=0\n    )\n    range_mat = torch.arange(max_frame_len, device=device)[None, None, :].expand(\n        batch_size, max_token_len, -1\n    )\n    idxs = torch.where(\n        (range_mat &gt;= start_frame_idxs) &amp; (range_mat &lt; end_frame_idxs),\n        range_mat,\n        -1,\n    )\n\n    # Compute the mean scalar value for each token\n    mask = idxs &gt;= 0\n    idxs = torch.clamp(idxs, min=0)\n\n    # Gather the frame scalar values using the index tensor\n    token_scalar = torch.gather(\n        frame_scalar.unsqueeze(1).expand(-1, max_token_len, -1), 2, idxs\n    )\n    token_scalar = torch.where(mask, token_scalar, 0)\n    token_scalar = token_scalar.sum(dim=2) / (mask.sum(dim=2) + 1e-10)\n\n    return token_scalar, duration_len\n</code></pre>"},{"location":"reference/module/encoder/","title":"encoder","text":""},{"location":"reference/module/encoder/asr/","title":"asr","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/encoder/asr/#module.encoder.asr.ASREncoder","title":"<code>ASREncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/encoder/asr.py</code> <pre><code>class ASREncoder(Module):\n    \"\"\"\"\"\"\n\n    def module_init(\n        self,\n        encoder: Dict,\n        frontend: Dict = None,\n        normalize: Dict or bool = None,\n        specaug: Dict or bool = None,\n        prenet: Dict = None,\n    ):\n        \"\"\"\n\n        Args:\n            (mandatory) frontend:\n            (optional) normalize:\n            (optional) specaug:\n            (optional) prenet:\n            (mandatory) encoder:\n\n        \"\"\"\n        # temporary register for connecting two sequential modules\n        _prev_output_size = None\n\n        # acoustic feature extraction frontend of the E2E ASR encoder\n        if frontend is not None:\n            frontend_class = import_class(\"speechain.module.\" + frontend[\"type\"])\n            frontend[\"conf\"] = (\n                dict() if \"conf\" not in frontend.keys() else frontend[\"conf\"]\n            )\n            self.frontend = frontend_class(**frontend[\"conf\"])\n            _prev_output_size = self.frontend.output_size\n\n        # feature normalization layer\n        if normalize is not None and normalize is not False:\n            normalize = dict() if normalize is True else normalize\n            self.normalize = FeatureNormalization(\n                input_size=_prev_output_size, distributed=self.distributed, **normalize\n            )\n            _prev_output_size = self.normalize.output_size\n\n        # SpecAugment layer\n        if specaug is not None and specaug is not False:\n            specaug = dict() if specaug is True else specaug\n            self.specaug = SpecAugment(\n                input_size=_prev_output_size,\n                feat_norm=normalize is not None and normalize is not False,\n                **specaug\n            )\n            _prev_output_size = self.specaug.output_size\n\n        # feature embedding layer of the E2E ASR encoder\n        if prenet is not None:\n            prenet_class = import_class(\"speechain.module.\" + prenet[\"type\"])\n            prenet[\"conf\"] = dict() if \"conf\" not in prenet.keys() else prenet[\"conf\"]\n            self.prenet = prenet_class(input_size=_prev_output_size, **prenet[\"conf\"])\n            _prev_output_size = self.prenet.output_size\n\n        # main body of the E2E ASR encoder\n        encoder_class = import_class(\"speechain.module.\" + encoder[\"type\"])\n        encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n        self.encoder = encoder_class(input_size=_prev_output_size, **encoder[\"conf\"])\n        self.output_size = self.encoder.output_size\n\n    def forward(\n        self,\n        feat: torch.Tensor,\n        feat_len: torch.Tensor,\n        epoch: int = None,\n        domain: str = None,\n    ):\n        \"\"\"\n\n        Args:\n            feat:\n            feat_len:\n            epoch:\n            domain:\n\n        Returns:\n\n        \"\"\"\n\n        # acoustic feature extraction for the waveform input, do nothing for the feature input\n        if feat.size(-1) == 1:\n            assert hasattr(\n                self, \"frontend\"\n            ), \"Currently, we don't support time-domain ASR. Please specify a feature extraction frontend!\"\n            # no amp operations for the frontend calculation to make sure the feature accuracy\n            with autocast(False):\n                feat, feat_len = self.frontend(feat, feat_len)\n\n        # feature normalization\n        if hasattr(self, \"normalize\"):\n            feat, feat_len = self.normalize(\n                feat, feat_len, epoch=epoch, group_ids=domain\n            )\n\n        # SpecAugment, only activated during training\n        if self.training and hasattr(self, \"specaug\"):\n            feat, feat_len = self.specaug(feat, feat_len)\n\n        # feature embedding by the encoder prenet\n        if hasattr(self, \"prenet\"):\n            feat, feat_len = self.prenet(feat, feat_len)\n\n        # mask generation for the embedded features\n        feat_mask = make_mask_from_len(feat_len)\n        if feat.is_cuda:\n            feat_mask = feat_mask.cuda(feat.device)\n\n        feat, feat_mask, attmat, hidden = self.encoder(feat, feat_mask)\n        return feat, feat_mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/encoder/asr/#module.encoder.asr.ASREncoder.forward","title":"<code>forward(feat, feat_len, epoch=None, domain=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>feat_len</code> <code>Tensor</code> required <code>epoch</code> <code>int</code> <code>None</code> <code>domain</code> <code>str</code> <code>None</code> <p>Returns:</p> Source code in <code>speechain/module/encoder/asr.py</code> <pre><code>def forward(\n    self,\n    feat: torch.Tensor,\n    feat_len: torch.Tensor,\n    epoch: int = None,\n    domain: str = None,\n):\n    \"\"\"\n\n    Args:\n        feat:\n        feat_len:\n        epoch:\n        domain:\n\n    Returns:\n\n    \"\"\"\n\n    # acoustic feature extraction for the waveform input, do nothing for the feature input\n    if feat.size(-1) == 1:\n        assert hasattr(\n            self, \"frontend\"\n        ), \"Currently, we don't support time-domain ASR. Please specify a feature extraction frontend!\"\n        # no amp operations for the frontend calculation to make sure the feature accuracy\n        with autocast(False):\n            feat, feat_len = self.frontend(feat, feat_len)\n\n    # feature normalization\n    if hasattr(self, \"normalize\"):\n        feat, feat_len = self.normalize(\n            feat, feat_len, epoch=epoch, group_ids=domain\n        )\n\n    # SpecAugment, only activated during training\n    if self.training and hasattr(self, \"specaug\"):\n        feat, feat_len = self.specaug(feat, feat_len)\n\n    # feature embedding by the encoder prenet\n    if hasattr(self, \"prenet\"):\n        feat, feat_len = self.prenet(feat, feat_len)\n\n    # mask generation for the embedded features\n    feat_mask = make_mask_from_len(feat_len)\n    if feat.is_cuda:\n        feat_mask = feat_mask.cuda(feat.device)\n\n    feat, feat_mask, attmat, hidden = self.encoder(feat, feat_mask)\n    return feat, feat_mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/encoder/asr/#module.encoder.asr.ASREncoder.module_init","title":"<code>module_init(encoder, frontend=None, normalize=None, specaug=None, prenet=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>(mandatory) frontend</code> required <code>(optional) normalize</code> required <code>(optional) specaug</code> required <code>(optional) prenet</code> required <code>(mandatory) encoder</code> required Source code in <code>speechain/module/encoder/asr.py</code> <pre><code>def module_init(\n    self,\n    encoder: Dict,\n    frontend: Dict = None,\n    normalize: Dict or bool = None,\n    specaug: Dict or bool = None,\n    prenet: Dict = None,\n):\n    \"\"\"\n\n    Args:\n        (mandatory) frontend:\n        (optional) normalize:\n        (optional) specaug:\n        (optional) prenet:\n        (mandatory) encoder:\n\n    \"\"\"\n    # temporary register for connecting two sequential modules\n    _prev_output_size = None\n\n    # acoustic feature extraction frontend of the E2E ASR encoder\n    if frontend is not None:\n        frontend_class = import_class(\"speechain.module.\" + frontend[\"type\"])\n        frontend[\"conf\"] = (\n            dict() if \"conf\" not in frontend.keys() else frontend[\"conf\"]\n        )\n        self.frontend = frontend_class(**frontend[\"conf\"])\n        _prev_output_size = self.frontend.output_size\n\n    # feature normalization layer\n    if normalize is not None and normalize is not False:\n        normalize = dict() if normalize is True else normalize\n        self.normalize = FeatureNormalization(\n            input_size=_prev_output_size, distributed=self.distributed, **normalize\n        )\n        _prev_output_size = self.normalize.output_size\n\n    # SpecAugment layer\n    if specaug is not None and specaug is not False:\n        specaug = dict() if specaug is True else specaug\n        self.specaug = SpecAugment(\n            input_size=_prev_output_size,\n            feat_norm=normalize is not None and normalize is not False,\n            **specaug\n        )\n        _prev_output_size = self.specaug.output_size\n\n    # feature embedding layer of the E2E ASR encoder\n    if prenet is not None:\n        prenet_class = import_class(\"speechain.module.\" + prenet[\"type\"])\n        prenet[\"conf\"] = dict() if \"conf\" not in prenet.keys() else prenet[\"conf\"]\n        self.prenet = prenet_class(input_size=_prev_output_size, **prenet[\"conf\"])\n        _prev_output_size = self.prenet.output_size\n\n    # main body of the E2E ASR encoder\n    encoder_class = import_class(\"speechain.module.\" + encoder[\"type\"])\n    encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n    self.encoder = encoder_class(input_size=_prev_output_size, **encoder[\"conf\"])\n    self.output_size = self.encoder.output_size\n</code></pre>"},{"location":"reference/module/encoder/speaker/","title":"speaker","text":""},{"location":"reference/module/encoder/speaker/#module.encoder.speaker.EncoderClassifier","title":"<code>EncoderClassifier</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/encoder/speaker.py</code> <pre><code>class EncoderClassifier(nn.Module):\n    def __init__(self, model_type=\"ecapa\"):\n        super().__init__()\n        self.model_type = model_type\n        self.model = self._create_model()\n\n    def _create_model(self):\n        if self.model_type == \"ecapa\":\n            return self._create_ecapa()\n        elif self.model_type == \"xvector\":\n            return self._create_xvector()\n        else:\n            raise ValueError(f\"Unknown model type: {self.model_type}\")\n\n    def _create_ecapa(self):\n        channels = 512\n        model = nn.Sequential(\n            nn.Conv1d(80, channels, 7, padding=3),\n            nn.BatchNorm1d(channels),\n            nn.ReLU(),\n            nn.Sequential(\n                Res2Block(channels), SEModule(channels), nn.BatchNorm1d(channels)\n            ),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(channels, 192),\n        )\n        return model\n\n    def _create_xvector(self):\n        model = nn.Sequential(\n            nn.Conv1d(80, 512, 5, padding=2),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Conv1d(512, 512, 3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Conv1d(512, 512, 3, padding=1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 192),\n        )\n        return model\n\n    def encode_batch(self, wavs, wav_lens=None):\n        self.eval()\n        with torch.no_grad():\n            x = wavs.transpose(1, 2)\n            embeddings = self.model(x)\n            return F.normalize(embeddings, p=2, dim=1)\n\n    @classmethod\n    def from_hparams(cls, source, savedir=None, run_opts=None):\n        \"\"\"Load pretrained model.\"\"\"\n        model = cls(model_type=\"ecapa\" if \"ecapa\" in source else \"xvector\")\n\n        if run_opts and \"device\" in run_opts:\n            model = model.to(run_opts[\"device\"])\n\n        if savedir:\n            weights_path = os.path.join(savedir, \"encoder.pth\")\n            if os.path.exists(weights_path):\n                model.load_state_dict(\n                    torch.load(weights_path, map_location=run_opts[\"device\"])\n                )\n\n        return model\n</code></pre>"},{"location":"reference/module/encoder/speaker/#module.encoder.speaker.EncoderClassifier.from_hparams","title":"<code>from_hparams(source, savedir=None, run_opts=None)</code>  <code>classmethod</code>","text":"<p>Load pretrained model.</p> Source code in <code>speechain/module/encoder/speaker.py</code> <pre><code>@classmethod\ndef from_hparams(cls, source, savedir=None, run_opts=None):\n    \"\"\"Load pretrained model.\"\"\"\n    model = cls(model_type=\"ecapa\" if \"ecapa\" in source else \"xvector\")\n\n    if run_opts and \"device\" in run_opts:\n        model = model.to(run_opts[\"device\"])\n\n    if savedir:\n        weights_path = os.path.join(savedir, \"encoder.pth\")\n        if os.path.exists(weights_path):\n            model.load_state_dict(\n                torch.load(weights_path, map_location=run_opts[\"device\"])\n            )\n\n    return model\n</code></pre>"},{"location":"reference/module/encoder/test_speaker/","title":"test_speaker","text":""},{"location":"reference/module/encoder/tts/","title":"tts","text":"<p>Author: Sashi Novitasari Affiliation: NAIST (-2022) Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/encoder/tts/#module.encoder.tts.TTSEncoder","title":"<code>TTSEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/encoder/tts.py</code> <pre><code>class TTSEncoder(Module):\n    \"\"\"\"\"\"\n\n    def module_init(\n        self,\n        embedding: Dict,\n        encoder: Dict,\n        prenet: Dict = None,\n        vocab_size: int = None,\n    ):\n        \"\"\"\n\n        Args:\n            encoder:\n            embedding:\n            prenet:\n            vocab_size:\n\n        \"\"\"\n        # temporary register for connecting two sequential modules\n        _prev_output_size = None\n\n        # Token embedding layer\n        embedding_class = import_class(\"speechain.module.\" + embedding[\"type\"])\n        embedding[\"conf\"] = (\n            dict() if \"conf\" not in embedding.keys() else embedding[\"conf\"]\n        )\n        self.embedding = embedding_class(vocab_size=vocab_size, **embedding[\"conf\"])\n        _prev_output_size = self.embedding.output_size\n\n        # TTS Encoder Prenet\n        if prenet is not None:\n            prenet_class = import_class(\"speechain.module.\" + prenet[\"type\"])\n            prenet[\"conf\"] = dict() if \"conf\" not in prenet.keys() else prenet[\"conf\"]\n            self.prenet = prenet_class(input_size=_prev_output_size, **prenet[\"conf\"])\n            _prev_output_size = self.prenet.output_size\n\n        # main body of the E2E TTS encoder\n        encoder_class = import_class(\"speechain.module.\" + encoder[\"type\"])\n        encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n        self.encoder = encoder_class(input_size=_prev_output_size, **encoder[\"conf\"])\n\n        # register the output size for assembly\n        self.output_size = self.encoder.output_size\n\n    def forward(self, text: torch.Tensor, text_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            text: (torch size: batch, maxlen)\n            text_len: (torch size: batch )\n\n        Returns:\n            enc_outputs: dict\n        \"\"\"\n        # Token Embedding\n        text = self.embedding(text)\n\n        # Prenet Processing if specified\n        if hasattr(self, \"prenet\"):\n            text, text_len = self.prenet(text, text_len)\n\n        # TTS Encoding\n        text, text_mask, attmat, hidden = self.encoder(\n            text, make_mask_from_len(text_len)\n        )\n        return text, text_mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/encoder/tts/#module.encoder.tts.TTSEncoder.forward","title":"<code>forward(text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>(torch size: batch, maxlen)</p> required <code>text_len</code> <code>Tensor</code> <p>(torch size: batch )</p> required <p>Returns:</p> Name Type Description <code>enc_outputs</code> <p>dict</p> Source code in <code>speechain/module/encoder/tts.py</code> <pre><code>def forward(self, text: torch.Tensor, text_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        text: (torch size: batch, maxlen)\n        text_len: (torch size: batch )\n\n    Returns:\n        enc_outputs: dict\n    \"\"\"\n    # Token Embedding\n    text = self.embedding(text)\n\n    # Prenet Processing if specified\n    if hasattr(self, \"prenet\"):\n        text, text_len = self.prenet(text, text_len)\n\n    # TTS Encoding\n    text, text_mask, attmat, hidden = self.encoder(\n        text, make_mask_from_len(text_len)\n    )\n    return text, text_mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/encoder/tts/#module.encoder.tts.TTSEncoder.module_init","title":"<code>module_init(embedding, encoder, prenet=None, vocab_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Dict</code> required <code>embedding</code> <code>Dict</code> required <code>prenet</code> <code>Dict</code> <code>None</code> <code>vocab_size</code> <code>int</code> <code>None</code> Source code in <code>speechain/module/encoder/tts.py</code> <pre><code>def module_init(\n    self,\n    embedding: Dict,\n    encoder: Dict,\n    prenet: Dict = None,\n    vocab_size: int = None,\n):\n    \"\"\"\n\n    Args:\n        encoder:\n        embedding:\n        prenet:\n        vocab_size:\n\n    \"\"\"\n    # temporary register for connecting two sequential modules\n    _prev_output_size = None\n\n    # Token embedding layer\n    embedding_class = import_class(\"speechain.module.\" + embedding[\"type\"])\n    embedding[\"conf\"] = (\n        dict() if \"conf\" not in embedding.keys() else embedding[\"conf\"]\n    )\n    self.embedding = embedding_class(vocab_size=vocab_size, **embedding[\"conf\"])\n    _prev_output_size = self.embedding.output_size\n\n    # TTS Encoder Prenet\n    if prenet is not None:\n        prenet_class = import_class(\"speechain.module.\" + prenet[\"type\"])\n        prenet[\"conf\"] = dict() if \"conf\" not in prenet.keys() else prenet[\"conf\"]\n        self.prenet = prenet_class(input_size=_prev_output_size, **prenet[\"conf\"])\n        _prev_output_size = self.prenet.output_size\n\n    # main body of the E2E TTS encoder\n    encoder_class = import_class(\"speechain.module.\" + encoder[\"type\"])\n    encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n    self.encoder = encoder_class(input_size=_prev_output_size, **encoder[\"conf\"])\n\n    # register the output size for assembly\n    self.output_size = self.encoder.output_size\n</code></pre>"},{"location":"reference/module/frontend/","title":"frontend","text":""},{"location":"reference/module/frontend/delta_feat/","title":"delta_feat","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/frontend/delta_feat/#module.frontend.delta_feat.DeltaFeature","title":"<code>DeltaFeature</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/frontend/delta_feat.py</code> <pre><code>class DeltaFeature(Module):\n    \"\"\"\"\"\"\n\n    def module_init(self, delta_order: int = 1, delta_N: int = 2):\n        \"\"\"\n        modified from https://github.com/jameslyons/python_speech_features/blob/master/python_speech_features/base.py#L195\n\n        Args:\n            delta_order:\n            delta_N:\n\n        Returns:\n\n        \"\"\"\n        assert isinstance(delta_order, int) and delta_order in [\n            1,\n            2,\n        ], f\"delta_order must be an integer of 1 or 2, but got delta_order={delta_order}\"\n        self.delta_order = delta_order\n\n        # initialze the filters for extracting delta features\n        assert (\n            isinstance(delta_N, int) and delta_N &gt;= 1\n        ), \"delta_N must be a positive integer equal to or larger than 1\"\n        self.delta_N = delta_N\n        _filter_weights = torch.arange(-delta_N, delta_N + 1) / (\n            2 * sum([i**2 for i in range(1, delta_N + 1)])\n        )\n        _delta_filters = torch.nn.Conv2d(\n            in_channels=1,\n            out_channels=1,\n            kernel_size=(2 * delta_N + 1, 1),\n            padding=(delta_N, 0),\n            bias=False,\n        )\n        _delta_filters.weight = torch.nn.Parameter(\n            _filter_weights.reshape(1, 1, -1, 1), requires_grad=False\n        )\n\n        # move the weight from _parameters to _buffers\n        _para_keys = list(_delta_filters._parameters.keys())\n        for name in _para_keys:\n            _delta_filters._buffers[name] = _delta_filters._parameters.pop(name)\n        self.delta_filters = _delta_filters\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat:\n            feat_len:\n\n        Returns:\n\n        \"\"\"\n\n        # first-order derivative\n        feat_stack = [feat]\n        delta_feat = self.delta_filters(feat.unsqueeze(1)).squeeze(1)\n        feat_stack.append(delta_feat)\n\n        # (Optional) second-order derivative\n        if self.delta_order == 2:\n            delta2_feat = self.delta_filters(delta_feat.unsqueeze(1)).squeeze(1)\n            feat_stack.append(delta2_feat)\n\n        # combine the original features with all delta features\n        feat = torch.cat(feat_stack, dim=-1)\n\n        return feat, feat_len\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\\n\"\n            f\"delta_order={self.delta_order}, \"\n            f\"delta_N={self.delta_N}\"\n            f\"\\n)\"\n        )\n</code></pre>"},{"location":"reference/module/frontend/delta_feat/#module.frontend.delta_feat.DeltaFeature.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>feat_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/frontend/delta_feat.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat:\n        feat_len:\n\n    Returns:\n\n    \"\"\"\n\n    # first-order derivative\n    feat_stack = [feat]\n    delta_feat = self.delta_filters(feat.unsqueeze(1)).squeeze(1)\n    feat_stack.append(delta_feat)\n\n    # (Optional) second-order derivative\n    if self.delta_order == 2:\n        delta2_feat = self.delta_filters(delta_feat.unsqueeze(1)).squeeze(1)\n        feat_stack.append(delta2_feat)\n\n    # combine the original features with all delta features\n    feat = torch.cat(feat_stack, dim=-1)\n\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/frontend/delta_feat/#module.frontend.delta_feat.DeltaFeature.module_init","title":"<code>module_init(delta_order=1, delta_N=2)</code>","text":"<p>modified from https://github.com/jameslyons/python_speech_features/blob/master/python_speech_features/base.py#L195</p> <p>Parameters:</p> Name Type Description Default <code>delta_order</code> <code>int</code> <code>1</code> <code>delta_N</code> <code>int</code> <code>2</code> <p>Returns:</p> Source code in <code>speechain/module/frontend/delta_feat.py</code> <pre><code>def module_init(self, delta_order: int = 1, delta_N: int = 2):\n    \"\"\"\n    modified from https://github.com/jameslyons/python_speech_features/blob/master/python_speech_features/base.py#L195\n\n    Args:\n        delta_order:\n        delta_N:\n\n    Returns:\n\n    \"\"\"\n    assert isinstance(delta_order, int) and delta_order in [\n        1,\n        2,\n    ], f\"delta_order must be an integer of 1 or 2, but got delta_order={delta_order}\"\n    self.delta_order = delta_order\n\n    # initialze the filters for extracting delta features\n    assert (\n        isinstance(delta_N, int) and delta_N &gt;= 1\n    ), \"delta_N must be a positive integer equal to or larger than 1\"\n    self.delta_N = delta_N\n    _filter_weights = torch.arange(-delta_N, delta_N + 1) / (\n        2 * sum([i**2 for i in range(1, delta_N + 1)])\n    )\n    _delta_filters = torch.nn.Conv2d(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=(2 * delta_N + 1, 1),\n        padding=(delta_N, 0),\n        bias=False,\n    )\n    _delta_filters.weight = torch.nn.Parameter(\n        _filter_weights.reshape(1, 1, -1, 1), requires_grad=False\n    )\n\n    # move the weight from _parameters to _buffers\n    _para_keys = list(_delta_filters._parameters.keys())\n    for name in _para_keys:\n        _delta_filters._buffers[name] = _delta_filters._parameters.pop(name)\n    self.delta_filters = _delta_filters\n</code></pre>"},{"location":"reference/module/frontend/linear2mel/","title":"linear2mel","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/frontend/linear2mel/#module.frontend.linear2mel.LinearSpec2MelSpec","title":"<code>LinearSpec2MelSpec</code>","text":"<p>               Bases: <code>Module</code></p> <p>The input is linear spectrogram extracted by STFT and the output is (log-)mel spectrogram The mel-fbank is implemented by a linear layer without the bias vector.</p> Source code in <code>speechain/module/frontend/linear2mel.py</code> <pre><code>class LinearSpec2MelSpec(Module):\n    \"\"\"The input is linear spectrogram extracted by STFT and the output is (log-)mel\n    spectrogram The mel-fbank is implemented by a linear layer without the bias\n    vector.\"\"\"\n\n    def module_init(\n        self,\n        n_fft: int,\n        n_mels: int,\n        sr: int = 16000,\n        fmin: float = 0.0,\n        fmax: float = None,\n        clamp: float = 1e-10,\n        logging: bool = True,\n        log_base: float = 10.0,\n        mel_scale: str = \"slaney\",\n        mel_norm: bool = True,\n        mag_spec: bool = False,\n    ):\n        \"\"\"The difference between two different options of mel_scale, i.e., 'htk' and\n        'slaney', is the relationship between the linear frequency (Hz) and mel\n        frequency.\n\n            1. For 'htk', the mel frequency is always logarithmic to the linear frequency by the following formula:\n                mel = 2595.0 * np.log10(1.0 + hz / 700.0)\n            2. For 'slaney', the mel frequency is linear to the linear frequency below 1K Hz and logarithmic above 1K Hz\n        In the initalization function, the default configuration is mel_scale = 'slaney' and mel_norm=True (the filters\n        will be normalized by the filter width).\n\n        A simple calculation procedure of 'htk'-scaled mel-fbank is shown below. For details about mel-scales, please\n        refer to http://librosa.org/doc/latest/generated/librosa.mel_frequencies.html?highlight=mel_frequencies#librosa.mel_frequencies\n            &gt;&gt;&gt; def hz2mel(hz: float or torch.Tensor):\n            ...     return 2595 * math.log10(1 + hz / 700)\n            &gt;&gt;&gt; def mel2hz(mel: float or torch.Tensor):\n            ...     return 700 * (10 ** (mel / 2595) - 1) \\\n\n            &gt;&gt;&gt; # --- Initialization for Mel-Fbank Matrix Production --- #\n            ... # frequency axis of the linear spectrogram\n            ... src_hz_points = torch.linspace(0, self.sr // 2, self.stft_dim).repeat(self.n_mels, 1)\n            ... # mel-frequency axis of the mel spectrogram, [mel(0), ..., mel(stft_dim + 1)]\n            ... # Note: there are two auxiliary points mel(0) and mel(stft_dim + 1)\n            ... mel_ranges = torch.linspace(hz2mel(self.fmin), hz2mel(self.fmax), n_mels + 2)\n            ... # frequency axis of the mel spectrogram\n            ... hz_ranges = mel2hz(mel_ranges)\n\n            &gt;&gt;&gt; # --- Left Slope Calculation --- #\n            ... # left mel-band width, [mel(1) - mel(0), ..., mel(stft_dim) - mel(stft_dim - 1)]\n            ... mel_left_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[:-1].repeat(self.stft_dim, 1).transpose(0, 1)\n            ... # left-shifted mel-frequency, [mel(0), ..., mel(stft_dim - 1)]\n            ... mel_left_hz_points = hz_ranges[: -2].repeat(self.stft_dim, 1).transpose(0, 1)\n            ... # slope values of the left mel-band\n            ... # i.e. (hz - mel(m - 1)) / (mel(m) - mel(m - 1)) where m in [1, ..., stft_dim]\n            ... left_slopes = (src_hz_points - mel_left_hz_points) / mel_left_hz_bands\n            ... # slope masks of the left mel-band\n            ... # True for the frequency in [mel(m - 1), mel(m)] where m in [1, ..., stft_dim]\n            ... left_masks = torch.logical_and(left_slopes &gt;= 0, left_slopes &lt;= 1)\n\n            &gt;&gt;&gt; # --- Right Slope Calculation --- #\n            ... # right mel-band width, [mel(2) - mel(1), ..., mel(stft_dim + 1) - mel(stft_dim)]\n            ... mel_right_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[1:].repeat(self.stft_dim, 1).transpose(0, 1)\n            ... # right-shifted mel-frequency, [mel(2), ..., mel(stft_dim + 1)]\n            ... mel_right_hz_points = hz_ranges[2:].repeat(self.stft_dim, 1).transpose(0, 1)\n            ... # slope values of the right mel-band\n            ... # i.e. (mel(m + 1) - hz) / (mel(m + 1) - mel(m)) where m in [1, ..., stft_dim]\n            ... right_slopes = (mel_right_hz_points - src_hz_points) / mel_right_hz_bands\n            ... # slope masks of the right mel-band\n            ... # True for the frequency in [mel(m), mel(m + 1)] where m in [1, ..., stft_dim]\n            ... right_masks = torch.logical_and(right_slopes &gt;= 0, right_slopes &lt; 1)\n\n            &gt;&gt;&gt; # --- Mel-Fbank Matrix Generation --- #\n            ... mel_matrix = torch.zeros(self.n_mels, self.stft_dim)\n            ... mel_matrix[left_masks] = left_slopes[left_masks]\n            ... mel_matrix[right_masks] = right_slopes[right_masks]\n\n        Args:\n            sr: int\n                The sampling rate of the input speech waveforms.\n            n_fft: int\n                The number of Fourier point used for STFT\n            n_mels: int\n                The number of filters in the mel-fbank\n            fmin: float\n                The minimal frequency for the mel-fbank\n            fmax: float\n                The maximal frequency for the mel-fbank\n            clamp: float\n                The minimal number for the log-mel spectrogram. Used for numerical stability.\n            logging: bool\n                Controls whether to take log for the mel spectrogram.\n            log_base: float\n                The log base for the log-mel spectrogram. None means the natural log base e.\n                This argument is effective when mel_norm=True (ESPNET style)\n            mel_scale: str\n                The tyle of mel-scale of the mel-fbank. 'htk' for SpeechBrain style and 'slaney' for ESPNET style.\n            mel_norm: bool\n                Whether perform the area normalization to the mel-fbank filters.\n                True for ESPNET style and False for SpeechBrain style.\n            mag_spec: bool\n                Whether the input linear spectrogram is the magnitude. Used for decibel calculation.\n                This argument is effective when mel_norm=False (SpeechBrain style)\n        \"\"\"\n        # fundamental arguments\n        self.sr = sr\n        self.stft_dim = n_fft // 2 + 1\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax if fmax is not None else sr // 2\n        assert (\n            self.fmax &gt; self.fmin\n        ), f\"fmax must be larger than fmin, but got fmax={self.fmax} and fmin={self.fmin}!\"\n\n        # mel-scale-related arguments\n        assert mel_scale in [\n            \"htk\",\n            \"slaney\",\n        ], f\"mel_scale must be either 'htk' or 'slaney', but got mel_scale={mel_scale}\"\n        self.mel_scale = mel_scale\n        self.mel_norm = mel_norm\n\n        # mel-fbank generation\n        mel_matrix = torchaudio.functional.melscale_fbanks(\n            sample_rate=self.sr,\n            n_mels=self.n_mels,\n            n_freqs=self.stft_dim,\n            f_min=self.fmin,\n            f_max=self.fmax,\n            norm=\"slaney\" if self.mel_norm else None,\n            mel_scale=self.mel_scale,\n        ).T\n\n        # implement mel-fbank extraction by a linear layer\n        mel_fbanks = torch.nn.Linear(\n            in_features=self.stft_dim, out_features=self.n_mels, bias=False\n        )\n        mel_fbanks.weight = torch.nn.Parameter(mel_matrix, requires_grad=False)\n\n        # move the weight from _parameters to _buffers so that these parameters won't influence the training\n        _para_keys = list(mel_fbanks._parameters.keys())\n        for name in _para_keys:\n            mel_fbanks._buffers[name] = mel_fbanks._parameters.pop(name)\n        self.mel_fbanks = mel_fbanks\n\n        # logging-related arguments\n        self.clamp = clamp\n        self.logging = logging\n        self.log_base = log_base\n        self.mag_spec = mag_spec\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, speech_maxlen, stft_dim)\n                The input linear spectrograms\n            feat_len: (batch,)\n                The lengths of the input linear spectrograms\n\n        Returns:\n            The log-mel spectrograms with their lengths.\n\n        \"\"\"\n        # extract mel-scale spectrogram\n        feat = self.mel_fbanks(feat)\n\n        # take the logarithm operation\n        if self.logging:\n            # pre-log clamping for numerical stability\n            feat = torch.clamp(input=feat, min=self.clamp)\n\n            # go through the logarithm operation for the mel-fbanks\n            feat = feat.log()\n            if self.log_base is not None:\n                feat /= math.log(self.log_base)\n\n        return feat, feat_len\n\n    def recover(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch_size, feat_maxlen, mel_dim)\n            feat_len: (batch_size,)\n\n        Returns:\n\n        \"\"\"\n        # recover the logarithm operation\n        if self.logging:\n            feat = torch.pow(\n                torch.full_like(\n                    feat, fill_value=torch.e if self.log_base is None else self.log_base\n                ),\n                feat,\n            )\n\n        # recover the mel spectrograms back to linear spectrograms\n        feat = torch.linalg.lstsq(\n            self.mel_fbanks.weight.data.unsqueeze(0),\n            feat.transpose(-2, -1),\n            # the default driver for CPU data is 'gelsy' which doesn't work and the result is zero\n            driver=\"gels\",\n        ).solution.transpose(-2, -1)\n\n        # turn the silence part of the shorter utterances to zeros\n        for i in range(len(feat_len)):\n            if feat_len[i] != feat_len.max():\n                feat[i][feat_len[i] :] = 0\n\n        # clamp the spectrogram for numerical stability\n        return torch.clamp(feat, min=1e-10)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\\n\"\n            f\"stft_dim={self.stft_dim}, \"\n            f\"n_mels={self.n_mels}, \"\n            f\"fmin={self.fmin}, \"\n            f\"fmax={self.fmax}, \"\n            f\"mel_scale={self.mel_scale}, \"\n            f\"mel_norm={self.mel_norm}\"\n            f\"\\n)\"\n        )\n</code></pre>"},{"location":"reference/module/frontend/linear2mel/#module.frontend.linear2mel.LinearSpec2MelSpec.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, speech_maxlen, stft_dim) The input linear spectrograms</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The lengths of the input linear spectrograms</p> required <p>Returns:</p> Type Description <p>The log-mel spectrograms with their lengths.</p> Source code in <code>speechain/module/frontend/linear2mel.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, speech_maxlen, stft_dim)\n            The input linear spectrograms\n        feat_len: (batch,)\n            The lengths of the input linear spectrograms\n\n    Returns:\n        The log-mel spectrograms with their lengths.\n\n    \"\"\"\n    # extract mel-scale spectrogram\n    feat = self.mel_fbanks(feat)\n\n    # take the logarithm operation\n    if self.logging:\n        # pre-log clamping for numerical stability\n        feat = torch.clamp(input=feat, min=self.clamp)\n\n        # go through the logarithm operation for the mel-fbanks\n        feat = feat.log()\n        if self.log_base is not None:\n            feat /= math.log(self.log_base)\n\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/frontend/linear2mel/#module.frontend.linear2mel.LinearSpec2MelSpec.module_init","title":"<code>module_init(n_fft, n_mels, sr=16000, fmin=0.0, fmax=None, clamp=1e-10, logging=True, log_base=10.0, mel_scale='slaney', mel_norm=True, mag_spec=False)</code>","text":"<p>The difference between two different options of mel_scale, i.e., 'htk' and 'slaney', is the relationship between the linear frequency (Hz) and mel frequency.</p> <pre><code>1. For 'htk', the mel frequency is always logarithmic to the linear frequency by the following formula:\n    mel = 2595.0 * np.log10(1.0 + hz / 700.0)\n2. For 'slaney', the mel frequency is linear to the linear frequency below 1K Hz and logarithmic above 1K Hz\n</code></pre> <p>In the initalization function, the default configuration is mel_scale = 'slaney' and mel_norm=True (the filters will be normalized by the filter width).</p> <p>A simple calculation procedure of 'htk'-scaled mel-fbank is shown below. For details about mel-scales, please refer to http://librosa.org/doc/latest/generated/librosa.mel_frequencies.html?highlight=mel_frequencies#librosa.mel_frequencies     &gt;&gt;&gt; def hz2mel(hz: float or torch.Tensor):     ...     return 2595 * math.log10(1 + hz / 700)     &gt;&gt;&gt; def mel2hz(mel: float or torch.Tensor):     ...     return 700 * (10 ** (mel / 2595) - 1)      &gt;&gt;&gt; # --- Initialization for Mel-Fbank Matrix Production --- #     ... # frequency axis of the linear spectrogram     ... src_hz_points = torch.linspace(0, self.sr // 2, self.stft_dim).repeat(self.n_mels, 1)     ... # mel-frequency axis of the mel spectrogram, [mel(0), ..., mel(stft_dim + 1)]     ... # Note: there are two auxiliary points mel(0) and mel(stft_dim + 1)     ... mel_ranges = torch.linspace(hz2mel(self.fmin), hz2mel(self.fmax), n_mels + 2)     ... # frequency axis of the mel spectrogram     ... hz_ranges = mel2hz(mel_ranges)</p> <pre><code>&gt;&gt;&gt; # --- Left Slope Calculation --- #\n... # left mel-band width, [mel(1) - mel(0), ..., mel(stft_dim) - mel(stft_dim - 1)]\n... mel_left_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[:-1].repeat(self.stft_dim, 1).transpose(0, 1)\n... # left-shifted mel-frequency, [mel(0), ..., mel(stft_dim - 1)]\n... mel_left_hz_points = hz_ranges[: -2].repeat(self.stft_dim, 1).transpose(0, 1)\n... # slope values of the left mel-band\n... # i.e. (hz - mel(m - 1)) / (mel(m) - mel(m - 1)) where m in [1, ..., stft_dim]\n... left_slopes = (src_hz_points - mel_left_hz_points) / mel_left_hz_bands\n... # slope masks of the left mel-band\n... # True for the frequency in [mel(m - 1), mel(m)] where m in [1, ..., stft_dim]\n... left_masks = torch.logical_and(left_slopes &gt;= 0, left_slopes &lt;= 1)\n\n&gt;&gt;&gt; # --- Right Slope Calculation --- #\n... # right mel-band width, [mel(2) - mel(1), ..., mel(stft_dim + 1) - mel(stft_dim)]\n... mel_right_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[1:].repeat(self.stft_dim, 1).transpose(0, 1)\n... # right-shifted mel-frequency, [mel(2), ..., mel(stft_dim + 1)]\n... mel_right_hz_points = hz_ranges[2:].repeat(self.stft_dim, 1).transpose(0, 1)\n... # slope values of the right mel-band\n... # i.e. (mel(m + 1) - hz) / (mel(m + 1) - mel(m)) where m in [1, ..., stft_dim]\n... right_slopes = (mel_right_hz_points - src_hz_points) / mel_right_hz_bands\n... # slope masks of the right mel-band\n... # True for the frequency in [mel(m), mel(m + 1)] where m in [1, ..., stft_dim]\n... right_masks = torch.logical_and(right_slopes &gt;= 0, right_slopes &lt; 1)\n\n&gt;&gt;&gt; # --- Mel-Fbank Matrix Generation --- #\n... mel_matrix = torch.zeros(self.n_mels, self.stft_dim)\n... mel_matrix[left_masks] = left_slopes[left_masks]\n... mel_matrix[right_masks] = right_slopes[right_masks]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sr</code> <code>int</code> <p>int The sampling rate of the input speech waveforms.</p> <code>16000</code> <code>n_fft</code> <code>int</code> <p>int The number of Fourier point used for STFT</p> required <code>n_mels</code> <code>int</code> <p>int The number of filters in the mel-fbank</p> required <code>fmin</code> <code>float</code> <p>float The minimal frequency for the mel-fbank</p> <code>0.0</code> <code>fmax</code> <code>float</code> <p>float The maximal frequency for the mel-fbank</p> <code>None</code> <code>clamp</code> <code>float</code> <p>float The minimal number for the log-mel spectrogram. Used for numerical stability.</p> <code>1e-10</code> <code>logging</code> <code>bool</code> <p>bool Controls whether to take log for the mel spectrogram.</p> <code>True</code> <code>log_base</code> <code>float</code> <p>float The log base for the log-mel spectrogram. None means the natural log base e. This argument is effective when mel_norm=True (ESPNET style)</p> <code>10.0</code> <code>mel_scale</code> <code>str</code> <p>str The tyle of mel-scale of the mel-fbank. 'htk' for SpeechBrain style and 'slaney' for ESPNET style.</p> <code>'slaney'</code> <code>mel_norm</code> <code>bool</code> <p>bool Whether perform the area normalization to the mel-fbank filters. True for ESPNET style and False for SpeechBrain style.</p> <code>True</code> <code>mag_spec</code> <code>bool</code> <p>bool Whether the input linear spectrogram is the magnitude. Used for decibel calculation. This argument is effective when mel_norm=False (SpeechBrain style)</p> <code>False</code> Source code in <code>speechain/module/frontend/linear2mel.py</code> <pre><code>def module_init(\n    self,\n    n_fft: int,\n    n_mels: int,\n    sr: int = 16000,\n    fmin: float = 0.0,\n    fmax: float = None,\n    clamp: float = 1e-10,\n    logging: bool = True,\n    log_base: float = 10.0,\n    mel_scale: str = \"slaney\",\n    mel_norm: bool = True,\n    mag_spec: bool = False,\n):\n    \"\"\"The difference between two different options of mel_scale, i.e., 'htk' and\n    'slaney', is the relationship between the linear frequency (Hz) and mel\n    frequency.\n\n        1. For 'htk', the mel frequency is always logarithmic to the linear frequency by the following formula:\n            mel = 2595.0 * np.log10(1.0 + hz / 700.0)\n        2. For 'slaney', the mel frequency is linear to the linear frequency below 1K Hz and logarithmic above 1K Hz\n    In the initalization function, the default configuration is mel_scale = 'slaney' and mel_norm=True (the filters\n    will be normalized by the filter width).\n\n    A simple calculation procedure of 'htk'-scaled mel-fbank is shown below. For details about mel-scales, please\n    refer to http://librosa.org/doc/latest/generated/librosa.mel_frequencies.html?highlight=mel_frequencies#librosa.mel_frequencies\n        &gt;&gt;&gt; def hz2mel(hz: float or torch.Tensor):\n        ...     return 2595 * math.log10(1 + hz / 700)\n        &gt;&gt;&gt; def mel2hz(mel: float or torch.Tensor):\n        ...     return 700 * (10 ** (mel / 2595) - 1) \\\n\n        &gt;&gt;&gt; # --- Initialization for Mel-Fbank Matrix Production --- #\n        ... # frequency axis of the linear spectrogram\n        ... src_hz_points = torch.linspace(0, self.sr // 2, self.stft_dim).repeat(self.n_mels, 1)\n        ... # mel-frequency axis of the mel spectrogram, [mel(0), ..., mel(stft_dim + 1)]\n        ... # Note: there are two auxiliary points mel(0) and mel(stft_dim + 1)\n        ... mel_ranges = torch.linspace(hz2mel(self.fmin), hz2mel(self.fmax), n_mels + 2)\n        ... # frequency axis of the mel spectrogram\n        ... hz_ranges = mel2hz(mel_ranges)\n\n        &gt;&gt;&gt; # --- Left Slope Calculation --- #\n        ... # left mel-band width, [mel(1) - mel(0), ..., mel(stft_dim) - mel(stft_dim - 1)]\n        ... mel_left_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[:-1].repeat(self.stft_dim, 1).transpose(0, 1)\n        ... # left-shifted mel-frequency, [mel(0), ..., mel(stft_dim - 1)]\n        ... mel_left_hz_points = hz_ranges[: -2].repeat(self.stft_dim, 1).transpose(0, 1)\n        ... # slope values of the left mel-band\n        ... # i.e. (hz - mel(m - 1)) / (mel(m) - mel(m - 1)) where m in [1, ..., stft_dim]\n        ... left_slopes = (src_hz_points - mel_left_hz_points) / mel_left_hz_bands\n        ... # slope masks of the left mel-band\n        ... # True for the frequency in [mel(m - 1), mel(m)] where m in [1, ..., stft_dim]\n        ... left_masks = torch.logical_and(left_slopes &gt;= 0, left_slopes &lt;= 1)\n\n        &gt;&gt;&gt; # --- Right Slope Calculation --- #\n        ... # right mel-band width, [mel(2) - mel(1), ..., mel(stft_dim + 1) - mel(stft_dim)]\n        ... mel_right_hz_bands = (hz_ranges[1:] - hz_ranges[:-1])[1:].repeat(self.stft_dim, 1).transpose(0, 1)\n        ... # right-shifted mel-frequency, [mel(2), ..., mel(stft_dim + 1)]\n        ... mel_right_hz_points = hz_ranges[2:].repeat(self.stft_dim, 1).transpose(0, 1)\n        ... # slope values of the right mel-band\n        ... # i.e. (mel(m + 1) - hz) / (mel(m + 1) - mel(m)) where m in [1, ..., stft_dim]\n        ... right_slopes = (mel_right_hz_points - src_hz_points) / mel_right_hz_bands\n        ... # slope masks of the right mel-band\n        ... # True for the frequency in [mel(m), mel(m + 1)] where m in [1, ..., stft_dim]\n        ... right_masks = torch.logical_and(right_slopes &gt;= 0, right_slopes &lt; 1)\n\n        &gt;&gt;&gt; # --- Mel-Fbank Matrix Generation --- #\n        ... mel_matrix = torch.zeros(self.n_mels, self.stft_dim)\n        ... mel_matrix[left_masks] = left_slopes[left_masks]\n        ... mel_matrix[right_masks] = right_slopes[right_masks]\n\n    Args:\n        sr: int\n            The sampling rate of the input speech waveforms.\n        n_fft: int\n            The number of Fourier point used for STFT\n        n_mels: int\n            The number of filters in the mel-fbank\n        fmin: float\n            The minimal frequency for the mel-fbank\n        fmax: float\n            The maximal frequency for the mel-fbank\n        clamp: float\n            The minimal number for the log-mel spectrogram. Used for numerical stability.\n        logging: bool\n            Controls whether to take log for the mel spectrogram.\n        log_base: float\n            The log base for the log-mel spectrogram. None means the natural log base e.\n            This argument is effective when mel_norm=True (ESPNET style)\n        mel_scale: str\n            The tyle of mel-scale of the mel-fbank. 'htk' for SpeechBrain style and 'slaney' for ESPNET style.\n        mel_norm: bool\n            Whether perform the area normalization to the mel-fbank filters.\n            True for ESPNET style and False for SpeechBrain style.\n        mag_spec: bool\n            Whether the input linear spectrogram is the magnitude. Used for decibel calculation.\n            This argument is effective when mel_norm=False (SpeechBrain style)\n    \"\"\"\n    # fundamental arguments\n    self.sr = sr\n    self.stft_dim = n_fft // 2 + 1\n    self.n_mels = n_mels\n    self.fmin = fmin\n    self.fmax = fmax if fmax is not None else sr // 2\n    assert (\n        self.fmax &gt; self.fmin\n    ), f\"fmax must be larger than fmin, but got fmax={self.fmax} and fmin={self.fmin}!\"\n\n    # mel-scale-related arguments\n    assert mel_scale in [\n        \"htk\",\n        \"slaney\",\n    ], f\"mel_scale must be either 'htk' or 'slaney', but got mel_scale={mel_scale}\"\n    self.mel_scale = mel_scale\n    self.mel_norm = mel_norm\n\n    # mel-fbank generation\n    mel_matrix = torchaudio.functional.melscale_fbanks(\n        sample_rate=self.sr,\n        n_mels=self.n_mels,\n        n_freqs=self.stft_dim,\n        f_min=self.fmin,\n        f_max=self.fmax,\n        norm=\"slaney\" if self.mel_norm else None,\n        mel_scale=self.mel_scale,\n    ).T\n\n    # implement mel-fbank extraction by a linear layer\n    mel_fbanks = torch.nn.Linear(\n        in_features=self.stft_dim, out_features=self.n_mels, bias=False\n    )\n    mel_fbanks.weight = torch.nn.Parameter(mel_matrix, requires_grad=False)\n\n    # move the weight from _parameters to _buffers so that these parameters won't influence the training\n    _para_keys = list(mel_fbanks._parameters.keys())\n    for name in _para_keys:\n        mel_fbanks._buffers[name] = mel_fbanks._parameters.pop(name)\n    self.mel_fbanks = mel_fbanks\n\n    # logging-related arguments\n    self.clamp = clamp\n    self.logging = logging\n    self.log_base = log_base\n    self.mag_spec = mag_spec\n</code></pre>"},{"location":"reference/module/frontend/linear2mel/#module.frontend.linear2mel.LinearSpec2MelSpec.recover","title":"<code>recover(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch_size, feat_maxlen, mel_dim)</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch_size,)</p> required <p>Returns:</p> Source code in <code>speechain/module/frontend/linear2mel.py</code> <pre><code>def recover(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch_size, feat_maxlen, mel_dim)\n        feat_len: (batch_size,)\n\n    Returns:\n\n    \"\"\"\n    # recover the logarithm operation\n    if self.logging:\n        feat = torch.pow(\n            torch.full_like(\n                feat, fill_value=torch.e if self.log_base is None else self.log_base\n            ),\n            feat,\n        )\n\n    # recover the mel spectrograms back to linear spectrograms\n    feat = torch.linalg.lstsq(\n        self.mel_fbanks.weight.data.unsqueeze(0),\n        feat.transpose(-2, -1),\n        # the default driver for CPU data is 'gelsy' which doesn't work and the result is zero\n        driver=\"gels\",\n    ).solution.transpose(-2, -1)\n\n    # turn the silence part of the shorter utterances to zeros\n    for i in range(len(feat_len)):\n        if feat_len[i] != feat_len.max():\n            feat[i][feat_len[i] :] = 0\n\n    # clamp the spectrogram for numerical stability\n    return torch.clamp(feat, min=1e-10)\n</code></pre>"},{"location":"reference/module/frontend/speech2linear/","title":"speech2linear","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/frontend/speech2linear/#module.frontend.speech2linear.Speech2LinearSpec","title":"<code>Speech2LinearSpec</code>","text":"<p>               Bases: <code>Module</code></p> <p>The acoustic frontend where the input is raw speech waveforms and the output is linear spectrogram.</p> Source code in <code>speechain/module/frontend/speech2linear.py</code> <pre><code>class Speech2LinearSpec(Module):\n    \"\"\"The acoustic frontend where the input is raw speech waveforms and the output is\n    linear spectrogram.\"\"\"\n\n    def module_init(\n        self,\n        hop_length: int or float,\n        win_length: int or float,\n        sr: int = 16000,\n        n_fft: int = None,\n        preemphasis: float = None,\n        pre_stft_norm: str = None,\n        window: str = \"hann\",\n        center: bool = True,\n        normalized: bool = False,\n        onesided: bool = True,\n        mag_spec: bool = False,\n        return_energy: bool = False,\n        clamp: float = 1e-10,\n        logging: bool = False,\n        log_base: float = None,\n    ):\n        \"\"\"\n        The transformation from waveform to linear spectrogram has 4 steps:\n            1. (optional) waveform pre-emphasis (implemented by Conv1d layer)\n            2. (optional) waveform pre-normalization (not recommended for TTS model)\n            3. STFT processing (implemented by torch.stft())\n            4. STFT postprocessing: zero masking, (optional)sqrt for magnitude, (optional)clamping + logging\n\n        Args:\n            hop_length: int or float\n                the distance between neighboring sliding window frames for STFT.\n                int means the absolute number of sampling point,\n                float means the duration of the speech segment (in seconds).\n            win_length: int or float\n                the size of window frame for STFT.\n                int means the absolute number of sampling point,\n                float means the duration of the speech segment (in seconds).\n            sr: int\n                The sampling rate of the input speech waveforms. Only used for window calculation.\n            n_fft: int\n                The number of Fourier point used for STFT\n            preemphasis: float\n                The preemphasis coefficient before STFT.\n            pre_stft_norm: str\n                The normalization type for the speech waveforms before STFT.\n            window: str\n                The window type for STFT.\n            center: bool\n                 whether to pad input on both sides so that the t-th frame is centered at time t \u00d7 hop_length.\n            normalized: bool\n                controls whether to return the normalized STFT results\n            onesided: bool\n                controls whether to return half of results to avoid redundancy for real inputs.\n            mag_spec: bool\n                controls whether to calculate the linear magnitude spectrogram during STFT.\n                True feeds the linear magnitude spectrogram into mel-fbank.\n                False feeds the linear energy spectrogram into mel-fbank.\n            return_energy: bool\n                Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram\n            clamp: float\n                The minimal number for the log-linear spectrogram. Used for numerical stability.\n            logging: bool\n                Controls whether to take log for the mel spectrogram.\n            log_base: float\n                The log base for the log-mel spectrogram. None means the natural log base e.\n\n        \"\"\"\n        # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n        hop_length = (\n            int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n        )\n        win_length = (\n            int(win_length * sr) if isinstance(win_length, float) else win_length\n        )\n\n        # if n_fft is not given, it will be initialized to the window length\n        if n_fft is None:\n            n_fft = win_length\n\n        # para recording\n        self.output_size = n_fft // 2 + 1 if onesided else n_fft\n        self.win_length = win_length\n        self.hop_length = hop_length\n        self.n_fft = n_fft\n        self.sr = sr\n\n        # preemphasis filter initialization\n        self.preemphasis = preemphasis\n        if preemphasis is not None:\n            _preemph_filter = torch.nn.Conv1d(1, 1, kernel_size=2, bias=False)\n            _filter_weight = torch.Tensor([-self.preemphasis, 1]).reshape(1, 1, 2)\n            _preemph_filter.weight = torch.nn.Parameter(\n                _filter_weight, requires_grad=False\n            )\n\n            # move the weight from _parameters to _buffers so that these parameters won't influence the training\n            _para_keys = list(_preemph_filter._parameters.keys())\n            for name in _para_keys:\n                _preemph_filter._buffers[name] = _preemph_filter._parameters.pop(name)\n            self.preemph_filter = _preemph_filter\n\n        # normalization type before STFT\n        self.pre_stft_norm = pre_stft_norm\n\n        # during stft\n        self.stft_config = dict(\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            window=window,\n            center=center,\n            normalized=normalized,\n            onesided=onesided,\n            return_complex=True,\n        )\n\n        # True=magnitude spectrogram, False=energy spectrogram\n        self.mag_spec = mag_spec\n        self.return_energy = return_energy\n\n        # logging-related arguments\n        self.clamp = clamp\n        self.logging = logging\n        self.log_base = log_base\n\n    def forward(self, speech: torch.Tensor, speech_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            speech: (batch, speech_maxlen, 1) or (batch, speech_maxlen)\n                The input speech data.\n            speech_len: (batch,)\n                The lengths of input speech data\n\n        Returns:\n            The linear spectrograms (energy or magnitude) with their lengths.\n\n        \"\"\"\n        # preparatory checking for the input speech data\n        if speech.dim() == 2:\n            speech.unsqueeze(-1)\n        elif speech.dim() == 3 and speech.size(-1) != 1:\n            raise RuntimeError(\n                f\"Currently, we don't support multi-channel speech waveforms. \"\n                f\"If the speech is given in 3D vectors, the last dimension must be 1. \"\n                f\"But got speech.shape={speech.shape}.\"\n            )\n\n        # --- 1. Waveform Pre-Emphasis --- #\n        # apply preemphasis if specified\n        if self.preemphasis is not None:\n            _previous_speech = F.pad(speech.transpose(1, 2), (1, 0))\n            speech = self.preemph_filter(_previous_speech).transpose(1, 2)\n            # remove redundant preemphasis calculations (there is one meaningless point at the end of some utterances)\n            for i in range(speech_len.size(0)):\n                if speech_len[i] &lt; speech.size(1):\n                    speech[i][speech_len[i] :] = 0.0\n\n        # --- 2. Waveform Pre-Normalization --- #\n        # normalization for audio signals before STFT\n        if self.pre_stft_norm is not None:\n            if self.pre_stft_norm == \"mean_std\":\n                speech = (speech - speech.mean(dim=1)) / speech.std(dim=1)\n            elif self.pre_stft_norm == \"min_max\":\n                speech_min, speech_max = (\n                    speech.min(dim=1, keepdim=True)[0],\n                    speech.max(dim=1, keepdim=True)[0],\n                )\n                speech = (speech - speech_min) / (speech_max - speech_min) * 2 - 1\n            else:\n                raise ValueError\n\n        # --- 3. STFT Processing --- #\n        # initialize the window function lazily at the first training step\n        # borrowed from https://github.com/espnet/espnet/blob/80e042099655822d6543c256910ae655a1a056fd/espnet2/layers/stft.py#L83\n        if isinstance(self.stft_config[\"window\"], str):\n            window_func = getattr(torch, f\"{self.stft_config['window']}_window\")\n            self.stft_config[\"window\"] = window_func(\n                self.stft_config[\"win_length\"], dtype=speech.dtype, device=speech.device\n            )\n        # extract linear spectrogram from signal by stft\n        stft_feat = torch.stft(speech.squeeze(-1), **self.stft_config).transpose(1, 2)\n\n        # calculate the number of frames after STFT\n        if self.stft_config[\"center\"]:\n            speech_len += 2 * torch.div(\n                self.stft_config[\"n_fft\"], 2, rounding_mode=\"floor\"\n            )\n        feat_len = (\n            torch.div(\n                speech_len - self.stft_config[\"n_fft\"],\n                self.stft_config[\"hop_length\"],\n                rounding_mode=\"floor\",\n            )\n            + 1\n        )\n        # get the energy spectrogram\n        linear_spec = stft_feat.real**2 + stft_feat.imag**2\n        # calculate L2-norm of each frame as the energy (magnitude)\n        if self.return_energy:\n            energy, energy_len = (\n                torch.sqrt(torch.clamp(linear_spec.sum(dim=-1), min=1e-10)),\n                feat_len,\n            )\n        else:\n            energy, energy_len = None, None\n\n        # --- 4. STFT Post-Processing --- #\n        # mask all the silence parts of the linear spectrogram to zeros\n        for i in range(feat_len.size(0)):\n            if feat_len[i] &lt; linear_spec.size(1):\n                linear_spec[i][feat_len[i] :] = 0.0\n        # mask all the silence parts of the frame-wise energy to zeros\n        if energy is not None:\n            for i in range(energy_len.size(0)):\n                if energy_len[i] &lt; energy.size(1):\n                    energy[i][energy_len[i] :] = 0.0\n\n        # convert the energy spectrogram to the magnitude spectrogram if specified\n        if self.mag_spec:\n            linear_spec = torch.sqrt(linear_spec)\n\n        # take the logarithm operation\n        if self.logging:\n            # pre-log clamping for numerical stability\n            linear_spec = torch.clamp(input=linear_spec, min=self.clamp)\n            linear_spec = linear_spec.log()\n            if self.log_base is not None:\n                linear_spec /= math.log(self.log_base)\n\n        if self.return_energy:\n            return linear_spec, feat_len, energy, energy_len\n        else:\n            return linear_spec, feat_len\n\n    def recover(\n        self, feat: torch.Tensor, feat_len: torch.Tensor, inv_preemph_winlen: int = 100\n    ):\n        \"\"\"\n\n        Args:\n            feat:\n            feat_len:\n            inv_preemph_winlen:\n\n        Returns:\n\n        \"\"\"\n        # --- STFT Recovery by the GL Algorithm --- #\n        # 1. Randomly initialize the phase between 0 and 2\u03a0\n        # 2. Recover the waveform by the magnitude and phase\n        # 3. Process the synthetic waveform and get the new magnitude and phase\n        # 4. Iteratively do step2 and step3 by the original magnitude and new phase\n        #\n        # Pseudo codes of GL algorithm could be as follow:\n        #     angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n        #     S_complex = np.abs(S).astype(np.complex)\n        #     y = _istft(S_complex * angles, hparams)\n        #     for i in range(hparams.griffin_lim_iters):\n        #         angles = np.exp(1j * np.angle(_stft(y, hparams)))\n        #         y = _istft(S_complex * angles, hparams)\n        # ----------------------------------------- #\n        if not hasattr(self, \"griffin_lim\"):\n            # lazily initialize the linear-to-waveform transformation\n            self.griffin_lim = torchaudio.transforms.GriffinLim(\n                n_fft=self.n_fft,\n                win_length=self.win_length,\n                hop_length=self.hop_length,\n                window_fn=(\n                    getattr(torch, f\"{self.stft_config['window']}_window\")\n                    if isinstance(self.stft_config[\"window\"], str)\n                    else self.stft_config[\"window\"]\n                ),\n                power=1 if self.mag_spec else 2,\n            )\n            self.griffin_lim.window = self.griffin_lim.window.to(feat.device)\n        wav = self.griffin_lim(feat.transpose(-2, -1))\n        wav_len = (feat_len - 1) * self.hop_length\n        assert wav_len.max() == wav.size(\n            1\n        ), \"Something wrong happens when calculating the length of synthetic utterances.\"\n\n        # pre-stft normalization cannot be recovered\n        assert (\n            self.pre_stft_norm is None\n        ), \"waveform pre-stft normalization cannot be recovered for TTS synthesis.\"\n\n        # --- Pre-Emphasis Recovery --- #\n        # Pre-emphasis: Y[n] = X[n] - 0.97 * X[n-1] where Y is the pre-emphasized signal and X is the original signal\n        # Inverse Pre-emphasis: X[n] = Y[n] + 0.97 * X[n-1] = Y[n] + 0.97 * Y[n-1] + ... + (0.97)^n * Y[0]\n        # However, since the signal is usually very long (n is in the unit of 10k), the power of n will infinitely\n        # approach 0 as n grows. So, a slide window is used to perform inverse pre-emphasis which only considers the\n        # previous time steps in a given range.\n        # ----------------------------- #\n        if self.preemphasis is not None:\n            # lazily initialize the inverse preemphasis filters (implemented by Conv1d)\n            if not hasattr(self, \"inv_preemph\"):\n                inv_preemph_filter = torch.nn.Conv1d(\n                    1, 1, kernel_size=inv_preemph_winlen, bias=False\n                )\n\n                # get the sliding window for the inverse pre-emphasis\n                inv_preemph_win = (\n                    torch.pow(\n                        torch.full((inv_preemph_winlen,), fill_value=self.preemphasis),\n                        torch.arange(\n                            start=inv_preemph_winlen - 1,\n                            end=-1,\n                            step=-1,\n                            dtype=torch.int,\n                        ),\n                    )\n                    .reshape(1, 1, -1)\n                    .to(wav.device)\n                )\n                inv_preemph_filter.weight = torch.nn.Parameter(\n                    inv_preemph_win, requires_grad=False\n                )\n\n                # move the weight from _parameters to _buffers so that these parameters won't influence the training\n                _para_keys = list(inv_preemph_filter._parameters.keys())\n                for name in _para_keys:\n                    inv_preemph_filter._buffers[name] = (\n                        inv_preemph_filter._parameters.pop(name)\n                    )\n                self.inv_preemph_filter = inv_preemph_filter\n\n            wav = F.pad(wav.unsqueeze(1), (inv_preemph_winlen - 1, 0))\n            wav = self.inv_preemph_filter(wav).transpose(-2, -1)\n\n        # make sure that the redundant parts are set to silence\n        for i in range(len(wav_len)):\n            wav[i][wav_len[i] :] = 0\n\n        return wav, wav_len\n\n    def get_sample_rate(self):\n        return self.sr\n\n    def __repr__(self) -&gt; str:\n        string = (\n            f\"{self.__class__.__name__}(\\n\"\n            f\"win_length={self.win_length}, \"\n            f\"hop_length={self.hop_length}, \"\n            f\"n_fft={self.n_fft}, \"\n            f\"mag_spec={self.mag_spec}, \"\n        )\n\n        if self.preemphasis is not None:\n            string += f\"preemphasis={self.preemphasis}, \"\n        if self.pre_stft_norm is not None:\n            string += f\"pre_stft_norm={self.pre_stft_norm}, \"\n\n        return string + \"\\n)\"\n</code></pre>"},{"location":"reference/module/frontend/speech2linear/#module.frontend.speech2linear.Speech2LinearSpec.forward","title":"<code>forward(speech, speech_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>speech</code> <code>Tensor</code> <p>(batch, speech_maxlen, 1) or (batch, speech_maxlen) The input speech data.</p> required <code>speech_len</code> <code>Tensor</code> <p>(batch,) The lengths of input speech data</p> required <p>Returns:</p> Type Description <p>The linear spectrograms (energy or magnitude) with their lengths.</p> Source code in <code>speechain/module/frontend/speech2linear.py</code> <pre><code>def forward(self, speech: torch.Tensor, speech_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        speech: (batch, speech_maxlen, 1) or (batch, speech_maxlen)\n            The input speech data.\n        speech_len: (batch,)\n            The lengths of input speech data\n\n    Returns:\n        The linear spectrograms (energy or magnitude) with their lengths.\n\n    \"\"\"\n    # preparatory checking for the input speech data\n    if speech.dim() == 2:\n        speech.unsqueeze(-1)\n    elif speech.dim() == 3 and speech.size(-1) != 1:\n        raise RuntimeError(\n            f\"Currently, we don't support multi-channel speech waveforms. \"\n            f\"If the speech is given in 3D vectors, the last dimension must be 1. \"\n            f\"But got speech.shape={speech.shape}.\"\n        )\n\n    # --- 1. Waveform Pre-Emphasis --- #\n    # apply preemphasis if specified\n    if self.preemphasis is not None:\n        _previous_speech = F.pad(speech.transpose(1, 2), (1, 0))\n        speech = self.preemph_filter(_previous_speech).transpose(1, 2)\n        # remove redundant preemphasis calculations (there is one meaningless point at the end of some utterances)\n        for i in range(speech_len.size(0)):\n            if speech_len[i] &lt; speech.size(1):\n                speech[i][speech_len[i] :] = 0.0\n\n    # --- 2. Waveform Pre-Normalization --- #\n    # normalization for audio signals before STFT\n    if self.pre_stft_norm is not None:\n        if self.pre_stft_norm == \"mean_std\":\n            speech = (speech - speech.mean(dim=1)) / speech.std(dim=1)\n        elif self.pre_stft_norm == \"min_max\":\n            speech_min, speech_max = (\n                speech.min(dim=1, keepdim=True)[0],\n                speech.max(dim=1, keepdim=True)[0],\n            )\n            speech = (speech - speech_min) / (speech_max - speech_min) * 2 - 1\n        else:\n            raise ValueError\n\n    # --- 3. STFT Processing --- #\n    # initialize the window function lazily at the first training step\n    # borrowed from https://github.com/espnet/espnet/blob/80e042099655822d6543c256910ae655a1a056fd/espnet2/layers/stft.py#L83\n    if isinstance(self.stft_config[\"window\"], str):\n        window_func = getattr(torch, f\"{self.stft_config['window']}_window\")\n        self.stft_config[\"window\"] = window_func(\n            self.stft_config[\"win_length\"], dtype=speech.dtype, device=speech.device\n        )\n    # extract linear spectrogram from signal by stft\n    stft_feat = torch.stft(speech.squeeze(-1), **self.stft_config).transpose(1, 2)\n\n    # calculate the number of frames after STFT\n    if self.stft_config[\"center\"]:\n        speech_len += 2 * torch.div(\n            self.stft_config[\"n_fft\"], 2, rounding_mode=\"floor\"\n        )\n    feat_len = (\n        torch.div(\n            speech_len - self.stft_config[\"n_fft\"],\n            self.stft_config[\"hop_length\"],\n            rounding_mode=\"floor\",\n        )\n        + 1\n    )\n    # get the energy spectrogram\n    linear_spec = stft_feat.real**2 + stft_feat.imag**2\n    # calculate L2-norm of each frame as the energy (magnitude)\n    if self.return_energy:\n        energy, energy_len = (\n            torch.sqrt(torch.clamp(linear_spec.sum(dim=-1), min=1e-10)),\n            feat_len,\n        )\n    else:\n        energy, energy_len = None, None\n\n    # --- 4. STFT Post-Processing --- #\n    # mask all the silence parts of the linear spectrogram to zeros\n    for i in range(feat_len.size(0)):\n        if feat_len[i] &lt; linear_spec.size(1):\n            linear_spec[i][feat_len[i] :] = 0.0\n    # mask all the silence parts of the frame-wise energy to zeros\n    if energy is not None:\n        for i in range(energy_len.size(0)):\n            if energy_len[i] &lt; energy.size(1):\n                energy[i][energy_len[i] :] = 0.0\n\n    # convert the energy spectrogram to the magnitude spectrogram if specified\n    if self.mag_spec:\n        linear_spec = torch.sqrt(linear_spec)\n\n    # take the logarithm operation\n    if self.logging:\n        # pre-log clamping for numerical stability\n        linear_spec = torch.clamp(input=linear_spec, min=self.clamp)\n        linear_spec = linear_spec.log()\n        if self.log_base is not None:\n            linear_spec /= math.log(self.log_base)\n\n    if self.return_energy:\n        return linear_spec, feat_len, energy, energy_len\n    else:\n        return linear_spec, feat_len\n</code></pre>"},{"location":"reference/module/frontend/speech2linear/#module.frontend.speech2linear.Speech2LinearSpec.module_init","title":"<code>module_init(hop_length, win_length, sr=16000, n_fft=None, preemphasis=None, pre_stft_norm=None, window='hann', center=True, normalized=False, onesided=True, mag_spec=False, return_energy=False, clamp=1e-10, logging=False, log_base=None)</code>","text":"The transformation from waveform to linear spectrogram has 4 steps <ol> <li>(optional) waveform pre-emphasis (implemented by Conv1d layer)</li> <li>(optional) waveform pre-normalization (not recommended for TTS model)</li> <li>STFT processing (implemented by torch.stft())</li> <li>STFT postprocessing: zero masking, (optional)sqrt for magnitude, (optional)clamping + logging</li> </ol> <p>Parameters:</p> Name Type Description Default <code>hop_length</code> <code>int or float</code> <p>int or float the distance between neighboring sliding window frames for STFT. int means the absolute number of sampling point, float means the duration of the speech segment (in seconds).</p> required <code>win_length</code> <code>int or float</code> <p>int or float the size of window frame for STFT. int means the absolute number of sampling point, float means the duration of the speech segment (in seconds).</p> required <code>sr</code> <code>int</code> <p>int The sampling rate of the input speech waveforms. Only used for window calculation.</p> <code>16000</code> <code>n_fft</code> <code>int</code> <p>int The number of Fourier point used for STFT</p> <code>None</code> <code>preemphasis</code> <code>float</code> <p>float The preemphasis coefficient before STFT.</p> <code>None</code> <code>pre_stft_norm</code> <code>str</code> <p>str The normalization type for the speech waveforms before STFT.</p> <code>None</code> <code>window</code> <code>str</code> <p>str The window type for STFT.</p> <code>'hann'</code> <code>center</code> <code>bool</code> <p>bool  whether to pad input on both sides so that the t-th frame is centered at time t \u00d7 hop_length.</p> <code>True</code> <code>normalized</code> <code>bool</code> <p>bool controls whether to return the normalized STFT results</p> <code>False</code> <code>onesided</code> <code>bool</code> <p>bool controls whether to return half of results to avoid redundancy for real inputs.</p> <code>True</code> <code>mag_spec</code> <code>bool</code> <p>bool controls whether to calculate the linear magnitude spectrogram during STFT. True feeds the linear magnitude spectrogram into mel-fbank. False feeds the linear energy spectrogram into mel-fbank.</p> <code>False</code> <code>return_energy</code> <code>bool</code> <p>bool Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram</p> <code>False</code> <code>clamp</code> <code>float</code> <p>float The minimal number for the log-linear spectrogram. Used for numerical stability.</p> <code>1e-10</code> <code>logging</code> <code>bool</code> <p>bool Controls whether to take log for the mel spectrogram.</p> <code>False</code> <code>log_base</code> <code>float</code> <p>float The log base for the log-mel spectrogram. None means the natural log base e.</p> <code>None</code> Source code in <code>speechain/module/frontend/speech2linear.py</code> <pre><code>def module_init(\n    self,\n    hop_length: int or float,\n    win_length: int or float,\n    sr: int = 16000,\n    n_fft: int = None,\n    preemphasis: float = None,\n    pre_stft_norm: str = None,\n    window: str = \"hann\",\n    center: bool = True,\n    normalized: bool = False,\n    onesided: bool = True,\n    mag_spec: bool = False,\n    return_energy: bool = False,\n    clamp: float = 1e-10,\n    logging: bool = False,\n    log_base: float = None,\n):\n    \"\"\"\n    The transformation from waveform to linear spectrogram has 4 steps:\n        1. (optional) waveform pre-emphasis (implemented by Conv1d layer)\n        2. (optional) waveform pre-normalization (not recommended for TTS model)\n        3. STFT processing (implemented by torch.stft())\n        4. STFT postprocessing: zero masking, (optional)sqrt for magnitude, (optional)clamping + logging\n\n    Args:\n        hop_length: int or float\n            the distance between neighboring sliding window frames for STFT.\n            int means the absolute number of sampling point,\n            float means the duration of the speech segment (in seconds).\n        win_length: int or float\n            the size of window frame for STFT.\n            int means the absolute number of sampling point,\n            float means the duration of the speech segment (in seconds).\n        sr: int\n            The sampling rate of the input speech waveforms. Only used for window calculation.\n        n_fft: int\n            The number of Fourier point used for STFT\n        preemphasis: float\n            The preemphasis coefficient before STFT.\n        pre_stft_norm: str\n            The normalization type for the speech waveforms before STFT.\n        window: str\n            The window type for STFT.\n        center: bool\n             whether to pad input on both sides so that the t-th frame is centered at time t \u00d7 hop_length.\n        normalized: bool\n            controls whether to return the normalized STFT results\n        onesided: bool\n            controls whether to return half of results to avoid redundancy for real inputs.\n        mag_spec: bool\n            controls whether to calculate the linear magnitude spectrogram during STFT.\n            True feeds the linear magnitude spectrogram into mel-fbank.\n            False feeds the linear energy spectrogram into mel-fbank.\n        return_energy: bool\n            Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram\n        clamp: float\n            The minimal number for the log-linear spectrogram. Used for numerical stability.\n        logging: bool\n            Controls whether to take log for the mel spectrogram.\n        log_base: float\n            The log base for the log-mel spectrogram. None means the natural log base e.\n\n    \"\"\"\n    # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n    hop_length = (\n        int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n    )\n    win_length = (\n        int(win_length * sr) if isinstance(win_length, float) else win_length\n    )\n\n    # if n_fft is not given, it will be initialized to the window length\n    if n_fft is None:\n        n_fft = win_length\n\n    # para recording\n    self.output_size = n_fft // 2 + 1 if onesided else n_fft\n    self.win_length = win_length\n    self.hop_length = hop_length\n    self.n_fft = n_fft\n    self.sr = sr\n\n    # preemphasis filter initialization\n    self.preemphasis = preemphasis\n    if preemphasis is not None:\n        _preemph_filter = torch.nn.Conv1d(1, 1, kernel_size=2, bias=False)\n        _filter_weight = torch.Tensor([-self.preemphasis, 1]).reshape(1, 1, 2)\n        _preemph_filter.weight = torch.nn.Parameter(\n            _filter_weight, requires_grad=False\n        )\n\n        # move the weight from _parameters to _buffers so that these parameters won't influence the training\n        _para_keys = list(_preemph_filter._parameters.keys())\n        for name in _para_keys:\n            _preemph_filter._buffers[name] = _preemph_filter._parameters.pop(name)\n        self.preemph_filter = _preemph_filter\n\n    # normalization type before STFT\n    self.pre_stft_norm = pre_stft_norm\n\n    # during stft\n    self.stft_config = dict(\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=window,\n        center=center,\n        normalized=normalized,\n        onesided=onesided,\n        return_complex=True,\n    )\n\n    # True=magnitude spectrogram, False=energy spectrogram\n    self.mag_spec = mag_spec\n    self.return_energy = return_energy\n\n    # logging-related arguments\n    self.clamp = clamp\n    self.logging = logging\n    self.log_base = log_base\n</code></pre>"},{"location":"reference/module/frontend/speech2linear/#module.frontend.speech2linear.Speech2LinearSpec.recover","title":"<code>recover(feat, feat_len, inv_preemph_winlen=100)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>feat_len</code> <code>Tensor</code> required <code>inv_preemph_winlen</code> <code>int</code> <code>100</code> <p>Returns:</p> Source code in <code>speechain/module/frontend/speech2linear.py</code> <pre><code>def recover(\n    self, feat: torch.Tensor, feat_len: torch.Tensor, inv_preemph_winlen: int = 100\n):\n    \"\"\"\n\n    Args:\n        feat:\n        feat_len:\n        inv_preemph_winlen:\n\n    Returns:\n\n    \"\"\"\n    # --- STFT Recovery by the GL Algorithm --- #\n    # 1. Randomly initialize the phase between 0 and 2\u03a0\n    # 2. Recover the waveform by the magnitude and phase\n    # 3. Process the synthetic waveform and get the new magnitude and phase\n    # 4. Iteratively do step2 and step3 by the original magnitude and new phase\n    #\n    # Pseudo codes of GL algorithm could be as follow:\n    #     angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n    #     S_complex = np.abs(S).astype(np.complex)\n    #     y = _istft(S_complex * angles, hparams)\n    #     for i in range(hparams.griffin_lim_iters):\n    #         angles = np.exp(1j * np.angle(_stft(y, hparams)))\n    #         y = _istft(S_complex * angles, hparams)\n    # ----------------------------------------- #\n    if not hasattr(self, \"griffin_lim\"):\n        # lazily initialize the linear-to-waveform transformation\n        self.griffin_lim = torchaudio.transforms.GriffinLim(\n            n_fft=self.n_fft,\n            win_length=self.win_length,\n            hop_length=self.hop_length,\n            window_fn=(\n                getattr(torch, f\"{self.stft_config['window']}_window\")\n                if isinstance(self.stft_config[\"window\"], str)\n                else self.stft_config[\"window\"]\n            ),\n            power=1 if self.mag_spec else 2,\n        )\n        self.griffin_lim.window = self.griffin_lim.window.to(feat.device)\n    wav = self.griffin_lim(feat.transpose(-2, -1))\n    wav_len = (feat_len - 1) * self.hop_length\n    assert wav_len.max() == wav.size(\n        1\n    ), \"Something wrong happens when calculating the length of synthetic utterances.\"\n\n    # pre-stft normalization cannot be recovered\n    assert (\n        self.pre_stft_norm is None\n    ), \"waveform pre-stft normalization cannot be recovered for TTS synthesis.\"\n\n    # --- Pre-Emphasis Recovery --- #\n    # Pre-emphasis: Y[n] = X[n] - 0.97 * X[n-1] where Y is the pre-emphasized signal and X is the original signal\n    # Inverse Pre-emphasis: X[n] = Y[n] + 0.97 * X[n-1] = Y[n] + 0.97 * Y[n-1] + ... + (0.97)^n * Y[0]\n    # However, since the signal is usually very long (n is in the unit of 10k), the power of n will infinitely\n    # approach 0 as n grows. So, a slide window is used to perform inverse pre-emphasis which only considers the\n    # previous time steps in a given range.\n    # ----------------------------- #\n    if self.preemphasis is not None:\n        # lazily initialize the inverse preemphasis filters (implemented by Conv1d)\n        if not hasattr(self, \"inv_preemph\"):\n            inv_preemph_filter = torch.nn.Conv1d(\n                1, 1, kernel_size=inv_preemph_winlen, bias=False\n            )\n\n            # get the sliding window for the inverse pre-emphasis\n            inv_preemph_win = (\n                torch.pow(\n                    torch.full((inv_preemph_winlen,), fill_value=self.preemphasis),\n                    torch.arange(\n                        start=inv_preemph_winlen - 1,\n                        end=-1,\n                        step=-1,\n                        dtype=torch.int,\n                    ),\n                )\n                .reshape(1, 1, -1)\n                .to(wav.device)\n            )\n            inv_preemph_filter.weight = torch.nn.Parameter(\n                inv_preemph_win, requires_grad=False\n            )\n\n            # move the weight from _parameters to _buffers so that these parameters won't influence the training\n            _para_keys = list(inv_preemph_filter._parameters.keys())\n            for name in _para_keys:\n                inv_preemph_filter._buffers[name] = (\n                    inv_preemph_filter._parameters.pop(name)\n                )\n            self.inv_preemph_filter = inv_preemph_filter\n\n        wav = F.pad(wav.unsqueeze(1), (inv_preemph_winlen - 1, 0))\n        wav = self.inv_preemph_filter(wav).transpose(-2, -1)\n\n    # make sure that the redundant parts are set to silence\n    for i in range(len(wav_len)):\n        wav[i][wav_len[i] :] = 0\n\n    return wav, wav_len\n</code></pre>"},{"location":"reference/module/frontend/speech2mel/","title":"speech2mel","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/frontend/speech2mel/#module.frontend.speech2mel.Speech2MelSpec","title":"<code>Speech2MelSpec</code>","text":"<p>               Bases: <code>Module</code></p> <p>The acoustic frontend where the input is raw speech waveforms and the output is log-mel spectrogram.</p> <p>The waveform is first converted into linear spectrogram by STFT. Then, the linear spectrogram is converted into log-mel spectrogram by mel-fbank filters. Finally, the delta features of log-mel spectrogram are calculated if specified.</p> Source code in <code>speechain/module/frontend/speech2mel.py</code> <pre><code>class Speech2MelSpec(Module):\n    \"\"\"The acoustic frontend where the input is raw speech waveforms and the output is\n    log-mel spectrogram.\n\n    The waveform is first converted into linear spectrogram by STFT. Then, the linear\n    spectrogram is converted into log-mel spectrogram by mel-fbank filters. Finally, the\n    delta features of log-mel spectrogram are calculated if specified.\n    \"\"\"\n\n    def module_init(\n        self,\n        n_mels: int,\n        hop_length: int or float,\n        win_length: int or float,\n        n_fft: int = None,\n        sr: int = 16000,\n        preemphasis: float = None,\n        pre_stft_norm: str = None,\n        window: str = \"hann\",\n        center: bool = True,\n        normalized: bool = False,\n        onesided: bool = True,\n        mag_spec: bool = False,\n        return_energy: bool = False,\n        fmin: float = 0.0,\n        fmax: float = None,\n        clamp: float = 1e-10,\n        logging: bool = True,\n        log_base: float = 10.0,\n        mel_scale: str = \"slaney\",\n        mel_norm: bool = True,\n        delta_order: int = None,\n        delta_N: int = 2,\n    ):\n        \"\"\"\n\n        Args:\n            n_mels: int\n                The number of filters in the mel-fbank\n            n_fft: int\n                The number of Fourier point used for STFT\n            hop_length: int or float\n                the distance between neighboring sliding window frames for STFT.\n                int means the absolute number of sampling point,\n                float means the duration of the speech segment (in seconds).\n            win_length: int or float\n                the size of window frame for STFT.\n                int means the absolute number of sampling point,\n                float means the duration of the speech segment (in seconds).\n            sr: int\n                The sampling rate of the input speech waveforms.\n            preemphasis: float\n                The preemphasis coefficient before STFT.\n            pre_stft_norm: str\n                The normalization method for the speech waveforms before STFT.\n            window: str\n                The window type for STFT.\n            center: bool\n                 whether to pad input on both sides so that the t-th frame is centered at time `t x hop_length`.\n            normalized: bool\n                controls whether to return the normalized STFT results\n            onesided: bool\n                controls whether to return half of results to avoid redundancy for real inputs.\n            mag_spec: bool\n                controls whether to calculate the linear magnitude spectrogram during STFT.\n                True feeds the linear magnitude (energy) spectrogram into mel-fbank.\n                False feeds the linear power spectrogram into mel-fbank.\n            return_energy: bool\n                Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram\n            fmin: float\n                The minimal frequency for the mel-fbank\n            fmax: float\n                The maximal frequency for the mel-fbank\n            clamp: float\n                The minimal number for the log-mel spectrogram. Used for stability.\n            logging: bool\n                Controls whether the mel spectrograms are logged\n            log_base: float\n                The log base for the log-mel spectrogram. None means the natural log base e.\n            mel_scale: str\n                The tyle of mel-scale of the mel-fbank.\n            mel_norm: bool\n                Whether perform the area normalization to the mel-fbank filters.\n            delta_order: int\n                The delta order you want to add to the original log-mel spectrogram.\n                1 means original log-mel spectrogram + $\\delta$ Log-mel spectrogram\n                2 means original log-mel spectrogram + $\\delta$ Log-mel spectrogram + $\\delta\\delta$ log-mel spectrogram\n            delta_N: int\n                The number of neighboring points used for calculating the delta features.\n\n        \"\"\"\n        # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n        hop_length = (\n            int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n        )\n        win_length = (\n            int(win_length * sr) if isinstance(win_length, float) else win_length\n        )\n\n        # if n_fft is not given, it will be initialized to the window length\n        if n_fft is None:\n            n_fft = win_length\n\n        # para recording\n        self.output_size = n_mels if delta_order is None else n_mels * (delta_order + 1)\n\n        # Speech -&gt; Linear Spectrogram (linear spectrograms are not logged for getting the mel spectrograms)\n        self.return_energy = return_energy\n        self.speech2linear = Speech2LinearSpec(\n            n_fft=n_fft,\n            sr=sr,\n            hop_length=hop_length,\n            win_length=win_length,\n            preemphasis=preemphasis,\n            pre_stft_norm=pre_stft_norm,\n            window=window,\n            center=center,\n            normalized=normalized,\n            onesided=onesided,\n            mag_spec=mag_spec,\n            return_energy=return_energy,\n            logging=False,\n        )\n        # Linear Spectrogram -&gt; (Log-)Mel Spectrogram\n        self.linear2mel = LinearSpec2MelSpec(\n            sr=sr,\n            n_fft=n_fft,\n            n_mels=n_mels,\n            fmin=fmin,\n            fmax=fmax,\n            clamp=clamp,\n            logging=logging,\n            log_base=log_base,\n            mel_scale=mel_scale,\n            mel_norm=mel_norm,\n            mag_spec=mag_spec,\n        )\n        # (Optional) (Log-)Mel Spectrogram -&gt; (Log-)Mel Spectrogram + Deltas\n        self.delta_order = delta_order\n        if delta_order is not None:\n            self.delta_N = delta_N\n            self.delta = DeltaFeature(delta_order=delta_order, delta_N=delta_N)\n\n    def forward(self, speech: torch.Tensor, speech_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            speech: (batch, speech_maxlen, 1) or (batch, speech_maxlen)\n                The input speech data.\n            speech_len: (batch,)\n                The lengths of input speech data\n\n        Returns:\n            The log-mel spectrograms with their lengths.\n\n        \"\"\"\n\n        # Speech -&gt; Linear Spectrogram\n        if self.return_energy:\n            feat, feat_len, energy, energy_len = self.speech2linear(speech, speech_len)\n        else:\n            feat, feat_len = self.speech2linear(speech, speech_len)\n            energy, energy_len = None, None\n\n        # Linear Spectrogram -&gt; Log-Mel Spectrogram\n        feat, feat_len = self.linear2mel(feat, feat_len)\n\n        # Log-Mel Spectrogram -&gt; Log-Mel Spectrogram + Deltas\n        if self.delta_order is not None:\n            feat, feat_len = self.delta(feat, feat_len)\n\n        if self.return_energy:\n            return feat, feat_len, energy, energy_len\n        else:\n            return feat, feat_len\n\n    def recover(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat:\n            feat_len:\n\n        Returns:\n\n        \"\"\"\n        # No delta recovery\n        assert self.delta_order is None\n\n        # Log-Mel Spectrogram -&gt; Linear Spectrogram\n        feat = self.linear2mel.recover(feat, feat_len)\n\n        # Linear Spectrogram -&gt; Waveforms (GL algorithm)\n        feat, feat_len = self.speech2linear.recover(feat, feat_len)\n\n        return feat, feat_len\n\n    def get_sample_rate(self):\n        return self.speech2linear.sr\n\n    def __repr__(self):\n        string = (\n            f\"{self.__class__.__name__}(\\n\"\n            + str(self.speech2linear)\n            + \"\\n\"\n            + str(self.linear2mel)\n        )\n\n        if self.delta_order is not None:\n            string += \"\\n\" + str(self.delta)\n\n        return string + \"\\n)\"\n</code></pre>"},{"location":"reference/module/frontend/speech2mel/#module.frontend.speech2mel.Speech2MelSpec.forward","title":"<code>forward(speech, speech_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>speech</code> <code>Tensor</code> <p>(batch, speech_maxlen, 1) or (batch, speech_maxlen) The input speech data.</p> required <code>speech_len</code> <code>Tensor</code> <p>(batch,) The lengths of input speech data</p> required <p>Returns:</p> Type Description <p>The log-mel spectrograms with their lengths.</p> Source code in <code>speechain/module/frontend/speech2mel.py</code> <pre><code>def forward(self, speech: torch.Tensor, speech_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        speech: (batch, speech_maxlen, 1) or (batch, speech_maxlen)\n            The input speech data.\n        speech_len: (batch,)\n            The lengths of input speech data\n\n    Returns:\n        The log-mel spectrograms with their lengths.\n\n    \"\"\"\n\n    # Speech -&gt; Linear Spectrogram\n    if self.return_energy:\n        feat, feat_len, energy, energy_len = self.speech2linear(speech, speech_len)\n    else:\n        feat, feat_len = self.speech2linear(speech, speech_len)\n        energy, energy_len = None, None\n\n    # Linear Spectrogram -&gt; Log-Mel Spectrogram\n    feat, feat_len = self.linear2mel(feat, feat_len)\n\n    # Log-Mel Spectrogram -&gt; Log-Mel Spectrogram + Deltas\n    if self.delta_order is not None:\n        feat, feat_len = self.delta(feat, feat_len)\n\n    if self.return_energy:\n        return feat, feat_len, energy, energy_len\n    else:\n        return feat, feat_len\n</code></pre>"},{"location":"reference/module/frontend/speech2mel/#module.frontend.speech2mel.Speech2MelSpec.module_init","title":"<code>module_init(n_mels, hop_length, win_length, n_fft=None, sr=16000, preemphasis=None, pre_stft_norm=None, window='hann', center=True, normalized=False, onesided=True, mag_spec=False, return_energy=False, fmin=0.0, fmax=None, clamp=1e-10, logging=True, log_base=10.0, mel_scale='slaney', mel_norm=True, delta_order=None, delta_N=2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_mels</code> <code>int</code> <p>int The number of filters in the mel-fbank</p> required <code>n_fft</code> <code>int</code> <p>int The number of Fourier point used for STFT</p> <code>None</code> <code>hop_length</code> <code>int or float</code> <p>int or float the distance between neighboring sliding window frames for STFT. int means the absolute number of sampling point, float means the duration of the speech segment (in seconds).</p> required <code>win_length</code> <code>int or float</code> <p>int or float the size of window frame for STFT. int means the absolute number of sampling point, float means the duration of the speech segment (in seconds).</p> required <code>sr</code> <code>int</code> <p>int The sampling rate of the input speech waveforms.</p> <code>16000</code> <code>preemphasis</code> <code>float</code> <p>float The preemphasis coefficient before STFT.</p> <code>None</code> <code>pre_stft_norm</code> <code>str</code> <p>str The normalization method for the speech waveforms before STFT.</p> <code>None</code> <code>window</code> <code>str</code> <p>str The window type for STFT.</p> <code>'hann'</code> <code>center</code> <code>bool</code> <p>bool  whether to pad input on both sides so that the t-th frame is centered at time <code>t x hop_length</code>.</p> <code>True</code> <code>normalized</code> <code>bool</code> <p>bool controls whether to return the normalized STFT results</p> <code>False</code> <code>onesided</code> <code>bool</code> <p>bool controls whether to return half of results to avoid redundancy for real inputs.</p> <code>True</code> <code>mag_spec</code> <code>bool</code> <p>bool controls whether to calculate the linear magnitude spectrogram during STFT. True feeds the linear magnitude (energy) spectrogram into mel-fbank. False feeds the linear power spectrogram into mel-fbank.</p> <code>False</code> <code>return_energy</code> <code>bool</code> <p>bool Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram</p> <code>False</code> <code>fmin</code> <code>float</code> <p>float The minimal frequency for the mel-fbank</p> <code>0.0</code> <code>fmax</code> <code>float</code> <p>float The maximal frequency for the mel-fbank</p> <code>None</code> <code>clamp</code> <code>float</code> <p>float The minimal number for the log-mel spectrogram. Used for stability.</p> <code>1e-10</code> <code>logging</code> <code>bool</code> <p>bool Controls whether the mel spectrograms are logged</p> <code>True</code> <code>log_base</code> <code>float</code> <p>float The log base for the log-mel spectrogram. None means the natural log base e.</p> <code>10.0</code> <code>mel_scale</code> <code>str</code> <p>str The tyle of mel-scale of the mel-fbank.</p> <code>'slaney'</code> <code>mel_norm</code> <code>bool</code> <p>bool Whether perform the area normalization to the mel-fbank filters.</p> <code>True</code> <code>delta_order</code> <code>int</code> <p>int The delta order you want to add to the original log-mel spectrogram. 1 means original log-mel spectrogram + \\(\\delta\\) Log-mel spectrogram 2 means original log-mel spectrogram + \\(\\delta\\) Log-mel spectrogram + \\(\\delta\\delta\\) log-mel spectrogram</p> <code>None</code> <code>delta_N</code> <code>int</code> <p>int The number of neighboring points used for calculating the delta features.</p> <code>2</code> Source code in <code>speechain/module/frontend/speech2mel.py</code> <pre><code>def module_init(\n    self,\n    n_mels: int,\n    hop_length: int or float,\n    win_length: int or float,\n    n_fft: int = None,\n    sr: int = 16000,\n    preemphasis: float = None,\n    pre_stft_norm: str = None,\n    window: str = \"hann\",\n    center: bool = True,\n    normalized: bool = False,\n    onesided: bool = True,\n    mag_spec: bool = False,\n    return_energy: bool = False,\n    fmin: float = 0.0,\n    fmax: float = None,\n    clamp: float = 1e-10,\n    logging: bool = True,\n    log_base: float = 10.0,\n    mel_scale: str = \"slaney\",\n    mel_norm: bool = True,\n    delta_order: int = None,\n    delta_N: int = 2,\n):\n    \"\"\"\n\n    Args:\n        n_mels: int\n            The number of filters in the mel-fbank\n        n_fft: int\n            The number of Fourier point used for STFT\n        hop_length: int or float\n            the distance between neighboring sliding window frames for STFT.\n            int means the absolute number of sampling point,\n            float means the duration of the speech segment (in seconds).\n        win_length: int or float\n            the size of window frame for STFT.\n            int means the absolute number of sampling point,\n            float means the duration of the speech segment (in seconds).\n        sr: int\n            The sampling rate of the input speech waveforms.\n        preemphasis: float\n            The preemphasis coefficient before STFT.\n        pre_stft_norm: str\n            The normalization method for the speech waveforms before STFT.\n        window: str\n            The window type for STFT.\n        center: bool\n             whether to pad input on both sides so that the t-th frame is centered at time `t x hop_length`.\n        normalized: bool\n            controls whether to return the normalized STFT results\n        onesided: bool\n            controls whether to return half of results to avoid redundancy for real inputs.\n        mag_spec: bool\n            controls whether to calculate the linear magnitude spectrogram during STFT.\n            True feeds the linear magnitude (energy) spectrogram into mel-fbank.\n            False feeds the linear power spectrogram into mel-fbank.\n        return_energy: bool\n            Whether to calculate the frame-wise energy for the linear magnitude (energy) spectrogram\n        fmin: float\n            The minimal frequency for the mel-fbank\n        fmax: float\n            The maximal frequency for the mel-fbank\n        clamp: float\n            The minimal number for the log-mel spectrogram. Used for stability.\n        logging: bool\n            Controls whether the mel spectrograms are logged\n        log_base: float\n            The log base for the log-mel spectrogram. None means the natural log base e.\n        mel_scale: str\n            The tyle of mel-scale of the mel-fbank.\n        mel_norm: bool\n            Whether perform the area normalization to the mel-fbank filters.\n        delta_order: int\n            The delta order you want to add to the original log-mel spectrogram.\n            1 means original log-mel spectrogram + $\\delta$ Log-mel spectrogram\n            2 means original log-mel spectrogram + $\\delta$ Log-mel spectrogram + $\\delta\\delta$ log-mel spectrogram\n        delta_N: int\n            The number of neighboring points used for calculating the delta features.\n\n    \"\"\"\n    # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n    hop_length = (\n        int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n    )\n    win_length = (\n        int(win_length * sr) if isinstance(win_length, float) else win_length\n    )\n\n    # if n_fft is not given, it will be initialized to the window length\n    if n_fft is None:\n        n_fft = win_length\n\n    # para recording\n    self.output_size = n_mels if delta_order is None else n_mels * (delta_order + 1)\n\n    # Speech -&gt; Linear Spectrogram (linear spectrograms are not logged for getting the mel spectrograms)\n    self.return_energy = return_energy\n    self.speech2linear = Speech2LinearSpec(\n        n_fft=n_fft,\n        sr=sr,\n        hop_length=hop_length,\n        win_length=win_length,\n        preemphasis=preemphasis,\n        pre_stft_norm=pre_stft_norm,\n        window=window,\n        center=center,\n        normalized=normalized,\n        onesided=onesided,\n        mag_spec=mag_spec,\n        return_energy=return_energy,\n        logging=False,\n    )\n    # Linear Spectrogram -&gt; (Log-)Mel Spectrogram\n    self.linear2mel = LinearSpec2MelSpec(\n        sr=sr,\n        n_fft=n_fft,\n        n_mels=n_mels,\n        fmin=fmin,\n        fmax=fmax,\n        clamp=clamp,\n        logging=logging,\n        log_base=log_base,\n        mel_scale=mel_scale,\n        mel_norm=mel_norm,\n        mag_spec=mag_spec,\n    )\n    # (Optional) (Log-)Mel Spectrogram -&gt; (Log-)Mel Spectrogram + Deltas\n    self.delta_order = delta_order\n    if delta_order is not None:\n        self.delta_N = delta_N\n        self.delta = DeltaFeature(delta_order=delta_order, delta_N=delta_N)\n</code></pre>"},{"location":"reference/module/frontend/speech2mel/#module.frontend.speech2mel.Speech2MelSpec.recover","title":"<code>recover(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>feat_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/frontend/speech2mel.py</code> <pre><code>def recover(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat:\n        feat_len:\n\n    Returns:\n\n    \"\"\"\n    # No delta recovery\n    assert self.delta_order is None\n\n    # Log-Mel Spectrogram -&gt; Linear Spectrogram\n    feat = self.linear2mel.recover(feat, feat_len)\n\n    # Linear Spectrogram -&gt; Waveforms (GL algorithm)\n    feat, feat_len = self.speech2linear.recover(feat, feat_len)\n\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/norm/","title":"norm","text":""},{"location":"reference/module/norm/feat_norm/","title":"feat_norm","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization","title":"<code>FeatureNormalization</code>","text":"<p>               Bases: <code>Module</code></p> <p>The feature normalization frontend that makes every feature dimension the distribution with 0 mean and 1 variance.</p> <p>As SpeechBrain, we also provide four kinds of feature normalization with different granularities.     1. utterance-level normalization: the mean and std are calculated on each individual utterance.     2. batch-level normalization: the mean and std are calculated on all the utterances in a training batch.     3. group-level normalization: the mean and std are calculated on all the utterances in a group.         The group here means where the utterance comes from, so it can be any kinds of data domains         such as different speakers, genders, source and target domains in Domain Adaptation scenario, and so on...     4. global-level normalization: the mean and std are calculated on all the utterances in the training set.</p> <p>We approximate group-level and global-level mean &amp; std by taking their moving average during training. Different from SpeechBrain, we initialize all the mean &amp; std variables lazily in the forward() function. Another difference is that our moving average is calculated by each batch as BatchNorm does.</p> <p>In the DDP mode, the mean &amp; std will be synchronized across all the processes before being used to normalize the input utterances. The synchronization method is different in different scenarios.</p> <ol> <li> <p>group-level normalization where each input utterance has different group id (group_ids = torch.Tensor,     e.g. different utterances in a single batch may belong to different speakers).     In this scenario, the mean &amp; std vectors of each utterance and the group ids will be gathered across all the     processes. Then, the mean &amp; std vectors will be picked up depending on the group id and the mean &amp; std of the     specific group will be calculated.</p> </li> <li> <p>global-level normalization or group-level normalization where all the input utterances have the same group id     (group_ids = str or int, e.g. all the utterances in the batch come from either the source domain or the target     domain). In this scenario, the summation of mean &amp; std vectors will be gathered instead of all of them to reduce     the data communication volume across all the processes. The real mean &amp; std vectors will be recovered by the     batch size of each process.</p> </li> </ol> Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>class FeatureNormalization(Module):\n    \"\"\"The feature normalization frontend that makes every feature dimension the\n    distribution with 0 mean and 1 variance.\n\n    As SpeechBrain, we also provide four kinds of feature normalization with different granularities.\n        1. utterance-level normalization: the mean and std are calculated on each individual utterance.\n        2. batch-level normalization: the mean and std are calculated on all the utterances in a training batch.\n        3. group-level normalization: the mean and std are calculated on all the utterances in a group.\n            The group here means where the utterance comes from, so it can be any kinds of data domains\n            such as different speakers, genders, source and target domains in Domain Adaptation scenario, and so on...\n        4. global-level normalization: the mean and std are calculated on all the utterances in the training set.\n\n    We approximate group-level and global-level mean &amp; std by taking their moving average during training.\n    Different from SpeechBrain, we initialize all the mean &amp; std variables lazily in the forward() function.\n    Another difference is that our moving average is calculated by each batch as BatchNorm does.\n\n    In the DDP mode, the mean &amp; std will be synchronized across all the processes before being used to normalize the\n    input utterances. The synchronization method is different in different scenarios.\n\n    1. group-level normalization where each input utterance has different group id (group_ids = torch.Tensor,\n        e.g. different utterances in a single batch may belong to different speakers).\n        In this scenario, the mean &amp; std vectors of each utterance and the group ids will be gathered across all the\n        processes. Then, the mean &amp; std vectors will be picked up depending on the group id and the mean &amp; std of the\n        specific group will be calculated.\n\n    2. global-level normalization or group-level normalization where all the input utterances have the same group id\n        (group_ids = str or int, e.g. all the utterances in the batch come from either the source domain or the target\n        domain). In this scenario, the summation of mean &amp; std vectors will be gathered instead of all of them to reduce\n        the data communication volume across all the processes. The real mean &amp; std vectors will be recovered by the\n        batch size of each process.\n    \"\"\"\n\n    def module_init(\n        self,\n        norm_type: str = \"global\",\n        mean_norm: bool = True,\n        std_norm: bool = True,\n        clamp: float = 1e-10,\n        max_epoch_num: int = 4,\n    ):\n        \"\"\"\n\n        Args:\n            norm_type: str\n                The type of feature normalization.\n                The type must be one of 'utterance', 'batch', 'group', and 'global'\n            mean_norm: bool\n                Controls whether the feature vectors will be normalized by their means\n            std_norm: bool\n                Controls whether the feature vectors will be normalized by their standard variance\n            clamp: float\n                Clamping threshold for the standard variance before division.\n            max_epoch_num: int\n                The maximum number of epochs used to calculate the moving average.\n                Usually, the value of this argument is lower than a half of the number of warmup epochs.\n\n        \"\"\"\n        self.norm_type = norm_type\n        self.mean_norm = mean_norm\n        self.std_norm = std_norm\n        self.clamp = clamp\n        self.max_epoch_num = max_epoch_num\n\n        if self.input_size is not None:\n            self.output_size = self.input_size\n\n    def forward(\n        self,\n        feat: torch.Tensor,\n        feat_len: torch.Tensor,\n        group_ids: torch.Tensor or str or int = None,\n        epoch: int = None,\n    ):\n        \"\"\"\n\n        Args:\n            feat: (batch, length, channel) or (batch, length)\n                The normalization will be done on the channel dimension.\n                If the feat is in the shape of (batch, length), it will be extended to (batch, length, 1)\n            feat_len: (batch)\n            group_ids: (batch)\n            epoch:\n\n        Returns:\n\n        \"\"\"\n        if self.norm_type == \"group\":\n            assert group_ids is not None, (\n                \"You are using group-level feature normalization, but group_ids is not given. \"\n                \"Please check 'data_cfg' in your configuration.\"\n            )\n        # para preparation\n        batch_size, squeeze_flag = feat.size(0), False\n        if len(feat.shape) == 2:\n            feat, squeeze_flag = feat.unsqueeze(-1), True\n        elif len(feat.shape) != 3:\n            raise RuntimeError(\n                f\"{self.__class__.__name__} only accepts the input vectors in the shape of \"\n                f\"(batch, length, channel) or (batch, length), but got shape={feat.shape}!\"\n            )\n\n        # --- Mean and Standard Variance Initialization --- #\n        # calculate the mean values of all channels of all the input utterances\n        curr_means = (\n            None\n            if not self.mean_norm\n            else torch.stack(\n                [feat[i][: feat_len[i]].mean(dim=0) for i in range(batch_size)]\n            )\n        )\n\n        # calculate the std values of all channels of all the input utterances\n        curr_stds = (\n            None\n            if not self.std_norm\n            else torch.clamp(\n                input=torch.stack(\n                    [feat[i][: feat_len[i]].std(dim=0) for i in range(batch_size)]\n                ),\n                min=self.clamp,\n            )\n        )\n\n        # --- Perform Normalization based on Different branches --- #\n        # utterance-level normalization or group-level normalization without group_ids\n        if self.norm_type == \"utterance\":\n            feat = feat - curr_means.unsqueeze(1) if curr_means is not None else feat\n            feat = feat / curr_stds.unsqueeze(1) if curr_stds is not None else feat\n\n        # global-level &amp; batch-level &amp; group-level normalization (with group_ids)\n        else:\n            # only gather the batch sizes from other processes in the DDP model of training\n            all_batch_size = None\n            if self.training:\n                all_batch_size = (\n                    self.gather_scalars(batch_size, feat.device)\n                    if self.distributed\n                    else batch_size\n                )\n\n            # group-level normalization with tensor group_ids (input utterances belong to different groups)\n            if self.norm_type == \"group\" and isinstance(group_ids, torch.Tensor):\n                # only update the mean and std of the specific group during training\n                if self.training:\n                    # DDP mode\n                    if self.distributed:\n                        # gather all the group ids from other processes\n                        all_group_ids = self.gather_vectors(group_ids, all_batch_size)\n                        # gather all the mean vectors from other processes\n                        all_curr_means = (\n                            None\n                            if curr_means is None\n                            else self.gather_matrices(curr_means, all_batch_size)\n                        )\n                        # gather all the std vectors from other processes\n                        all_curr_stds = (\n                            None\n                            if curr_stds is None\n                            else self.gather_matrices(curr_stds, all_batch_size)\n                        )\n                    # single-GPU mode\n                    else:\n                        # not perform gathering\n                        all_group_ids = group_ids\n                        all_curr_means = curr_means\n                        all_curr_stds = curr_stds\n\n                    # record the mean of all groups in the current batch\n                    group_mean_dict = self.sort_data_by_group(\n                        raw_data=all_curr_means, group_ids=all_group_ids\n                    )\n\n                    # record the std of all groups in the current batch\n                    group_std_dict = self.sort_data_by_group(\n                        raw_data=all_curr_stds, group_ids=all_group_ids\n                    )\n\n                    # register the mean, std, and batch numbers into the buffer\n                    group_keys = (\n                        list(group_mean_dict.keys())\n                        if group_mean_dict is not None\n                        else list(group_std_dict.keys())\n                    )\n                    for group_id in group_keys:\n                        self.register_mean_std_batch(\n                            curr_aver_mean=(\n                                group_mean_dict[group_id].mean(dim=0)\n                                if group_mean_dict is not None\n                                else None\n                            ),\n                            curr_aver_std=(\n                                group_std_dict[group_id].mean(dim=0)\n                                if group_std_dict is not None\n                                else None\n                            ),\n                            prefix=group_id,\n                            epoch=epoch,\n                        )\n                    # update the average mean &amp; std of all the groups\n                    # (i.e. the average distribution for unknown samples during inference)\n                    self.update_aver_mean_std(epoch)\n\n                # During training, normalize the known features by the group mean &amp; std\n                # During inference, normalize the unknown features by the average mean &amp; std of all groups\n                for i in range(batch_size):\n                    group_id = group_ids[i].item() if group_ids is not None else None\n\n                    if self.mean_norm:\n                        feat[i] -= (\n                            self.get_buffer(\"aver_mean\")\n                            if not hasattr(self, f\"{group_id}_mean\")\n                            else self.get_buffer(f\"{group_id}_mean\")\n                        )\n                    if self.std_norm:\n                        feat[i] /= (\n                            self.get_buffer(\"aver_std\")\n                            if not hasattr(self, f\"{group_id}_std\")\n                            else self.get_buffer(f\"{group_id}_std\")\n                        )\n\n            # batch-level &amp; global-level normalization (these two scenarios share the batch-level mean &amp; std)\n            else:\n                # only calculate the batch-level mean and std during training\n                if self.training:\n                    # gather the mean and std from the other processes in the DDP mode\n                    if self.distributed:\n                        # gather the sums of batch means from all the processes\n                        batch_mean_sum = (\n                            curr_means.sum(dim=0) if curr_means is not None else None\n                        )\n                        all_batch_mean_sums = (\n                            self.gather_vectors(batch_mean_sum)\n                            if batch_mean_sum is not None\n                            else None\n                        )\n                        batch_mean = (\n                            None\n                            if all_batch_mean_sums is None\n                            else all_batch_mean_sums.sum(dim=0) / all_batch_size.sum()\n                        )\n\n                        # gather the sums of batch stds from all the processes\n                        batch_std_sum = (\n                            curr_stds.sum(dim=0) if curr_stds is not None else None\n                        )\n                        all_batch_std_sums = (\n                            self.gather_vectors(batch_std_sum)\n                            if batch_std_sum is not None\n                            else None\n                        )\n                        batch_std = (\n                            None\n                            if all_batch_std_sums is None\n                            else all_batch_std_sums.sum(dim=0) / all_batch_size.sum()\n                        )\n\n                    # single-GPU mode\n                    else:\n                        batch_mean = (\n                            curr_means.mean(dim=0) if curr_means is not None else None\n                        )\n                        batch_std = (\n                            curr_stds.mean(dim=0) if curr_stds is not None else None\n                        )\n\n                # do nothing for batch-level mean and std during evaluation\n                else:\n                    batch_mean = None\n                    batch_std = None\n\n                # batch-level normalization\n                if self.norm_type == \"batch\":\n                    # normalize the input utterances by the batch mean and std during training\n                    if self.training:\n                        feat = feat - batch_mean if batch_mean is not None else feat\n                        feat = feat / batch_std if batch_std is not None else feat\n                    # normalize the input utterances by the utterance-specific mean and std during evaluation\n                    else:\n                        feat = (\n                            feat - curr_means.unsqueeze(1)\n                            if curr_means is not None\n                            else feat\n                        )\n                        feat = (\n                            feat / curr_stds.unsqueeze(1)\n                            if curr_stds is not None\n                            else feat\n                        )\n\n                # global-level normalization or\n                # group-level normalization with str or int group_ids (input utterances belong to the same group)\n                else:\n                    assert self.norm_type in [\"global\", \"group\"], (\n                        f\"norm_type can only be one of 'utterance', 'batch', 'group', 'global', \"\n                        f\"but got norm_type={self.norm_type}!\"\n                    )\n                    if self.norm_type == \"group\":\n                        assert isinstance(group_ids, (str, int)), (\n                            f\"If all the utterances in a single batch belong to the same group, \"\n                            f\"you should give group_ids as a string or integer. \"\n                            f\"But got type(group_ids)={type(group_ids)}.\"\n                        )\n\n                    # only update the mean and std during training\n                    prefix = \"global\" if self.norm_type == \"global\" else group_ids\n                    if self.training:\n                        self.register_mean_std_batch(\n                            curr_aver_mean=batch_mean,\n                            curr_aver_std=batch_std,\n                            prefix=prefix,\n                            epoch=epoch,\n                        )\n\n                    # if the group_ids is given as a string or int,\n                    # we assume that there are no unknown testing samples during inference\n                    feat = (\n                        feat - self.get_buffer(f\"{prefix}_mean\")\n                        if curr_means is not None\n                        else feat\n                    )\n                    feat = (\n                        feat / self.get_buffer(f\"{prefix}_std\")\n                        if curr_stds is not None\n                        else feat\n                    )\n\n        return feat.squeeze(-1) if squeeze_flag else feat, feat_len\n\n    @staticmethod\n    def gather_scalars(scalar: int, device: torch.device) -&gt; torch.LongTensor:\n        # gather the input scalars\n        all_scalars = [\n            torch.LongTensor([0]).cuda(device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(\n            all_scalars, torch.LongTensor([scalar]).cuda(device)\n        )\n        return torch.LongTensor(all_scalars)\n\n    @staticmethod\n    def gather_vectors(\n        vector: torch.Tensor, all_batch_size: torch.Tensor = None\n    ) -&gt; torch.Tensor:\n        # vectors of all the processes may have different length\n        if all_batch_size is not None:\n            curr_batch_size = all_batch_size[torch.distributed.get_rank()].item()\n            max_batch_size = all_batch_size.max().item()\n            if curr_batch_size &lt; max_batch_size:\n                vector = torch.cat(\n                    (\n                        vector,\n                        torch.zeros(\n                            max_batch_size - curr_batch_size,\n                            dtype=vector.dtype,\n                            device=vector.device,\n                        ),\n                    )\n                )\n            all_vectors = [\n                torch.Tensor([0 for _ in range(max_batch_size)])\n                .type_as(vector)\n                .cuda(vector.device)\n                for _ in range(torch.distributed.get_world_size())\n            ]\n        # all the vectors are equal in length\n        else:\n            all_vectors = [\n                torch.zeros_like(vector, device=vector.device)\n                for _ in range(torch.distributed.get_world_size())\n            ]\n\n        # gather the vectors from other processes to all_vectors\n        torch.distributed.all_gather(all_vectors, vector)\n\n        # remove the padding\n        return (\n            torch.stack(all_vectors)\n            if all_batch_size is None\n            else torch.cat(\n                [all_vectors[i][: all_batch_size[i]] for i in range(len(all_vectors))]\n            )\n        )\n\n    @staticmethod\n    def gather_matrices(\n        matrix: torch.Tensor, all_batch_size: torch.Tensor\n    ) -&gt; torch.Tensor:\n        curr_batch_size = all_batch_size[torch.distributed.get_rank()].item()\n        max_batch_size = all_batch_size.max().item()\n        # padding the matrix if necessary\n        if curr_batch_size &lt; max_batch_size:\n            matrix = torch.cat(\n                (\n                    matrix,\n                    torch.zeros(\n                        max_batch_size - curr_batch_size,\n                        matrix.size(-1),\n                        device=matrix.device,\n                    ),\n                )\n            )\n\n        # gather the matrices from other processes to all_matrices\n        all_matrices = [\n            torch.zeros_like(matrix, device=matrix.device)\n            for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(all_matrices, matrix)\n\n        # remove the padding\n        return torch.cat(\n            [all_matrices[i][: all_batch_size[i]] for i in range(len(all_matrices))]\n        )\n\n    @staticmethod\n    def sort_data_by_group(raw_data: torch.Tensor, group_ids: torch.Tensor):\n        \"\"\"\n\n        Args:\n            raw_data:\n            group_ids:\n\n        Returns:\n\n        \"\"\"\n        if raw_data is None:\n            return None\n        else:\n            group_dict = dict()\n            # loop each group id\n            for i in range(group_ids.size(0)):\n                curr_group = group_ids[i].item()\n                # initialize the group list if not existed\n                if curr_group not in group_dict.keys():\n                    group_dict[curr_group] = []\n                group_dict[curr_group].append(raw_data[i])\n            # turn each group list into a 2d tensor\n            return {\n                group_id: torch.stack(group_list)\n                for group_id, group_list in group_dict.items()\n            }\n\n    def register_mean_std_batch(\n        self,\n        curr_aver_mean: torch.Tensor,\n        curr_aver_std: torch.Tensor,\n        prefix: str,\n        epoch: int,\n    ):\n        \"\"\"\n\n        Args:\n            curr_aver_mean:\n            curr_aver_std:\n            prefix:\n            epoch:\n\n        \"\"\"\n        # update the observed global batch number\n        if epoch is None or not hasattr(self, f\"{prefix}_batch\"):\n            self.register_buffer(\n                f\"{prefix}_batch\",\n                torch.LongTensor([1]).cuda(device=curr_aver_mean.device),\n            )\n        elif epoch &lt;= self.max_epoch_num:\n            self.register_buffer(\n                f\"{prefix}_batch\", self.get_buffer(f\"{prefix}_batch\") + 1\n            )\n\n        # update the observed global mean &amp; std only in the predefined batch number\n        if epoch is None or epoch &lt;= self.max_epoch_num:\n            # get the weight of the global average values\n            curr_weight = 1 / self.get_buffer(f\"{prefix}_batch\")\n\n            # update the observed global mean\n            if self.mean_norm:\n                if not hasattr(self, f\"{prefix}_mean\"):\n                    self.register_buffer(f\"{prefix}_mean\", curr_aver_mean)\n                else:\n                    prev_aver_mean = self.get_buffer(f\"{prefix}_mean\")\n                    self.register_buffer(\n                        f\"{prefix}_mean\",\n                        curr_weight * curr_aver_mean\n                        + (1 - curr_weight) * prev_aver_mean,\n                    )\n\n            # update the observed global std\n            if self.std_norm:\n                if not hasattr(self, f\"{prefix}_std\"):\n                    self.register_buffer(f\"{prefix}_std\", curr_aver_std)\n                else:\n                    prev_aver_std = self.get_buffer(f\"{prefix}_std\")\n                    self.register_buffer(\n                        f\"{prefix}_std\",\n                        curr_weight * curr_aver_std + (1 - curr_weight) * prev_aver_std,\n                    )\n\n    def update_aver_mean_std(self, epoch: int):\n        \"\"\"\n\n        Args:\n            epoch:\n\n        \"\"\"\n        if epoch is None or epoch &lt;= self.max_epoch_num:\n            _group_mean_num, _group_std_num = 0, 0\n            _aver_mean, _aver_std = None, None\n            for name, buff in self.named_buffers():\n                if name.endswith(\"_mean\"):\n                    _group_mean_num += 1\n                    _aver_mean = (\n                        buff.clone() if _aver_mean is None else _aver_mean + buff\n                    )\n                elif name.endswith(\"_std\"):\n                    _group_std_num += 1\n                    _aver_std = buff.clone() if _aver_std is None else _aver_std + buff\n\n            self.register_buffer(\"aver_mean\", _aver_mean / _group_mean_num)\n            self.register_buffer(\"aver_std\", _aver_std / _group_std_num)\n\n    def recover(self, feat: torch.Tensor, group_ids: torch.Tensor or str or int = None):\n        \"\"\"\n\n        Args:\n            feat:\n            group_ids:\n\n        Returns:\n\n        \"\"\"\n        assert self.norm_type not in [\n            \"utterance\",\n            \"batch\",\n        ], \"If norm_type is either 'utterance' or 'batch', the normalized features cannot be recovered.\"\n\n        # global normalization or\n        # group-level normalization with str or int group_ids (input utterances belong to the same group)\n        if self.norm_type == \"global\" or (\n            self.norm_type == \"group\" and isinstance(group_ids, (str, int))\n        ):\n            prefix = \"global\" if self.norm_type == \"global\" else str(group_ids)\n            feat = feat * self.get_buffer(f\"{prefix}_std\") if self.std_norm else feat\n            feat = feat + self.get_buffer(f\"{prefix}_mean\") if self.mean_norm else feat\n        # group-level normalization with tensor group_ids (input utterances belong to different groups)\n        # recover by the average mean &amp; std when meeting an unknown group during inference\n        elif self.norm_type == \"group\" and isinstance(group_ids, torch.Tensor):\n            feat = (\n                feat\n                * torch.stack(\n                    [\n                        (\n                            self.get_buffer(f\"{g_id.item():d}_std\")\n                            if hasattr(self, f\"{g_id.item():d}_std\")\n                            else self.get_buffer(\"aver_std\")\n                        )\n                        for g_id in group_ids\n                    ],\n                    dim=0,\n                ).unsqueeze(1)\n                if self.std_norm\n                else feat\n            )\n\n            feat = (\n                feat\n                + torch.stack(\n                    [\n                        (\n                            self.get_buffer(f\"{g_id.item():d}_mean\")\n                            if hasattr(self, f\"{g_id.item():d}_mean\")\n                            else self.get_buffer(\"aver_mean\")\n                        )\n                        for g_id in group_ids\n                    ],\n                    dim=0,\n                ).unsqueeze(1)\n                if self.mean_norm\n                else feat\n            )\n        # group-level normalization with None group_ids, recover by the average mean &amp; std\n        elif self.norm_type == \"group\" and group_ids is None:\n            feat = (\n                feat * self.get_buffer(\"aver_std\").expand(1, 1, -1)\n                if self.std_norm\n                else feat\n            )\n            feat = (\n                feat + self.get_buffer(\"aver_mean\").expand(1, 1, -1)\n                if self.mean_norm\n                else feat\n            )\n        else:\n            raise RuntimeError\n\n        return feat\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        \"\"\"Lazily register all the buffer variables ending with '_batch', '_std', or\n        '_mean' from state_dict to self.\"\"\"\n        for key in state_dict.keys():\n            if key.startswith(prefix):\n                input_name = key[len(prefix) :].split(\".\", 1)[0]\n\n                if \"_\" in input_name and input_name.split(\"_\")[-1] in [\n                    \"batch\",\n                    \"std\",\n                    \"mean\",\n                ]:\n                    self.register_buffer(input_name, state_dict[key])\n                else:\n                    unexpected_keys.append(key)\n\n    def extra_repr(self) -&gt; str:\n        return f\"norm_type={self.norm_type}, mean_norm={self.mean_norm}, std_norm={self.std_norm}\"\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.forward","title":"<code>forward(feat, feat_len, group_ids=None, epoch=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, length, channel) or (batch, length) The normalization will be done on the channel dimension. If the feat is in the shape of (batch, length), it will be extended to (batch, length, 1)</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch)</p> required <code>group_ids</code> <code>Tensor or str or int</code> <p>(batch)</p> <code>None</code> <code>epoch</code> <code>int</code> <code>None</code> <p>Returns:</p> Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>def forward(\n    self,\n    feat: torch.Tensor,\n    feat_len: torch.Tensor,\n    group_ids: torch.Tensor or str or int = None,\n    epoch: int = None,\n):\n    \"\"\"\n\n    Args:\n        feat: (batch, length, channel) or (batch, length)\n            The normalization will be done on the channel dimension.\n            If the feat is in the shape of (batch, length), it will be extended to (batch, length, 1)\n        feat_len: (batch)\n        group_ids: (batch)\n        epoch:\n\n    Returns:\n\n    \"\"\"\n    if self.norm_type == \"group\":\n        assert group_ids is not None, (\n            \"You are using group-level feature normalization, but group_ids is not given. \"\n            \"Please check 'data_cfg' in your configuration.\"\n        )\n    # para preparation\n    batch_size, squeeze_flag = feat.size(0), False\n    if len(feat.shape) == 2:\n        feat, squeeze_flag = feat.unsqueeze(-1), True\n    elif len(feat.shape) != 3:\n        raise RuntimeError(\n            f\"{self.__class__.__name__} only accepts the input vectors in the shape of \"\n            f\"(batch, length, channel) or (batch, length), but got shape={feat.shape}!\"\n        )\n\n    # --- Mean and Standard Variance Initialization --- #\n    # calculate the mean values of all channels of all the input utterances\n    curr_means = (\n        None\n        if not self.mean_norm\n        else torch.stack(\n            [feat[i][: feat_len[i]].mean(dim=0) for i in range(batch_size)]\n        )\n    )\n\n    # calculate the std values of all channels of all the input utterances\n    curr_stds = (\n        None\n        if not self.std_norm\n        else torch.clamp(\n            input=torch.stack(\n                [feat[i][: feat_len[i]].std(dim=0) for i in range(batch_size)]\n            ),\n            min=self.clamp,\n        )\n    )\n\n    # --- Perform Normalization based on Different branches --- #\n    # utterance-level normalization or group-level normalization without group_ids\n    if self.norm_type == \"utterance\":\n        feat = feat - curr_means.unsqueeze(1) if curr_means is not None else feat\n        feat = feat / curr_stds.unsqueeze(1) if curr_stds is not None else feat\n\n    # global-level &amp; batch-level &amp; group-level normalization (with group_ids)\n    else:\n        # only gather the batch sizes from other processes in the DDP model of training\n        all_batch_size = None\n        if self.training:\n            all_batch_size = (\n                self.gather_scalars(batch_size, feat.device)\n                if self.distributed\n                else batch_size\n            )\n\n        # group-level normalization with tensor group_ids (input utterances belong to different groups)\n        if self.norm_type == \"group\" and isinstance(group_ids, torch.Tensor):\n            # only update the mean and std of the specific group during training\n            if self.training:\n                # DDP mode\n                if self.distributed:\n                    # gather all the group ids from other processes\n                    all_group_ids = self.gather_vectors(group_ids, all_batch_size)\n                    # gather all the mean vectors from other processes\n                    all_curr_means = (\n                        None\n                        if curr_means is None\n                        else self.gather_matrices(curr_means, all_batch_size)\n                    )\n                    # gather all the std vectors from other processes\n                    all_curr_stds = (\n                        None\n                        if curr_stds is None\n                        else self.gather_matrices(curr_stds, all_batch_size)\n                    )\n                # single-GPU mode\n                else:\n                    # not perform gathering\n                    all_group_ids = group_ids\n                    all_curr_means = curr_means\n                    all_curr_stds = curr_stds\n\n                # record the mean of all groups in the current batch\n                group_mean_dict = self.sort_data_by_group(\n                    raw_data=all_curr_means, group_ids=all_group_ids\n                )\n\n                # record the std of all groups in the current batch\n                group_std_dict = self.sort_data_by_group(\n                    raw_data=all_curr_stds, group_ids=all_group_ids\n                )\n\n                # register the mean, std, and batch numbers into the buffer\n                group_keys = (\n                    list(group_mean_dict.keys())\n                    if group_mean_dict is not None\n                    else list(group_std_dict.keys())\n                )\n                for group_id in group_keys:\n                    self.register_mean_std_batch(\n                        curr_aver_mean=(\n                            group_mean_dict[group_id].mean(dim=0)\n                            if group_mean_dict is not None\n                            else None\n                        ),\n                        curr_aver_std=(\n                            group_std_dict[group_id].mean(dim=0)\n                            if group_std_dict is not None\n                            else None\n                        ),\n                        prefix=group_id,\n                        epoch=epoch,\n                    )\n                # update the average mean &amp; std of all the groups\n                # (i.e. the average distribution for unknown samples during inference)\n                self.update_aver_mean_std(epoch)\n\n            # During training, normalize the known features by the group mean &amp; std\n            # During inference, normalize the unknown features by the average mean &amp; std of all groups\n            for i in range(batch_size):\n                group_id = group_ids[i].item() if group_ids is not None else None\n\n                if self.mean_norm:\n                    feat[i] -= (\n                        self.get_buffer(\"aver_mean\")\n                        if not hasattr(self, f\"{group_id}_mean\")\n                        else self.get_buffer(f\"{group_id}_mean\")\n                    )\n                if self.std_norm:\n                    feat[i] /= (\n                        self.get_buffer(\"aver_std\")\n                        if not hasattr(self, f\"{group_id}_std\")\n                        else self.get_buffer(f\"{group_id}_std\")\n                    )\n\n        # batch-level &amp; global-level normalization (these two scenarios share the batch-level mean &amp; std)\n        else:\n            # only calculate the batch-level mean and std during training\n            if self.training:\n                # gather the mean and std from the other processes in the DDP mode\n                if self.distributed:\n                    # gather the sums of batch means from all the processes\n                    batch_mean_sum = (\n                        curr_means.sum(dim=0) if curr_means is not None else None\n                    )\n                    all_batch_mean_sums = (\n                        self.gather_vectors(batch_mean_sum)\n                        if batch_mean_sum is not None\n                        else None\n                    )\n                    batch_mean = (\n                        None\n                        if all_batch_mean_sums is None\n                        else all_batch_mean_sums.sum(dim=0) / all_batch_size.sum()\n                    )\n\n                    # gather the sums of batch stds from all the processes\n                    batch_std_sum = (\n                        curr_stds.sum(dim=0) if curr_stds is not None else None\n                    )\n                    all_batch_std_sums = (\n                        self.gather_vectors(batch_std_sum)\n                        if batch_std_sum is not None\n                        else None\n                    )\n                    batch_std = (\n                        None\n                        if all_batch_std_sums is None\n                        else all_batch_std_sums.sum(dim=0) / all_batch_size.sum()\n                    )\n\n                # single-GPU mode\n                else:\n                    batch_mean = (\n                        curr_means.mean(dim=0) if curr_means is not None else None\n                    )\n                    batch_std = (\n                        curr_stds.mean(dim=0) if curr_stds is not None else None\n                    )\n\n            # do nothing for batch-level mean and std during evaluation\n            else:\n                batch_mean = None\n                batch_std = None\n\n            # batch-level normalization\n            if self.norm_type == \"batch\":\n                # normalize the input utterances by the batch mean and std during training\n                if self.training:\n                    feat = feat - batch_mean if batch_mean is not None else feat\n                    feat = feat / batch_std if batch_std is not None else feat\n                # normalize the input utterances by the utterance-specific mean and std during evaluation\n                else:\n                    feat = (\n                        feat - curr_means.unsqueeze(1)\n                        if curr_means is not None\n                        else feat\n                    )\n                    feat = (\n                        feat / curr_stds.unsqueeze(1)\n                        if curr_stds is not None\n                        else feat\n                    )\n\n            # global-level normalization or\n            # group-level normalization with str or int group_ids (input utterances belong to the same group)\n            else:\n                assert self.norm_type in [\"global\", \"group\"], (\n                    f\"norm_type can only be one of 'utterance', 'batch', 'group', 'global', \"\n                    f\"but got norm_type={self.norm_type}!\"\n                )\n                if self.norm_type == \"group\":\n                    assert isinstance(group_ids, (str, int)), (\n                        f\"If all the utterances in a single batch belong to the same group, \"\n                        f\"you should give group_ids as a string or integer. \"\n                        f\"But got type(group_ids)={type(group_ids)}.\"\n                    )\n\n                # only update the mean and std during training\n                prefix = \"global\" if self.norm_type == \"global\" else group_ids\n                if self.training:\n                    self.register_mean_std_batch(\n                        curr_aver_mean=batch_mean,\n                        curr_aver_std=batch_std,\n                        prefix=prefix,\n                        epoch=epoch,\n                    )\n\n                # if the group_ids is given as a string or int,\n                # we assume that there are no unknown testing samples during inference\n                feat = (\n                    feat - self.get_buffer(f\"{prefix}_mean\")\n                    if curr_means is not None\n                    else feat\n                )\n                feat = (\n                    feat / self.get_buffer(f\"{prefix}_std\")\n                    if curr_stds is not None\n                    else feat\n                )\n\n    return feat.squeeze(-1) if squeeze_flag else feat, feat_len\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.module_init","title":"<code>module_init(norm_type='global', mean_norm=True, std_norm=True, clamp=1e-10, max_epoch_num=4)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>norm_type</code> <code>str</code> <p>str The type of feature normalization. The type must be one of 'utterance', 'batch', 'group', and 'global'</p> <code>'global'</code> <code>mean_norm</code> <code>bool</code> <p>bool Controls whether the feature vectors will be normalized by their means</p> <code>True</code> <code>std_norm</code> <code>bool</code> <p>bool Controls whether the feature vectors will be normalized by their standard variance</p> <code>True</code> <code>clamp</code> <code>float</code> <p>float Clamping threshold for the standard variance before division.</p> <code>1e-10</code> <code>max_epoch_num</code> <code>int</code> <p>int The maximum number of epochs used to calculate the moving average. Usually, the value of this argument is lower than a half of the number of warmup epochs.</p> <code>4</code> Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>def module_init(\n    self,\n    norm_type: str = \"global\",\n    mean_norm: bool = True,\n    std_norm: bool = True,\n    clamp: float = 1e-10,\n    max_epoch_num: int = 4,\n):\n    \"\"\"\n\n    Args:\n        norm_type: str\n            The type of feature normalization.\n            The type must be one of 'utterance', 'batch', 'group', and 'global'\n        mean_norm: bool\n            Controls whether the feature vectors will be normalized by their means\n        std_norm: bool\n            Controls whether the feature vectors will be normalized by their standard variance\n        clamp: float\n            Clamping threshold for the standard variance before division.\n        max_epoch_num: int\n            The maximum number of epochs used to calculate the moving average.\n            Usually, the value of this argument is lower than a half of the number of warmup epochs.\n\n    \"\"\"\n    self.norm_type = norm_type\n    self.mean_norm = mean_norm\n    self.std_norm = std_norm\n    self.clamp = clamp\n    self.max_epoch_num = max_epoch_num\n\n    if self.input_size is not None:\n        self.output_size = self.input_size\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.recover","title":"<code>recover(feat, group_ids=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> required <code>group_ids</code> <code>Tensor or str or int</code> <code>None</code> <p>Returns:</p> Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>def recover(self, feat: torch.Tensor, group_ids: torch.Tensor or str or int = None):\n    \"\"\"\n\n    Args:\n        feat:\n        group_ids:\n\n    Returns:\n\n    \"\"\"\n    assert self.norm_type not in [\n        \"utterance\",\n        \"batch\",\n    ], \"If norm_type is either 'utterance' or 'batch', the normalized features cannot be recovered.\"\n\n    # global normalization or\n    # group-level normalization with str or int group_ids (input utterances belong to the same group)\n    if self.norm_type == \"global\" or (\n        self.norm_type == \"group\" and isinstance(group_ids, (str, int))\n    ):\n        prefix = \"global\" if self.norm_type == \"global\" else str(group_ids)\n        feat = feat * self.get_buffer(f\"{prefix}_std\") if self.std_norm else feat\n        feat = feat + self.get_buffer(f\"{prefix}_mean\") if self.mean_norm else feat\n    # group-level normalization with tensor group_ids (input utterances belong to different groups)\n    # recover by the average mean &amp; std when meeting an unknown group during inference\n    elif self.norm_type == \"group\" and isinstance(group_ids, torch.Tensor):\n        feat = (\n            feat\n            * torch.stack(\n                [\n                    (\n                        self.get_buffer(f\"{g_id.item():d}_std\")\n                        if hasattr(self, f\"{g_id.item():d}_std\")\n                        else self.get_buffer(\"aver_std\")\n                    )\n                    for g_id in group_ids\n                ],\n                dim=0,\n            ).unsqueeze(1)\n            if self.std_norm\n            else feat\n        )\n\n        feat = (\n            feat\n            + torch.stack(\n                [\n                    (\n                        self.get_buffer(f\"{g_id.item():d}_mean\")\n                        if hasattr(self, f\"{g_id.item():d}_mean\")\n                        else self.get_buffer(\"aver_mean\")\n                    )\n                    for g_id in group_ids\n                ],\n                dim=0,\n            ).unsqueeze(1)\n            if self.mean_norm\n            else feat\n        )\n    # group-level normalization with None group_ids, recover by the average mean &amp; std\n    elif self.norm_type == \"group\" and group_ids is None:\n        feat = (\n            feat * self.get_buffer(\"aver_std\").expand(1, 1, -1)\n            if self.std_norm\n            else feat\n        )\n        feat = (\n            feat + self.get_buffer(\"aver_mean\").expand(1, 1, -1)\n            if self.mean_norm\n            else feat\n        )\n    else:\n        raise RuntimeError\n\n    return feat\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.register_mean_std_batch","title":"<code>register_mean_std_batch(curr_aver_mean, curr_aver_std, prefix, epoch)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>curr_aver_mean</code> <code>Tensor</code> required <code>curr_aver_std</code> <code>Tensor</code> required <code>prefix</code> <code>str</code> required <code>epoch</code> <code>int</code> required Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>def register_mean_std_batch(\n    self,\n    curr_aver_mean: torch.Tensor,\n    curr_aver_std: torch.Tensor,\n    prefix: str,\n    epoch: int,\n):\n    \"\"\"\n\n    Args:\n        curr_aver_mean:\n        curr_aver_std:\n        prefix:\n        epoch:\n\n    \"\"\"\n    # update the observed global batch number\n    if epoch is None or not hasattr(self, f\"{prefix}_batch\"):\n        self.register_buffer(\n            f\"{prefix}_batch\",\n            torch.LongTensor([1]).cuda(device=curr_aver_mean.device),\n        )\n    elif epoch &lt;= self.max_epoch_num:\n        self.register_buffer(\n            f\"{prefix}_batch\", self.get_buffer(f\"{prefix}_batch\") + 1\n        )\n\n    # update the observed global mean &amp; std only in the predefined batch number\n    if epoch is None or epoch &lt;= self.max_epoch_num:\n        # get the weight of the global average values\n        curr_weight = 1 / self.get_buffer(f\"{prefix}_batch\")\n\n        # update the observed global mean\n        if self.mean_norm:\n            if not hasattr(self, f\"{prefix}_mean\"):\n                self.register_buffer(f\"{prefix}_mean\", curr_aver_mean)\n            else:\n                prev_aver_mean = self.get_buffer(f\"{prefix}_mean\")\n                self.register_buffer(\n                    f\"{prefix}_mean\",\n                    curr_weight * curr_aver_mean\n                    + (1 - curr_weight) * prev_aver_mean,\n                )\n\n        # update the observed global std\n        if self.std_norm:\n            if not hasattr(self, f\"{prefix}_std\"):\n                self.register_buffer(f\"{prefix}_std\", curr_aver_std)\n            else:\n                prev_aver_std = self.get_buffer(f\"{prefix}_std\")\n                self.register_buffer(\n                    f\"{prefix}_std\",\n                    curr_weight * curr_aver_std + (1 - curr_weight) * prev_aver_std,\n                )\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.sort_data_by_group","title":"<code>sort_data_by_group(raw_data, group_ids)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>Tensor</code> required <code>group_ids</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>@staticmethod\ndef sort_data_by_group(raw_data: torch.Tensor, group_ids: torch.Tensor):\n    \"\"\"\n\n    Args:\n        raw_data:\n        group_ids:\n\n    Returns:\n\n    \"\"\"\n    if raw_data is None:\n        return None\n    else:\n        group_dict = dict()\n        # loop each group id\n        for i in range(group_ids.size(0)):\n            curr_group = group_ids[i].item()\n            # initialize the group list if not existed\n            if curr_group not in group_dict.keys():\n                group_dict[curr_group] = []\n            group_dict[curr_group].append(raw_data[i])\n        # turn each group list into a 2d tensor\n        return {\n            group_id: torch.stack(group_list)\n            for group_id, group_list in group_dict.items()\n        }\n</code></pre>"},{"location":"reference/module/norm/feat_norm/#module.norm.feat_norm.FeatureNormalization.update_aver_mean_std","title":"<code>update_aver_mean_std(epoch)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> required Source code in <code>speechain/module/norm/feat_norm.py</code> <pre><code>def update_aver_mean_std(self, epoch: int):\n    \"\"\"\n\n    Args:\n        epoch:\n\n    \"\"\"\n    if epoch is None or epoch &lt;= self.max_epoch_num:\n        _group_mean_num, _group_std_num = 0, 0\n        _aver_mean, _aver_std = None, None\n        for name, buff in self.named_buffers():\n            if name.endswith(\"_mean\"):\n                _group_mean_num += 1\n                _aver_mean = (\n                    buff.clone() if _aver_mean is None else _aver_mean + buff\n                )\n            elif name.endswith(\"_std\"):\n                _group_std_num += 1\n                _aver_std = buff.clone() if _aver_std is None else _aver_std + buff\n\n        self.register_buffer(\"aver_mean\", _aver_mean / _group_mean_num)\n        self.register_buffer(\"aver_std\", _aver_std / _group_std_num)\n</code></pre>"},{"location":"reference/module/postnet/","title":"postnet","text":""},{"location":"reference/module/postnet/conv1d/","title":"conv1d","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/postnet/conv1d/#module.postnet.conv1d.Conv1dPostnet","title":"<code>Conv1dPostnet</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Conv1d postnet for TTS. Usually used after the Transformer TTS decoder. This prenet is made up of only Conv1d blocks each of which contains:  1. a Conv1d layer  2. a BatchNorm1d layer  3. an activation function  4. a Dropout layer.</p> Reference <p>Neural Speech Synthesis with Transformer Network https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> Source code in <code>speechain/module/postnet/conv1d.py</code> <pre><code>class Conv1dPostnet(Module):\n    \"\"\"\n    The Conv1d postnet for TTS. Usually used after the Transformer TTS decoder.\n    This prenet is made up of only Conv1d blocks each of which contains:\n     1. a Conv1d layer\n     2. a BatchNorm1d layer\n     3. an activation function\n     4. a Dropout layer.\n\n    Reference:\n        Neural Speech Synthesis with Transformer Network\n        https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n    \"\"\"\n\n    def module_init(\n        self,\n        feat_dim: int = None,\n        conv_dims: int or List[int] = [512, 512, 512, 512, 0],\n        conv_kernel: int = 5,\n        conv_stride: int = 1,\n        conv_padding_mode: str = \"same\",\n        conv_batchnorm: bool = True,\n        conv_activation: str = \"Tanh\",\n        conv_dropout: float or List[float] = None,\n        zero_centered: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            feat_dim: int\n                The dimension of input acoustic feature tensors.\n                Used for calculating the in_features of the first Linear layer.\n            conv_dims: List[int] or int\n                The values of out_channels of each Conv1d layer.\n                If a list of integers is given, multiple Conv1d layers will be initialized.\n                If an integer is given, there will be only one Conv1d layer\n                -1: same size as the previous convolutional layer's dim\n                0: same size as the input feat_dim\n            conv_kernel: int\n                The value of kernel_size of all Conv1d layers.\n            conv_stride: int\n                The value of stride of all Conv1d layers.\n            conv_padding_mode: str\n                The padding mode of convolutional layers. Must be one of ['valid', 'full', 'same', 'causal'].\n            conv_batchnorm: bool\n                Whether a BatchNorm1d layer is added right after a Conv1d layer\n            conv_activation: str\n                The type of the activation function after all Conv1d layers.\n                None means no activation function is needed.\n            conv_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            zero_centered: bool\n                Whether the output of this module is centered at 0.\n                If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n                LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n                to True.\n\n        \"\"\"\n        # Convolution arguments checking\n        assert isinstance(\n            conv_dims, (List, int)\n        ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_kernel, int\n        ), \"The sizes of convolutional kernels must be given as an integer!\"\n        assert isinstance(\n            conv_stride, int\n        ), \"The lengths of convolutional strides must be given as an integer!\"\n        if conv_dropout is not None:\n            assert isinstance(\n                conv_dropout, (List, float)\n            ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n        # input_size initialization\n        if self.input_size is not None:\n            feat_dim = self.input_size\n        else:\n            assert feat_dim is not None\n\n        # register convolution arguments\n        self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n        self.conv_kernel = conv_kernel\n        self.conv_stride = conv_stride\n        self.conv_padding_mode = conv_padding_mode\n        self.conv_dropout = conv_dropout\n\n        # Conv1d layers initialization\n        _prev_dim = feat_dim\n        _tmp_conv = []\n        for i in range(len(self.conv_dims)):\n            # 0 means go back to the input feat_dim\n            if self.conv_dims[i] == 0:\n                self.conv_dims[i] = feat_dim\n            # -1 means equal to the previous layer\n            elif self.conv_dims[i] == -1:\n                self.conv_dims[i] = self.conv_dims[i - 1]\n            _tmp_conv.append(\n                # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n                # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n                Conv1dEv(\n                    in_channels=_prev_dim,\n                    out_channels=self.conv_dims[i],\n                    kernel_size=self.conv_kernel,\n                    stride=self.conv_stride,\n                    padding_mode=self.conv_padding_mode,\n                    bias=not conv_batchnorm,\n                )\n            )\n            # BatchNorm is better to be placed before activation\n            # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n            if conv_batchnorm:\n                _tmp_conv.append(torch.nn.BatchNorm1d(self.conv_dims[i]))\n            if conv_activation is not None:\n                # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n                if i != len(self.conv_dims) - 1 or not (\n                    zero_centered and \"ReLU\" in conv_activation\n                ):\n                    _tmp_conv.append(getattr(torch.nn, conv_activation)())\n            if conv_dropout is not None:\n                _tmp_conv.append(\n                    torch.nn.Dropout(\n                        p=(\n                            self.conv_dropout\n                            if not isinstance(self.conv_dropout, List)\n                            else self.conv_dropout[i]\n                        )\n                    )\n                )\n            _prev_dim = conv_dims[i]\n        self.conv = torch.nn.Sequential(*_tmp_conv)\n        self.output_size = _prev_dim\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input feature tensors.\n            feat_len: (batch,)\n                The length of each feature tensor.\n\n        Returns: feat, feat_len\n            The embedded feature vectors with their lengths.\n\n        \"\"\"\n        # forward the convolutional layers\n        # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n        feat = feat.transpose(1, 2)\n        # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n        feat = self.conv(feat)\n        # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n        return feat.transpose(1, 2)\n</code></pre>"},{"location":"reference/module/postnet/conv1d/#module.postnet.conv1d.Conv1dPostnet.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input feature tensors.</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The length of each feature tensor.</p> required <p>feat, feat_len</p> Type Description <p>The embedded feature vectors with their lengths.</p> Source code in <code>speechain/module/postnet/conv1d.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input feature tensors.\n        feat_len: (batch,)\n            The length of each feature tensor.\n\n    Returns: feat, feat_len\n        The embedded feature vectors with their lengths.\n\n    \"\"\"\n    # forward the convolutional layers\n    # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n    feat = feat.transpose(1, 2)\n    # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n    feat = self.conv(feat)\n    # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n    return feat.transpose(1, 2)\n</code></pre>"},{"location":"reference/module/postnet/conv1d/#module.postnet.conv1d.Conv1dPostnet.module_init","title":"<code>module_init(feat_dim=None, conv_dims=[512, 512, 512, 512, 0], conv_kernel=5, conv_stride=1, conv_padding_mode='same', conv_batchnorm=True, conv_activation='Tanh', conv_dropout=None, zero_centered=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat_dim</code> <code>int</code> <p>int The dimension of input acoustic feature tensors. Used for calculating the in_features of the first Linear layer.</p> <code>None</code> <code>conv_dims</code> <code>int or List[int]</code> <p>List[int] or int The values of out_channels of each Conv1d layer. If a list of integers is given, multiple Conv1d layers will be initialized. If an integer is given, there will be only one Conv1d layer -1: same size as the previous convolutional layer's dim 0: same size as the input feat_dim</p> <code>[512, 512, 512, 512, 0]</code> <code>conv_kernel</code> <code>int</code> <p>int The value of kernel_size of all Conv1d layers.</p> <code>5</code> <code>conv_stride</code> <code>int</code> <p>int The value of stride of all Conv1d layers.</p> <code>1</code> <code>conv_padding_mode</code> <code>str</code> <p>str The padding mode of convolutional layers. Must be one of ['valid', 'full', 'same', 'causal'].</p> <code>'same'</code> <code>conv_batchnorm</code> <code>bool</code> <p>bool Whether a BatchNorm1d layer is added right after a Conv1d layer</p> <code>True</code> <code>conv_activation</code> <code>str</code> <p>str The type of the activation function after all Conv1d layers. None means no activation function is needed.</p> <code>'Tanh'</code> <code>conv_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>zero_centered</code> <code>bool</code> <p>bool Whether the output of this module is centered at 0. If the specified activation function changes the centroid of the output distribution, e.g. ReLU and LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set to True.</p> <code>False</code> Source code in <code>speechain/module/postnet/conv1d.py</code> <pre><code>def module_init(\n    self,\n    feat_dim: int = None,\n    conv_dims: int or List[int] = [512, 512, 512, 512, 0],\n    conv_kernel: int = 5,\n    conv_stride: int = 1,\n    conv_padding_mode: str = \"same\",\n    conv_batchnorm: bool = True,\n    conv_activation: str = \"Tanh\",\n    conv_dropout: float or List[float] = None,\n    zero_centered: bool = False,\n):\n    \"\"\"\n\n    Args:\n        feat_dim: int\n            The dimension of input acoustic feature tensors.\n            Used for calculating the in_features of the first Linear layer.\n        conv_dims: List[int] or int\n            The values of out_channels of each Conv1d layer.\n            If a list of integers is given, multiple Conv1d layers will be initialized.\n            If an integer is given, there will be only one Conv1d layer\n            -1: same size as the previous convolutional layer's dim\n            0: same size as the input feat_dim\n        conv_kernel: int\n            The value of kernel_size of all Conv1d layers.\n        conv_stride: int\n            The value of stride of all Conv1d layers.\n        conv_padding_mode: str\n            The padding mode of convolutional layers. Must be one of ['valid', 'full', 'same', 'causal'].\n        conv_batchnorm: bool\n            Whether a BatchNorm1d layer is added right after a Conv1d layer\n        conv_activation: str\n            The type of the activation function after all Conv1d layers.\n            None means no activation function is needed.\n        conv_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        zero_centered: bool\n            Whether the output of this module is centered at 0.\n            If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n            LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n            to True.\n\n    \"\"\"\n    # Convolution arguments checking\n    assert isinstance(\n        conv_dims, (List, int)\n    ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_kernel, int\n    ), \"The sizes of convolutional kernels must be given as an integer!\"\n    assert isinstance(\n        conv_stride, int\n    ), \"The lengths of convolutional strides must be given as an integer!\"\n    if conv_dropout is not None:\n        assert isinstance(\n            conv_dropout, (List, float)\n        ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n    # input_size initialization\n    if self.input_size is not None:\n        feat_dim = self.input_size\n    else:\n        assert feat_dim is not None\n\n    # register convolution arguments\n    self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n    self.conv_kernel = conv_kernel\n    self.conv_stride = conv_stride\n    self.conv_padding_mode = conv_padding_mode\n    self.conv_dropout = conv_dropout\n\n    # Conv1d layers initialization\n    _prev_dim = feat_dim\n    _tmp_conv = []\n    for i in range(len(self.conv_dims)):\n        # 0 means go back to the input feat_dim\n        if self.conv_dims[i] == 0:\n            self.conv_dims[i] = feat_dim\n        # -1 means equal to the previous layer\n        elif self.conv_dims[i] == -1:\n            self.conv_dims[i] = self.conv_dims[i - 1]\n        _tmp_conv.append(\n            # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n            # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n            Conv1dEv(\n                in_channels=_prev_dim,\n                out_channels=self.conv_dims[i],\n                kernel_size=self.conv_kernel,\n                stride=self.conv_stride,\n                padding_mode=self.conv_padding_mode,\n                bias=not conv_batchnorm,\n            )\n        )\n        # BatchNorm is better to be placed before activation\n        # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n        if conv_batchnorm:\n            _tmp_conv.append(torch.nn.BatchNorm1d(self.conv_dims[i]))\n        if conv_activation is not None:\n            # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n            if i != len(self.conv_dims) - 1 or not (\n                zero_centered and \"ReLU\" in conv_activation\n            ):\n                _tmp_conv.append(getattr(torch.nn, conv_activation)())\n        if conv_dropout is not None:\n            _tmp_conv.append(\n                torch.nn.Dropout(\n                    p=(\n                        self.conv_dropout\n                        if not isinstance(self.conv_dropout, List)\n                        else self.conv_dropout[i]\n                    )\n                )\n            )\n        _prev_dim = conv_dims[i]\n    self.conv = torch.nn.Sequential(*_tmp_conv)\n    self.output_size = _prev_dim\n</code></pre>"},{"location":"reference/module/postnet/token/","title":"token","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/postnet/token/#module.postnet.token.TokenPostnet","title":"<code>TokenPostnet</code>","text":"<p>               Bases: <code>Module</code></p> <p>The decoder postnet that projects the model output vectors into token predictions.</p> Source code in <code>speechain/module/postnet/token.py</code> <pre><code>class TokenPostnet(Module):\n    \"\"\"The decoder postnet that projects the model output vectors into token\n    predictions.\"\"\"\n\n    def module_init(self, vocab_size: int, input_dim: int = None):\n        \"\"\"\n\n        Args:\n            input_dim: int\n                The dimension of the output vectors from the decoder\n            vocab_size: int\n                The number of tokens in the dictionary.\n\n        \"\"\"\n        # input_size and output_size initialization\n        if self.input_size is not None:\n            input_dim = self.input_size\n        else:\n            assert input_dim is not None\n        self.output_size = vocab_size\n\n        # para recording\n        self.input_dim = input_dim\n        self.vocab_size = vocab_size\n\n        # initialize the output layer\n        self.linear = torch.nn.Linear(in_features=input_dim, out_features=vocab_size)\n\n    def forward(self, input: torch.Tensor):\n        \"\"\"\n\n        Args:\n            input:\n\n        Returns:\n\n        \"\"\"\n        return self.linear(input)\n</code></pre>"},{"location":"reference/module/postnet/token/#module.postnet.token.TokenPostnet.forward","title":"<code>forward(input)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/postnet/token.py</code> <pre><code>def forward(self, input: torch.Tensor):\n    \"\"\"\n\n    Args:\n        input:\n\n    Returns:\n\n    \"\"\"\n    return self.linear(input)\n</code></pre>"},{"location":"reference/module/postnet/token/#module.postnet.token.TokenPostnet.module_init","title":"<code>module_init(vocab_size, input_dim=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>int The dimension of the output vectors from the decoder</p> <code>None</code> <code>vocab_size</code> <code>int</code> <p>int The number of tokens in the dictionary.</p> required Source code in <code>speechain/module/postnet/token.py</code> <pre><code>def module_init(self, vocab_size: int, input_dim: int = None):\n    \"\"\"\n\n    Args:\n        input_dim: int\n            The dimension of the output vectors from the decoder\n        vocab_size: int\n            The number of tokens in the dictionary.\n\n    \"\"\"\n    # input_size and output_size initialization\n    if self.input_size is not None:\n        input_dim = self.input_size\n    else:\n        assert input_dim is not None\n    self.output_size = vocab_size\n\n    # para recording\n    self.input_dim = input_dim\n    self.vocab_size = vocab_size\n\n    # initialize the output layer\n    self.linear = torch.nn.Linear(in_features=input_dim, out_features=vocab_size)\n</code></pre>"},{"location":"reference/module/prenet/","title":"prenet","text":""},{"location":"reference/module/prenet/conv1d/","title":"conv1d","text":"<p>Author: Sashi Novitasari Affiliation: NAIST (-2022) Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dEv","title":"<code>Conv1dEv</code>","text":"<p>               Bases: <code>Module</code></p> <p>A 1D convolutional layer with support for different padding modes.</p> <p>Attributes:</p> Name Type Description <code>cutoff</code> <code>bool</code> <p>Indicates whether the output should be cut off for the 'same' padding mode.</p> <code>causal_padding</code> <code>int</code> <p>Additional padding required for the 'causal' padding mode.</p> <code>dilation</code> <code>int</code> <p>The dilation rate of the convolutional layer.</p> <code>conv_lyr</code> <code>Conv1d</code> <p>The 1D convolutional layer.</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>class Conv1dEv(torch.nn.Module):\n    \"\"\"A 1D convolutional layer with support for different padding modes.\n\n    Attributes:\n        cutoff (bool):\n            Indicates whether the output should be cut off for the 'same' padding mode.\n        causal_padding (int):\n            Additional padding required for the 'causal' padding mode.\n        dilation (int):\n            The dilation rate of the convolutional layer.\n        conv_lyr (torch.nn.Conv1d):\n            The 1D convolutional layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        dilation: int = 1,\n        padding_mode: str = \"same\",\n        bias: bool = True,\n        use_weight_norm: bool = False,\n        groups: int = 1,\n    ):\n        \"\"\"Initializes the Conv1dEv module with the specified parameters.\n\n        Args:\n            in_channels (int):\n                Number of channels in the input feature.\n            out_channels (int):\n                Number of channels produced by the convolution.\n            kernel_size (int):\n                Size of the convolutional kernel.\n            stride (int, optional):\n                Stride of the convolution. Defaults to 1.\n            dilation (int, optional):\n                The dilation rate of the kernel. Defaults to 1.\n            padding_mode (str, optional):\n                Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.\n            bias (bool, optional):\n                If True, adds a learnable bias to the output. Defaults to True.\n\n        Raises:\n            ValueError: If an unsupported padding mode is specified.\n        \"\"\"\n        super().__init__()\n\n        self.cutoff = False\n        self.causal_padding = 0\n        self.dilation = dilation\n\n        # no padding is used\n        if padding_mode == \"valid\":\n            padding = 0\n        # full padding\n        elif padding_mode == \"full\":\n            padding = dilation * (kernel_size - 1)\n        # same padding, the output is the same in dimension with input\n        elif padding_mode == \"same\":\n            assert stride == 1, \"Stride should be 1 for 'same' padding mode\"\n            if kernel_size % 2 == 0:\n                padding = dilation * kernel_size // 2\n                self.cutoff = True\n            else:\n                padding = dilation * (kernel_size - 1) // 2\n        # causal padding\n        elif padding_mode == \"causal\":\n            padding = 0\n            self.causal_padding = dilation * (kernel_size - 1)\n        else:\n            raise ValueError(\n                \"Unsupported padding mode. Supported modes are 'valid', 'full', 'same' and 'causal'.\"\n            )\n\n        self.conv_lyr = torch.nn.Conv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n        if use_weight_norm:\n            self.conv_lyr = weight_norm(self.conv_lyr)\n\n    def forward(self, feat: torch.Tensor):\n        \"\"\"Performs a forward pass through the convolutional layer.\n\n        Args:\n            feat (torch.Tensor):\n                The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).\n\n        Returns:\n            torch.Tensor:\n                The output tensor. Shape: (batch, out_channels, output_len).\n        \"\"\"\n        # attach additional paddings at the end for the 'causal' padding mode\n        if self.causal_padding &gt; 0:\n            feat = F.pad(feat, (self.causal_padding, 0))\n        output = self.conv_lyr(feat)\n        # cut off the redundant tails for the 'same' padding mode\n        if self.cutoff:\n            output = output[:, :, : -self.dilation]\n        return output\n</code></pre>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dEv.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, dilation=1, padding_mode='same', bias=True, use_weight_norm=False, groups=1)</code>","text":"<p>Initializes the Conv1dEv module with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input feature.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution. Defaults to 1.</p> <code>1</code> <code>dilation</code> <code>int</code> <p>The dilation rate of the kernel. Defaults to 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.</p> <code>'same'</code> <code>bias</code> <code>bool</code> <p>If True, adds a learnable bias to the output. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported padding mode is specified.</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    dilation: int = 1,\n    padding_mode: str = \"same\",\n    bias: bool = True,\n    use_weight_norm: bool = False,\n    groups: int = 1,\n):\n    \"\"\"Initializes the Conv1dEv module with the specified parameters.\n\n    Args:\n        in_channels (int):\n            Number of channels in the input feature.\n        out_channels (int):\n            Number of channels produced by the convolution.\n        kernel_size (int):\n            Size of the convolutional kernel.\n        stride (int, optional):\n            Stride of the convolution. Defaults to 1.\n        dilation (int, optional):\n            The dilation rate of the kernel. Defaults to 1.\n        padding_mode (str, optional):\n            Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.\n        bias (bool, optional):\n            If True, adds a learnable bias to the output. Defaults to True.\n\n    Raises:\n        ValueError: If an unsupported padding mode is specified.\n    \"\"\"\n    super().__init__()\n\n    self.cutoff = False\n    self.causal_padding = 0\n    self.dilation = dilation\n\n    # no padding is used\n    if padding_mode == \"valid\":\n        padding = 0\n    # full padding\n    elif padding_mode == \"full\":\n        padding = dilation * (kernel_size - 1)\n    # same padding, the output is the same in dimension with input\n    elif padding_mode == \"same\":\n        assert stride == 1, \"Stride should be 1 for 'same' padding mode\"\n        if kernel_size % 2 == 0:\n            padding = dilation * kernel_size // 2\n            self.cutoff = True\n        else:\n            padding = dilation * (kernel_size - 1) // 2\n    # causal padding\n    elif padding_mode == \"causal\":\n        padding = 0\n        self.causal_padding = dilation * (kernel_size - 1)\n    else:\n        raise ValueError(\n            \"Unsupported padding mode. Supported modes are 'valid', 'full', 'same' and 'causal'.\"\n        )\n\n    self.conv_lyr = torch.nn.Conv1d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        dilation=dilation,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n    if use_weight_norm:\n        self.conv_lyr = weight_norm(self.conv_lyr)\n</code></pre>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dEv.forward","title":"<code>forward(feat)</code>","text":"<p>Performs a forward pass through the convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor. Shape: (batch, out_channels, output_len).</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def forward(self, feat: torch.Tensor):\n    \"\"\"Performs a forward pass through the convolutional layer.\n\n    Args:\n        feat (torch.Tensor):\n            The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).\n\n    Returns:\n        torch.Tensor:\n            The output tensor. Shape: (batch, out_channels, output_len).\n    \"\"\"\n    # attach additional paddings at the end for the 'causal' padding mode\n    if self.causal_padding &gt; 0:\n        feat = F.pad(feat, (self.causal_padding, 0))\n    output = self.conv_lyr(feat)\n    # cut off the redundant tails for the 'same' padding mode\n    if self.cutoff:\n        output = output[:, :, : -self.dilation]\n    return output\n</code></pre>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dPrenet","title":"<code>Conv1dPrenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Conv1d prenet. Usually used before the TTS encoder. This prenet is made up of two parts:     1. (mandatory) The Conv1d part contains one or more Conv1d blocks which are composed of the components below         1. (mandatory) a Conv1d layer         2. (optional) a BatchNorm1d layer         3. (optional) an activation function         4. (optional) a Dropout layer.     2. (optional) The Linear part contains one or more Linear blocks which are composed of the components below         1. (mandatory) a Linear layer         2. (optional) an activation function         3. (optional) a Dropout layer.</p> Reference <p>Neural Speech Synthesis with Transformer Network https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>class Conv1dPrenet(Module):\n    \"\"\"\n    The Conv1d prenet. Usually used before the TTS encoder.\n    This prenet is made up of two parts:\n        1. (mandatory) The Conv1d part contains one or more Conv1d blocks which are composed of the components below\n            1. (mandatory) a Conv1d layer\n            2. (optional) a BatchNorm1d layer\n            3. (optional) an activation function\n            4. (optional) a Dropout layer.\n        2. (optional) The Linear part contains one or more Linear blocks which are composed of the components below\n            1. (mandatory) a Linear layer\n            2. (optional) an activation function\n            3. (optional) a Dropout layer.\n\n    Reference:\n        Neural Speech Synthesis with Transformer Network\n        https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n    \"\"\"\n\n    def module_init(\n        self,\n        feat_dim: int = None,\n        conv_dims: int or List[int] = [512, 512, 512],\n        conv_kernel: int = 5,\n        conv_stride: int = 1,\n        conv_batchnorm: bool = True,\n        conv_activation: str = \"ReLU\",\n        conv_dropout: float or List[float] = None,\n        lnr_dims: int or List[int] = -1,\n        lnr_activation: str = None,\n        lnr_dropout: int or List[int] = None,\n        zero_centered: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            feat_dim: int\n                The dimension of input acoustic feature tensors.\n                Used for calculating the in_features of the first Linear layer.\n            conv_dims: List[int] or int\n                The values of out_channels of each Conv1d layer.\n                If a list of integers is given, multiple Conv1d layers will be initialized.\n                If an integer is given, there will be only one Conv1d layer\n            conv_kernel: int\n                The value of kernel_size of all Conv1d layers.\n            conv_stride: int\n                The value of stride of all Conv1d layers.\n            conv_batchnorm: bool\n                Whether a BatchNorm1d layer is added right after a Conv1d layer\n            conv_activation: str\n                The type of the activation function after all Conv1d layers.\n                None means no activation function is needed.\n            conv_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            lnr_dims: int or List[int]\n                The values of out_features of each Linear layer.\n                The first value in the List represents the out_features of the first Linear layer.\n                -1: same size as the last convolutional layer's dim\n            lnr_activation: str\n                The type of the activation function after all Linear layers.\n                None means no activation function is needed.\n            lnr_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            zero_centered: bool\n                Whether the output of this module is centered at 0.\n                If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n                LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n                to True.\n\n        \"\"\"\n        # --- 0. Argument Checking --- #\n        # Convolution arguments checking\n        assert isinstance(\n            conv_dims, (List, int)\n        ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_kernel, int\n        ), \"The sizes of convolutional kernels must be given as an integer!\"\n        assert isinstance(\n            conv_stride, int\n        ), \"The lengths of convolutional strides must be given as an integer!\"\n        if conv_dropout is not None:\n            assert isinstance(\n                conv_dropout, (List, float)\n            ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n        # Linear arguments checking\n        if lnr_dropout is not None:\n            assert isinstance(\n                lnr_dropout, (List, float)\n            ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n        if lnr_dims is not None:\n            assert isinstance(\n                lnr_dims, (List, int)\n            ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n        # input_size initialization\n        if self.input_size is not None:\n            feat_dim = self.input_size\n        else:\n            assert feat_dim is not None\n\n        # --- 1. Convolutional Part Initialization --- #\n        # register convolution arguments\n        self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n        self.conv_kernel = conv_kernel\n        self.conv_stride = conv_stride\n        self.conv_dropout = conv_dropout\n\n        # Conv1d blocks construction\n        _prev_dim = feat_dim\n        _tmp_conv = []\n        for i in range(len(self.conv_dims)):\n            _tmp_conv.append(\n                # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n                # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n                Conv1dEv(\n                    in_channels=_prev_dim,\n                    out_channels=self.conv_dims[i],\n                    kernel_size=self.conv_kernel,\n                    stride=self.conv_stride,\n                    padding_mode=\"same\",\n                    bias=not conv_batchnorm,\n                )\n            )\n            # BatchNorm is better to be placed before activation\n            # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n            if conv_batchnorm:\n                _tmp_conv.append(torch.nn.BatchNorm1d(self.conv_dims[i]))\n            if conv_activation is not None:\n                # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n                if not (i == len(self.conv_dims) - 1 and lnr_dims is None) or not (\n                    zero_centered and \"ReLU\" in conv_activation\n                ):\n                    _tmp_conv.append(getattr(torch.nn, conv_activation)())\n            if conv_dropout is not None:\n                _tmp_conv.append(\n                    torch.nn.Dropout(\n                        p=(\n                            self.conv_dropout\n                            if not isinstance(self.conv_dropout, List)\n                            else self.conv_dropout[i]\n                        )\n                    )\n                )\n            _prev_dim = conv_dims[i]\n        self.conv = torch.nn.Sequential(*_tmp_conv)\n        self.output_size = _prev_dim\n\n        # --- 2. Linear Part Initialization --- #\n        if lnr_dims is not None:\n            lnr_dims = lnr_dims if isinstance(lnr_dims, List) else [lnr_dims]\n            for i in range(len(lnr_dims)):\n                _prev_dim = self.conv_dims[-1] if i == 0 else lnr_dims[i - 1]\n                if lnr_dims[i] == -1:\n                    lnr_dims[i] = _prev_dim\n\n            self.linear = LinearPrenet(\n                feat_dim=self.output_size,\n                lnr_dims=lnr_dims,\n                lnr_activation=lnr_activation,\n                lnr_dropout=lnr_dropout,\n                zero_centered=zero_centered,\n            )\n            self.output_size = self.linear.output_size\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input feature tensors.\n            feat_len: (batch,)\n                The length of each feature tensor.\n\n        Returns: feat, feat_len\n            The embedded feature vectors with their lengths.\n\n        \"\"\"\n        # forward the convolutional layers\n        # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n        feat = feat.transpose(1, 2)\n        # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n        feat = self.conv(feat)\n        # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n        feat = feat.transpose(1, 2)\n\n        # forward the linear layers\n        if hasattr(self, \"linear\"):\n            # (batch, feat_maxlen, conv_dim) -&gt; (batch, feat_maxlen, lnr_dim)\n            feat, feat_len = self.linear(feat, feat_len)\n\n        # return both feat &amp; feat_len for the compatibility with other prenet\n        return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dPrenet.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input feature tensors.</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The length of each feature tensor.</p> required <p>feat, feat_len</p> Type Description <p>The embedded feature vectors with their lengths.</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input feature tensors.\n        feat_len: (batch,)\n            The length of each feature tensor.\n\n    Returns: feat, feat_len\n        The embedded feature vectors with their lengths.\n\n    \"\"\"\n    # forward the convolutional layers\n    # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n    feat = feat.transpose(1, 2)\n    # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n    feat = self.conv(feat)\n    # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n    feat = feat.transpose(1, 2)\n\n    # forward the linear layers\n    if hasattr(self, \"linear\"):\n        # (batch, feat_maxlen, conv_dim) -&gt; (batch, feat_maxlen, lnr_dim)\n        feat, feat_len = self.linear(feat, feat_len)\n\n    # return both feat &amp; feat_len for the compatibility with other prenet\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/conv1d/#module.prenet.conv1d.Conv1dPrenet.module_init","title":"<code>module_init(feat_dim=None, conv_dims=[512, 512, 512], conv_kernel=5, conv_stride=1, conv_batchnorm=True, conv_activation='ReLU', conv_dropout=None, lnr_dims=-1, lnr_activation=None, lnr_dropout=None, zero_centered=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat_dim</code> <code>int</code> <p>int The dimension of input acoustic feature tensors. Used for calculating the in_features of the first Linear layer.</p> <code>None</code> <code>conv_dims</code> <code>int or List[int]</code> <p>List[int] or int The values of out_channels of each Conv1d layer. If a list of integers is given, multiple Conv1d layers will be initialized. If an integer is given, there will be only one Conv1d layer</p> <code>[512, 512, 512]</code> <code>conv_kernel</code> <code>int</code> <p>int The value of kernel_size of all Conv1d layers.</p> <code>5</code> <code>conv_stride</code> <code>int</code> <p>int The value of stride of all Conv1d layers.</p> <code>1</code> <code>conv_batchnorm</code> <code>bool</code> <p>bool Whether a BatchNorm1d layer is added right after a Conv1d layer</p> <code>True</code> <code>conv_activation</code> <code>str</code> <p>str The type of the activation function after all Conv1d layers. None means no activation function is needed.</p> <code>'ReLU'</code> <code>conv_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>lnr_dims</code> <code>int or List[int]</code> <p>int or List[int] The values of out_features of each Linear layer. The first value in the List represents the out_features of the first Linear layer. -1: same size as the last convolutional layer's dim</p> <code>-1</code> <code>lnr_activation</code> <code>str</code> <p>str The type of the activation function after all Linear layers. None means no activation function is needed.</p> <code>None</code> <code>lnr_dropout</code> <code>int or List[int]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>zero_centered</code> <code>bool</code> <p>bool Whether the output of this module is centered at 0. If the specified activation function changes the centroid of the output distribution, e.g. ReLU and LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set to True.</p> <code>False</code> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def module_init(\n    self,\n    feat_dim: int = None,\n    conv_dims: int or List[int] = [512, 512, 512],\n    conv_kernel: int = 5,\n    conv_stride: int = 1,\n    conv_batchnorm: bool = True,\n    conv_activation: str = \"ReLU\",\n    conv_dropout: float or List[float] = None,\n    lnr_dims: int or List[int] = -1,\n    lnr_activation: str = None,\n    lnr_dropout: int or List[int] = None,\n    zero_centered: bool = False,\n):\n    \"\"\"\n\n    Args:\n        feat_dim: int\n            The dimension of input acoustic feature tensors.\n            Used for calculating the in_features of the first Linear layer.\n        conv_dims: List[int] or int\n            The values of out_channels of each Conv1d layer.\n            If a list of integers is given, multiple Conv1d layers will be initialized.\n            If an integer is given, there will be only one Conv1d layer\n        conv_kernel: int\n            The value of kernel_size of all Conv1d layers.\n        conv_stride: int\n            The value of stride of all Conv1d layers.\n        conv_batchnorm: bool\n            Whether a BatchNorm1d layer is added right after a Conv1d layer\n        conv_activation: str\n            The type of the activation function after all Conv1d layers.\n            None means no activation function is needed.\n        conv_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        lnr_dims: int or List[int]\n            The values of out_features of each Linear layer.\n            The first value in the List represents the out_features of the first Linear layer.\n            -1: same size as the last convolutional layer's dim\n        lnr_activation: str\n            The type of the activation function after all Linear layers.\n            None means no activation function is needed.\n        lnr_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        zero_centered: bool\n            Whether the output of this module is centered at 0.\n            If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n            LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n            to True.\n\n    \"\"\"\n    # --- 0. Argument Checking --- #\n    # Convolution arguments checking\n    assert isinstance(\n        conv_dims, (List, int)\n    ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_kernel, int\n    ), \"The sizes of convolutional kernels must be given as an integer!\"\n    assert isinstance(\n        conv_stride, int\n    ), \"The lengths of convolutional strides must be given as an integer!\"\n    if conv_dropout is not None:\n        assert isinstance(\n            conv_dropout, (List, float)\n        ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n    # Linear arguments checking\n    if lnr_dropout is not None:\n        assert isinstance(\n            lnr_dropout, (List, float)\n        ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n    if lnr_dims is not None:\n        assert isinstance(\n            lnr_dims, (List, int)\n        ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n    # input_size initialization\n    if self.input_size is not None:\n        feat_dim = self.input_size\n    else:\n        assert feat_dim is not None\n\n    # --- 1. Convolutional Part Initialization --- #\n    # register convolution arguments\n    self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n    self.conv_kernel = conv_kernel\n    self.conv_stride = conv_stride\n    self.conv_dropout = conv_dropout\n\n    # Conv1d blocks construction\n    _prev_dim = feat_dim\n    _tmp_conv = []\n    for i in range(len(self.conv_dims)):\n        _tmp_conv.append(\n            # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n            # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n            Conv1dEv(\n                in_channels=_prev_dim,\n                out_channels=self.conv_dims[i],\n                kernel_size=self.conv_kernel,\n                stride=self.conv_stride,\n                padding_mode=\"same\",\n                bias=not conv_batchnorm,\n            )\n        )\n        # BatchNorm is better to be placed before activation\n        # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n        if conv_batchnorm:\n            _tmp_conv.append(torch.nn.BatchNorm1d(self.conv_dims[i]))\n        if conv_activation is not None:\n            # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n            if not (i == len(self.conv_dims) - 1 and lnr_dims is None) or not (\n                zero_centered and \"ReLU\" in conv_activation\n            ):\n                _tmp_conv.append(getattr(torch.nn, conv_activation)())\n        if conv_dropout is not None:\n            _tmp_conv.append(\n                torch.nn.Dropout(\n                    p=(\n                        self.conv_dropout\n                        if not isinstance(self.conv_dropout, List)\n                        else self.conv_dropout[i]\n                    )\n                )\n            )\n        _prev_dim = conv_dims[i]\n    self.conv = torch.nn.Sequential(*_tmp_conv)\n    self.output_size = _prev_dim\n\n    # --- 2. Linear Part Initialization --- #\n    if lnr_dims is not None:\n        lnr_dims = lnr_dims if isinstance(lnr_dims, List) else [lnr_dims]\n        for i in range(len(lnr_dims)):\n            _prev_dim = self.conv_dims[-1] if i == 0 else lnr_dims[i - 1]\n            if lnr_dims[i] == -1:\n                lnr_dims[i] = _prev_dim\n\n        self.linear = LinearPrenet(\n            feat_dim=self.output_size,\n            lnr_dims=lnr_dims,\n            lnr_activation=lnr_activation,\n            lnr_dropout=lnr_dropout,\n            zero_centered=zero_centered,\n        )\n        self.output_size = self.linear.output_size\n</code></pre>"},{"location":"reference/module/prenet/conv2d/","title":"conv2d","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/prenet/conv2d/#module.prenet.conv2d.Conv2dPrenet","title":"<code>Conv2dPrenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Conv2d prenet. Usually used before the ASR encoder. This prenet is made up of two parts:     1. (mandatory) The Conv2d part contains one or more Conv2d blocks which are composed of the components below         1. (mandatory) a Conv2d layer         2. (optional) a BatchNorm2d layer         3. (optional) an activation function         4. (optional) a Dropout layer     2. (optional) The Linear part contains one or more Linear blocks which are composed of the components below         1. (mandatory) a Linear layer         2. (optional) an activation function         3. (optional) a Dropout layer.</p> Reference <p>Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition https://ieeexplore.ieee.org/abstract/document/8462506/</p> Source code in <code>speechain/module/prenet/conv2d.py</code> <pre><code>class Conv2dPrenet(Module):\n    \"\"\"\n    The Conv2d prenet. Usually used before the ASR encoder.\n    This prenet is made up of two parts:\n        1. (mandatory) The Conv2d part contains one or more Conv2d blocks which are composed of the components below\n            1. (mandatory) a Conv2d layer\n            2. (optional) a BatchNorm2d layer\n            3. (optional) an activation function\n            4. (optional) a Dropout layer\n        2. (optional) The Linear part contains one or more Linear blocks which are composed of the components below\n            1. (mandatory) a Linear layer\n            2. (optional) an activation function\n            3. (optional) a Dropout layer.\n\n    Reference:\n        Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition\n        https://ieeexplore.ieee.org/abstract/document/8462506/\n    \"\"\"\n\n    def module_init(\n        self,\n        feat_dim: int = None,\n        conv_dims: int or List[int] = [64, 64],\n        conv_kernel: int or List[int] = 3,\n        conv_stride: int or List[int] = 2,\n        conv_padding: int or List[int] = 0,\n        conv_batchnorm: bool = False,\n        conv_activation: str = \"ReLU\",\n        conv_dropout: float or List[float] = None,\n        lnr_dims: int or List[int] = 512,\n        lnr_activation: str = None,\n        lnr_dropout: float or List[float] = None,\n        zero_centered: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            feat_dim: int\n                The dimension of input acoustic feature tensors.\n                Used for calculating the in_features of the first Linear layer.\n            conv_dims: List[int] or int\n                The values of out_channels of each Conv2d layer.\n                If a list of integers is given, multiple Conv2d layers will be initialized.\n                If an integer is given, there will be only one Conv2d layer\n            conv_kernel: int or List[int]\n                The value of kernel_size of all Conv2d layers.\n                An integer means the same kernel size for time and frequency dimension.\n                List[int] is needed if you would like to make different dimensions have different kernel sizes.\n            conv_stride: int or List[int]\n                The value of stride of all Conv2d layers.\n                An integer means the same stride for time and frequency dimension.\n                List[int] is needed if you would like to make different dimensions have different strides.\n            conv_padding: int or List[int]\n                The padding added to all four sides of the input. It can be either a string {\u2018valid\u2019, \u2018same\u2019} or a\n                list of integers giving the amount of implicit padding applied on both sides.\n            conv_batchnorm: bool\n                Whether a BatchNorm2d layer is added after each Conv2d layer\n            conv_activation: str\n                The type of the activation function after all Conv2d layers.\n                None means no activation function is needed.\n            conv_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            lnr_dims: int or List[int]\n                The values of out_features of each Linear layer.\n                The first value in the List represents the out_features of the first Linear layer.\n            lnr_activation: str\n                The type of the activation function after all Linear layers. None means no activation function is needed.\n                For transformer training, it's better not to add a non-negative ReLU activation function to the last\n                linear layer because the ReLU activation will make the range of the output (&gt;= 0) different from the\n                sinusoidal positional encoding [-1, 1]. For more details, please refer to Section 3.3 of the paper below:\n                    'Neural Speech Synthesis with Transformer Network'\n                    https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n            lnr_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            zero_centered: bool\n                Whether the output of this module is centered at 0.\n                If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n                LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n                to True.\n\n        \"\"\"\n        # --- 0. Argument Checking --- #\n        # Convolution arguments checking\n        assert isinstance(\n            conv_dims, (List, int)\n        ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_kernel, (List, int)\n        ), \"The sizes of convolutional kernels must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_stride, (List, int)\n        ), \"The lengths of convolutional strides must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_padding, (List, int)\n        ), \"The lengths of convolutional paddings must be given as a list of integers, an integer, or a string!\"\n        if conv_dropout is not None:\n            assert isinstance(\n                conv_dropout, (List, float)\n            ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n        # Linear arguments checking\n        if lnr_dropout is not None:\n            assert isinstance(\n                lnr_dropout, (List, float)\n            ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n        if lnr_dims is not None:\n            assert isinstance(\n                lnr_dims, (List, int)\n            ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n        # input_size initialization\n        if self.input_size is not None:\n            feat_dim = self.input_size\n        elif feat_dim is None:\n            raise RuntimeError\n\n        # --- 1. Convolutional Part Initialization --- #\n        # register convolution arguments\n        self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n        self.conv_kernel = (\n            tuple(conv_kernel)\n            if isinstance(conv_kernel, List)\n            else (conv_kernel, conv_kernel)\n        )\n        self.conv_stride = (\n            tuple(conv_stride)\n            if isinstance(conv_stride, List)\n            else (conv_stride, conv_stride)\n        )\n        self.conv_padding = (\n            tuple(conv_padding)\n            if isinstance(conv_padding, List)\n            else (conv_padding, conv_padding)\n        )\n        self.conv_dropout = conv_dropout\n\n        self.min_height, self.min_width = 1, 1\n        for _ in self.conv_dims:\n            self.min_height = (\n                (self.min_height - 1) * self.conv_stride[0]\n                + self.conv_kernel[0]\n                - 2 * self.conv_padding[0]\n            )\n            self.min_width = (\n                (self.min_width - 1) * self.conv_stride[0]\n                + self.conv_kernel[0]\n                - 2 * self.conv_padding[0]\n            )\n\n        # Conv2d blocks construction\n        _prev_dim = 1\n        _tmp_conv = []\n        for i in range(len(self.conv_dims)):\n            _tmp_conv.append(\n                # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n                # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n                torch.nn.Conv2d(\n                    in_channels=_prev_dim,\n                    out_channels=self.conv_dims[i],\n                    kernel_size=self.conv_kernel,\n                    stride=self.conv_stride,\n                    padding=self.conv_padding,\n                    bias=not conv_batchnorm,\n                )\n            )\n            # BatchNorm is better to be placed before activation\n            # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n            if conv_batchnorm:\n                _tmp_conv.append(torch.nn.BatchNorm2d(self.conv_dims[i]))\n            if conv_activation is not None:\n                # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n                if not (i == len(self.conv_dims) - 1 and lnr_dims is None) or not (\n                    zero_centered and \"ReLU\" in conv_activation\n                ):\n                    _tmp_conv.append(getattr(torch.nn, conv_activation)())\n            if conv_dropout is not None:\n                _tmp_conv.append(\n                    torch.nn.Dropout(\n                        p=(\n                            self.conv_dropout\n                            if not isinstance(self.conv_dropout, List)\n                            else self.conv_dropout[i]\n                        )\n                    )\n                )\n            _prev_dim = self.conv_dims[i]\n        self.conv = torch.nn.Sequential(*_tmp_conv)\n\n        # feature dimension recalculation after convolutional layers\n        for _ in self.conv_dims:\n            feat_dim = (feat_dim - self.conv_kernel[-1]) // self.conv_stride[-1] + 1\n        _prev_dim *= feat_dim\n        self.output_size = _prev_dim\n\n        # --- 2. Linear Part Initialization --- #\n        if lnr_dims is not None:\n            self.linear = LinearPrenet(\n                feat_dim=self.output_size,\n                lnr_dims=lnr_dims,\n                lnr_activation=lnr_activation,\n                lnr_dropout=lnr_dropout,\n                zero_centered=zero_centered,\n            )\n            self.output_size = self.linear.output_size\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input acoustic feature tensors.\n            feat_len: (batch,)\n                The length of each acoustic feature tensor.\n\n        Returns:\n            The embedded feature vectors with their lengths.\n\n        \"\"\"\n        # check the height and width before forwarding\n        if feat.size(1) &lt; self.min_height:\n            len_diff = self.min_height - feat.size(1)\n            feat = torch.nn.functional.pad(\n                feat, (0, 0, int(len_diff / 2), len_diff - int(len_diff / 2))\n            )\n            feat_len += len_diff\n\n        if feat.size(2) &lt; self.min_width:\n            len_diff = self.min_width - feat.size(2)\n            feat = torch.nn.functional.pad(\n                feat, (int(len_diff / 2), len_diff - int(len_diff / 2))\n            )\n\n        # forward the convolutional layers\n        # (batch, feat_maxlen, feat_dim) -&gt; (batch, 1, feat_maxlen, feat_dim)\n        feat = feat.unsqueeze(1)\n        # (batch, 1, feat_maxlen, feat_dim) -&gt; (batch, conv_dim, feat_maxlen_after, feat_dim_after)\n        feat = self.conv(feat)\n        batch, channels, feat_maxlen, feat_dim = feat.size()\n        # (batch, conv_dim, feat_maxlen_after, feat_dim_after) -&gt; (batch, feat_maxlen_after, feat_dim_after \u00d7 conv_dim)\n        feat = feat.transpose(1, 2).contiguous().view(batch, feat_maxlen, -1)\n\n        # modify the feature length\n        for _ in self.conv_dims:\n            # torch version of 'feat_len = (feat_len - self.conv_kernel[0]) // self.conv_stride[0] + 1'\n            feat_len = (\n                torch.div(\n                    feat_len - self.conv_kernel[0],\n                    self.conv_stride[0],\n                    rounding_mode=\"floor\",\n                )\n                + 1\n            )\n\n        # check the input feat length\n        if max(feat_len) != feat.size(1):\n            raise RuntimeError(\n                f\"There is a bug in the {self.__class__.__name__}.\"\n                f\"The calculation of the feature lengths has something wrong.\"\n            )\n\n        # forward the linear layers\n        # (batch, feat_maxlen_after, feat_dim_after \u00d7 conv_dim) -&gt; (batch, feat_maxlen_after, lnr_dim)\n        if hasattr(self, \"linear\"):\n            feat, feat_len = self.linear(feat, feat_len)\n\n        return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/conv2d/#module.prenet.conv2d.Conv2dPrenet.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input acoustic feature tensors.</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The length of each acoustic feature tensor.</p> required <p>Returns:</p> Type Description <p>The embedded feature vectors with their lengths.</p> Source code in <code>speechain/module/prenet/conv2d.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input acoustic feature tensors.\n        feat_len: (batch,)\n            The length of each acoustic feature tensor.\n\n    Returns:\n        The embedded feature vectors with their lengths.\n\n    \"\"\"\n    # check the height and width before forwarding\n    if feat.size(1) &lt; self.min_height:\n        len_diff = self.min_height - feat.size(1)\n        feat = torch.nn.functional.pad(\n            feat, (0, 0, int(len_diff / 2), len_diff - int(len_diff / 2))\n        )\n        feat_len += len_diff\n\n    if feat.size(2) &lt; self.min_width:\n        len_diff = self.min_width - feat.size(2)\n        feat = torch.nn.functional.pad(\n            feat, (int(len_diff / 2), len_diff - int(len_diff / 2))\n        )\n\n    # forward the convolutional layers\n    # (batch, feat_maxlen, feat_dim) -&gt; (batch, 1, feat_maxlen, feat_dim)\n    feat = feat.unsqueeze(1)\n    # (batch, 1, feat_maxlen, feat_dim) -&gt; (batch, conv_dim, feat_maxlen_after, feat_dim_after)\n    feat = self.conv(feat)\n    batch, channels, feat_maxlen, feat_dim = feat.size()\n    # (batch, conv_dim, feat_maxlen_after, feat_dim_after) -&gt; (batch, feat_maxlen_after, feat_dim_after \u00d7 conv_dim)\n    feat = feat.transpose(1, 2).contiguous().view(batch, feat_maxlen, -1)\n\n    # modify the feature length\n    for _ in self.conv_dims:\n        # torch version of 'feat_len = (feat_len - self.conv_kernel[0]) // self.conv_stride[0] + 1'\n        feat_len = (\n            torch.div(\n                feat_len - self.conv_kernel[0],\n                self.conv_stride[0],\n                rounding_mode=\"floor\",\n            )\n            + 1\n        )\n\n    # check the input feat length\n    if max(feat_len) != feat.size(1):\n        raise RuntimeError(\n            f\"There is a bug in the {self.__class__.__name__}.\"\n            f\"The calculation of the feature lengths has something wrong.\"\n        )\n\n    # forward the linear layers\n    # (batch, feat_maxlen_after, feat_dim_after \u00d7 conv_dim) -&gt; (batch, feat_maxlen_after, lnr_dim)\n    if hasattr(self, \"linear\"):\n        feat, feat_len = self.linear(feat, feat_len)\n\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/conv2d/#module.prenet.conv2d.Conv2dPrenet.module_init","title":"<code>module_init(feat_dim=None, conv_dims=[64, 64], conv_kernel=3, conv_stride=2, conv_padding=0, conv_batchnorm=False, conv_activation='ReLU', conv_dropout=None, lnr_dims=512, lnr_activation=None, lnr_dropout=None, zero_centered=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat_dim</code> <code>int</code> <p>int The dimension of input acoustic feature tensors. Used for calculating the in_features of the first Linear layer.</p> <code>None</code> <code>conv_dims</code> <code>int or List[int]</code> <p>List[int] or int The values of out_channels of each Conv2d layer. If a list of integers is given, multiple Conv2d layers will be initialized. If an integer is given, there will be only one Conv2d layer</p> <code>[64, 64]</code> <code>conv_kernel</code> <code>int or List[int]</code> <p>int or List[int] The value of kernel_size of all Conv2d layers. An integer means the same kernel size for time and frequency dimension. List[int] is needed if you would like to make different dimensions have different kernel sizes.</p> <code>3</code> <code>conv_stride</code> <code>int or List[int]</code> <p>int or List[int] The value of stride of all Conv2d layers. An integer means the same stride for time and frequency dimension. List[int] is needed if you would like to make different dimensions have different strides.</p> <code>2</code> <code>conv_padding</code> <code>int or List[int]</code> <p>int or List[int] The padding added to all four sides of the input. It can be either a string {\u2018valid\u2019, \u2018same\u2019} or a list of integers giving the amount of implicit padding applied on both sides.</p> <code>0</code> <code>conv_batchnorm</code> <code>bool</code> <p>bool Whether a BatchNorm2d layer is added after each Conv2d layer</p> <code>False</code> <code>conv_activation</code> <code>str</code> <p>str The type of the activation function after all Conv2d layers. None means no activation function is needed.</p> <code>'ReLU'</code> <code>conv_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>lnr_dims</code> <code>int or List[int]</code> <p>int or List[int] The values of out_features of each Linear layer. The first value in the List represents the out_features of the first Linear layer.</p> <code>512</code> <code>lnr_activation</code> <code>str</code> <p>str The type of the activation function after all Linear layers. None means no activation function is needed. For transformer training, it's better not to add a non-negative ReLU activation function to the last linear layer because the ReLU activation will make the range of the output (&gt;= 0) different from the sinusoidal positional encoding [-1, 1]. For more details, please refer to Section 3.3 of the paper below:     'Neural Speech Synthesis with Transformer Network'     https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> <code>None</code> <code>lnr_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>zero_centered</code> <code>bool</code> <p>bool Whether the output of this module is centered at 0. If the specified activation function changes the centroid of the output distribution, e.g. ReLU and LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set to True.</p> <code>False</code> Source code in <code>speechain/module/prenet/conv2d.py</code> <pre><code>def module_init(\n    self,\n    feat_dim: int = None,\n    conv_dims: int or List[int] = [64, 64],\n    conv_kernel: int or List[int] = 3,\n    conv_stride: int or List[int] = 2,\n    conv_padding: int or List[int] = 0,\n    conv_batchnorm: bool = False,\n    conv_activation: str = \"ReLU\",\n    conv_dropout: float or List[float] = None,\n    lnr_dims: int or List[int] = 512,\n    lnr_activation: str = None,\n    lnr_dropout: float or List[float] = None,\n    zero_centered: bool = False,\n):\n    \"\"\"\n\n    Args:\n        feat_dim: int\n            The dimension of input acoustic feature tensors.\n            Used for calculating the in_features of the first Linear layer.\n        conv_dims: List[int] or int\n            The values of out_channels of each Conv2d layer.\n            If a list of integers is given, multiple Conv2d layers will be initialized.\n            If an integer is given, there will be only one Conv2d layer\n        conv_kernel: int or List[int]\n            The value of kernel_size of all Conv2d layers.\n            An integer means the same kernel size for time and frequency dimension.\n            List[int] is needed if you would like to make different dimensions have different kernel sizes.\n        conv_stride: int or List[int]\n            The value of stride of all Conv2d layers.\n            An integer means the same stride for time and frequency dimension.\n            List[int] is needed if you would like to make different dimensions have different strides.\n        conv_padding: int or List[int]\n            The padding added to all four sides of the input. It can be either a string {\u2018valid\u2019, \u2018same\u2019} or a\n            list of integers giving the amount of implicit padding applied on both sides.\n        conv_batchnorm: bool\n            Whether a BatchNorm2d layer is added after each Conv2d layer\n        conv_activation: str\n            The type of the activation function after all Conv2d layers.\n            None means no activation function is needed.\n        conv_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        lnr_dims: int or List[int]\n            The values of out_features of each Linear layer.\n            The first value in the List represents the out_features of the first Linear layer.\n        lnr_activation: str\n            The type of the activation function after all Linear layers. None means no activation function is needed.\n            For transformer training, it's better not to add a non-negative ReLU activation function to the last\n            linear layer because the ReLU activation will make the range of the output (&gt;= 0) different from the\n            sinusoidal positional encoding [-1, 1]. For more details, please refer to Section 3.3 of the paper below:\n                'Neural Speech Synthesis with Transformer Network'\n                https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n        lnr_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        zero_centered: bool\n            Whether the output of this module is centered at 0.\n            If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n            LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n            to True.\n\n    \"\"\"\n    # --- 0. Argument Checking --- #\n    # Convolution arguments checking\n    assert isinstance(\n        conv_dims, (List, int)\n    ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_kernel, (List, int)\n    ), \"The sizes of convolutional kernels must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_stride, (List, int)\n    ), \"The lengths of convolutional strides must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_padding, (List, int)\n    ), \"The lengths of convolutional paddings must be given as a list of integers, an integer, or a string!\"\n    if conv_dropout is not None:\n        assert isinstance(\n            conv_dropout, (List, float)\n        ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n    # Linear arguments checking\n    if lnr_dropout is not None:\n        assert isinstance(\n            lnr_dropout, (List, float)\n        ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n    if lnr_dims is not None:\n        assert isinstance(\n            lnr_dims, (List, int)\n        ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n    # input_size initialization\n    if self.input_size is not None:\n        feat_dim = self.input_size\n    elif feat_dim is None:\n        raise RuntimeError\n\n    # --- 1. Convolutional Part Initialization --- #\n    # register convolution arguments\n    self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n    self.conv_kernel = (\n        tuple(conv_kernel)\n        if isinstance(conv_kernel, List)\n        else (conv_kernel, conv_kernel)\n    )\n    self.conv_stride = (\n        tuple(conv_stride)\n        if isinstance(conv_stride, List)\n        else (conv_stride, conv_stride)\n    )\n    self.conv_padding = (\n        tuple(conv_padding)\n        if isinstance(conv_padding, List)\n        else (conv_padding, conv_padding)\n    )\n    self.conv_dropout = conv_dropout\n\n    self.min_height, self.min_width = 1, 1\n    for _ in self.conv_dims:\n        self.min_height = (\n            (self.min_height - 1) * self.conv_stride[0]\n            + self.conv_kernel[0]\n            - 2 * self.conv_padding[0]\n        )\n        self.min_width = (\n            (self.min_width - 1) * self.conv_stride[0]\n            + self.conv_kernel[0]\n            - 2 * self.conv_padding[0]\n        )\n\n    # Conv2d blocks construction\n    _prev_dim = 1\n    _tmp_conv = []\n    for i in range(len(self.conv_dims)):\n        _tmp_conv.append(\n            # don't include bias in the convolutional layer if it is followed by a batchnorm layer\n            # reference: https://stackoverflow.com/questions/46256747/can-not-use-both-bias-and-batch-normalization-in-convolution-layers\n            torch.nn.Conv2d(\n                in_channels=_prev_dim,\n                out_channels=self.conv_dims[i],\n                kernel_size=self.conv_kernel,\n                stride=self.conv_stride,\n                padding=self.conv_padding,\n                bias=not conv_batchnorm,\n            )\n        )\n        # BatchNorm is better to be placed before activation\n        # reference: https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout\n        if conv_batchnorm:\n            _tmp_conv.append(torch.nn.BatchNorm2d(self.conv_dims[i]))\n        if conv_activation is not None:\n            # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n            if not (i == len(self.conv_dims) - 1 and lnr_dims is None) or not (\n                zero_centered and \"ReLU\" in conv_activation\n            ):\n                _tmp_conv.append(getattr(torch.nn, conv_activation)())\n        if conv_dropout is not None:\n            _tmp_conv.append(\n                torch.nn.Dropout(\n                    p=(\n                        self.conv_dropout\n                        if not isinstance(self.conv_dropout, List)\n                        else self.conv_dropout[i]\n                    )\n                )\n            )\n        _prev_dim = self.conv_dims[i]\n    self.conv = torch.nn.Sequential(*_tmp_conv)\n\n    # feature dimension recalculation after convolutional layers\n    for _ in self.conv_dims:\n        feat_dim = (feat_dim - self.conv_kernel[-1]) // self.conv_stride[-1] + 1\n    _prev_dim *= feat_dim\n    self.output_size = _prev_dim\n\n    # --- 2. Linear Part Initialization --- #\n    if lnr_dims is not None:\n        self.linear = LinearPrenet(\n            feat_dim=self.output_size,\n            lnr_dims=lnr_dims,\n            lnr_activation=lnr_activation,\n            lnr_dropout=lnr_dropout,\n            zero_centered=zero_centered,\n        )\n        self.output_size = self.linear.output_size\n</code></pre>"},{"location":"reference/module/prenet/embed/","title":"embed","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/prenet/embed/#module.prenet.embed.EmbedPrenet","title":"<code>EmbedPrenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Create new embeddings for the vocabulary.</p> <p>Use scaling for the Transformer.</p> Source code in <code>speechain/module/prenet/embed.py</code> <pre><code>class EmbedPrenet(Module):\n    \"\"\"Create new embeddings for the vocabulary.\n\n    Use scaling for the Transformer.\n    \"\"\"\n\n    def module_init(\n        self, embedding_dim, vocab_size, scale: bool = False, padding_idx: int = 0\n    ):\n        \"\"\"\n\n        Args:\n            embedding_dim: int\n                The dimension of token embedding vectors\n            scale: bool\n                Controls whether the values of embedding vectors are scaled according to the embedding dimension.\n                Useful for the Transformer model.\n            vocab_size: int\n                The number of tokens in the dictionary.\n            padding_idx: int\n                The token index used for padding the tail areas of the short sentences.\n\n        \"\"\"\n        # output_size initialization\n        self.output_size = embedding_dim\n\n        # para recording\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.scale = scale\n        self.padding_idx = padding_idx\n\n        # initialize Embedding layer\n        self.embed = torch.nn.Embedding(\n            num_embeddings=vocab_size,\n            embedding_dim=embedding_dim,\n            padding_idx=padding_idx,\n        )\n\n    def forward(self, text: torch.Tensor):\n        \"\"\"Perform lookup for input `x` in the embedding table.\n\n        Args:\n            text: (batch, seq_len)\n            index in the vocabulary\n\n        Returns:\n            embedded representation for `x`\n        \"\"\"\n        if self.scale:\n            return self.embed(text) * math.sqrt(self.embedding_dim)\n        else:\n            return self.embed(text)\n</code></pre>"},{"location":"reference/module/prenet/embed/#module.prenet.embed.EmbedPrenet.forward","title":"<code>forward(text)</code>","text":"<p>Perform lookup for input <code>x</code> in the embedding table.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>(batch, seq_len)</p> required <p>Returns:</p> Type Description <p>embedded representation for <code>x</code></p> Source code in <code>speechain/module/prenet/embed.py</code> <pre><code>def forward(self, text: torch.Tensor):\n    \"\"\"Perform lookup for input `x` in the embedding table.\n\n    Args:\n        text: (batch, seq_len)\n        index in the vocabulary\n\n    Returns:\n        embedded representation for `x`\n    \"\"\"\n    if self.scale:\n        return self.embed(text) * math.sqrt(self.embedding_dim)\n    else:\n        return self.embed(text)\n</code></pre>"},{"location":"reference/module/prenet/embed/#module.prenet.embed.EmbedPrenet.module_init","title":"<code>module_init(embedding_dim, vocab_size, scale=False, padding_idx=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <p>int The dimension of token embedding vectors</p> required <code>scale</code> <code>bool</code> <p>bool Controls whether the values of embedding vectors are scaled according to the embedding dimension. Useful for the Transformer model.</p> <code>False</code> <code>vocab_size</code> <p>int The number of tokens in the dictionary.</p> required <code>padding_idx</code> <code>int</code> <p>int The token index used for padding the tail areas of the short sentences.</p> <code>0</code> Source code in <code>speechain/module/prenet/embed.py</code> <pre><code>def module_init(\n    self, embedding_dim, vocab_size, scale: bool = False, padding_idx: int = 0\n):\n    \"\"\"\n\n    Args:\n        embedding_dim: int\n            The dimension of token embedding vectors\n        scale: bool\n            Controls whether the values of embedding vectors are scaled according to the embedding dimension.\n            Useful for the Transformer model.\n        vocab_size: int\n            The number of tokens in the dictionary.\n        padding_idx: int\n            The token index used for padding the tail areas of the short sentences.\n\n    \"\"\"\n    # output_size initialization\n    self.output_size = embedding_dim\n\n    # para recording\n    self.embedding_dim = embedding_dim\n    self.vocab_size = vocab_size\n    self.scale = scale\n    self.padding_idx = padding_idx\n\n    # initialize Embedding layer\n    self.embed = torch.nn.Embedding(\n        num_embeddings=vocab_size,\n        embedding_dim=embedding_dim,\n        padding_idx=padding_idx,\n    )\n</code></pre>"},{"location":"reference/module/prenet/linear/","title":"linear","text":"<p>Author: Sashi Novitasari Affiliation: NAIST (-2022) Date: 2022.08</p> <p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/module/prenet/linear/#module.prenet.linear.LinearPrenet","title":"<code>LinearPrenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Linear prenet. Usually used before the Transformer TTS decoder. This prenet is made up of one or more Linear blocks which is composed of the components below:     1. (mandatory) a Linear layer     2. (optional) an activation function     3. (optional) a Dropout layer</p> Reference <p>Neural Speech Synthesis with Transformer Network https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> Source code in <code>speechain/module/prenet/linear.py</code> <pre><code>class LinearPrenet(Module):\n    \"\"\"\n    The Linear prenet. Usually used before the Transformer TTS decoder.\n    This prenet is made up of one or more Linear blocks which is composed of the components below:\n        1. (mandatory) a Linear layer\n        2. (optional) an activation function\n        3. (optional) a Dropout layer\n\n    Reference:\n        Neural Speech Synthesis with Transformer Network\n        https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n    \"\"\"\n\n    def module_init(\n        self,\n        feat_dim: int = None,\n        lnr_dims: int or List[int] = [256, 256],\n        lnr_activation: str = \"ReLU\",\n        lnr_dropout: float or List[float] = None,\n        zero_centered: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            feat_dim: int\n                The dimension of input acoustic feature tensors.\n                Used for calculating the in_features of the first Linear layer.\n            lnr_dims: int or List[int]\n                The values of out_features of each Linear layer.\n                The first value in the List represents the out_features of the first Linear layer.\n            lnr_activation: str\n                The type of the activation function after all Linear layers.\n                None means no activation function is needed.\n            lnr_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            zero_centered: bool\n                Whether the output of this module is centered at 0.\n                If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n                LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n                to True.\n\n        \"\"\"\n        # --- 0. Argument Checking --- #\n        # arguments checking\n        if lnr_dropout is not None:\n            assert isinstance(\n                lnr_dropout, (List, float)\n            ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n        assert isinstance(\n            lnr_dims, (List, int)\n        ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n        # input_size initialization\n        if self.input_size is not None:\n            feat_dim = self.input_size\n        else:\n            assert feat_dim is not None\n\n        # para recording\n        if lnr_dims is not None:\n            self.lnr_dims = lnr_dims if isinstance(lnr_dims, List) else [lnr_dims]\n        self.lnr_dropout = lnr_dropout\n\n        # --- 1. Linear Part Initialization --- #\n        # Linear layers construction\n        _tmp_lnr = []\n        _prev_dim = feat_dim\n        # The order of activation function and dropout layer is somewhat not a big deal\n        # a useful blog: https://sebastianraschka.com/faq/docs/dropout-activation.html\n        for i in range(len(self.lnr_dims)):\n            _tmp_lnr.append(\n                torch.nn.Linear(in_features=_prev_dim, out_features=self.lnr_dims[i])\n            )\n            if lnr_activation is not None:\n                # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n                if i != len(self.lnr_dims) - 1 or not (\n                    zero_centered and \"ReLU\" in lnr_activation\n                ):\n                    _tmp_lnr.append(getattr(torch.nn, lnr_activation)())\n            if lnr_dropout is not None:\n                _tmp_lnr.append(\n                    torch.nn.Dropout(\n                        p=(\n                            self.lnr_dropout\n                            if not isinstance(self.lnr_dropout, List)\n                            else self.lnr_dropout[i]\n                        )\n                    )\n                )\n            _prev_dim = self.lnr_dims[i]\n\n        self.linear = torch.nn.Sequential(*_tmp_lnr)\n        self.output_size = self.lnr_dims[-1]\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input feature tensors.\n            feat_len: (batch,)\n                The length of each feature tensor.\n                feat_len is not used in this forward function, but it's better to include this argument here for\n                compatibility with other prenet classes.\n\n        Returns: feat, feat_len\n            The embedded feature vectors with their lengths.\n\n        \"\"\"\n        feat = self.linear(feat)\n        return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/linear/#module.prenet.linear.LinearPrenet.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input feature tensors.</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The length of each feature tensor. feat_len is not used in this forward function, but it's better to include this argument here for compatibility with other prenet classes.</p> required <p>feat, feat_len</p> Type Description <p>The embedded feature vectors with their lengths.</p> Source code in <code>speechain/module/prenet/linear.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input feature tensors.\n        feat_len: (batch,)\n            The length of each feature tensor.\n            feat_len is not used in this forward function, but it's better to include this argument here for\n            compatibility with other prenet classes.\n\n    Returns: feat, feat_len\n        The embedded feature vectors with their lengths.\n\n    \"\"\"\n    feat = self.linear(feat)\n    return feat, feat_len\n</code></pre>"},{"location":"reference/module/prenet/linear/#module.prenet.linear.LinearPrenet.module_init","title":"<code>module_init(feat_dim=None, lnr_dims=[256, 256], lnr_activation='ReLU', lnr_dropout=None, zero_centered=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat_dim</code> <code>int</code> <p>int The dimension of input acoustic feature tensors. Used for calculating the in_features of the first Linear layer.</p> <code>None</code> <code>lnr_dims</code> <code>int or List[int]</code> <p>int or List[int] The values of out_features of each Linear layer. The first value in the List represents the out_features of the first Linear layer.</p> <code>[256, 256]</code> <code>lnr_activation</code> <code>str</code> <p>str The type of the activation function after all Linear layers. None means no activation function is needed.</p> <code>'ReLU'</code> <code>lnr_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>None</code> <code>zero_centered</code> <code>bool</code> <p>bool Whether the output of this module is centered at 0. If the specified activation function changes the centroid of the output distribution, e.g. ReLU and LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set to True.</p> <code>False</code> Source code in <code>speechain/module/prenet/linear.py</code> <pre><code>def module_init(\n    self,\n    feat_dim: int = None,\n    lnr_dims: int or List[int] = [256, 256],\n    lnr_activation: str = \"ReLU\",\n    lnr_dropout: float or List[float] = None,\n    zero_centered: bool = False,\n):\n    \"\"\"\n\n    Args:\n        feat_dim: int\n            The dimension of input acoustic feature tensors.\n            Used for calculating the in_features of the first Linear layer.\n        lnr_dims: int or List[int]\n            The values of out_features of each Linear layer.\n            The first value in the List represents the out_features of the first Linear layer.\n        lnr_activation: str\n            The type of the activation function after all Linear layers.\n            None means no activation function is needed.\n        lnr_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        zero_centered: bool\n            Whether the output of this module is centered at 0.\n            If the specified activation function changes the centroid of the output distribution, e.g. ReLU and\n            LeakyReLU, the activation function won't be attached to the final Linear layer if zer_centered is set\n            to True.\n\n    \"\"\"\n    # --- 0. Argument Checking --- #\n    # arguments checking\n    if lnr_dropout is not None:\n        assert isinstance(\n            lnr_dropout, (List, float)\n        ), \"The dropout rates of linear layers must be given as a list of integers or an integer!\"\n    assert isinstance(\n        lnr_dims, (List, int)\n    ), \"The dimensions of linear layers must be given as a list of integers or an integer!\"\n\n    # input_size initialization\n    if self.input_size is not None:\n        feat_dim = self.input_size\n    else:\n        assert feat_dim is not None\n\n    # para recording\n    if lnr_dims is not None:\n        self.lnr_dims = lnr_dims if isinstance(lnr_dims, List) else [lnr_dims]\n    self.lnr_dropout = lnr_dropout\n\n    # --- 1. Linear Part Initialization --- #\n    # Linear layers construction\n    _tmp_lnr = []\n    _prev_dim = feat_dim\n    # The order of activation function and dropout layer is somewhat not a big deal\n    # a useful blog: https://sebastianraschka.com/faq/docs/dropout-activation.html\n    for i in range(len(self.lnr_dims)):\n        _tmp_lnr.append(\n            torch.nn.Linear(in_features=_prev_dim, out_features=self.lnr_dims[i])\n        )\n        if lnr_activation is not None:\n            # no 'ReLU'-series activation is added for the last layer if zero_centered is specified\n            if i != len(self.lnr_dims) - 1 or not (\n                zero_centered and \"ReLU\" in lnr_activation\n            ):\n                _tmp_lnr.append(getattr(torch.nn, lnr_activation)())\n        if lnr_dropout is not None:\n            _tmp_lnr.append(\n                torch.nn.Dropout(\n                    p=(\n                        self.lnr_dropout\n                        if not isinstance(self.lnr_dropout, List)\n                        else self.lnr_dropout[i]\n                    )\n                )\n            )\n        _prev_dim = self.lnr_dims[i]\n\n    self.linear = torch.nn.Sequential(*_tmp_lnr)\n    self.output_size = self.lnr_dims[-1]\n</code></pre>"},{"location":"reference/module/prenet/spk_embed/","title":"spk_embed","text":""},{"location":"reference/module/prenet/spk_embed/#module.prenet.spk_embed.SpeakerEmbedPrenet","title":"<code>SpeakerEmbedPrenet</code>","text":"<p>               Bases: <code>Module</code></p> <p>SpeakerEmbedPrenet is a module for integrating speaker embeddings into a TTS model.</p> <p>It supports both open-set and close-set multi-speaker TTS and can combine speaker embeddings with encoder and/or decoder inputs.</p> Source code in <code>speechain/module/prenet/spk_embed.py</code> <pre><code>class SpeakerEmbedPrenet(Module):\n    \"\"\"SpeakerEmbedPrenet is a module for integrating speaker embeddings into a TTS\n    model.\n\n    It supports both open-set and close-set multi-speaker TTS and can combine speaker\n    embeddings with encoder and/or decoder inputs.\n    \"\"\"\n\n    def module_init(\n        self,\n        d_model: int,\n        spk_emb_dim_lookup: int = None,\n        spk_emb_dim_pretrained: int = None,\n        spk_num: int = None,\n        spk_emb_comb: str = \"concat\",\n        dec_comb: bool = False,\n        encdec_same_proj: bool = True,\n    ):\n        \"\"\"Initialize the SpeakerEmbedPrenet module with the given configuration.\n\n        Args:\n            d_model: int\n                input/output feature dimension for the TTS model\n            spk_emb_dim_lookup: int\n                speaker embedding dimension for close-set TTS with lookup table (optional)\n            spk_emb_dim_pretrained: int\n                speaker embedding dimension for open-set TTS with pretrained embeddings (optional)\n            spk_num: int\n                number of speakers for close-set TTS (optional)\n            spk_emb_comb: str\n                method for combining speaker embeddings with encoder/decoder inputs, either 'add' or 'concat'\n            dec_comb: bool\n                whether to combine speaker embeddings with decoder inputs\n            encdec_same_proj: bool\n                whether to use the same projection layer for encoder and decoder\n\n        Returns:\n            None\n        \"\"\"\n        self.use_lookup, self.use_pretrain = (\n            spk_emb_dim_lookup is not None,\n            spk_emb_dim_pretrained is not None,\n        )\n        assert self.use_lookup or self.use_pretrain, (\n            \"spk_emb_dim_lookup and spk_emb_dim_pretrained cannot be None at the same time! \"\n            \"Please specify the value of at least one of them.\"\n        )\n        assert spk_emb_comb in [\n            \"add\",\n            \"concat\",\n        ], f\"spk_emb_comb must be either 'add' or 'concat', but got {spk_emb_comb}.\"\n\n        # for close-set multi-speaker TTS, speaker lookup table is created for extracting embeddings from speaker IDs\n        if self.use_lookup:\n            assert spk_emb_dim_lookup is not None\n            self.spk_emb_dim_lookup = spk_emb_dim_lookup\n            self.spk_lookup = EmbedPrenet(\n                embedding_dim=self.spk_emb_dim_lookup, vocab_size=spk_num\n            )\n            # for dimension consistency with the model\n            if spk_emb_comb == \"add\" and self.spk_emb_dim_lookup != d_model:\n                self.pre_add_proj_lookup = torch.nn.Linear(\n                    self.spk_emb_dim_lookup, d_model\n                )\n\n        # initialize the linear projection layer\n        self.spk_emb_comb = spk_emb_comb\n        self.spk_emb_dim_pretrained = spk_emb_dim_pretrained\n        # for open-set multi-speaker TTS, speaker embedding is extracted by a pretrained speaker embedding model\n        if self.use_pretrain:\n            assert spk_emb_dim_pretrained is not None\n            self.spk_emb_dim_pretrained = spk_emb_dim_pretrained\n            # for dimension consistency with the model\n            if spk_emb_comb == \"add\" and self.spk_emb_dim_pretrained != d_model:\n                self.pre_add_proj_pretrain = torch.nn.Linear(\n                    self.spk_emb_dim_pretrained, d_model\n                )\n\n        # at the end of SpeakerEmbedPrenet, there is a linear projection layer shared by both open-set and close-set\n        # multi-speaker TTS models before passing the results to the TTS decoder\n        proj_in_size = d_model\n        if self.use_lookup and spk_emb_comb == \"concat\":\n            proj_in_size += self.spk_emb_dim_lookup\n        if self.use_pretrain and spk_emb_comb == \"concat\":\n            proj_in_size += self.spk_emb_dim_pretrained\n        self.final_proj_enc = torch.nn.Linear(proj_in_size, d_model)\n\n        # the projection layer can also be applied to the input of AR-TTS decoder\n        self.dec_comb = dec_comb\n        self.encdec_same_proj = encdec_same_proj\n        if self.dec_comb and not self.encdec_same_proj:\n            self.final_proj_dec = torch.nn.Linear(proj_in_size, d_model)\n\n    def forward(self, spk_ids: torch.Tensor = None, spk_feat: torch.Tensor = None):\n        \"\"\"Forward pass of the SpeakerEmbedPrenet module to obtain speaker features.\n\n        Args:\n            spk_ids: torch.Tensor\n                speaker IDs for close-set TTS (optional)\n            spk_feat: torch.Tensor\n                pretrained speaker embeddings for open-set TTS (optional)\n\n        Returns:\n            tuple: speaker features from lookup table and pretrained embeddings\n        \"\"\"\n        # 1. extract the speaker feature from the embedding look-up table\n        if self.use_lookup:\n            assert (\n                spk_ids is not None\n            ), \"For lookup-based close-set TTS, you must pass spk_ids to SpeakerEmbedPrenet.\"\n            spk_feat_lookup = self.spk_lookup(spk_ids)\n            # 1.2. project the lookup speaker feature to the same dimension with the model input\n            if hasattr(self, \"pre_add_proj_lookup\"):\n                spk_feat_lookup = self.pre_add_proj_lookup(spk_feat_lookup)\n        else:\n            spk_feat_lookup = None\n\n        # 2. project pretrained speaker feature to the same dimension with the model input\n        if self.use_pretrain:\n            assert (\n                spk_feat is not None\n            ), \"For pretrained-bsed open-set TTS, you must pass spk_feat to SpeakerEmbedPrenet.\"\n            if hasattr(self, \"pre_add_proj_pretrain\"):\n                spk_feat = self.pre_add_proj_pretrain(spk_feat)\n        else:\n            spk_feat = None\n\n        # 3. activate speaker embeddings before fusion\n        # process the lookup speaker embeddings by non-linear function softsign\n        spk_feat_lookup = (\n            torch.nn.functional.normalize(spk_feat_lookup, dim=-1)\n            if spk_feat_lookup is not None\n            else spk_feat_lookup\n        )\n        # normalize the pretrained speaker embeddings to transform it to the unit sphere\n        spk_feat = (\n            torch.nn.functional.normalize(spk_feat, dim=-1)\n            if spk_feat is not None\n            else spk_feat\n        )\n\n        return spk_feat_lookup, spk_feat\n\n    def combine_spk_feat(\n        self,\n        spk_feat: torch.Tensor,\n        spk_feat_lookup: torch.Tensor,\n        enc_output: torch.Tensor,\n        dec_input: torch.Tensor = None,\n    ):\n        \"\"\"Combine speaker features with TTS model's encoder and/or decoder inputs.\n\n        Args:\n            spk_feat: torch.Tensor\n                pretrained speaker embeddings for open-set TTS (optional)\n            spk_feat_lookup: torch.Tensor\n                speaker features from lookup table for close-set TTS (optional)\n            enc_output: torch.Tensor\n                TTS encoder output\n            dec_input: torch.Tensor\n                TTS decoder input (optional)\n\n        Returns:\n            tuple: TTS encoder output and decoder input combined with speaker features\n        \"\"\"\n        if spk_feat is not None:\n            if spk_feat.dim() == 2:\n                spk_feat = spk_feat.unsqueeze(1)\n            else:\n                assert (\n                    spk_feat.dim() == 3 and spk_feat.size(1) == 1\n                ), f\"Something wrong happens to the dimension of the input spk_feat. Its dimension is {spk_feat.shape}.\"\n\n        if spk_feat_lookup is not None:\n            if spk_feat_lookup.dim() == 2:\n                spk_feat_lookup = spk_feat_lookup.unsqueeze(1)\n            else:\n                assert spk_feat_lookup.dim() == 3 and spk_feat_lookup.size(1) == 1, (\n                    f\"Something wrong happens to the dimension of the input spk_feat_lookup. \"\n                    f\"Its dimension is {spk_feat_lookup.shape}.\"\n                )\n\n        def combine_spk_feat_to_tgt(\n            tgt_feat: torch.Tensor, proj_layer: torch.nn.Linear\n        ):\n            # directly add the speaker embedding into the target features\n            if self.spk_emb_comb == \"add\":\n                if spk_feat is not None:\n                    tgt_feat = tgt_feat + spk_feat\n                if spk_feat_lookup is not None:\n                    tgt_feat = tgt_feat + spk_feat_lookup\n            # concatenate the target features with the speaker features in the last dimension\n            elif self.spk_emb_comb == \"concat\":\n                if spk_feat is not None:\n                    tgt_feat = torch.cat(\n                        [tgt_feat, spk_feat.expand(-1, tgt_feat.size(1), -1)], dim=-1\n                    )\n                if spk_feat_lookup is not None:\n                    tgt_feat = torch.cat(\n                        [tgt_feat, spk_feat_lookup.expand(-1, tgt_feat.size(1), -1)],\n                        dim=-1,\n                    )\n            else:\n                raise RuntimeError\n            # project the concatenated vectors to the same dimension as self.d_model\n            return proj_layer(tgt_feat)\n\n        # (mandatory) combine the speaker embedding to the TTS encoder outputs\n        enc_output = combine_spk_feat_to_tgt(enc_output, self.final_proj_enc)\n        # (optional) combine the speaker embedding to the TTS decoder inputs\n        if self.dec_comb:\n            assert dec_input is not None, (\n                \"If you want to combine speaker embeddings with decoder input vectors, \"\n                \"please give the decoder input vectors by the argument 'dec_input' in combine_spk_feat()\"\n            )\n            dec_input = combine_spk_feat_to_tgt(\n                dec_input,\n                self.final_proj_enc if self.encdec_same_proj else self.final_proj_dec,\n            )\n\n        return enc_output, dec_input\n\n    def extra_repr(self) -&gt; str:\n        return f\"dec_comb={self.dec_comb}\"\n</code></pre>"},{"location":"reference/module/prenet/spk_embed/#module.prenet.spk_embed.SpeakerEmbedPrenet.combine_spk_feat","title":"<code>combine_spk_feat(spk_feat, spk_feat_lookup, enc_output, dec_input=None)</code>","text":"<p>Combine speaker features with TTS model's encoder and/or decoder inputs.</p> <p>Parameters:</p> Name Type Description Default <code>spk_feat</code> <code>Tensor</code> <p>torch.Tensor pretrained speaker embeddings for open-set TTS (optional)</p> required <code>spk_feat_lookup</code> <code>Tensor</code> <p>torch.Tensor speaker features from lookup table for close-set TTS (optional)</p> required <code>enc_output</code> <code>Tensor</code> <p>torch.Tensor TTS encoder output</p> required <code>dec_input</code> <code>Tensor</code> <p>torch.Tensor TTS decoder input (optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>TTS encoder output and decoder input combined with speaker features</p> Source code in <code>speechain/module/prenet/spk_embed.py</code> <pre><code>def combine_spk_feat(\n    self,\n    spk_feat: torch.Tensor,\n    spk_feat_lookup: torch.Tensor,\n    enc_output: torch.Tensor,\n    dec_input: torch.Tensor = None,\n):\n    \"\"\"Combine speaker features with TTS model's encoder and/or decoder inputs.\n\n    Args:\n        spk_feat: torch.Tensor\n            pretrained speaker embeddings for open-set TTS (optional)\n        spk_feat_lookup: torch.Tensor\n            speaker features from lookup table for close-set TTS (optional)\n        enc_output: torch.Tensor\n            TTS encoder output\n        dec_input: torch.Tensor\n            TTS decoder input (optional)\n\n    Returns:\n        tuple: TTS encoder output and decoder input combined with speaker features\n    \"\"\"\n    if spk_feat is not None:\n        if spk_feat.dim() == 2:\n            spk_feat = spk_feat.unsqueeze(1)\n        else:\n            assert (\n                spk_feat.dim() == 3 and spk_feat.size(1) == 1\n            ), f\"Something wrong happens to the dimension of the input spk_feat. Its dimension is {spk_feat.shape}.\"\n\n    if spk_feat_lookup is not None:\n        if spk_feat_lookup.dim() == 2:\n            spk_feat_lookup = spk_feat_lookup.unsqueeze(1)\n        else:\n            assert spk_feat_lookup.dim() == 3 and spk_feat_lookup.size(1) == 1, (\n                f\"Something wrong happens to the dimension of the input spk_feat_lookup. \"\n                f\"Its dimension is {spk_feat_lookup.shape}.\"\n            )\n\n    def combine_spk_feat_to_tgt(\n        tgt_feat: torch.Tensor, proj_layer: torch.nn.Linear\n    ):\n        # directly add the speaker embedding into the target features\n        if self.spk_emb_comb == \"add\":\n            if spk_feat is not None:\n                tgt_feat = tgt_feat + spk_feat\n            if spk_feat_lookup is not None:\n                tgt_feat = tgt_feat + spk_feat_lookup\n        # concatenate the target features with the speaker features in the last dimension\n        elif self.spk_emb_comb == \"concat\":\n            if spk_feat is not None:\n                tgt_feat = torch.cat(\n                    [tgt_feat, spk_feat.expand(-1, tgt_feat.size(1), -1)], dim=-1\n                )\n            if spk_feat_lookup is not None:\n                tgt_feat = torch.cat(\n                    [tgt_feat, spk_feat_lookup.expand(-1, tgt_feat.size(1), -1)],\n                    dim=-1,\n                )\n        else:\n            raise RuntimeError\n        # project the concatenated vectors to the same dimension as self.d_model\n        return proj_layer(tgt_feat)\n\n    # (mandatory) combine the speaker embedding to the TTS encoder outputs\n    enc_output = combine_spk_feat_to_tgt(enc_output, self.final_proj_enc)\n    # (optional) combine the speaker embedding to the TTS decoder inputs\n    if self.dec_comb:\n        assert dec_input is not None, (\n            \"If you want to combine speaker embeddings with decoder input vectors, \"\n            \"please give the decoder input vectors by the argument 'dec_input' in combine_spk_feat()\"\n        )\n        dec_input = combine_spk_feat_to_tgt(\n            dec_input,\n            self.final_proj_enc if self.encdec_same_proj else self.final_proj_dec,\n        )\n\n    return enc_output, dec_input\n</code></pre>"},{"location":"reference/module/prenet/spk_embed/#module.prenet.spk_embed.SpeakerEmbedPrenet.forward","title":"<code>forward(spk_ids=None, spk_feat=None)</code>","text":"<p>Forward pass of the SpeakerEmbedPrenet module to obtain speaker features.</p> <p>Parameters:</p> Name Type Description Default <code>spk_ids</code> <code>Tensor</code> <p>torch.Tensor speaker IDs for close-set TTS (optional)</p> <code>None</code> <code>spk_feat</code> <code>Tensor</code> <p>torch.Tensor pretrained speaker embeddings for open-set TTS (optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>speaker features from lookup table and pretrained embeddings</p> Source code in <code>speechain/module/prenet/spk_embed.py</code> <pre><code>def forward(self, spk_ids: torch.Tensor = None, spk_feat: torch.Tensor = None):\n    \"\"\"Forward pass of the SpeakerEmbedPrenet module to obtain speaker features.\n\n    Args:\n        spk_ids: torch.Tensor\n            speaker IDs for close-set TTS (optional)\n        spk_feat: torch.Tensor\n            pretrained speaker embeddings for open-set TTS (optional)\n\n    Returns:\n        tuple: speaker features from lookup table and pretrained embeddings\n    \"\"\"\n    # 1. extract the speaker feature from the embedding look-up table\n    if self.use_lookup:\n        assert (\n            spk_ids is not None\n        ), \"For lookup-based close-set TTS, you must pass spk_ids to SpeakerEmbedPrenet.\"\n        spk_feat_lookup = self.spk_lookup(spk_ids)\n        # 1.2. project the lookup speaker feature to the same dimension with the model input\n        if hasattr(self, \"pre_add_proj_lookup\"):\n            spk_feat_lookup = self.pre_add_proj_lookup(spk_feat_lookup)\n    else:\n        spk_feat_lookup = None\n\n    # 2. project pretrained speaker feature to the same dimension with the model input\n    if self.use_pretrain:\n        assert (\n            spk_feat is not None\n        ), \"For pretrained-bsed open-set TTS, you must pass spk_feat to SpeakerEmbedPrenet.\"\n        if hasattr(self, \"pre_add_proj_pretrain\"):\n            spk_feat = self.pre_add_proj_pretrain(spk_feat)\n    else:\n        spk_feat = None\n\n    # 3. activate speaker embeddings before fusion\n    # process the lookup speaker embeddings by non-linear function softsign\n    spk_feat_lookup = (\n        torch.nn.functional.normalize(spk_feat_lookup, dim=-1)\n        if spk_feat_lookup is not None\n        else spk_feat_lookup\n    )\n    # normalize the pretrained speaker embeddings to transform it to the unit sphere\n    spk_feat = (\n        torch.nn.functional.normalize(spk_feat, dim=-1)\n        if spk_feat is not None\n        else spk_feat\n    )\n\n    return spk_feat_lookup, spk_feat\n</code></pre>"},{"location":"reference/module/prenet/spk_embed/#module.prenet.spk_embed.SpeakerEmbedPrenet.module_init","title":"<code>module_init(d_model, spk_emb_dim_lookup=None, spk_emb_dim_pretrained=None, spk_num=None, spk_emb_comb='concat', dec_comb=False, encdec_same_proj=True)</code>","text":"<p>Initialize the SpeakerEmbedPrenet module with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int input/output feature dimension for the TTS model</p> required <code>spk_emb_dim_lookup</code> <code>int</code> <p>int speaker embedding dimension for close-set TTS with lookup table (optional)</p> <code>None</code> <code>spk_emb_dim_pretrained</code> <code>int</code> <p>int speaker embedding dimension for open-set TTS with pretrained embeddings (optional)</p> <code>None</code> <code>spk_num</code> <code>int</code> <p>int number of speakers for close-set TTS (optional)</p> <code>None</code> <code>spk_emb_comb</code> <code>str</code> <p>str method for combining speaker embeddings with encoder/decoder inputs, either 'add' or 'concat'</p> <code>'concat'</code> <code>dec_comb</code> <code>bool</code> <p>bool whether to combine speaker embeddings with decoder inputs</p> <code>False</code> <code>encdec_same_proj</code> <code>bool</code> <p>bool whether to use the same projection layer for encoder and decoder</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>speechain/module/prenet/spk_embed.py</code> <pre><code>def module_init(\n    self,\n    d_model: int,\n    spk_emb_dim_lookup: int = None,\n    spk_emb_dim_pretrained: int = None,\n    spk_num: int = None,\n    spk_emb_comb: str = \"concat\",\n    dec_comb: bool = False,\n    encdec_same_proj: bool = True,\n):\n    \"\"\"Initialize the SpeakerEmbedPrenet module with the given configuration.\n\n    Args:\n        d_model: int\n            input/output feature dimension for the TTS model\n        spk_emb_dim_lookup: int\n            speaker embedding dimension for close-set TTS with lookup table (optional)\n        spk_emb_dim_pretrained: int\n            speaker embedding dimension for open-set TTS with pretrained embeddings (optional)\n        spk_num: int\n            number of speakers for close-set TTS (optional)\n        spk_emb_comb: str\n            method for combining speaker embeddings with encoder/decoder inputs, either 'add' or 'concat'\n        dec_comb: bool\n            whether to combine speaker embeddings with decoder inputs\n        encdec_same_proj: bool\n            whether to use the same projection layer for encoder and decoder\n\n    Returns:\n        None\n    \"\"\"\n    self.use_lookup, self.use_pretrain = (\n        spk_emb_dim_lookup is not None,\n        spk_emb_dim_pretrained is not None,\n    )\n    assert self.use_lookup or self.use_pretrain, (\n        \"spk_emb_dim_lookup and spk_emb_dim_pretrained cannot be None at the same time! \"\n        \"Please specify the value of at least one of them.\"\n    )\n    assert spk_emb_comb in [\n        \"add\",\n        \"concat\",\n    ], f\"spk_emb_comb must be either 'add' or 'concat', but got {spk_emb_comb}.\"\n\n    # for close-set multi-speaker TTS, speaker lookup table is created for extracting embeddings from speaker IDs\n    if self.use_lookup:\n        assert spk_emb_dim_lookup is not None\n        self.spk_emb_dim_lookup = spk_emb_dim_lookup\n        self.spk_lookup = EmbedPrenet(\n            embedding_dim=self.spk_emb_dim_lookup, vocab_size=spk_num\n        )\n        # for dimension consistency with the model\n        if spk_emb_comb == \"add\" and self.spk_emb_dim_lookup != d_model:\n            self.pre_add_proj_lookup = torch.nn.Linear(\n                self.spk_emb_dim_lookup, d_model\n            )\n\n    # initialize the linear projection layer\n    self.spk_emb_comb = spk_emb_comb\n    self.spk_emb_dim_pretrained = spk_emb_dim_pretrained\n    # for open-set multi-speaker TTS, speaker embedding is extracted by a pretrained speaker embedding model\n    if self.use_pretrain:\n        assert spk_emb_dim_pretrained is not None\n        self.spk_emb_dim_pretrained = spk_emb_dim_pretrained\n        # for dimension consistency with the model\n        if spk_emb_comb == \"add\" and self.spk_emb_dim_pretrained != d_model:\n            self.pre_add_proj_pretrain = torch.nn.Linear(\n                self.spk_emb_dim_pretrained, d_model\n            )\n\n    # at the end of SpeakerEmbedPrenet, there is a linear projection layer shared by both open-set and close-set\n    # multi-speaker TTS models before passing the results to the TTS decoder\n    proj_in_size = d_model\n    if self.use_lookup and spk_emb_comb == \"concat\":\n        proj_in_size += self.spk_emb_dim_lookup\n    if self.use_pretrain and spk_emb_comb == \"concat\":\n        proj_in_size += self.spk_emb_dim_pretrained\n    self.final_proj_enc = torch.nn.Linear(proj_in_size, d_model)\n\n    # the projection layer can also be applied to the input of AR-TTS decoder\n    self.dec_comb = dec_comb\n    self.encdec_same_proj = encdec_same_proj\n    if self.dec_comb and not self.encdec_same_proj:\n        self.final_proj_dec = torch.nn.Linear(proj_in_size, d_model)\n</code></pre>"},{"location":"reference/module/prenet/var_pred/","title":"var_pred","text":""},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.Conv1dVarPredictor","title":"<code>Conv1dVarPredictor</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Conv1d variance predictor for FastSpeech2. This module is made up of:     1. (mandatory) The Conv1d part contains two or more Conv1d blocks which are composed of the components below         1. (mandatory) a Conv1d layer         2. (mandatory) a ReLU function         3. (mandatory) a LayerNorm layer         4. (mandatory) a Dropout layer.     2. (mandatory) The Linear part contains one Linear block which is composed of the component below         1. (mandatory) a Linear layer</p> Reference <p>Fastspeech 2: Fast and high-quality end-to-end text to speech https://arxiv.org/pdf/2006.04558</p> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>class Conv1dVarPredictor(Module):\n    \"\"\"\n    The Conv1d variance predictor for FastSpeech2.\n    This module is made up of:\n        1. (mandatory) The Conv1d part contains two or more Conv1d blocks which are composed of the components below\n            1. (mandatory) a Conv1d layer\n            2. (mandatory) a ReLU function\n            3. (mandatory) a LayerNorm layer\n            4. (mandatory) a Dropout layer.\n        2. (mandatory) The Linear part contains one Linear block which is composed of the component below\n            1. (mandatory) a Linear layer\n\n    Reference:\n        Fastspeech 2: Fast and high-quality end-to-end text to speech\n        https://arxiv.org/pdf/2006.04558\n    \"\"\"\n\n    def module_init(\n        self,\n        feat_dim: int = None,\n        conv_dims: int or List[int] = [256, 256],\n        conv_kernel: int = 3,\n        conv_stride: int = 1,\n        use_gate: bool = False,\n        conv_dropout: float or List[float] = 0.5,\n        use_conv_emb: bool = True,\n        conv_emb_kernel: int = 1,\n        conv_emb_dropout: float = 0.0,\n    ):\n        \"\"\"\n\n        Args:\n            feat_dim: int\n                The dimension of input acoustic feature tensors.\n                Used for calculating the in_features of the first Linear layer.\n            conv_dims: List[int] or int\n                The values of out_channels of each Conv1d layer.\n                If a list of integers is given, multiple Conv1d layers will be initialized.\n                If an integer is given, there will be only one Conv1d layer\n            conv_kernel: int\n                The value of kernel_size of all Conv1d layers.\n            conv_stride: int\n                The value of stride of all Conv1d layers.\n            conv_dropout: float or List[float]\n                The values of p rate of the Dropout layer after each Linear layer.\n            use_conv_emb: bool\n                Whether to embed the predicted scalar back to an embedding vector.\n                This argument needs to be False for duration predictor.\n            conv_emb_kernel: int\n                The value of kernel_size for the conv1d embedding layer.\n                Only effective when use_conv_emb is True.\n            conv_emb_dropout: float\n                The value of p reate of the Dropout layer after the conv1d embedding layer.\n                Only effective when use_conv_emb is True.\n        \"\"\"\n        # --- 0. Argument Checking --- #\n        # Convolution arguments checking\n        assert isinstance(\n            conv_dims, (List, int)\n        ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n        assert isinstance(\n            conv_kernel, int\n        ), \"The sizes of convolutional kernels must be given as an integer!\"\n        assert isinstance(\n            conv_stride, int\n        ), \"The lengths of convolutional strides must be given as an integer!\"\n        if conv_dropout is not None:\n            assert isinstance(\n                conv_dropout, (List, float)\n            ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n        # input_size initialization\n        if self.input_size is not None:\n            feat_dim = self.input_size\n        else:\n            assert feat_dim is not None\n            self.input_size = feat_dim\n\n        # --- 1. Convolutional Part Initialization --- #\n        # register convolution arguments\n        self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n        self.conv_kernel = conv_kernel\n        self.conv_stride = conv_stride\n        self.conv_dropout = conv_dropout\n\n        # Conv1d blocks construction\n        _prev_dim = feat_dim\n        _tmp_conv = []\n        for i in range(len(self.conv_dims)):\n            # 0 means go back to the input feat_dim\n            if self.conv_dims[i] == 0:\n                self.conv_dims[i] = feat_dim\n            # -1 means equal to the previous layer\n            elif self.conv_dims[i] == -1:\n                self.conv_dims[i] = self.conv_dims[i - 1]\n            # Conv1d layer\n            _tmp_conv.append(\n                Conv1dEv(\n                    in_channels=_prev_dim,\n                    out_channels=self.conv_dims[i],\n                    kernel_size=self.conv_kernel,\n                    stride=self.conv_stride,\n                    padding_mode=\"same\",\n                )\n            )\n            # ReLU function\n            _tmp_conv.append(torch.nn.ReLU())\n            # LayerNorm layer\n            _tmp_conv.append(LayerNorm(self.conv_dims[i], dim=1))\n            # Dropout layer\n            if conv_dropout is not None:\n                _tmp_conv.append(\n                    torch.nn.Dropout(\n                        p=(\n                            self.conv_dropout\n                            if not isinstance(self.conv_dropout, List)\n                            else self.conv_dropout[i]\n                        )\n                    )\n                )\n            _prev_dim = conv_dims[i]\n        self.conv = torch.nn.Sequential(*_tmp_conv)\n\n        # --- 2. Linear Part Initialization --- #\n        self.linear = torch.nn.Linear(_prev_dim, 1)\n        if use_gate:\n            self.gate_linear = torch.nn.Linear(_prev_dim, 1)\n\n        # --- 3. Scalar Embedding Part Initialization --- #\n        if use_conv_emb:\n            _tmp_conv_emb = [\n                Conv1dEv(\n                    in_channels=1,\n                    out_channels=self.input_size,\n                    kernel_size=conv_emb_kernel,\n                    padding_mode=\"same\",\n                )\n            ]\n            if conv_emb_dropout &gt; 0:\n                _tmp_conv_emb.append(torch.nn.Dropout(p=conv_emb_dropout))\n            self.conv_emb = torch.nn.Sequential(*_tmp_conv_emb)\n        self.output_size = self.input_size\n\n    def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            feat: (batch, feat_maxlen, feat_dim)\n                The input feature tensors.\n            feat_len: (batch,)\n                The length of each feature tensor.\n\n        Returns: feat, feat_len\n            The embedded feature vectors with their lengths.\n\n        \"\"\"\n        # forward the convolutional layers\n        # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n        feat = feat.transpose(1, 2)\n        # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n        feat = self.conv(feat)\n        # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n        feat = feat.transpose(1, 2)\n\n        # forward the linear layer\n        # (batch, feat_maxlen, conv_dim) -&gt; (batch, feat_maxlen, 1) -&gt; (batch, feat_maxlen)\n        feat_pred = self.linear(feat).squeeze(-1)\n\n        # return feat_len for the compatibility with other prenets\n        if not hasattr(self, \"gate_linear\"):\n            return feat_pred, feat_len\n        else:\n            feat_gate = self.gate_linear(feat).squeeze(-1)\n            return feat_pred, feat_len, feat_gate\n\n    def emb_pred_scalar(self, pred_scalar: torch.Tensor):\n        \"\"\"\n\n        Args:\n            pred_scalar: (batch, feat_maxlen, 1) or (batch, feat_maxlen)\n                The predicted scalar vectors calculated in the forward().\n\n        \"\"\"\n        assert hasattr(\n            self, \"conv_emb\"\n        ), \"Please set the argument 'use_conv_emb' to True if you want to embed the predicted scalar!\"\n        if len(pred_scalar.shape) == 2:\n            pred_scalar = pred_scalar.unsqueeze(-1)\n        else:\n            assert len(pred_scalar.shape) == 3 and pred_scalar.size(-1) == 1\n\n        return self.conv_emb(pred_scalar.transpose(1, 2)).transpose(1, 2)\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.Conv1dVarPredictor.emb_pred_scalar","title":"<code>emb_pred_scalar(pred_scalar)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pred_scalar</code> <code>Tensor</code> <p>(batch, feat_maxlen, 1) or (batch, feat_maxlen) The predicted scalar vectors calculated in the forward().</p> required Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>def emb_pred_scalar(self, pred_scalar: torch.Tensor):\n    \"\"\"\n\n    Args:\n        pred_scalar: (batch, feat_maxlen, 1) or (batch, feat_maxlen)\n            The predicted scalar vectors calculated in the forward().\n\n    \"\"\"\n    assert hasattr(\n        self, \"conv_emb\"\n    ), \"Please set the argument 'use_conv_emb' to True if you want to embed the predicted scalar!\"\n    if len(pred_scalar.shape) == 2:\n        pred_scalar = pred_scalar.unsqueeze(-1)\n    else:\n        assert len(pred_scalar.shape) == 3 and pred_scalar.size(-1) == 1\n\n    return self.conv_emb(pred_scalar.transpose(1, 2)).transpose(1, 2)\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.Conv1dVarPredictor.forward","title":"<code>forward(feat, feat_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>(batch, feat_maxlen, feat_dim) The input feature tensors.</p> required <code>feat_len</code> <code>Tensor</code> <p>(batch,) The length of each feature tensor.</p> required <p>feat, feat_len</p> Type Description <p>The embedded feature vectors with their lengths.</p> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>def forward(self, feat: torch.Tensor, feat_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        feat: (batch, feat_maxlen, feat_dim)\n            The input feature tensors.\n        feat_len: (batch,)\n            The length of each feature tensor.\n\n    Returns: feat, feat_len\n        The embedded feature vectors with their lengths.\n\n    \"\"\"\n    # forward the convolutional layers\n    # (batch, feat_maxlen, feat_dim) -&gt; (batch, feat_dim, feat_maxlen)\n    feat = feat.transpose(1, 2)\n    # (batch, feat_dim, feat_maxlen) -&gt; (batch, conv_dim, feat_maxlen)\n    feat = self.conv(feat)\n    # (batch, conv_dim, feat_maxlen) -&gt; (batch, feat_maxlen, conv_dim)\n    feat = feat.transpose(1, 2)\n\n    # forward the linear layer\n    # (batch, feat_maxlen, conv_dim) -&gt; (batch, feat_maxlen, 1) -&gt; (batch, feat_maxlen)\n    feat_pred = self.linear(feat).squeeze(-1)\n\n    # return feat_len for the compatibility with other prenets\n    if not hasattr(self, \"gate_linear\"):\n        return feat_pred, feat_len\n    else:\n        feat_gate = self.gate_linear(feat).squeeze(-1)\n        return feat_pred, feat_len, feat_gate\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.Conv1dVarPredictor.module_init","title":"<code>module_init(feat_dim=None, conv_dims=[256, 256], conv_kernel=3, conv_stride=1, use_gate=False, conv_dropout=0.5, use_conv_emb=True, conv_emb_kernel=1, conv_emb_dropout=0.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feat_dim</code> <code>int</code> <p>int The dimension of input acoustic feature tensors. Used for calculating the in_features of the first Linear layer.</p> <code>None</code> <code>conv_dims</code> <code>int or List[int]</code> <p>List[int] or int The values of out_channels of each Conv1d layer. If a list of integers is given, multiple Conv1d layers will be initialized. If an integer is given, there will be only one Conv1d layer</p> <code>[256, 256]</code> <code>conv_kernel</code> <code>int</code> <p>int The value of kernel_size of all Conv1d layers.</p> <code>3</code> <code>conv_stride</code> <code>int</code> <p>int The value of stride of all Conv1d layers.</p> <code>1</code> <code>conv_dropout</code> <code>float or List[float]</code> <p>float or List[float] The values of p rate of the Dropout layer after each Linear layer.</p> <code>0.5</code> <code>use_conv_emb</code> <code>bool</code> <p>bool Whether to embed the predicted scalar back to an embedding vector. This argument needs to be False for duration predictor.</p> <code>True</code> <code>conv_emb_kernel</code> <code>int</code> <p>int The value of kernel_size for the conv1d embedding layer. Only effective when use_conv_emb is True.</p> <code>1</code> <code>conv_emb_dropout</code> <code>float</code> <p>float The value of p reate of the Dropout layer after the conv1d embedding layer. Only effective when use_conv_emb is True.</p> <code>0.0</code> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>def module_init(\n    self,\n    feat_dim: int = None,\n    conv_dims: int or List[int] = [256, 256],\n    conv_kernel: int = 3,\n    conv_stride: int = 1,\n    use_gate: bool = False,\n    conv_dropout: float or List[float] = 0.5,\n    use_conv_emb: bool = True,\n    conv_emb_kernel: int = 1,\n    conv_emb_dropout: float = 0.0,\n):\n    \"\"\"\n\n    Args:\n        feat_dim: int\n            The dimension of input acoustic feature tensors.\n            Used for calculating the in_features of the first Linear layer.\n        conv_dims: List[int] or int\n            The values of out_channels of each Conv1d layer.\n            If a list of integers is given, multiple Conv1d layers will be initialized.\n            If an integer is given, there will be only one Conv1d layer\n        conv_kernel: int\n            The value of kernel_size of all Conv1d layers.\n        conv_stride: int\n            The value of stride of all Conv1d layers.\n        conv_dropout: float or List[float]\n            The values of p rate of the Dropout layer after each Linear layer.\n        use_conv_emb: bool\n            Whether to embed the predicted scalar back to an embedding vector.\n            This argument needs to be False for duration predictor.\n        conv_emb_kernel: int\n            The value of kernel_size for the conv1d embedding layer.\n            Only effective when use_conv_emb is True.\n        conv_emb_dropout: float\n            The value of p reate of the Dropout layer after the conv1d embedding layer.\n            Only effective when use_conv_emb is True.\n    \"\"\"\n    # --- 0. Argument Checking --- #\n    # Convolution arguments checking\n    assert isinstance(\n        conv_dims, (List, int)\n    ), \"The dimensions of convolutional layers must be given as a list of integers or an integer!\"\n    assert isinstance(\n        conv_kernel, int\n    ), \"The sizes of convolutional kernels must be given as an integer!\"\n    assert isinstance(\n        conv_stride, int\n    ), \"The lengths of convolutional strides must be given as an integer!\"\n    if conv_dropout is not None:\n        assert isinstance(\n            conv_dropout, (List, float)\n        ), \"The dropout rates of convolutional layers must be given as a list of integers or an integer!\"\n\n    # input_size initialization\n    if self.input_size is not None:\n        feat_dim = self.input_size\n    else:\n        assert feat_dim is not None\n        self.input_size = feat_dim\n\n    # --- 1. Convolutional Part Initialization --- #\n    # register convolution arguments\n    self.conv_dims = conv_dims if isinstance(conv_dims, List) else [conv_dims]\n    self.conv_kernel = conv_kernel\n    self.conv_stride = conv_stride\n    self.conv_dropout = conv_dropout\n\n    # Conv1d blocks construction\n    _prev_dim = feat_dim\n    _tmp_conv = []\n    for i in range(len(self.conv_dims)):\n        # 0 means go back to the input feat_dim\n        if self.conv_dims[i] == 0:\n            self.conv_dims[i] = feat_dim\n        # -1 means equal to the previous layer\n        elif self.conv_dims[i] == -1:\n            self.conv_dims[i] = self.conv_dims[i - 1]\n        # Conv1d layer\n        _tmp_conv.append(\n            Conv1dEv(\n                in_channels=_prev_dim,\n                out_channels=self.conv_dims[i],\n                kernel_size=self.conv_kernel,\n                stride=self.conv_stride,\n                padding_mode=\"same\",\n            )\n        )\n        # ReLU function\n        _tmp_conv.append(torch.nn.ReLU())\n        # LayerNorm layer\n        _tmp_conv.append(LayerNorm(self.conv_dims[i], dim=1))\n        # Dropout layer\n        if conv_dropout is not None:\n            _tmp_conv.append(\n                torch.nn.Dropout(\n                    p=(\n                        self.conv_dropout\n                        if not isinstance(self.conv_dropout, List)\n                        else self.conv_dropout[i]\n                    )\n                )\n            )\n        _prev_dim = conv_dims[i]\n    self.conv = torch.nn.Sequential(*_tmp_conv)\n\n    # --- 2. Linear Part Initialization --- #\n    self.linear = torch.nn.Linear(_prev_dim, 1)\n    if use_gate:\n        self.gate_linear = torch.nn.Linear(_prev_dim, 1)\n\n    # --- 3. Scalar Embedding Part Initialization --- #\n    if use_conv_emb:\n        _tmp_conv_emb = [\n            Conv1dEv(\n                in_channels=1,\n                out_channels=self.input_size,\n                kernel_size=conv_emb_kernel,\n                padding_mode=\"same\",\n            )\n        ]\n        if conv_emb_dropout &gt; 0:\n            _tmp_conv_emb.append(torch.nn.Dropout(p=conv_emb_dropout))\n        self.conv_emb = torch.nn.Sequential(*_tmp_conv_emb)\n    self.output_size = self.input_size\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.LayerNorm","title":"<code>LayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> <p>Layer normalization module. Borrowed from     https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/transformer/layer_norm.py</p> <p>Parameters:</p> Name Type Description Default <code>nout</code> <code>int</code> <p>Output dim size.</p> required <code>dim</code> <code>int</code> <p>Dimension to be normalized.</p> <code>-1</code> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>class LayerNorm(torch.nn.LayerNorm):\n    \"\"\"\n    Layer normalization module.\n    Borrowed from\n        https://github.com/espnet/espnet/blob/master/espnet/nets/pytorch_backend/transformer/layer_norm.py\n\n    Args:\n        nout (int): Output dim size.\n        dim (int): Dimension to be normalized.\n    \"\"\"\n\n    def __init__(self, nout, dim=-1):\n        \"\"\"Construct an LayerNorm object.\"\"\"\n        super(LayerNorm, self).__init__(nout, eps=1e-12)\n        self.dim = dim\n\n    def forward(self, x):\n        \"\"\"Apply layer normalization.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n        Returns:\n            torch.Tensor: Normalized tensor.\n        \"\"\"\n        if self.dim == -1:\n            return super(LayerNorm, self).forward(x)\n        return (\n            super(LayerNorm, self)\n            .forward(x.transpose(self.dim, -1))\n            .transpose(self.dim, -1)\n        )\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.LayerNorm.__init__","title":"<code>__init__(nout, dim=-1)</code>","text":"<p>Construct an LayerNorm object.</p> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>def __init__(self, nout, dim=-1):\n    \"\"\"Construct an LayerNorm object.\"\"\"\n    super(LayerNorm, self).__init__(nout, eps=1e-12)\n    self.dim = dim\n</code></pre>"},{"location":"reference/module/prenet/var_pred/#module.prenet.var_pred.LayerNorm.forward","title":"<code>forward(x)</code>","text":"<p>Apply layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:     torch.Tensor: Normalized tensor.</p> Source code in <code>speechain/module/prenet/var_pred.py</code> <pre><code>def forward(self, x):\n    \"\"\"Apply layer normalization.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n    Returns:\n        torch.Tensor: Normalized tensor.\n    \"\"\"\n    if self.dim == -1:\n        return super(LayerNorm, self).forward(x)\n    return (\n        super(LayerNorm, self)\n        .forward(x.transpose(self.dim, -1))\n        .transpose(self.dim, -1)\n    )\n</code></pre>"},{"location":"reference/module/standalone/","title":"standalone","text":""},{"location":"reference/module/standalone/lm/","title":"lm","text":""},{"location":"reference/module/standalone/lm/#module.standalone.lm.LanguageModel","title":"<code>LanguageModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Stand-Alone module of the autoregressive language model.</p> <p>This module has two usages: 1. language model training by speechain.model.lm.LM 2. ASR-LM joint decoding by speechain.model.ar_asr.ARASR</p> Source code in <code>speechain/module/standalone/lm.py</code> <pre><code>class LanguageModel(Module):\n    \"\"\"Stand-Alone module of the autoregressive language model.\n\n    This module has two usages:\n    1. language model training by speechain.model.lm.LM\n    2. ASR-LM joint decoding by speechain.model.ar_asr.ARASR\n    \"\"\"\n\n    embedding_class_dict = dict(embed=EmbedPrenet)\n\n    encoder_class_dict = dict(transformer=TransformerEncoder)\n\n    def module_init(self, vocab_size: int, emb: Dict, encoder: Dict):\n        \"\"\"\n\n        Args:\n            vocab_size:\n            emb:\n            encoder:\n\n        \"\"\"\n        # LM embedding layer\n        assert (\n            \"type\" in emb.keys()\n        ), \"There must a key named 'type' in model['module_conf']['embedding']!\"\n        embedding_class = self.embedding_class_dict[emb[\"type\"]]\n        emb[\"conf\"] = dict() if \"conf\" not in emb.keys() else emb[\"conf\"]\n        self.embedding = embedding_class(vocab_size=vocab_size, **emb[\"conf\"])\n\n        # LM encoder part\n        assert (\n            \"type\" in encoder.keys()\n        ), \"There must a key named 'type' in model['module_conf']['encoder']!\"\n        encoder_class = self.encoder_class_dict[encoder[\"type\"]]\n        encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n        # the LM encoder is automatically set to unidirectional\n        encoder[\"conf\"][\"uni_direction\"] = True\n        self.encoder = encoder_class(\n            input_size=self.embedding.output_size, **encoder[\"conf\"]\n        )\n\n        # LM token prediction layer\n        self.postnet = TokenPostnet(\n            input_size=self.encoder.output_size, vocab_size=vocab_size\n        )\n\n    def forward(self, text: torch.Tensor, text_len: torch.Tensor):\n        \"\"\"\n\n        Args:\n            text:\n            text_len:\n\n        Returns:\n\n        \"\"\"\n        # Text Embedding\n        emb_text = self.embedding(text)\n\n        # mask generation for the input text\n        text_mask = make_mask_from_len(text_len)\n        if text.is_cuda:\n            text_mask = text_mask.cuda(text.device)\n\n        # Encoding\n        enc_returns = self.encoder(src=emb_text, mask=text_mask)\n        # Transformer-based encoder additionally returns the encoder self-attention\n        if len(enc_returns) == 4:\n            enc_feat, enc_feat_mask, enc_attmat, enc_hidden = enc_returns\n        # RNN-based encoder doesn't return any attention\n        elif len(enc_returns) == 3:\n            (enc_feat, enc_feat_mask, enc_hidden), enc_attmat = enc_returns, None\n        else:\n            raise RuntimeError\n\n        # Token prediction\n        logits = self.postnet(enc_feat)\n\n        return logits, enc_feat_mask, enc_attmat\n</code></pre>"},{"location":"reference/module/standalone/lm/#module.standalone.lm.LanguageModel.forward","title":"<code>forward(text, text_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> required <code>text_len</code> <code>Tensor</code> required <p>Returns:</p> Source code in <code>speechain/module/standalone/lm.py</code> <pre><code>def forward(self, text: torch.Tensor, text_len: torch.Tensor):\n    \"\"\"\n\n    Args:\n        text:\n        text_len:\n\n    Returns:\n\n    \"\"\"\n    # Text Embedding\n    emb_text = self.embedding(text)\n\n    # mask generation for the input text\n    text_mask = make_mask_from_len(text_len)\n    if text.is_cuda:\n        text_mask = text_mask.cuda(text.device)\n\n    # Encoding\n    enc_returns = self.encoder(src=emb_text, mask=text_mask)\n    # Transformer-based encoder additionally returns the encoder self-attention\n    if len(enc_returns) == 4:\n        enc_feat, enc_feat_mask, enc_attmat, enc_hidden = enc_returns\n    # RNN-based encoder doesn't return any attention\n    elif len(enc_returns) == 3:\n        (enc_feat, enc_feat_mask, enc_hidden), enc_attmat = enc_returns, None\n    else:\n        raise RuntimeError\n\n    # Token prediction\n    logits = self.postnet(enc_feat)\n\n    return logits, enc_feat_mask, enc_attmat\n</code></pre>"},{"location":"reference/module/standalone/lm/#module.standalone.lm.LanguageModel.module_init","title":"<code>module_init(vocab_size, emb, encoder)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> required <code>emb</code> <code>Dict</code> required <code>encoder</code> <code>Dict</code> required Source code in <code>speechain/module/standalone/lm.py</code> <pre><code>def module_init(self, vocab_size: int, emb: Dict, encoder: Dict):\n    \"\"\"\n\n    Args:\n        vocab_size:\n        emb:\n        encoder:\n\n    \"\"\"\n    # LM embedding layer\n    assert (\n        \"type\" in emb.keys()\n    ), \"There must a key named 'type' in model['module_conf']['embedding']!\"\n    embedding_class = self.embedding_class_dict[emb[\"type\"]]\n    emb[\"conf\"] = dict() if \"conf\" not in emb.keys() else emb[\"conf\"]\n    self.embedding = embedding_class(vocab_size=vocab_size, **emb[\"conf\"])\n\n    # LM encoder part\n    assert (\n        \"type\" in encoder.keys()\n    ), \"There must a key named 'type' in model['module_conf']['encoder']!\"\n    encoder_class = self.encoder_class_dict[encoder[\"type\"]]\n    encoder[\"conf\"] = dict() if \"conf\" not in encoder.keys() else encoder[\"conf\"]\n    # the LM encoder is automatically set to unidirectional\n    encoder[\"conf\"][\"uni_direction\"] = True\n    self.encoder = encoder_class(\n        input_size=self.embedding.output_size, **encoder[\"conf\"]\n    )\n\n    # LM token prediction layer\n    self.postnet = TokenPostnet(\n        input_size=self.encoder.output_size, vocab_size=vocab_size\n    )\n</code></pre>"},{"location":"reference/module/transformer/","title":"transformer","text":""},{"location":"reference/module/transformer/attention/","title":"attention","text":"<p>Origin: Sashi Novitasari Modification: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/transformer/attention/#module.transformer.attention.MultiHeadedAttention","title":"<code>MultiHeadedAttention</code>","text":"<p>               Bases: <code>Module</code></p> A Multi-Head Attention layer has <p>\u00b7 Query linear layer \u00b7 Key linear layer \u00b7 Value linear layer \u00b7 Softmax layer \u00b7 Attention Dropout layer \u00b7 Output linear layer</p> <p>Implementation modified from OpenNMT-py. https://github.com/OpenNMT/OpenNMT-py</p> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>class MultiHeadedAttention(Module):\n    \"\"\"\n    A Multi-Head Attention layer has:\n        \u00b7 Query linear layer\n        \u00b7 Key linear layer\n        \u00b7 Value linear layer\n        \u00b7 Softmax layer\n        \u00b7 Attention Dropout layer\n        \u00b7 Output linear layer\n\n    Implementation modified from OpenNMT-py.\n    https://github.com/OpenNMT/OpenNMT-py\n    \"\"\"\n\n    def module_init(\n        self,\n        num_heads: int,\n        d_model: int,\n        dropout: float = 0.1,\n        scale_dp_by_head: bool = False,\n    ):\n        \"\"\"Create a multi-headed attention layer.\n\n        Args:\n            num_heads:\n                The number of heads\n            d_model:\n                Model size (must be divisible by num_heads)\n            dropout:\n                The dropout rate of the Dropout layer after the softmax operation\n        \"\"\"\n        assert d_model % num_heads == 0, \"d_model is not divisible by num_heads!\"\n\n        self.head_size = d_model // num_heads\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        self.k_layer = nn.Linear(d_model, num_heads * self.head_size)\n        self.v_layer = nn.Linear(d_model, num_heads * self.head_size)\n        self.q_layer = nn.Linear(d_model, num_heads * self.head_size)\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n        self.output_layer = nn.Linear(d_model, d_model)\n\n        self.scale = (\n            1 / math.sqrt(self.head_size)\n            if scale_dp_by_head\n            else 1 / math.sqrt(self.d_model)\n        )\n\n    def kvq_forward(self, k: torch.Tensor, v: torch.Tensor, q: torch.Tensor):\n\n        batch_size = k.size(0)\n\n        # project the queries (q), keys (k), and values (v)\n        k = self.k_layer(k)\n        v = self.v_layer(v)\n        q = self.q_layer(q)\n\n        # separate all heads of q, k, v\n        k = k.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        v = v.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        q = q.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n\n        return k, v, q\n\n    def attention_forward(\n        self, v: torch.Tensor, scores: torch.Tensor, mask: torch.Tensor\n    ):\n\n        # apply the mask (if we have one)\n        # we add a dimension for the heads to it below: [B, 1, 1, M]\n        if mask is not None:\n            scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n\n        # apply attention dropout and compute context vectors.\n        attention = self.softmax(scores)\n        score_soft = attention.clone()\n        attention = self.dropout(attention)\n\n        # get context vector (select values with attention) and reshape\n        # back to [B, M, D]\n        context = torch.matmul(attention, v)\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(v.size(0), -1, self.num_heads * self.head_size)\n        )\n\n        output = self.output_layer(context)\n\n        return output, score_soft\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        v: torch.Tensor,\n        q: torch.Tensor,\n        mask: torch.Tensor = None,\n    ):\n        \"\"\"Computes multi-headed attention.\n\n        Args:\n            k: keys   [B, M, D] with M being the sentence length.\n            v: values [B, M, D]\n            q: query  [B, M, D]\n            mask: optional mask [B, 1, M]\n\n        Returns:\n        \"\"\"\n\n        k, v, q = self.kvq_forward(k, v, q)\n\n        # compute scaled attention scores\n        scores = torch.matmul(q, k.transpose(2, 3)) * self.scale\n\n        return self.attention_forward(v, scores, mask)\n</code></pre>"},{"location":"reference/module/transformer/attention/#module.transformer.attention.MultiHeadedAttention.forward","title":"<code>forward(k, v, q, mask=None)</code>","text":"<p>Computes multi-headed attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>keys   [B, M, D] with M being the sentence length.</p> required <code>v</code> <code>Tensor</code> <p>values [B, M, D]</p> required <code>q</code> <code>Tensor</code> <p>query  [B, M, D]</p> required <code>mask</code> <code>Tensor</code> <p>optional mask [B, 1, M]</p> <code>None</code> <p>Returns:</p> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    q: torch.Tensor,\n    mask: torch.Tensor = None,\n):\n    \"\"\"Computes multi-headed attention.\n\n    Args:\n        k: keys   [B, M, D] with M being the sentence length.\n        v: values [B, M, D]\n        q: query  [B, M, D]\n        mask: optional mask [B, 1, M]\n\n    Returns:\n    \"\"\"\n\n    k, v, q = self.kvq_forward(k, v, q)\n\n    # compute scaled attention scores\n    scores = torch.matmul(q, k.transpose(2, 3)) * self.scale\n\n    return self.attention_forward(v, scores, mask)\n</code></pre>"},{"location":"reference/module/transformer/attention/#module.transformer.attention.MultiHeadedAttention.module_init","title":"<code>module_init(num_heads, d_model, dropout=0.1, scale_dp_by_head=False)</code>","text":"<p>Create a multi-headed attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>The number of heads</p> required <code>d_model</code> <code>int</code> <p>Model size (must be divisible by num_heads)</p> required <code>dropout</code> <code>float</code> <p>The dropout rate of the Dropout layer after the softmax operation</p> <code>0.1</code> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>def module_init(\n    self,\n    num_heads: int,\n    d_model: int,\n    dropout: float = 0.1,\n    scale_dp_by_head: bool = False,\n):\n    \"\"\"Create a multi-headed attention layer.\n\n    Args:\n        num_heads:\n            The number of heads\n        d_model:\n            Model size (must be divisible by num_heads)\n        dropout:\n            The dropout rate of the Dropout layer after the softmax operation\n    \"\"\"\n    assert d_model % num_heads == 0, \"d_model is not divisible by num_heads!\"\n\n    self.head_size = d_model // num_heads\n    self.d_model = d_model\n    self.num_heads = num_heads\n\n    self.k_layer = nn.Linear(d_model, num_heads * self.head_size)\n    self.v_layer = nn.Linear(d_model, num_heads * self.head_size)\n    self.q_layer = nn.Linear(d_model, num_heads * self.head_size)\n\n    self.softmax = nn.Softmax(dim=-1)\n    self.dropout = nn.Dropout(dropout)\n    self.output_layer = nn.Linear(d_model, d_model)\n\n    self.scale = (\n        1 / math.sqrt(self.head_size)\n        if scale_dp_by_head\n        else 1 / math.sqrt(self.d_model)\n    )\n</code></pre>"},{"location":"reference/module/transformer/decoder/","title":"decoder","text":"<p>Origin: Sashi Novitasari Modification: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.Conv1dEv","title":"<code>Conv1dEv</code>","text":"<p>               Bases: <code>Module</code></p> <p>A 1D convolutional layer with support for different padding modes.</p> <p>Attributes:</p> Name Type Description <code>cutoff</code> <code>bool</code> <p>Indicates whether the output should be cut off for the 'same' padding mode.</p> <code>causal_padding</code> <code>int</code> <p>Additional padding required for the 'causal' padding mode.</p> <code>dilation</code> <code>int</code> <p>The dilation rate of the convolutional layer.</p> <code>conv_lyr</code> <code>Conv1d</code> <p>The 1D convolutional layer.</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>class Conv1dEv(torch.nn.Module):\n    \"\"\"A 1D convolutional layer with support for different padding modes.\n\n    Attributes:\n        cutoff (bool):\n            Indicates whether the output should be cut off for the 'same' padding mode.\n        causal_padding (int):\n            Additional padding required for the 'causal' padding mode.\n        dilation (int):\n            The dilation rate of the convolutional layer.\n        conv_lyr (torch.nn.Conv1d):\n            The 1D convolutional layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        dilation: int = 1,\n        padding_mode: str = \"same\",\n        bias: bool = True,\n        use_weight_norm: bool = False,\n        groups: int = 1,\n    ):\n        \"\"\"Initializes the Conv1dEv module with the specified parameters.\n\n        Args:\n            in_channels (int):\n                Number of channels in the input feature.\n            out_channels (int):\n                Number of channels produced by the convolution.\n            kernel_size (int):\n                Size of the convolutional kernel.\n            stride (int, optional):\n                Stride of the convolution. Defaults to 1.\n            dilation (int, optional):\n                The dilation rate of the kernel. Defaults to 1.\n            padding_mode (str, optional):\n                Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.\n            bias (bool, optional):\n                If True, adds a learnable bias to the output. Defaults to True.\n\n        Raises:\n            ValueError: If an unsupported padding mode is specified.\n        \"\"\"\n        super().__init__()\n\n        self.cutoff = False\n        self.causal_padding = 0\n        self.dilation = dilation\n\n        # no padding is used\n        if padding_mode == \"valid\":\n            padding = 0\n        # full padding\n        elif padding_mode == \"full\":\n            padding = dilation * (kernel_size - 1)\n        # same padding, the output is the same in dimension with input\n        elif padding_mode == \"same\":\n            assert stride == 1, \"Stride should be 1 for 'same' padding mode\"\n            if kernel_size % 2 == 0:\n                padding = dilation * kernel_size // 2\n                self.cutoff = True\n            else:\n                padding = dilation * (kernel_size - 1) // 2\n        # causal padding\n        elif padding_mode == \"causal\":\n            padding = 0\n            self.causal_padding = dilation * (kernel_size - 1)\n        else:\n            raise ValueError(\n                \"Unsupported padding mode. Supported modes are 'valid', 'full', 'same' and 'causal'.\"\n            )\n\n        self.conv_lyr = torch.nn.Conv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n        if use_weight_norm:\n            self.conv_lyr = weight_norm(self.conv_lyr)\n\n    def forward(self, feat: torch.Tensor):\n        \"\"\"Performs a forward pass through the convolutional layer.\n\n        Args:\n            feat (torch.Tensor):\n                The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).\n\n        Returns:\n            torch.Tensor:\n                The output tensor. Shape: (batch, out_channels, output_len).\n        \"\"\"\n        # attach additional paddings at the end for the 'causal' padding mode\n        if self.causal_padding &gt; 0:\n            feat = F.pad(feat, (self.causal_padding, 0))\n        output = self.conv_lyr(feat)\n        # cut off the redundant tails for the 'same' padding mode\n        if self.cutoff:\n            output = output[:, :, : -self.dilation]\n        return output\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.Conv1dEv.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, dilation=1, padding_mode='same', bias=True, use_weight_norm=False, groups=1)</code>","text":"<p>Initializes the Conv1dEv module with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input feature.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolutional kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution. Defaults to 1.</p> <code>1</code> <code>dilation</code> <code>int</code> <p>The dilation rate of the kernel. Defaults to 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.</p> <code>'same'</code> <code>bias</code> <code>bool</code> <p>If True, adds a learnable bias to the output. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported padding mode is specified.</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    dilation: int = 1,\n    padding_mode: str = \"same\",\n    bias: bool = True,\n    use_weight_norm: bool = False,\n    groups: int = 1,\n):\n    \"\"\"Initializes the Conv1dEv module with the specified parameters.\n\n    Args:\n        in_channels (int):\n            Number of channels in the input feature.\n        out_channels (int):\n            Number of channels produced by the convolution.\n        kernel_size (int):\n            Size of the convolutional kernel.\n        stride (int, optional):\n            Stride of the convolution. Defaults to 1.\n        dilation (int, optional):\n            The dilation rate of the kernel. Defaults to 1.\n        padding_mode (str, optional):\n            Padding mode. Supported values are 'valid', 'full', 'same' and 'causal'. Defaults to 'same'.\n        bias (bool, optional):\n            If True, adds a learnable bias to the output. Defaults to True.\n\n    Raises:\n        ValueError: If an unsupported padding mode is specified.\n    \"\"\"\n    super().__init__()\n\n    self.cutoff = False\n    self.causal_padding = 0\n    self.dilation = dilation\n\n    # no padding is used\n    if padding_mode == \"valid\":\n        padding = 0\n    # full padding\n    elif padding_mode == \"full\":\n        padding = dilation * (kernel_size - 1)\n    # same padding, the output is the same in dimension with input\n    elif padding_mode == \"same\":\n        assert stride == 1, \"Stride should be 1 for 'same' padding mode\"\n        if kernel_size % 2 == 0:\n            padding = dilation * kernel_size // 2\n            self.cutoff = True\n        else:\n            padding = dilation * (kernel_size - 1) // 2\n    # causal padding\n    elif padding_mode == \"causal\":\n        padding = 0\n        self.causal_padding = dilation * (kernel_size - 1)\n    else:\n        raise ValueError(\n            \"Unsupported padding mode. Supported modes are 'valid', 'full', 'same' and 'causal'.\"\n        )\n\n    self.conv_lyr = torch.nn.Conv1d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        dilation=dilation,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n    if use_weight_norm:\n        self.conv_lyr = weight_norm(self.conv_lyr)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.Conv1dEv.forward","title":"<code>forward(feat)</code>","text":"<p>Performs a forward pass through the convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor. Shape: (batch, out_channels, output_len).</p> Source code in <code>speechain/module/prenet/conv1d.py</code> <pre><code>def forward(self, feat: torch.Tensor):\n    \"\"\"Performs a forward pass through the convolutional layer.\n\n    Args:\n        feat (torch.Tensor):\n            The input feature tensor. Shape: (batch, feat_dim, feat_maxlen).\n\n    Returns:\n        torch.Tensor:\n            The output tensor. Shape: (batch, out_channels, output_len).\n    \"\"\"\n    # attach additional paddings at the end for the 'causal' padding mode\n    if self.causal_padding &gt; 0:\n        feat = F.pad(feat, (self.causal_padding, 0))\n    output = self.conv_lyr(feat)\n    # cut off the redundant tails for the 'same' padding mode\n    if self.cutoff:\n        output = output[:, :, : -self.dilation]\n    return output\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.MultiHeadedAttention","title":"<code>MultiHeadedAttention</code>","text":"<p>               Bases: <code>Module</code></p> A Multi-Head Attention layer has <p>\u00b7 Query linear layer \u00b7 Key linear layer \u00b7 Value linear layer \u00b7 Softmax layer \u00b7 Attention Dropout layer \u00b7 Output linear layer</p> <p>Implementation modified from OpenNMT-py. https://github.com/OpenNMT/OpenNMT-py</p> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>class MultiHeadedAttention(Module):\n    \"\"\"\n    A Multi-Head Attention layer has:\n        \u00b7 Query linear layer\n        \u00b7 Key linear layer\n        \u00b7 Value linear layer\n        \u00b7 Softmax layer\n        \u00b7 Attention Dropout layer\n        \u00b7 Output linear layer\n\n    Implementation modified from OpenNMT-py.\n    https://github.com/OpenNMT/OpenNMT-py\n    \"\"\"\n\n    def module_init(\n        self,\n        num_heads: int,\n        d_model: int,\n        dropout: float = 0.1,\n        scale_dp_by_head: bool = False,\n    ):\n        \"\"\"Create a multi-headed attention layer.\n\n        Args:\n            num_heads:\n                The number of heads\n            d_model:\n                Model size (must be divisible by num_heads)\n            dropout:\n                The dropout rate of the Dropout layer after the softmax operation\n        \"\"\"\n        assert d_model % num_heads == 0, \"d_model is not divisible by num_heads!\"\n\n        self.head_size = d_model // num_heads\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        self.k_layer = nn.Linear(d_model, num_heads * self.head_size)\n        self.v_layer = nn.Linear(d_model, num_heads * self.head_size)\n        self.q_layer = nn.Linear(d_model, num_heads * self.head_size)\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n        self.output_layer = nn.Linear(d_model, d_model)\n\n        self.scale = (\n            1 / math.sqrt(self.head_size)\n            if scale_dp_by_head\n            else 1 / math.sqrt(self.d_model)\n        )\n\n    def kvq_forward(self, k: torch.Tensor, v: torch.Tensor, q: torch.Tensor):\n\n        batch_size = k.size(0)\n\n        # project the queries (q), keys (k), and values (v)\n        k = self.k_layer(k)\n        v = self.v_layer(v)\n        q = self.q_layer(q)\n\n        # separate all heads of q, k, v\n        k = k.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        v = v.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        q = q.view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n\n        return k, v, q\n\n    def attention_forward(\n        self, v: torch.Tensor, scores: torch.Tensor, mask: torch.Tensor\n    ):\n\n        # apply the mask (if we have one)\n        # we add a dimension for the heads to it below: [B, 1, 1, M]\n        if mask is not None:\n            scores = scores.masked_fill(~mask.unsqueeze(1), float(\"-inf\"))\n\n        # apply attention dropout and compute context vectors.\n        attention = self.softmax(scores)\n        score_soft = attention.clone()\n        attention = self.dropout(attention)\n\n        # get context vector (select values with attention) and reshape\n        # back to [B, M, D]\n        context = torch.matmul(attention, v)\n        context = (\n            context.transpose(1, 2)\n            .contiguous()\n            .view(v.size(0), -1, self.num_heads * self.head_size)\n        )\n\n        output = self.output_layer(context)\n\n        return output, score_soft\n\n    def forward(\n        self,\n        k: torch.Tensor,\n        v: torch.Tensor,\n        q: torch.Tensor,\n        mask: torch.Tensor = None,\n    ):\n        \"\"\"Computes multi-headed attention.\n\n        Args:\n            k: keys   [B, M, D] with M being the sentence length.\n            v: values [B, M, D]\n            q: query  [B, M, D]\n            mask: optional mask [B, 1, M]\n\n        Returns:\n        \"\"\"\n\n        k, v, q = self.kvq_forward(k, v, q)\n\n        # compute scaled attention scores\n        scores = torch.matmul(q, k.transpose(2, 3)) * self.scale\n\n        return self.attention_forward(v, scores, mask)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.MultiHeadedAttention.forward","title":"<code>forward(k, v, q, mask=None)</code>","text":"<p>Computes multi-headed attention.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>Tensor</code> <p>keys   [B, M, D] with M being the sentence length.</p> required <code>v</code> <code>Tensor</code> <p>values [B, M, D]</p> required <code>q</code> <code>Tensor</code> <p>query  [B, M, D]</p> required <code>mask</code> <code>Tensor</code> <p>optional mask [B, 1, M]</p> <code>None</code> <p>Returns:</p> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>def forward(\n    self,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    q: torch.Tensor,\n    mask: torch.Tensor = None,\n):\n    \"\"\"Computes multi-headed attention.\n\n    Args:\n        k: keys   [B, M, D] with M being the sentence length.\n        v: values [B, M, D]\n        q: query  [B, M, D]\n        mask: optional mask [B, 1, M]\n\n    Returns:\n    \"\"\"\n\n    k, v, q = self.kvq_forward(k, v, q)\n\n    # compute scaled attention scores\n    scores = torch.matmul(q, k.transpose(2, 3)) * self.scale\n\n    return self.attention_forward(v, scores, mask)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.MultiHeadedAttention.module_init","title":"<code>module_init(num_heads, d_model, dropout=0.1, scale_dp_by_head=False)</code>","text":"<p>Create a multi-headed attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>The number of heads</p> required <code>d_model</code> <code>int</code> <p>Model size (must be divisible by num_heads)</p> required <code>dropout</code> <code>float</code> <p>The dropout rate of the Dropout layer after the softmax operation</p> <code>0.1</code> Source code in <code>speechain/module/transformer/attention.py</code> <pre><code>def module_init(\n    self,\n    num_heads: int,\n    d_model: int,\n    dropout: float = 0.1,\n    scale_dp_by_head: bool = False,\n):\n    \"\"\"Create a multi-headed attention layer.\n\n    Args:\n        num_heads:\n            The number of heads\n        d_model:\n            Model size (must be divisible by num_heads)\n        dropout:\n            The dropout rate of the Dropout layer after the softmax operation\n    \"\"\"\n    assert d_model % num_heads == 0, \"d_model is not divisible by num_heads!\"\n\n    self.head_size = d_model // num_heads\n    self.d_model = d_model\n    self.num_heads = num_heads\n\n    self.k_layer = nn.Linear(d_model, num_heads * self.head_size)\n    self.v_layer = nn.Linear(d_model, num_heads * self.head_size)\n    self.q_layer = nn.Linear(d_model, num_heads * self.head_size)\n\n    self.softmax = nn.Softmax(dim=-1)\n    self.dropout = nn.Dropout(dropout)\n    self.output_layer = nn.Linear(d_model, d_model)\n\n    self.scale = (\n        1 / math.sqrt(self.head_size)\n        if scale_dp_by_head\n        else 1 / math.sqrt(self.d_model)\n    )\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Pre-compute position encodings (PE).</p> <p>In forward pass, this module adds the positional encodings to the embedded feature vectors to make the Transformer aware of the positional information of the sequences.</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>class PositionalEncoding(Module):\n    \"\"\"Pre-compute position encodings (PE).\n\n    In forward pass, this module adds the positional encodings to the embedded feature\n    vectors to make the Transformer aware of the positional information of the\n    sequences.\n    \"\"\"\n\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        d_model: int = 512,\n        emb_scale: bool = False,\n        emb_layernorm: bool = False,\n        posenc_scale: bool = False,\n        init_alpha: float = 1.0,\n        max_len: int = 5000,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Positional Encoding with maximum length max_len.\n\n        Args:\n            posenc_type: str\n                The type of positional encoding (must be either 'mix' or 'sep').\n                For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions.\n                The equations are as below:\n                    PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    Reference:\n                        'Attention Is All You Need'\n                        https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n                For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half\n                of dimensions. The equations are as below:\n                    PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}\n                    Reference:\n                        'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'\n                        https://ieeexplore.ieee.org/abstract/document/8462506/\n            d_model: int\n                The dimension of the hidden feature vectors of the Transformer layers.\n            emb_scale: bool\n                Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n                encoding or not.\n                References:\n                    Section 3.4 in 'Attention Is All You Need'\n                    https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n                In most cases, we don't recommend you to turn it on especially when you don't have a large training set\n                (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it\n                only when you want to emphasize the embedded features over the positional encodings.\n            emb_layernorm: bool\n                Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n                encoding or not.\n            posenc_scale: bool\n                Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n                embedded features or not.\n                Reference:\n                    'Neural Speech Synthesis with Transformer Network'\n                    https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n            init_alpha: float\n                The initial value of the alpha used for positional encoding scaling.\n                Only effective when posenc_scale is True.\n            max_len: int\n                The maximum length of the input feature sequences.\n            dropout: float\n                The dropout rate for the Dropout layer after adding the positional encoding to the input\n        \"\"\"\n\n        assert posenc_type in [\n            \"mix\",\n            \"sep\",\n        ], f\"The type of PositionalEncoding layer must be either 'mix' or 'sep', but got type={posenc_type}!\"\n        assert (\n            d_model % 2 == 0\n        ), f\"Cannot apply sin/cos positional encoding to the vectors with odd dimensions (got d_model={d_model:d}).\"\n\n        self.posenc_type = posenc_type\n        self.d_model = d_model\n        self.emb_scale = emb_scale\n        if emb_layernorm:\n            self.emb_layernorm = torch.nn.LayerNorm(d_model)\n\n        self.init_alpha = (\n            init_alpha if isinstance(init_alpha, float) else float(init_alpha)\n        )\n        if posenc_scale:\n            self.alpha = torch.nn.Parameter(torch.tensor(self.init_alpha))\n\n        # positional encoding matrix\n        self.update_posenc(max_len)\n\n        # positional encoding Dropout layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def reset_parameters(self):\n        \"\"\"Make sure that the scalar value is not influenced by different model\n        initialization methods.\"\"\"\n        if hasattr(self, \"alpha\"):\n            self.alpha.data = torch.tensor(self.init_alpha)\n\n    def update_posenc(self, max_len: int):\n        \"\"\"\n\n        Args:\n            max_len:\n\n        \"\"\"\n\n        # positional encoding calculation\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float)\n            * (math.log(10000.0) / self.d_model)\n        )\n        posenc = torch.zeros(max_len, self.d_model)\n\n        # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n        if self.posenc_type == \"mix\":\n            posenc[:, 0::2] = torch.sin(position / div_term)\n            posenc[:, 1::2] = torch.cos(position / div_term)\n        # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n        elif self.posenc_type == \"sep\":\n            div_term_ext = torch.exp(\n                torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n                * (math.log(10000.0) / self.d_model)\n            )\n            posenc[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n            posenc[:, int(self.d_model / 2) :] = torch.cos(position / div_term_ext)\n\n        # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n        # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n        self.register_buffer(\"posenc\", posenc.unsqueeze(0))\n\n    def forward(self, emb_feat: torch.Tensor):\n        \"\"\"Embedded feature.\n\n            -&gt; LayerNorm(Embedded feature)\n                -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n                    -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n                        -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n\n        Args:\n            emb_feat: (batch_size, seq_len, d_model)\n                Embedded input feature sequences\n\n        Returns:\n            Embedded input feature sequences with positional encoding\n        \"\"\"\n        # in case that the input sequence is longer than the preset max_len\n        if emb_feat.size(1) &gt; self.posenc.size(1):\n            self.update_posenc(emb_feat.size(1), self.d_model)\n\n        # 1. (optional) normalize the embedded feature by LayerNorm\n        if hasattr(self, \"emb_layernorm\"):\n            emb_feat = self.emb_layernorm(emb_feat)\n\n        # 2. (optional) scale the embedded feature up by sqrt(d_model)\n        if self.emb_scale:\n            emb_feat *= math.sqrt(self.d_model)\n\n        # 3. (optional) scale the positional encoding vectors\n        posenc = self.posenc[:, : emb_feat.size(1)]\n        if hasattr(self, \"alpha\"):\n            # avoid posenc *= self.alpha to protect the original positional encoding\n            posenc = posenc * self.alpha\n\n        # 4. (mandatory) add positional encoding into embedded feature and apply the dropout\n        return self.dropout(emb_feat + posenc)\n\n    def get_recordable_para(self) -&gt; Dict or None:\n        if hasattr(self, \"alpha\"):\n            return dict(alpha=self.alpha)\n        else:\n            return None\n\n    def extra_repr(self) -&gt; str:\n        return f\"emb_scale={self.emb_scale}\\n\" f\"posenc_scale={hasattr(self, 'alpha')}\"\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionalEncoding.forward","title":"<code>forward(emb_feat)</code>","text":"<p>Embedded feature.</p> <pre><code>-&gt; LayerNorm(Embedded feature)\n    -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n        -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n            -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>emb_feat</code> <code>Tensor</code> <p>(batch_size, seq_len, d_model) Embedded input feature sequences</p> required <p>Returns:</p> Type Description <p>Embedded input feature sequences with positional encoding</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def forward(self, emb_feat: torch.Tensor):\n    \"\"\"Embedded feature.\n\n        -&gt; LayerNorm(Embedded feature)\n            -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n                -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n                    -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n\n    Args:\n        emb_feat: (batch_size, seq_len, d_model)\n            Embedded input feature sequences\n\n    Returns:\n        Embedded input feature sequences with positional encoding\n    \"\"\"\n    # in case that the input sequence is longer than the preset max_len\n    if emb_feat.size(1) &gt; self.posenc.size(1):\n        self.update_posenc(emb_feat.size(1), self.d_model)\n\n    # 1. (optional) normalize the embedded feature by LayerNorm\n    if hasattr(self, \"emb_layernorm\"):\n        emb_feat = self.emb_layernorm(emb_feat)\n\n    # 2. (optional) scale the embedded feature up by sqrt(d_model)\n    if self.emb_scale:\n        emb_feat *= math.sqrt(self.d_model)\n\n    # 3. (optional) scale the positional encoding vectors\n    posenc = self.posenc[:, : emb_feat.size(1)]\n    if hasattr(self, \"alpha\"):\n        # avoid posenc *= self.alpha to protect the original positional encoding\n        posenc = posenc * self.alpha\n\n    # 4. (mandatory) add positional encoding into embedded feature and apply the dropout\n    return self.dropout(emb_feat + posenc)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionalEncoding.module_init","title":"<code>module_init(posenc_type='mix', d_model=512, emb_scale=False, emb_layernorm=False, posenc_scale=False, init_alpha=1.0, max_len=5000, dropout=0.0)</code>","text":"<p>Positional Encoding with maximum length max_len.</p> <p>Parameters:</p> Name Type Description Default <code>posenc_type</code> <code>str</code> <p>str The type of positional encoding (must be either 'mix' or 'sep'). For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions. The equations are as below:     PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     Reference:         'Attention Is All You Need'         https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half of dimensions. The equations are as below:     PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}     Reference:         'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'         https://ieeexplore.ieee.org/abstract/document/8462506/</p> <code>'mix'</code> <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vectors of the Transformer layers.</p> <code>512</code> <code>emb_scale</code> <code>bool</code> <p>bool Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional encoding or not. References:     Section 3.4 in 'Attention Is All You Need'     https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf In most cases, we don't recommend you to turn it on especially when you don't have a large training set (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it only when you want to emphasize the embedded features over the positional encodings.</p> <code>False</code> <code>emb_layernorm</code> <code>bool</code> <p>bool Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional encoding or not.</p> <code>False</code> <code>posenc_scale</code> <code>bool</code> <p>bool Controls whether the positional encodings are scaled up by a trainable scalar before adding into the embedded features or not. Reference:     'Neural Speech Synthesis with Transformer Network'     https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> <code>False</code> <code>init_alpha</code> <code>float</code> <p>float The initial value of the alpha used for positional encoding scaling. Only effective when posenc_scale is True.</p> <code>1.0</code> <code>max_len</code> <code>int</code> <p>int The maximum length of the input feature sequences.</p> <code>5000</code> <code>dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after adding the positional encoding to the input</p> <code>0.0</code> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def module_init(\n    self,\n    posenc_type: str = \"mix\",\n    d_model: int = 512,\n    emb_scale: bool = False,\n    emb_layernorm: bool = False,\n    posenc_scale: bool = False,\n    init_alpha: float = 1.0,\n    max_len: int = 5000,\n    dropout: float = 0.0,\n):\n    \"\"\"Positional Encoding with maximum length max_len.\n\n    Args:\n        posenc_type: str\n            The type of positional encoding (must be either 'mix' or 'sep').\n            For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions.\n            The equations are as below:\n                PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                Reference:\n                    'Attention Is All You Need'\n                    https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n            For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half\n            of dimensions. The equations are as below:\n                PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}\n                Reference:\n                    'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'\n                    https://ieeexplore.ieee.org/abstract/document/8462506/\n        d_model: int\n            The dimension of the hidden feature vectors of the Transformer layers.\n        emb_scale: bool\n            Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n            encoding or not.\n            References:\n                Section 3.4 in 'Attention Is All You Need'\n                https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n            In most cases, we don't recommend you to turn it on especially when you don't have a large training set\n            (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it\n            only when you want to emphasize the embedded features over the positional encodings.\n        emb_layernorm: bool\n            Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n            encoding or not.\n        posenc_scale: bool\n            Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n            embedded features or not.\n            Reference:\n                'Neural Speech Synthesis with Transformer Network'\n                https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n        init_alpha: float\n            The initial value of the alpha used for positional encoding scaling.\n            Only effective when posenc_scale is True.\n        max_len: int\n            The maximum length of the input feature sequences.\n        dropout: float\n            The dropout rate for the Dropout layer after adding the positional encoding to the input\n    \"\"\"\n\n    assert posenc_type in [\n        \"mix\",\n        \"sep\",\n    ], f\"The type of PositionalEncoding layer must be either 'mix' or 'sep', but got type={posenc_type}!\"\n    assert (\n        d_model % 2 == 0\n    ), f\"Cannot apply sin/cos positional encoding to the vectors with odd dimensions (got d_model={d_model:d}).\"\n\n    self.posenc_type = posenc_type\n    self.d_model = d_model\n    self.emb_scale = emb_scale\n    if emb_layernorm:\n        self.emb_layernorm = torch.nn.LayerNorm(d_model)\n\n    self.init_alpha = (\n        init_alpha if isinstance(init_alpha, float) else float(init_alpha)\n    )\n    if posenc_scale:\n        self.alpha = torch.nn.Parameter(torch.tensor(self.init_alpha))\n\n    # positional encoding matrix\n    self.update_posenc(max_len)\n\n    # positional encoding Dropout layer\n    self.dropout = torch.nn.Dropout(p=dropout)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionalEncoding.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Make sure that the scalar value is not influenced by different model initialization methods.</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"Make sure that the scalar value is not influenced by different model\n    initialization methods.\"\"\"\n    if hasattr(self, \"alpha\"):\n        self.alpha.data = torch.tensor(self.init_alpha)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionalEncoding.update_posenc","title":"<code>update_posenc(max_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>max_len</code> <code>int</code> required Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def update_posenc(self, max_len: int):\n    \"\"\"\n\n    Args:\n        max_len:\n\n    \"\"\"\n\n    # positional encoding calculation\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, self.d_model, 2, dtype=torch.float)\n        * (math.log(10000.0) / self.d_model)\n    )\n    posenc = torch.zeros(max_len, self.d_model)\n\n    # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n    if self.posenc_type == \"mix\":\n        posenc[:, 0::2] = torch.sin(position / div_term)\n        posenc[:, 1::2] = torch.cos(position / div_term)\n    # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n    elif self.posenc_type == \"sep\":\n        div_term_ext = torch.exp(\n            torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n            * (math.log(10000.0) / self.d_model)\n        )\n        posenc[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n        posenc[:, int(self.d_model / 2) :] = torch.cos(position / div_term_ext)\n\n    # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n    # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n    self.register_buffer(\"posenc\", posenc.unsqueeze(0))\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionwiseFeedForward","title":"<code>PositionwiseFeedForward</code>","text":"<p>               Bases: <code>Module</code></p> <p>Position-wise Feed-forward layer Projects the output vectors of multi- head attention layer to fdfwd_dim and then back to d_model.</p> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>class PositionwiseFeedForward(Module):\n    \"\"\"Position-wise Feed-forward layer Projects the output vectors of multi- head\n    attention layer to fdfwd_dim and then back to d_model.\"\"\"\n\n    def module_init(\n        self,\n        d_model: int = 512,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        dropout=0.1,\n    ):\n        \"\"\"Initializes position-wise feed-forward layer.\n\n        Args:\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer\n            fdfwd_type: str\n                The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_kernel: int\n                The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n            dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer\n        \"\"\"\n        if len(fdfwd_args) == 0:\n            if fdfwd_type == \"conv\":\n                fdfwd_args = dict(kernel_size=3)\n\n        # In-layer at the beginning\n        if fdfwd_type == \"linear\":\n            self.in_layer = nn.Linear(d_model, fdfwd_dim, **fdfwd_args)\n        elif fdfwd_type == \"conv\":\n            self.in_layer = Conv1dEv(d_model, fdfwd_dim, **fdfwd_args)\n        else:\n            raise NotImplementedError(\n                f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n                f\"But got {fdfwd_type}!\"\n            )\n\n        # ReLU and DropOut layers in the middle\n        self.activation = getattr(torch.nn, fdfwd_activation)()\n        self.dropout = nn.Dropout(dropout)\n\n        # Out-layer at the end\n        if fdfwd_type == \"linear\":\n            self.out_layer = nn.Linear(fdfwd_dim, d_model, **fdfwd_args)\n        elif fdfwd_type == \"conv\":\n            self.out_layer = Conv1dEv(fdfwd_dim, d_model, **fdfwd_args)\n        else:\n            raise NotImplementedError(\n                f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n                f\"But got {fdfwd_type}!\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n\n        Args:\n            x: (batch, seq_maxlen, d_model)\n\n        Returns:\n\n        \"\"\"\n        # forward the convolutional layers\n        if isinstance(self.in_layer, Conv1dEv):\n            # (batch, seq_maxlen, d_model) -&gt; (batch, d_model, seq_maxlen)\n            x = x.transpose(1, 2)\n        # pass the in-layer at the beginning\n        # (batch, d_model, seq_maxlen) -&gt; (batch, fdfwd_dim, seq_maxlen) or\n        # (batch, seq_maxlen, d_model) -&gt; (batch, seq_maxlen, fdfwd_dim)\n        x = self.in_layer(x)\n\n        # pass the middle layers\n        x = self.dropout(self.activation(x))\n\n        # pass the out-layer at the end\n        # (batch, fdfwd_dim, seq_maxlen) -&gt; (batch, d_model, seq_maxlen) or\n        # (batch, seq_maxlen, fdfwd_dim) -&gt; (batch, seq_maxlen, d_model)\n        x = self.out_layer(x)\n        # forward the convolutional layers\n        if isinstance(self.out_layer, Conv1dEv):\n            # (batch, d_model, seq_maxlen) -&gt; (batch, seq_maxlen, d_model)\n            x = x.transpose(1, 2)\n        return x\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionwiseFeedForward.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>(batch, seq_maxlen, d_model)</p> required <p>Returns:</p> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n\n    Args:\n        x: (batch, seq_maxlen, d_model)\n\n    Returns:\n\n    \"\"\"\n    # forward the convolutional layers\n    if isinstance(self.in_layer, Conv1dEv):\n        # (batch, seq_maxlen, d_model) -&gt; (batch, d_model, seq_maxlen)\n        x = x.transpose(1, 2)\n    # pass the in-layer at the beginning\n    # (batch, d_model, seq_maxlen) -&gt; (batch, fdfwd_dim, seq_maxlen) or\n    # (batch, seq_maxlen, d_model) -&gt; (batch, seq_maxlen, fdfwd_dim)\n    x = self.in_layer(x)\n\n    # pass the middle layers\n    x = self.dropout(self.activation(x))\n\n    # pass the out-layer at the end\n    # (batch, fdfwd_dim, seq_maxlen) -&gt; (batch, d_model, seq_maxlen) or\n    # (batch, seq_maxlen, fdfwd_dim) -&gt; (batch, seq_maxlen, d_model)\n    x = self.out_layer(x)\n    # forward the convolutional layers\n    if isinstance(self.out_layer, Conv1dEv):\n        # (batch, d_model, seq_maxlen) -&gt; (batch, seq_maxlen, d_model)\n        x = x.transpose(1, 2)\n    return x\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.PositionwiseFeedForward.module_init","title":"<code>module_init(d_model=512, fdfwd_dim=2048, fdfwd_type='linear', fdfwd_activation='ReLU', fdfwd_args={}, dropout=0.1)</code>","text":"<p>Initializes position-wise feed-forward layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer</p> <code>2048</code> <code>fdfwd_type</code> <code>str</code> <p>str The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.</p> <code>'linear'</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_kernel</code> <p>int The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.</p> required <code>dropout</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer</p> <code>0.1</code> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>def module_init(\n    self,\n    d_model: int = 512,\n    fdfwd_dim: int = 2048,\n    fdfwd_type: str = \"linear\",\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_args: Dict[str, Any] = {},\n    dropout=0.1,\n):\n    \"\"\"Initializes position-wise feed-forward layer.\n\n    Args:\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer\n        fdfwd_type: str\n            The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_kernel: int\n            The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n        dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer\n    \"\"\"\n    if len(fdfwd_args) == 0:\n        if fdfwd_type == \"conv\":\n            fdfwd_args = dict(kernel_size=3)\n\n    # In-layer at the beginning\n    if fdfwd_type == \"linear\":\n        self.in_layer = nn.Linear(d_model, fdfwd_dim, **fdfwd_args)\n    elif fdfwd_type == \"conv\":\n        self.in_layer = Conv1dEv(d_model, fdfwd_dim, **fdfwd_args)\n    else:\n        raise NotImplementedError(\n            f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n            f\"But got {fdfwd_type}!\"\n        )\n\n    # ReLU and DropOut layers in the middle\n    self.activation = getattr(torch.nn, fdfwd_activation)()\n    self.dropout = nn.Dropout(dropout)\n\n    # Out-layer at the end\n    if fdfwd_type == \"linear\":\n        self.out_layer = nn.Linear(fdfwd_dim, d_model, **fdfwd_args)\n    elif fdfwd_type == \"conv\":\n        self.out_layer = Conv1dEv(fdfwd_dim, d_model, **fdfwd_args)\n    else:\n        raise NotImplementedError(\n            f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n            f\"But got {fdfwd_type}!\"\n        )\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoder","title":"<code>TransformerDecoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>class TransformerDecoder(Module):\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        posenc_maxlen: int = 5000,\n        posenc_dropout: float = 0.1,\n        posenc_scale: bool = False,\n        posenc_init_alpha: float = 1.0,\n        emb_layernorm: bool = False,\n        emb_scale: bool = True,\n        d_model: int = 512,\n        num_heads: int = 4,\n        num_layers: int = 8,\n        scale_dp_by_head: bool = False,\n        fdfwd_dim: int = 2048,\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_dropout: float = 0.1,\n        att_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n    ):\n        \"\"\"\n\n        Args:\n            posenc_type: str\n                Specify the positional encoding type you would like to use in your Transformer blocks.\n            posenc_maxlen: int\n                Maximal length when calculating the positional encoding.\n                Usually, the default value of this argument is enough for the research.\n            posenc_dropout: float\n                The dropout rate for the Dropout layer after adding the positional encoding to the input\n            posenc_scale: bool\n                Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n                embedded features or not.\n                Reference:\n                    'Neural Speech Synthesis with Transformer Network'\n                    https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n            posenc_init_alpha: float = 1.0\n                The initial value of the alpha used for positional encoding scaling.\n                Only effective when posenc_scale is True.\n            emb_layernorm: bool\n                Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n                encoding or not.\n            emb_scale: bool\n                Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n                encoding or not.\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            num_heads: int\n                The number of attention heads in each Transformer layer\n            num_layers: int\n                The number of Transformer layers\n            att_dropout: float\n                The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer in each Transformer layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n            res_dropout: float\n                The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n            layernorm_first: bool\n                controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer.\n                True means the LayerNorm layer appears at the beginning; False means the LayerNorm layer appears at the end.\n\n        \"\"\"\n\n        # input_size and output_size initialization\n        if self.input_size is not None:\n            d_model = self.input_size\n        self.output_size = d_model\n\n        # para recording\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.layernorm_first = layernorm_first\n\n        # initialize the positional encoding layer\n        self.posenc = PositionalEncoding(\n            posenc_type=posenc_type,\n            d_model=d_model,\n            emb_scale=emb_scale,\n            emb_layernorm=emb_layernorm,\n            posenc_scale=posenc_scale,\n            init_alpha=posenc_init_alpha,\n            max_len=posenc_maxlen,\n            dropout=posenc_dropout,\n        )\n\n        # create num_layers decoder layers and put them in a list\n        self.trfm_layers = torch.nn.ModuleList(\n            [\n                TransformerDecoderLayer(\n                    d_model=d_model,\n                    num_heads=num_heads,\n                    scale_dp_by_head=scale_dp_by_head,\n                    att_dropout=att_dropout,\n                    fdfwd_dim=fdfwd_dim,\n                    fdfwd_activation=fdfwd_activation,\n                    fdfwd_dropout=fdfwd_dropout,\n                    res_dropout=res_dropout,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        # initialize layernorm layer if necessary\n        if self.layernorm_first:\n            self.layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n    @staticmethod\n    def subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n        \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n        Transformer helper function.\n\n        Args:\n            batch_size:\n            maxlen: int\n                size of mask (2nd and 3rd dim)\n\n        Returns:\n        \"\"\"\n        return ~torch.triu(\n            torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n        )\n\n    def forward(\n        self,\n        tgt: torch.Tensor,\n        src: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        src_mask: torch.Tensor,\n        return_att: bool = False,\n        return_hidden: bool = False,\n    ):\n        \"\"\"Transformer decoder forward pass.\n\n        Args:\n            tgt: (batch, tgt_maxlen, d_model)\n                embedded targets\n            src: (batch, src_maxlen, d_model)\n                source representations\n            tgt_mask: (batch, 1, tgt_maxlen)\n                to mask out target paddings\n                Note that a subsequent mask is applied here.\n            src_mask: (batch, 1, src_maxlen)\n                to mask out source paddings\n            return_att:\n            return_hidden:\n\n        Returns:\n            The output of the Transformer decoder.\n            The outputs of each Transformer decoder layer will be returned as a List.\n            The attention matrix (self and enc-dec) of each Transformer decoder layer will also be returned as a List.\n        \"\"\"\n        assert tgt_mask is not None, \"tgt_mask is required for Transformer!\"\n\n        # pass the positional encoding layer\n        tgt = self.posenc(tgt)\n\n        # generate the diagonal mask for self-attention layers\n        batch_size, _, tgt_maxlen = tgt_mask.size()\n        tgt_mask = torch.logical_and(\n            tgt_mask.repeat(1, tgt_maxlen, 1),\n            self.subsequent_mask(batch_size, tgt_maxlen).to(tgt_mask.device),\n        )\n\n        # pass the transformer layers\n        self_attmat, encdec_attmat, hidden = [], [], []\n        for layer in self.trfm_layers:\n            tgt, _self_attmat, _encdec_attmat = layer(\n                tgt=tgt, tgt_mask=tgt_mask, src=src, src_mask=src_mask\n            )\n            self_attmat.append(_self_attmat)\n            encdec_attmat.append(_encdec_attmat)\n            hidden.append(tgt.clone())\n\n        # pass the layernorm layer if necessary\n        if self.layernorm_first:\n            tgt = self.layernorm(tgt)\n\n        return tgt, self_attmat, encdec_attmat, hidden\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoder.forward","title":"<code>forward(tgt, src, tgt_mask, src_mask, return_att=False, return_hidden=False)</code>","text":"<p>Transformer decoder forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>tgt</code> <code>Tensor</code> <p>(batch, tgt_maxlen, d_model) embedded targets</p> required <code>src</code> <code>Tensor</code> <p>(batch, src_maxlen, d_model) source representations</p> required <code>tgt_mask</code> <code>Tensor</code> <p>(batch, 1, tgt_maxlen) to mask out target paddings Note that a subsequent mask is applied here.</p> required <code>src_mask</code> <code>Tensor</code> <p>(batch, 1, src_maxlen) to mask out source paddings</p> required <code>return_att</code> <code>bool</code> <code>False</code> <code>return_hidden</code> <code>bool</code> <code>False</code> <p>Returns:</p> Type Description <p>The output of the Transformer decoder.</p> <p>The outputs of each Transformer decoder layer will be returned as a List.</p> <p>The attention matrix (self and enc-dec) of each Transformer decoder layer will also be returned as a List.</p> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>def forward(\n    self,\n    tgt: torch.Tensor,\n    src: torch.Tensor,\n    tgt_mask: torch.Tensor,\n    src_mask: torch.Tensor,\n    return_att: bool = False,\n    return_hidden: bool = False,\n):\n    \"\"\"Transformer decoder forward pass.\n\n    Args:\n        tgt: (batch, tgt_maxlen, d_model)\n            embedded targets\n        src: (batch, src_maxlen, d_model)\n            source representations\n        tgt_mask: (batch, 1, tgt_maxlen)\n            to mask out target paddings\n            Note that a subsequent mask is applied here.\n        src_mask: (batch, 1, src_maxlen)\n            to mask out source paddings\n        return_att:\n        return_hidden:\n\n    Returns:\n        The output of the Transformer decoder.\n        The outputs of each Transformer decoder layer will be returned as a List.\n        The attention matrix (self and enc-dec) of each Transformer decoder layer will also be returned as a List.\n    \"\"\"\n    assert tgt_mask is not None, \"tgt_mask is required for Transformer!\"\n\n    # pass the positional encoding layer\n    tgt = self.posenc(tgt)\n\n    # generate the diagonal mask for self-attention layers\n    batch_size, _, tgt_maxlen = tgt_mask.size()\n    tgt_mask = torch.logical_and(\n        tgt_mask.repeat(1, tgt_maxlen, 1),\n        self.subsequent_mask(batch_size, tgt_maxlen).to(tgt_mask.device),\n    )\n\n    # pass the transformer layers\n    self_attmat, encdec_attmat, hidden = [], [], []\n    for layer in self.trfm_layers:\n        tgt, _self_attmat, _encdec_attmat = layer(\n            tgt=tgt, tgt_mask=tgt_mask, src=src, src_mask=src_mask\n        )\n        self_attmat.append(_self_attmat)\n        encdec_attmat.append(_encdec_attmat)\n        hidden.append(tgt.clone())\n\n    # pass the layernorm layer if necessary\n    if self.layernorm_first:\n        tgt = self.layernorm(tgt)\n\n    return tgt, self_attmat, encdec_attmat, hidden\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoder.module_init","title":"<code>module_init(posenc_type='mix', posenc_maxlen=5000, posenc_dropout=0.1, posenc_scale=False, posenc_init_alpha=1.0, emb_layernorm=False, emb_scale=True, d_model=512, num_heads=4, num_layers=8, scale_dp_by_head=False, fdfwd_dim=2048, fdfwd_activation='ReLU', fdfwd_dropout=0.1, att_dropout=0.1, res_dropout=0.1, layernorm_first=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>posenc_type</code> <code>str</code> <p>str Specify the positional encoding type you would like to use in your Transformer blocks.</p> <code>'mix'</code> <code>posenc_maxlen</code> <code>int</code> <p>int Maximal length when calculating the positional encoding. Usually, the default value of this argument is enough for the research.</p> <code>5000</code> <code>posenc_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after adding the positional encoding to the input</p> <code>0.1</code> <code>posenc_scale</code> <code>bool</code> <p>bool Controls whether the positional encodings are scaled up by a trainable scalar before adding into the embedded features or not. Reference:     'Neural Speech Synthesis with Transformer Network'     https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> <code>False</code> <code>posenc_init_alpha</code> <code>float</code> <p>float = 1.0 The initial value of the alpha used for positional encoding scaling. Only effective when posenc_scale is True.</p> <code>1.0</code> <code>emb_layernorm</code> <code>bool</code> <p>bool Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional encoding or not.</p> <code>False</code> <code>emb_scale</code> <code>bool</code> <p>bool Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional encoding or not.</p> <code>True</code> <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>int The number of attention heads in each Transformer layer</p> <code>4</code> <code>num_layers</code> <code>int</code> <p>int The number of Transformer layers</p> <code>8</code> <code>att_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after calculating the weights in each Transformer layer</p> <code>0.1</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer in each Transformer layer.</p> <code>2048</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer</p> <code>0.1</code> <code>res_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input</p> <code>0.1</code> <code>layernorm_first</code> <code>bool</code> <p>bool controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer. True means the LayerNorm layer appears at the beginning; False means the LayerNorm layer appears at the end.</p> <code>True</code> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>def module_init(\n    self,\n    posenc_type: str = \"mix\",\n    posenc_maxlen: int = 5000,\n    posenc_dropout: float = 0.1,\n    posenc_scale: bool = False,\n    posenc_init_alpha: float = 1.0,\n    emb_layernorm: bool = False,\n    emb_scale: bool = True,\n    d_model: int = 512,\n    num_heads: int = 4,\n    num_layers: int = 8,\n    scale_dp_by_head: bool = False,\n    fdfwd_dim: int = 2048,\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_dropout: float = 0.1,\n    att_dropout: float = 0.1,\n    res_dropout: float = 0.1,\n    layernorm_first: bool = True,\n):\n    \"\"\"\n\n    Args:\n        posenc_type: str\n            Specify the positional encoding type you would like to use in your Transformer blocks.\n        posenc_maxlen: int\n            Maximal length when calculating the positional encoding.\n            Usually, the default value of this argument is enough for the research.\n        posenc_dropout: float\n            The dropout rate for the Dropout layer after adding the positional encoding to the input\n        posenc_scale: bool\n            Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n            embedded features or not.\n            Reference:\n                'Neural Speech Synthesis with Transformer Network'\n                https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n        posenc_init_alpha: float = 1.0\n            The initial value of the alpha used for positional encoding scaling.\n            Only effective when posenc_scale is True.\n        emb_layernorm: bool\n            Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n            encoding or not.\n        emb_scale: bool\n            Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n            encoding or not.\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        num_heads: int\n            The number of attention heads in each Transformer layer\n        num_layers: int\n            The number of Transformer layers\n        att_dropout: float\n            The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer in each Transformer layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n        res_dropout: float\n            The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n        layernorm_first: bool\n            controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer.\n            True means the LayerNorm layer appears at the beginning; False means the LayerNorm layer appears at the end.\n\n    \"\"\"\n\n    # input_size and output_size initialization\n    if self.input_size is not None:\n        d_model = self.input_size\n    self.output_size = d_model\n\n    # para recording\n    self.d_model = d_model\n    self.num_layers = num_layers\n    self.num_heads = num_heads\n    self.layernorm_first = layernorm_first\n\n    # initialize the positional encoding layer\n    self.posenc = PositionalEncoding(\n        posenc_type=posenc_type,\n        d_model=d_model,\n        emb_scale=emb_scale,\n        emb_layernorm=emb_layernorm,\n        posenc_scale=posenc_scale,\n        init_alpha=posenc_init_alpha,\n        max_len=posenc_maxlen,\n        dropout=posenc_dropout,\n    )\n\n    # create num_layers decoder layers and put them in a list\n    self.trfm_layers = torch.nn.ModuleList(\n        [\n            TransformerDecoderLayer(\n                d_model=d_model,\n                num_heads=num_heads,\n                scale_dp_by_head=scale_dp_by_head,\n                att_dropout=att_dropout,\n                fdfwd_dim=fdfwd_dim,\n                fdfwd_activation=fdfwd_activation,\n                fdfwd_dropout=fdfwd_dropout,\n                res_dropout=res_dropout,\n            )\n            for _ in range(num_layers)\n        ]\n    )\n\n    # initialize layernorm layer if necessary\n    if self.layernorm_first:\n        self.layernorm = nn.LayerNorm(d_model, eps=1e-6)\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoder.subsequent_mask","title":"<code>subsequent_mask(batch_size, maxlen)</code>  <code>staticmethod</code>","text":"<p>Mask out subsequent positions (to prevent attending to future positions) Transformer helper function.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> required <code>maxlen</code> <code>int</code> <p>int size of mask (2nd and 3rd dim)</p> required <p>Returns:</p> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>@staticmethod\ndef subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n    \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n    Transformer helper function.\n\n    Args:\n        batch_size:\n        maxlen: int\n            size of mask (2nd and 3rd dim)\n\n    Returns:\n    \"\"\"\n    return ~torch.triu(\n        torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n    )\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoderLayer","title":"<code>TransformerDecoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single Transformer decoder layer has: \u00b7 a self multi-head attention sublayer \u00b7 a LayerNorm layer exclusively for the self-attention sublayer \u00b7 a encoder-decoder multi-head attention sublayer \u00b7 a LayerNorm layer exclusively for the encoder-decoder attention sublayer \u00b7 a position-wise feed-forward sublayer \u00b7 a LayerNorm layer exclusively for the feed-forward sublayer \u00b7 a residual dropout layer</p> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>class TransformerDecoderLayer(Module):\n    \"\"\"\n    A single Transformer decoder layer has:\n    \u00b7 a self multi-head attention sublayer\n    \u00b7 a LayerNorm layer exclusively for the self-attention sublayer\n    \u00b7 a encoder-decoder multi-head attention sublayer\n    \u00b7 a LayerNorm layer exclusively for the encoder-decoder attention sublayer\n    \u00b7 a position-wise feed-forward sublayer\n    \u00b7 a LayerNorm layer exclusively for the feed-forward sublayer\n    \u00b7 a residual dropout layer\n\n    \"\"\"\n\n    def module_init(\n        self,\n        d_model: int = 512,\n        num_heads: int = 8,\n        scale_dp_by_head: bool = False,\n        att_dropout: float = 0.1,\n        fdfwd_dim: int = 0,\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n    ):\n        \"\"\"Represents a single Transformer decoder layer. It attends to the source\n        representation and the previous decoder states.\n\n        Args:\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            num_heads: int\n                The number of attention heads in each Transformer layer\n            att_dropout: float\n                The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer in each Transformer layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n            res_dropout: float\n                The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n            layernorm_first: bool\n                Whether layernorm is performed before feeding src into sublayers.\n                if layernorm_first is True:\n                    output = input + Sublayer(LayerNorm(input))\n                elif layernorm_first is False:\n                    output = LayerNorm(input + Sublayer(input))\n        \"\"\"\n        # initialize the self attention layer\n        self.self_att = MultiHeadedAttention(\n            num_heads=num_heads,\n            d_model=d_model,\n            dropout=att_dropout,\n            scale_dp_by_head=scale_dp_by_head,\n        )\n\n        # initialize the encoder-decoder attention layer\n        self.encdec_att = MultiHeadedAttention(\n            num_heads=num_heads, d_model=d_model, dropout=att_dropout\n        )\n\n        # initialize feedforward layer\n        self.feed_forward = PositionwiseFeedForward(\n            d_model=d_model,\n            fdfwd_dim=fdfwd_dim,\n            fdfwd_activation=fdfwd_activation,\n            dropout=fdfwd_dropout,\n        )\n\n        # initialize layernorm layers\n        self.layernorm_first = layernorm_first\n        self.self_att_ln = nn.LayerNorm(d_model, eps=1e-6)\n        self.encdec_att_ln = nn.LayerNorm(d_model, eps=1e-6)\n        self.fdfwd_ln = nn.LayerNorm(d_model, eps=1e-6)\n\n        # initialize residual dropout layer\n        self.dropout = nn.Dropout(res_dropout)\n\n    def forward(\n        self,\n        tgt: torch.Tensor,\n        src: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        src_mask: torch.Tensor,\n    ):\n        \"\"\"Forward pass of a single Transformer decoder layer.\n\n        Args:\n            tgt: (batch, tgt_maxlen, d_model)\n                target inputs\n            src: (batch, src_maxlen, d_model)\n                source representations\n            tgt_mask: (batch, tgt_maxlen, tgt_maxlen)\n                target mask (so as to not condition on future steps)\n            src_mask: (batch, 1, src_maxlen)\n                source mask\n\n        Returns:\n            The output of this Transformer decoder layer and the attention matrix (self and enc-dec)\n        \"\"\"\n\n        # --- 1. Self Attention Layer part --- #\n        # go through the LayerNorm layer before the self attention layer or not\n        tgt_norm = self.self_att_ln(tgt) if self.layernorm_first else tgt\n\n        # go through the self attention layer and perform the residual connection\n        self_att_hidden, self_attmat = self.self_att(\n            tgt_norm, tgt_norm, tgt_norm, mask=tgt_mask\n        )\n        self_att_output = self.dropout(self_att_hidden) + tgt\n\n        # go through the LayerNorm layer after the self attention layer or not\n        self_att_output = (\n            self.self_att_ln(self_att_output)\n            if not self.layernorm_first\n            else self_att_output\n        )\n\n        # --- 2. Enc-Dec Attention Layer part --- #\n        # go through the LayerNorm layer before the enc-dec attention layer or not\n        self_att_output_norm = (\n            self.encdec_att_ln(self_att_output)\n            if self.layernorm_first\n            else self_att_output\n        )\n\n        # go through the enc-dec attention layer and perform the residual connection\n        encdec_att_hidden, encdec_attmat = self.encdec_att(\n            src, src, self_att_output_norm, mask=src_mask\n        )\n        encdec_att_output = self.dropout(encdec_att_hidden) + self_att_output\n\n        # go through the LayerNorm layer after the enc-dec attention layer or not\n        encdec_att_output = (\n            self.encdec_att_ln(encdec_att_output)\n            if not self.layernorm_first\n            else encdec_att_output\n        )\n\n        # --- 3. Positional FeedForward Layer part --- #\n        # go through the LayerNorm layer before the feedforward layer or not\n        encdec_att_output_norm = (\n            self.fdfwd_ln(encdec_att_output)\n            if self.layernorm_first\n            else encdec_att_output\n        )\n\n        # go through the feedforward layer and perform the residual connection\n        fdfwd_hidden = self.feed_forward(encdec_att_output_norm)\n        fdfwd_output = self.dropout(fdfwd_hidden) + encdec_att_output\n\n        # go through the LayerNorm layer after the feedforward layer or not\n        fdfwd_output = (\n            self.fdfwd_ln(fdfwd_output) if not self.layernorm_first else fdfwd_output\n        )\n\n        return fdfwd_output, self_attmat, encdec_attmat\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoderLayer.forward","title":"<code>forward(tgt, src, tgt_mask, src_mask)</code>","text":"<p>Forward pass of a single Transformer decoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>tgt</code> <code>Tensor</code> <p>(batch, tgt_maxlen, d_model) target inputs</p> required <code>src</code> <code>Tensor</code> <p>(batch, src_maxlen, d_model) source representations</p> required <code>tgt_mask</code> <code>Tensor</code> <p>(batch, tgt_maxlen, tgt_maxlen) target mask (so as to not condition on future steps)</p> required <code>src_mask</code> <code>Tensor</code> <p>(batch, 1, src_maxlen) source mask</p> required <p>Returns:</p> Type Description <p>The output of this Transformer decoder layer and the attention matrix (self and enc-dec)</p> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>def forward(\n    self,\n    tgt: torch.Tensor,\n    src: torch.Tensor,\n    tgt_mask: torch.Tensor,\n    src_mask: torch.Tensor,\n):\n    \"\"\"Forward pass of a single Transformer decoder layer.\n\n    Args:\n        tgt: (batch, tgt_maxlen, d_model)\n            target inputs\n        src: (batch, src_maxlen, d_model)\n            source representations\n        tgt_mask: (batch, tgt_maxlen, tgt_maxlen)\n            target mask (so as to not condition on future steps)\n        src_mask: (batch, 1, src_maxlen)\n            source mask\n\n    Returns:\n        The output of this Transformer decoder layer and the attention matrix (self and enc-dec)\n    \"\"\"\n\n    # --- 1. Self Attention Layer part --- #\n    # go through the LayerNorm layer before the self attention layer or not\n    tgt_norm = self.self_att_ln(tgt) if self.layernorm_first else tgt\n\n    # go through the self attention layer and perform the residual connection\n    self_att_hidden, self_attmat = self.self_att(\n        tgt_norm, tgt_norm, tgt_norm, mask=tgt_mask\n    )\n    self_att_output = self.dropout(self_att_hidden) + tgt\n\n    # go through the LayerNorm layer after the self attention layer or not\n    self_att_output = (\n        self.self_att_ln(self_att_output)\n        if not self.layernorm_first\n        else self_att_output\n    )\n\n    # --- 2. Enc-Dec Attention Layer part --- #\n    # go through the LayerNorm layer before the enc-dec attention layer or not\n    self_att_output_norm = (\n        self.encdec_att_ln(self_att_output)\n        if self.layernorm_first\n        else self_att_output\n    )\n\n    # go through the enc-dec attention layer and perform the residual connection\n    encdec_att_hidden, encdec_attmat = self.encdec_att(\n        src, src, self_att_output_norm, mask=src_mask\n    )\n    encdec_att_output = self.dropout(encdec_att_hidden) + self_att_output\n\n    # go through the LayerNorm layer after the enc-dec attention layer or not\n    encdec_att_output = (\n        self.encdec_att_ln(encdec_att_output)\n        if not self.layernorm_first\n        else encdec_att_output\n    )\n\n    # --- 3. Positional FeedForward Layer part --- #\n    # go through the LayerNorm layer before the feedforward layer or not\n    encdec_att_output_norm = (\n        self.fdfwd_ln(encdec_att_output)\n        if self.layernorm_first\n        else encdec_att_output\n    )\n\n    # go through the feedforward layer and perform the residual connection\n    fdfwd_hidden = self.feed_forward(encdec_att_output_norm)\n    fdfwd_output = self.dropout(fdfwd_hidden) + encdec_att_output\n\n    # go through the LayerNorm layer after the feedforward layer or not\n    fdfwd_output = (\n        self.fdfwd_ln(fdfwd_output) if not self.layernorm_first else fdfwd_output\n    )\n\n    return fdfwd_output, self_attmat, encdec_attmat\n</code></pre>"},{"location":"reference/module/transformer/decoder/#module.transformer.decoder.TransformerDecoderLayer.module_init","title":"<code>module_init(d_model=512, num_heads=8, scale_dp_by_head=False, att_dropout=0.1, fdfwd_dim=0, fdfwd_activation='ReLU', fdfwd_dropout=0.1, res_dropout=0.1, layernorm_first=True)</code>","text":"<p>Represents a single Transformer decoder layer. It attends to the source representation and the previous decoder states.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>int The number of attention heads in each Transformer layer</p> <code>8</code> <code>att_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after calculating the weights in each Transformer layer</p> <code>0.1</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer in each Transformer layer.</p> <code>0</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer</p> <code>0.1</code> <code>res_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input</p> <code>0.1</code> <code>layernorm_first</code> <code>bool</code> <p>bool Whether layernorm is performed before feeding src into sublayers. if layernorm_first is True:     output = input + Sublayer(LayerNorm(input)) elif layernorm_first is False:     output = LayerNorm(input + Sublayer(input))</p> <code>True</code> Source code in <code>speechain/module/transformer/decoder.py</code> <pre><code>def module_init(\n    self,\n    d_model: int = 512,\n    num_heads: int = 8,\n    scale_dp_by_head: bool = False,\n    att_dropout: float = 0.1,\n    fdfwd_dim: int = 0,\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_dropout: float = 0.1,\n    res_dropout: float = 0.1,\n    layernorm_first: bool = True,\n):\n    \"\"\"Represents a single Transformer decoder layer. It attends to the source\n    representation and the previous decoder states.\n\n    Args:\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        num_heads: int\n            The number of attention heads in each Transformer layer\n        att_dropout: float\n            The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer in each Transformer layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n        res_dropout: float\n            The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n        layernorm_first: bool\n            Whether layernorm is performed before feeding src into sublayers.\n            if layernorm_first is True:\n                output = input + Sublayer(LayerNorm(input))\n            elif layernorm_first is False:\n                output = LayerNorm(input + Sublayer(input))\n    \"\"\"\n    # initialize the self attention layer\n    self.self_att = MultiHeadedAttention(\n        num_heads=num_heads,\n        d_model=d_model,\n        dropout=att_dropout,\n        scale_dp_by_head=scale_dp_by_head,\n    )\n\n    # initialize the encoder-decoder attention layer\n    self.encdec_att = MultiHeadedAttention(\n        num_heads=num_heads, d_model=d_model, dropout=att_dropout\n    )\n\n    # initialize feedforward layer\n    self.feed_forward = PositionwiseFeedForward(\n        d_model=d_model,\n        fdfwd_dim=fdfwd_dim,\n        fdfwd_activation=fdfwd_activation,\n        dropout=fdfwd_dropout,\n    )\n\n    # initialize layernorm layers\n    self.layernorm_first = layernorm_first\n    self.self_att_ln = nn.LayerNorm(d_model, eps=1e-6)\n    self.encdec_att_ln = nn.LayerNorm(d_model, eps=1e-6)\n    self.fdfwd_ln = nn.LayerNorm(d_model, eps=1e-6)\n\n    # initialize residual dropout layer\n    self.dropout = nn.Dropout(res_dropout)\n</code></pre>"},{"location":"reference/module/transformer/encoder/","title":"encoder","text":"<p>Origin: Sashi Novitasari Modification: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoder","title":"<code>TransformerEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>The Transformer encoder for any Sequence-to-Sequence tasks. Reference:     Attention is all you need     https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</p> Our Transformer encoder implements the following properties <ol> <li>Different positional encoding. (Mix or Sep)</li> <li>Different positions of the LayerNorm layer (first or last)</li> <li>Time Frame Downsampling (pool or concat)</li> </ol> <p>For the details, please refer to the docstrings of PositionalEncoding and TransformerEncoderLayer.</p> <p>In our Transformer implementation, there are 4 places to place the Dropout layers:     1. After adding the positional encoding into the embedded features.     2. After the softmax operation and before reweighting all the values by these weights in the         multi-head attention layer.     3. Between two feedforward linear layers there will be a Dropout layer.     4. Before performing residual connect in a Transformer layer.</p> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>class TransformerEncoder(Module):\n    \"\"\"\n    The Transformer encoder for any Sequence-to-Sequence tasks.\n    Reference:\n        Attention is all you need\n        https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n    Our Transformer encoder implements the following properties:\n        1. Different positional encoding. (Mix or Sep)\n        2. Different positions of the LayerNorm layer (first or last)\n        3. Time Frame Downsampling (pool or concat)\n    For the details, please refer to the docstrings of PositionalEncoding and TransformerEncoderLayer.\n\n    In our Transformer implementation, there are 4 places to place the Dropout layers:\n        1. After adding the positional encoding into the embedded features.\n        2. After the softmax operation and before reweighting all the values by these weights in the\n            multi-head attention layer.\n        3. Between two feedforward linear layers there will be a Dropout layer.\n        4. Before performing residual connect in a Transformer layer.\n\n    \"\"\"\n\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        posenc_maxlen: int = 5000,\n        posenc_dropout: float = 0.1,\n        posenc_scale: bool = False,\n        posenc_init_alpha: float = 1.0,\n        emb_layernorm: bool = False,\n        emb_scale: bool = False,\n        d_model: int = 512,\n        num_heads: int = 4,\n        num_layers: int = 8,\n        scale_dp_by_head: bool = False,\n        att_dropout: float = 0.1,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        fdfwd_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n        uni_direction: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            posenc_type: str\n                Specify the positional encoding type you would like to use in your Transformer blocks.\n            posenc_maxlen: int\n                Maximal length when calculating the positional encoding.\n                Usually, the default value of this argument is enough for the research.\n            posenc_dropout: float\n                The dropout rate for the Dropout layer after adding the positional encoding to the input\n            posenc_scale: bool\n                Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n                embedded features or not.\n                Reference:\n                    'Neural Speech Synthesis with Transformer Network'\n                    https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n            posenc_init_alpha: float = 1.0\n                The initial value of the alpha used for positional encoding scaling.\n                Only effective when posenc_scale is True.\n            emb_layernorm: bool\n                Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n                encoding or not.\n            emb_scale: bool\n                Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n                encoding or not.\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            num_heads: int\n                The number of attention heads in each Transformer layer\n            num_layers: int\n                The number of Transformer layers\n            att_dropout: float\n                The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer in each Transformer layer.\n            fdfwd_type: str\n                The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n            res_dropout: float\n                The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n            uni_direction: bool = False\n                Whether the encoder is unidirectional or not. If True, the attention matrix will be masked into a\n                lower-triangular matrix.\n            layernorm_first: bool\n                controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer.\n                    True means the LayerNorm layer appears at the beginning\n                    False means the LayerNorm layer appears at the end.\n                For LayerNorm first, there will be an additional LayerNorm at the end of the Transformer Encoder to\n                perform the final normalization.\n\n        \"\"\"\n        # input_size and output_size initialization\n        if self.input_size is not None:\n            d_model = self.input_size\n        self.output_size = d_model\n\n        # para recording\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.layernorm_first = layernorm_first\n        self.uni_direction = uni_direction\n\n        # initialize positional encoding layer\n        self.posenc = PositionalEncoding(\n            posenc_type=posenc_type,\n            d_model=d_model,\n            emb_scale=emb_scale,\n            emb_layernorm=emb_layernorm,\n            posenc_scale=posenc_scale,\n            init_alpha=posenc_init_alpha,\n            max_len=posenc_maxlen,\n            dropout=posenc_dropout,\n        )\n\n        # initialize transformer layers\n        self.trfm_layers = torch.nn.ModuleList(\n            [\n                TransformerEncoderLayer(\n                    d_model=d_model,\n                    num_heads=num_heads,\n                    scale_dp_by_head=scale_dp_by_head,\n                    att_dropout=att_dropout,\n                    fdfwd_dim=fdfwd_dim,\n                    fdfwd_type=fdfwd_type,\n                    fdfwd_activation=fdfwd_activation,\n                    fdfwd_args=fdfwd_args,\n                    fdfwd_dropout=fdfwd_dropout,\n                    res_dropout=res_dropout,\n                    layernorm_first=layernorm_first,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n\n        # initialize layernorm layer if necessary\n        if self.layernorm_first:\n            self.layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n    @staticmethod\n    def subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n        \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n        Transformer helper function.\n\n        Args:\n            batch_size:\n            maxlen: int\n                size of mask (2nd and 3rd dim)\n\n        Returns:\n        \"\"\"\n        return ~torch.triu(\n            torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n        )\n\n    def forward(self, src: torch.Tensor, mask: torch.Tensor):\n        \"\"\"Pass the input (and mask) through each layer in turn. Applies a Transformer\n        encoder to sequence of embeddings x. The input mini-batch x needs to be sorted\n        by src length. x and mask should have the same dimensions [batch, time, dim].\n\n        Args:\n            src: (batch_size, src_maxlen, embed_size)\n                embedded src inputs,\n            mask: (batch_size, 1, src_maxlen)\n                indicates padding areas (zeros where padding)\n\n        Returns:\n            The output of the Transformer encoder with its mask.\n            The outputs of each Transformer encoder layer will be returned as a List.\n            The attention matrix of each Transformer encoder layer will also be returned as a List.\n        \"\"\"\n\n        # add position encoding to word embeddings\n        src = self.posenc(src)\n\n        # generate the low-triangular mask for self-attention layers\n        if self.uni_direction:\n            batch_size, _, src_maxlen = mask.size()\n            mask = torch.logical_and(\n                mask.repeat(1, src_maxlen, 1),\n                self.subsequent_mask(batch_size, src_maxlen).to(mask.device),\n            )\n\n        # go through the Transformer layers\n        attmat, hidden = [], []\n        for l in range(len(self.trfm_layers)):\n            src, _tmp_attmat = self.trfm_layers[l](src, mask)\n            attmat.append(_tmp_attmat)\n            hidden.append(src.clone())\n\n        # go through the final layernorm layer if necessary\n        if self.layernorm_first:\n            src = self.layernorm(src)\n\n        return src, mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoder.forward","title":"<code>forward(src, mask)</code>","text":"<p>Pass the input (and mask) through each layer in turn. Applies a Transformer encoder to sequence of embeddings x. The input mini-batch x needs to be sorted by src length. x and mask should have the same dimensions [batch, time, dim].</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>(batch_size, src_maxlen, embed_size) embedded src inputs,</p> required <code>mask</code> <code>Tensor</code> <p>(batch_size, 1, src_maxlen) indicates padding areas (zeros where padding)</p> required <p>Returns:</p> Type Description <p>The output of the Transformer encoder with its mask.</p> <p>The outputs of each Transformer encoder layer will be returned as a List.</p> <p>The attention matrix of each Transformer encoder layer will also be returned as a List.</p> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>def forward(self, src: torch.Tensor, mask: torch.Tensor):\n    \"\"\"Pass the input (and mask) through each layer in turn. Applies a Transformer\n    encoder to sequence of embeddings x. The input mini-batch x needs to be sorted\n    by src length. x and mask should have the same dimensions [batch, time, dim].\n\n    Args:\n        src: (batch_size, src_maxlen, embed_size)\n            embedded src inputs,\n        mask: (batch_size, 1, src_maxlen)\n            indicates padding areas (zeros where padding)\n\n    Returns:\n        The output of the Transformer encoder with its mask.\n        The outputs of each Transformer encoder layer will be returned as a List.\n        The attention matrix of each Transformer encoder layer will also be returned as a List.\n    \"\"\"\n\n    # add position encoding to word embeddings\n    src = self.posenc(src)\n\n    # generate the low-triangular mask for self-attention layers\n    if self.uni_direction:\n        batch_size, _, src_maxlen = mask.size()\n        mask = torch.logical_and(\n            mask.repeat(1, src_maxlen, 1),\n            self.subsequent_mask(batch_size, src_maxlen).to(mask.device),\n        )\n\n    # go through the Transformer layers\n    attmat, hidden = [], []\n    for l in range(len(self.trfm_layers)):\n        src, _tmp_attmat = self.trfm_layers[l](src, mask)\n        attmat.append(_tmp_attmat)\n        hidden.append(src.clone())\n\n    # go through the final layernorm layer if necessary\n    if self.layernorm_first:\n        src = self.layernorm(src)\n\n    return src, mask, attmat, hidden\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoder.module_init","title":"<code>module_init(posenc_type='mix', posenc_maxlen=5000, posenc_dropout=0.1, posenc_scale=False, posenc_init_alpha=1.0, emb_layernorm=False, emb_scale=False, d_model=512, num_heads=4, num_layers=8, scale_dp_by_head=False, att_dropout=0.1, fdfwd_dim=2048, fdfwd_type='linear', fdfwd_activation='ReLU', fdfwd_args={}, fdfwd_dropout=0.1, res_dropout=0.1, layernorm_first=True, uni_direction=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>posenc_type</code> <code>str</code> <p>str Specify the positional encoding type you would like to use in your Transformer blocks.</p> <code>'mix'</code> <code>posenc_maxlen</code> <code>int</code> <p>int Maximal length when calculating the positional encoding. Usually, the default value of this argument is enough for the research.</p> <code>5000</code> <code>posenc_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after adding the positional encoding to the input</p> <code>0.1</code> <code>posenc_scale</code> <code>bool</code> <p>bool Controls whether the positional encodings are scaled up by a trainable scalar before adding into the embedded features or not. Reference:     'Neural Speech Synthesis with Transformer Network'     https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> <code>False</code> <code>posenc_init_alpha</code> <code>float</code> <p>float = 1.0 The initial value of the alpha used for positional encoding scaling. Only effective when posenc_scale is True.</p> <code>1.0</code> <code>emb_layernorm</code> <code>bool</code> <p>bool Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional encoding or not.</p> <code>False</code> <code>emb_scale</code> <code>bool</code> <p>bool Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional encoding or not.</p> <code>False</code> <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>int The number of attention heads in each Transformer layer</p> <code>4</code> <code>num_layers</code> <code>int</code> <p>int The number of Transformer layers</p> <code>8</code> <code>att_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after calculating the weights in each Transformer layer</p> <code>0.1</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer in each Transformer layer.</p> <code>2048</code> <code>fdfwd_type</code> <code>str</code> <p>str The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.</p> <code>'linear'</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer</p> <code>0.1</code> <code>res_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input</p> <code>0.1</code> <code>uni_direction</code> <code>bool</code> <p>bool = False Whether the encoder is unidirectional or not. If True, the attention matrix will be masked into a lower-triangular matrix.</p> <code>False</code> <code>layernorm_first</code> <code>bool</code> <p>bool controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer.     True means the LayerNorm layer appears at the beginning     False means the LayerNorm layer appears at the end. For LayerNorm first, there will be an additional LayerNorm at the end of the Transformer Encoder to perform the final normalization.</p> <code>True</code> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>def module_init(\n    self,\n    posenc_type: str = \"mix\",\n    posenc_maxlen: int = 5000,\n    posenc_dropout: float = 0.1,\n    posenc_scale: bool = False,\n    posenc_init_alpha: float = 1.0,\n    emb_layernorm: bool = False,\n    emb_scale: bool = False,\n    d_model: int = 512,\n    num_heads: int = 4,\n    num_layers: int = 8,\n    scale_dp_by_head: bool = False,\n    att_dropout: float = 0.1,\n    fdfwd_dim: int = 2048,\n    fdfwd_type: str = \"linear\",\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_args: Dict[str, Any] = {},\n    fdfwd_dropout: float = 0.1,\n    res_dropout: float = 0.1,\n    layernorm_first: bool = True,\n    uni_direction: bool = False,\n):\n    \"\"\"\n\n    Args:\n        posenc_type: str\n            Specify the positional encoding type you would like to use in your Transformer blocks.\n        posenc_maxlen: int\n            Maximal length when calculating the positional encoding.\n            Usually, the default value of this argument is enough for the research.\n        posenc_dropout: float\n            The dropout rate for the Dropout layer after adding the positional encoding to the input\n        posenc_scale: bool\n            Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n            embedded features or not.\n            Reference:\n                'Neural Speech Synthesis with Transformer Network'\n                https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n        posenc_init_alpha: float = 1.0\n            The initial value of the alpha used for positional encoding scaling.\n            Only effective when posenc_scale is True.\n        emb_layernorm: bool\n            Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n            encoding or not.\n        emb_scale: bool\n            Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n            encoding or not.\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        num_heads: int\n            The number of attention heads in each Transformer layer\n        num_layers: int\n            The number of Transformer layers\n        att_dropout: float\n            The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer in each Transformer layer.\n        fdfwd_type: str\n            The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n        res_dropout: float\n            The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n        uni_direction: bool = False\n            Whether the encoder is unidirectional or not. If True, the attention matrix will be masked into a\n            lower-triangular matrix.\n        layernorm_first: bool\n            controls whether the LayerNorm layer appears at the beginning or at the end of each Transformer layer.\n                True means the LayerNorm layer appears at the beginning\n                False means the LayerNorm layer appears at the end.\n            For LayerNorm first, there will be an additional LayerNorm at the end of the Transformer Encoder to\n            perform the final normalization.\n\n    \"\"\"\n    # input_size and output_size initialization\n    if self.input_size is not None:\n        d_model = self.input_size\n    self.output_size = d_model\n\n    # para recording\n    self.d_model = d_model\n    self.num_layers = num_layers\n    self.num_heads = num_heads\n    self.layernorm_first = layernorm_first\n    self.uni_direction = uni_direction\n\n    # initialize positional encoding layer\n    self.posenc = PositionalEncoding(\n        posenc_type=posenc_type,\n        d_model=d_model,\n        emb_scale=emb_scale,\n        emb_layernorm=emb_layernorm,\n        posenc_scale=posenc_scale,\n        init_alpha=posenc_init_alpha,\n        max_len=posenc_maxlen,\n        dropout=posenc_dropout,\n    )\n\n    # initialize transformer layers\n    self.trfm_layers = torch.nn.ModuleList(\n        [\n            TransformerEncoderLayer(\n                d_model=d_model,\n                num_heads=num_heads,\n                scale_dp_by_head=scale_dp_by_head,\n                att_dropout=att_dropout,\n                fdfwd_dim=fdfwd_dim,\n                fdfwd_type=fdfwd_type,\n                fdfwd_activation=fdfwd_activation,\n                fdfwd_args=fdfwd_args,\n                fdfwd_dropout=fdfwd_dropout,\n                res_dropout=res_dropout,\n                layernorm_first=layernorm_first,\n            )\n            for _ in range(num_layers)\n        ]\n    )\n\n    # initialize layernorm layer if necessary\n    if self.layernorm_first:\n        self.layernorm = nn.LayerNorm(d_model, eps=1e-6)\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoder.subsequent_mask","title":"<code>subsequent_mask(batch_size, maxlen)</code>  <code>staticmethod</code>","text":"<p>Mask out subsequent positions (to prevent attending to future positions) Transformer helper function.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> required <code>maxlen</code> <code>int</code> <p>int size of mask (2nd and 3rd dim)</p> required <p>Returns:</p> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>@staticmethod\ndef subsequent_mask(batch_size, maxlen: int) -&gt; torch.Tensor:\n    \"\"\"Mask out subsequent positions (to prevent attending to future positions)\n    Transformer helper function.\n\n    Args:\n        batch_size:\n        maxlen: int\n            size of mask (2nd and 3rd dim)\n\n    Returns:\n    \"\"\"\n    return ~torch.triu(\n        torch.ones(batch_size, maxlen, maxlen, dtype=torch.bool), diagonal=1\n    )\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoderLayer","title":"<code>TransformerEncoderLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single Transformer encoder layer has: \u00b7 a Multi-head attention sublayer \u00b7 a LayerNorm layer exclusively for the attention sublayer \u00b7 a position-wise feed-forward sublayer \u00b7 a LayerNorm layer exclusively for the feed-forward sublayer \u00b7 a residual dropout layer</p> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>class TransformerEncoderLayer(Module):\n    \"\"\"\n    A single Transformer encoder layer has:\n    \u00b7 a Multi-head attention sublayer\n    \u00b7 a LayerNorm layer exclusively for the attention sublayer\n    \u00b7 a position-wise feed-forward sublayer\n    \u00b7 a LayerNorm layer exclusively for the feed-forward sublayer\n    \u00b7 a residual dropout layer\n\n    \"\"\"\n\n    def module_init(\n        self,\n        d_model: int = 512,\n        num_heads: int = 8,\n        scale_dp_by_head: bool = False,\n        att_dropout: float = 0.1,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        fdfwd_dropout: float = 0.1,\n        res_dropout: float = 0.1,\n        layernorm_first: bool = True,\n    ):\n        \"\"\"\n\n        Args:\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            num_heads: int\n                The number of attention heads in each Transformer layer\n            att_dropout: float\n                The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer in each Transformer layer.\n            fdfwd_type: str\n                The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_kernel: int\n                The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n            fdfwd_dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n            res_dropout: float\n                The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n            layernorm_first: bool\n                Whether layernorm is performed before feeding src into sublayers.\n                if layernorm_first is True:\n                    output = input + Sublayer(LayerNorm(input))\n                elif layernorm_first is False:\n                    output = LayerNorm(input + Sublayer(input))\n\n        \"\"\"\n        # initialize multi-head attention layer\n        self.multihead_att = MultiHeadedAttention(\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout=att_dropout,\n            scale_dp_by_head=scale_dp_by_head,\n        )\n\n        # initialize feedforward layer\n        self.feed_forward = PositionwiseFeedForward(\n            d_model=d_model,\n            fdfwd_dim=fdfwd_dim,\n            fdfwd_type=fdfwd_type,\n            fdfwd_activation=fdfwd_activation,\n            fdfwd_args=fdfwd_args,\n            dropout=fdfwd_dropout,\n        )\n\n        # initialize residual dropout layer\n        self.dropout = nn.Dropout(res_dropout)\n\n        # initialize layernorm layers, each sublayer has an exclusive LayerNorm layer\n        self.layernorm_first = layernorm_first\n        self.att_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n        self.fdfwd_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, src: torch.Tensor, mask: torch.Tensor):\n        \"\"\"Forward pass for a single transformer encoder layer.\n\n        Args:\n            src: (batch, src_maxlen, d_model)\n                source input for the encoder\n            mask: (batch, 1, src_maxlen)\n                input mask\n\n        Returns:\n            The output of this Transformer encoder layer and the attention matrix\n        \"\"\"\n\n        \"Multi-head Attention Layer part\"\n        # go through the LayerNorm layer before the multi-head attention layer or not\n        src_norm = self.att_layernorm(src) if self.layernorm_first else src\n\n        # go through the multi-head attention layer and perform the residual connection\n        att_hidden, attmat = self.multihead_att(src_norm, src_norm, src_norm, mask)\n        att_output = self.dropout(att_hidden) + src\n\n        # go through the LayerNorm layer after the multi-head attention layer or not\n        att_output = (\n            self.att_layernorm(att_output) if not self.layernorm_first else att_output\n        )\n\n        \"Positional FeedForward Layer part\"\n        # go through the LayerNorm layer before the feedforward layer or not\n        att_output_norm = (\n            self.fdfwd_layernorm(att_output) if self.layernorm_first else att_output\n        )\n\n        # go through the feedforward layer and perform the residual connection\n        fdfwd_hidden = self.feed_forward(att_output_norm)\n        fdfwd_output = self.dropout(fdfwd_hidden) + att_output\n\n        # go through the LayerNorm layer after the feedforward layer or not\n        fdfwd_output = (\n            self.fdfwd_layernorm(fdfwd_output)\n            if not self.layernorm_first\n            else fdfwd_output\n        )\n\n        return fdfwd_output, attmat\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoderLayer.forward","title":"<code>forward(src, mask)</code>","text":"<p>Forward pass for a single transformer encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>(batch, src_maxlen, d_model) source input for the encoder</p> required <code>mask</code> <code>Tensor</code> <p>(batch, 1, src_maxlen) input mask</p> required <p>Returns:</p> Type Description <p>The output of this Transformer encoder layer and the attention matrix</p> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>def forward(self, src: torch.Tensor, mask: torch.Tensor):\n    \"\"\"Forward pass for a single transformer encoder layer.\n\n    Args:\n        src: (batch, src_maxlen, d_model)\n            source input for the encoder\n        mask: (batch, 1, src_maxlen)\n            input mask\n\n    Returns:\n        The output of this Transformer encoder layer and the attention matrix\n    \"\"\"\n\n    \"Multi-head Attention Layer part\"\n    # go through the LayerNorm layer before the multi-head attention layer or not\n    src_norm = self.att_layernorm(src) if self.layernorm_first else src\n\n    # go through the multi-head attention layer and perform the residual connection\n    att_hidden, attmat = self.multihead_att(src_norm, src_norm, src_norm, mask)\n    att_output = self.dropout(att_hidden) + src\n\n    # go through the LayerNorm layer after the multi-head attention layer or not\n    att_output = (\n        self.att_layernorm(att_output) if not self.layernorm_first else att_output\n    )\n\n    \"Positional FeedForward Layer part\"\n    # go through the LayerNorm layer before the feedforward layer or not\n    att_output_norm = (\n        self.fdfwd_layernorm(att_output) if self.layernorm_first else att_output\n    )\n\n    # go through the feedforward layer and perform the residual connection\n    fdfwd_hidden = self.feed_forward(att_output_norm)\n    fdfwd_output = self.dropout(fdfwd_hidden) + att_output\n\n    # go through the LayerNorm layer after the feedforward layer or not\n    fdfwd_output = (\n        self.fdfwd_layernorm(fdfwd_output)\n        if not self.layernorm_first\n        else fdfwd_output\n    )\n\n    return fdfwd_output, attmat\n</code></pre>"},{"location":"reference/module/transformer/encoder/#module.transformer.encoder.TransformerEncoderLayer.module_init","title":"<code>module_init(d_model=512, num_heads=8, scale_dp_by_head=False, att_dropout=0.1, fdfwd_dim=2048, fdfwd_type='linear', fdfwd_activation='ReLU', fdfwd_args={}, fdfwd_dropout=0.1, res_dropout=0.1, layernorm_first=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>int The number of attention heads in each Transformer layer</p> <code>8</code> <code>att_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after calculating the weights in each Transformer layer</p> <code>0.1</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer in each Transformer layer.</p> <code>2048</code> <code>fdfwd_type</code> <code>str</code> <p>str The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.</p> <code>'linear'</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_kernel</code> <p>int The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.</p> required <code>fdfwd_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer</p> <code>0.1</code> <code>res_dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input</p> <code>0.1</code> <code>layernorm_first</code> <code>bool</code> <p>bool Whether layernorm is performed before feeding src into sublayers. if layernorm_first is True:     output = input + Sublayer(LayerNorm(input)) elif layernorm_first is False:     output = LayerNorm(input + Sublayer(input))</p> <code>True</code> Source code in <code>speechain/module/transformer/encoder.py</code> <pre><code>def module_init(\n    self,\n    d_model: int = 512,\n    num_heads: int = 8,\n    scale_dp_by_head: bool = False,\n    att_dropout: float = 0.1,\n    fdfwd_dim: int = 2048,\n    fdfwd_type: str = \"linear\",\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_args: Dict[str, Any] = {},\n    fdfwd_dropout: float = 0.1,\n    res_dropout: float = 0.1,\n    layernorm_first: bool = True,\n):\n    \"\"\"\n\n    Args:\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        num_heads: int\n            The number of attention heads in each Transformer layer\n        att_dropout: float\n            The dropout rate for the Dropout layer after calculating the weights in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer in each Transformer layer.\n        fdfwd_type: str\n            The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_kernel: int\n            The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n        fdfwd_dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer in each Transformer layer\n        res_dropout: float\n            The dropout rate for the Dropout layer before adding the output of each Transformer layer into its input\n        layernorm_first: bool\n            Whether layernorm is performed before feeding src into sublayers.\n            if layernorm_first is True:\n                output = input + Sublayer(LayerNorm(input))\n            elif layernorm_first is False:\n                output = LayerNorm(input + Sublayer(input))\n\n    \"\"\"\n    # initialize multi-head attention layer\n    self.multihead_att = MultiHeadedAttention(\n        d_model=d_model,\n        num_heads=num_heads,\n        dropout=att_dropout,\n        scale_dp_by_head=scale_dp_by_head,\n    )\n\n    # initialize feedforward layer\n    self.feed_forward = PositionwiseFeedForward(\n        d_model=d_model,\n        fdfwd_dim=fdfwd_dim,\n        fdfwd_type=fdfwd_type,\n        fdfwd_activation=fdfwd_activation,\n        fdfwd_args=fdfwd_args,\n        dropout=fdfwd_dropout,\n    )\n\n    # initialize residual dropout layer\n    self.dropout = nn.Dropout(res_dropout)\n\n    # initialize layernorm layers, each sublayer has an exclusive LayerNorm layer\n    self.layernorm_first = layernorm_first\n    self.att_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n    self.fdfwd_layernorm = nn.LayerNorm(d_model, eps=1e-6)\n</code></pre>"},{"location":"reference/module/transformer/feed_forward/","title":"feed_forward","text":"<p>Origin: Sashi Novitasari Modification: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/transformer/feed_forward/#module.transformer.feed_forward.PositionwiseFeedForward","title":"<code>PositionwiseFeedForward</code>","text":"<p>               Bases: <code>Module</code></p> <p>Position-wise Feed-forward layer Projects the output vectors of multi- head attention layer to fdfwd_dim and then back to d_model.</p> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>class PositionwiseFeedForward(Module):\n    \"\"\"Position-wise Feed-forward layer Projects the output vectors of multi- head\n    attention layer to fdfwd_dim and then back to d_model.\"\"\"\n\n    def module_init(\n        self,\n        d_model: int = 512,\n        fdfwd_dim: int = 2048,\n        fdfwd_type: str = \"linear\",\n        fdfwd_activation: str = \"ReLU\",\n        fdfwd_args: Dict[str, Any] = {},\n        dropout=0.1,\n    ):\n        \"\"\"Initializes position-wise feed-forward layer.\n\n        Args:\n            d_model: int\n                The dimension of the hidden feature vector in each Transformer layer\n            fdfwd_dim: int\n                The value of the out_features of the first linear feedforward layer and the in_features of the second\n                linear feedforward layer\n            fdfwd_type: str\n                The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n            fdfwd_activation: str\n                The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n            fdfwd_kernel: int\n                The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n            dropout: float\n                The dropout rate for the Dropout layer after the first linear feedforward layer\n        \"\"\"\n        if len(fdfwd_args) == 0:\n            if fdfwd_type == \"conv\":\n                fdfwd_args = dict(kernel_size=3)\n\n        # In-layer at the beginning\n        if fdfwd_type == \"linear\":\n            self.in_layer = nn.Linear(d_model, fdfwd_dim, **fdfwd_args)\n        elif fdfwd_type == \"conv\":\n            self.in_layer = Conv1dEv(d_model, fdfwd_dim, **fdfwd_args)\n        else:\n            raise NotImplementedError(\n                f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n                f\"But got {fdfwd_type}!\"\n            )\n\n        # ReLU and DropOut layers in the middle\n        self.activation = getattr(torch.nn, fdfwd_activation)()\n        self.dropout = nn.Dropout(dropout)\n\n        # Out-layer at the end\n        if fdfwd_type == \"linear\":\n            self.out_layer = nn.Linear(fdfwd_dim, d_model, **fdfwd_args)\n        elif fdfwd_type == \"conv\":\n            self.out_layer = Conv1dEv(fdfwd_dim, d_model, **fdfwd_args)\n        else:\n            raise NotImplementedError(\n                f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n                f\"But got {fdfwd_type}!\"\n            )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n\n        Args:\n            x: (batch, seq_maxlen, d_model)\n\n        Returns:\n\n        \"\"\"\n        # forward the convolutional layers\n        if isinstance(self.in_layer, Conv1dEv):\n            # (batch, seq_maxlen, d_model) -&gt; (batch, d_model, seq_maxlen)\n            x = x.transpose(1, 2)\n        # pass the in-layer at the beginning\n        # (batch, d_model, seq_maxlen) -&gt; (batch, fdfwd_dim, seq_maxlen) or\n        # (batch, seq_maxlen, d_model) -&gt; (batch, seq_maxlen, fdfwd_dim)\n        x = self.in_layer(x)\n\n        # pass the middle layers\n        x = self.dropout(self.activation(x))\n\n        # pass the out-layer at the end\n        # (batch, fdfwd_dim, seq_maxlen) -&gt; (batch, d_model, seq_maxlen) or\n        # (batch, seq_maxlen, fdfwd_dim) -&gt; (batch, seq_maxlen, d_model)\n        x = self.out_layer(x)\n        # forward the convolutional layers\n        if isinstance(self.out_layer, Conv1dEv):\n            # (batch, d_model, seq_maxlen) -&gt; (batch, seq_maxlen, d_model)\n            x = x.transpose(1, 2)\n        return x\n</code></pre>"},{"location":"reference/module/transformer/feed_forward/#module.transformer.feed_forward.PositionwiseFeedForward.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>(batch, seq_maxlen, d_model)</p> required <p>Returns:</p> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n\n    Args:\n        x: (batch, seq_maxlen, d_model)\n\n    Returns:\n\n    \"\"\"\n    # forward the convolutional layers\n    if isinstance(self.in_layer, Conv1dEv):\n        # (batch, seq_maxlen, d_model) -&gt; (batch, d_model, seq_maxlen)\n        x = x.transpose(1, 2)\n    # pass the in-layer at the beginning\n    # (batch, d_model, seq_maxlen) -&gt; (batch, fdfwd_dim, seq_maxlen) or\n    # (batch, seq_maxlen, d_model) -&gt; (batch, seq_maxlen, fdfwd_dim)\n    x = self.in_layer(x)\n\n    # pass the middle layers\n    x = self.dropout(self.activation(x))\n\n    # pass the out-layer at the end\n    # (batch, fdfwd_dim, seq_maxlen) -&gt; (batch, d_model, seq_maxlen) or\n    # (batch, seq_maxlen, fdfwd_dim) -&gt; (batch, seq_maxlen, d_model)\n    x = self.out_layer(x)\n    # forward the convolutional layers\n    if isinstance(self.out_layer, Conv1dEv):\n        # (batch, d_model, seq_maxlen) -&gt; (batch, seq_maxlen, d_model)\n        x = x.transpose(1, 2)\n    return x\n</code></pre>"},{"location":"reference/module/transformer/feed_forward/#module.transformer.feed_forward.PositionwiseFeedForward.module_init","title":"<code>module_init(d_model=512, fdfwd_dim=2048, fdfwd_type='linear', fdfwd_activation='ReLU', fdfwd_args={}, dropout=0.1)</code>","text":"<p>Initializes position-wise feed-forward layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vector in each Transformer layer</p> <code>512</code> <code>fdfwd_dim</code> <code>int</code> <p>int The value of the out_features of the first linear feedforward layer and the in_features of the second linear feedforward layer</p> <code>2048</code> <code>fdfwd_type</code> <code>str</code> <p>str The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.</p> <code>'linear'</code> <code>fdfwd_activation</code> <code>str</code> <p>str The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.</p> <code>'ReLU'</code> <code>fdfwd_kernel</code> <p>int The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.</p> required <code>dropout</code> <p>float The dropout rate for the Dropout layer after the first linear feedforward layer</p> <code>0.1</code> Source code in <code>speechain/module/transformer/feed_forward.py</code> <pre><code>def module_init(\n    self,\n    d_model: int = 512,\n    fdfwd_dim: int = 2048,\n    fdfwd_type: str = \"linear\",\n    fdfwd_activation: str = \"ReLU\",\n    fdfwd_args: Dict[str, Any] = {},\n    dropout=0.1,\n):\n    \"\"\"Initializes position-wise feed-forward layer.\n\n    Args:\n        d_model: int\n            The dimension of the hidden feature vector in each Transformer layer\n        fdfwd_dim: int\n            The value of the out_features of the first linear feedforward layer and the in_features of the second\n            linear feedforward layer\n        fdfwd_type: str\n            The type of the feed-forward layer. 'linear' means the Linear layer while 'conv' means the Conv1d layer.\n        fdfwd_activation: str\n            The name of the activation function of feedforward layers. Should be the name of functions in 'torch.nn'.\n        fdfwd_kernel: int\n            The kernal size of the Conv1d feed-forward layer. This argument is not effective if fdfwd_type == 'linear'.\n        dropout: float\n            The dropout rate for the Dropout layer after the first linear feedforward layer\n    \"\"\"\n    if len(fdfwd_args) == 0:\n        if fdfwd_type == \"conv\":\n            fdfwd_args = dict(kernel_size=3)\n\n    # In-layer at the beginning\n    if fdfwd_type == \"linear\":\n        self.in_layer = nn.Linear(d_model, fdfwd_dim, **fdfwd_args)\n    elif fdfwd_type == \"conv\":\n        self.in_layer = Conv1dEv(d_model, fdfwd_dim, **fdfwd_args)\n    else:\n        raise NotImplementedError(\n            f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n            f\"But got {fdfwd_type}!\"\n        )\n\n    # ReLU and DropOut layers in the middle\n    self.activation = getattr(torch.nn, fdfwd_activation)()\n    self.dropout = nn.Dropout(dropout)\n\n    # Out-layer at the end\n    if fdfwd_type == \"linear\":\n        self.out_layer = nn.Linear(fdfwd_dim, d_model, **fdfwd_args)\n    elif fdfwd_type == \"conv\":\n        self.out_layer = Conv1dEv(fdfwd_dim, d_model, **fdfwd_args)\n    else:\n        raise NotImplementedError(\n            f\"Currently, fdfwd_type can only be one of 'linear' and 'conv'. \"\n            f\"But got {fdfwd_type}!\"\n        )\n</code></pre>"},{"location":"reference/module/transformer/pos_enc/","title":"pos_enc","text":"<p>Origin: Sashi Novitasari Modification: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/module/transformer/pos_enc/#module.transformer.pos_enc.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Pre-compute position encodings (PE).</p> <p>In forward pass, this module adds the positional encodings to the embedded feature vectors to make the Transformer aware of the positional information of the sequences.</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>class PositionalEncoding(Module):\n    \"\"\"Pre-compute position encodings (PE).\n\n    In forward pass, this module adds the positional encodings to the embedded feature\n    vectors to make the Transformer aware of the positional information of the\n    sequences.\n    \"\"\"\n\n    def module_init(\n        self,\n        posenc_type: str = \"mix\",\n        d_model: int = 512,\n        emb_scale: bool = False,\n        emb_layernorm: bool = False,\n        posenc_scale: bool = False,\n        init_alpha: float = 1.0,\n        max_len: int = 5000,\n        dropout: float = 0.0,\n    ):\n        \"\"\"Positional Encoding with maximum length max_len.\n\n        Args:\n            posenc_type: str\n                The type of positional encoding (must be either 'mix' or 'sep').\n                For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions.\n                The equations are as below:\n                    PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    Reference:\n                        'Attention Is All You Need'\n                        https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n                For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half\n                of dimensions. The equations are as below:\n                    PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                    PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}\n                    Reference:\n                        'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'\n                        https://ieeexplore.ieee.org/abstract/document/8462506/\n            d_model: int\n                The dimension of the hidden feature vectors of the Transformer layers.\n            emb_scale: bool\n                Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n                encoding or not.\n                References:\n                    Section 3.4 in 'Attention Is All You Need'\n                    https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n                In most cases, we don't recommend you to turn it on especially when you don't have a large training set\n                (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it\n                only when you want to emphasize the embedded features over the positional encodings.\n            emb_layernorm: bool\n                Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n                encoding or not.\n            posenc_scale: bool\n                Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n                embedded features or not.\n                Reference:\n                    'Neural Speech Synthesis with Transformer Network'\n                    https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n            init_alpha: float\n                The initial value of the alpha used for positional encoding scaling.\n                Only effective when posenc_scale is True.\n            max_len: int\n                The maximum length of the input feature sequences.\n            dropout: float\n                The dropout rate for the Dropout layer after adding the positional encoding to the input\n        \"\"\"\n\n        assert posenc_type in [\n            \"mix\",\n            \"sep\",\n        ], f\"The type of PositionalEncoding layer must be either 'mix' or 'sep', but got type={posenc_type}!\"\n        assert (\n            d_model % 2 == 0\n        ), f\"Cannot apply sin/cos positional encoding to the vectors with odd dimensions (got d_model={d_model:d}).\"\n\n        self.posenc_type = posenc_type\n        self.d_model = d_model\n        self.emb_scale = emb_scale\n        if emb_layernorm:\n            self.emb_layernorm = torch.nn.LayerNorm(d_model)\n\n        self.init_alpha = (\n            init_alpha if isinstance(init_alpha, float) else float(init_alpha)\n        )\n        if posenc_scale:\n            self.alpha = torch.nn.Parameter(torch.tensor(self.init_alpha))\n\n        # positional encoding matrix\n        self.update_posenc(max_len)\n\n        # positional encoding Dropout layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def reset_parameters(self):\n        \"\"\"Make sure that the scalar value is not influenced by different model\n        initialization methods.\"\"\"\n        if hasattr(self, \"alpha\"):\n            self.alpha.data = torch.tensor(self.init_alpha)\n\n    def update_posenc(self, max_len: int):\n        \"\"\"\n\n        Args:\n            max_len:\n\n        \"\"\"\n\n        # positional encoding calculation\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float)\n            * (math.log(10000.0) / self.d_model)\n        )\n        posenc = torch.zeros(max_len, self.d_model)\n\n        # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n        if self.posenc_type == \"mix\":\n            posenc[:, 0::2] = torch.sin(position / div_term)\n            posenc[:, 1::2] = torch.cos(position / div_term)\n        # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n        elif self.posenc_type == \"sep\":\n            div_term_ext = torch.exp(\n                torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n                * (math.log(10000.0) / self.d_model)\n            )\n            posenc[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n            posenc[:, int(self.d_model / 2) :] = torch.cos(position / div_term_ext)\n\n        # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n        # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n        self.register_buffer(\"posenc\", posenc.unsqueeze(0))\n\n    def forward(self, emb_feat: torch.Tensor):\n        \"\"\"Embedded feature.\n\n            -&gt; LayerNorm(Embedded feature)\n                -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n                    -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n                        -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n\n        Args:\n            emb_feat: (batch_size, seq_len, d_model)\n                Embedded input feature sequences\n\n        Returns:\n            Embedded input feature sequences with positional encoding\n        \"\"\"\n        # in case that the input sequence is longer than the preset max_len\n        if emb_feat.size(1) &gt; self.posenc.size(1):\n            self.update_posenc(emb_feat.size(1), self.d_model)\n\n        # 1. (optional) normalize the embedded feature by LayerNorm\n        if hasattr(self, \"emb_layernorm\"):\n            emb_feat = self.emb_layernorm(emb_feat)\n\n        # 2. (optional) scale the embedded feature up by sqrt(d_model)\n        if self.emb_scale:\n            emb_feat *= math.sqrt(self.d_model)\n\n        # 3. (optional) scale the positional encoding vectors\n        posenc = self.posenc[:, : emb_feat.size(1)]\n        if hasattr(self, \"alpha\"):\n            # avoid posenc *= self.alpha to protect the original positional encoding\n            posenc = posenc * self.alpha\n\n        # 4. (mandatory) add positional encoding into embedded feature and apply the dropout\n        return self.dropout(emb_feat + posenc)\n\n    def get_recordable_para(self) -&gt; Dict or None:\n        if hasattr(self, \"alpha\"):\n            return dict(alpha=self.alpha)\n        else:\n            return None\n\n    def extra_repr(self) -&gt; str:\n        return f\"emb_scale={self.emb_scale}\\n\" f\"posenc_scale={hasattr(self, 'alpha')}\"\n</code></pre>"},{"location":"reference/module/transformer/pos_enc/#module.transformer.pos_enc.PositionalEncoding.forward","title":"<code>forward(emb_feat)</code>","text":"<p>Embedded feature.</p> <pre><code>-&gt; LayerNorm(Embedded feature)\n    -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n        -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n            -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>emb_feat</code> <code>Tensor</code> <p>(batch_size, seq_len, d_model) Embedded input feature sequences</p> required <p>Returns:</p> Type Description <p>Embedded input feature sequences with positional encoding</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def forward(self, emb_feat: torch.Tensor):\n    \"\"\"Embedded feature.\n\n        -&gt; LayerNorm(Embedded feature)\n            -&gt; LayerNorm(Embedded feature) * sqrt(d_model)\n                -&gt; LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar\n                    -&gt; Dropout(LayerNorm(Embedded feature) * sqrt(d_model) + Positional Encoding * learnable scalar)\n\n    Args:\n        emb_feat: (batch_size, seq_len, d_model)\n            Embedded input feature sequences\n\n    Returns:\n        Embedded input feature sequences with positional encoding\n    \"\"\"\n    # in case that the input sequence is longer than the preset max_len\n    if emb_feat.size(1) &gt; self.posenc.size(1):\n        self.update_posenc(emb_feat.size(1), self.d_model)\n\n    # 1. (optional) normalize the embedded feature by LayerNorm\n    if hasattr(self, \"emb_layernorm\"):\n        emb_feat = self.emb_layernorm(emb_feat)\n\n    # 2. (optional) scale the embedded feature up by sqrt(d_model)\n    if self.emb_scale:\n        emb_feat *= math.sqrt(self.d_model)\n\n    # 3. (optional) scale the positional encoding vectors\n    posenc = self.posenc[:, : emb_feat.size(1)]\n    if hasattr(self, \"alpha\"):\n        # avoid posenc *= self.alpha to protect the original positional encoding\n        posenc = posenc * self.alpha\n\n    # 4. (mandatory) add positional encoding into embedded feature and apply the dropout\n    return self.dropout(emb_feat + posenc)\n</code></pre>"},{"location":"reference/module/transformer/pos_enc/#module.transformer.pos_enc.PositionalEncoding.module_init","title":"<code>module_init(posenc_type='mix', d_model=512, emb_scale=False, emb_layernorm=False, posenc_scale=False, init_alpha=1.0, max_len=5000, dropout=0.0)</code>","text":"<p>Positional Encoding with maximum length max_len.</p> <p>Parameters:</p> Name Type Description Default <code>posenc_type</code> <code>str</code> <p>str The type of positional encoding (must be either 'mix' or 'sep'). For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions. The equations are as below:     PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     Reference:         'Attention Is All You Need'         https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half of dimensions. The equations are as below:     PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}     PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}     Reference:         'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'         https://ieeexplore.ieee.org/abstract/document/8462506/</p> <code>'mix'</code> <code>d_model</code> <code>int</code> <p>int The dimension of the hidden feature vectors of the Transformer layers.</p> <code>512</code> <code>emb_scale</code> <code>bool</code> <p>bool Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional encoding or not. References:     Section 3.4 in 'Attention Is All You Need'     https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf In most cases, we don't recommend you to turn it on especially when you don't have a large training set (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it only when you want to emphasize the embedded features over the positional encodings.</p> <code>False</code> <code>emb_layernorm</code> <code>bool</code> <p>bool Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional encoding or not.</p> <code>False</code> <code>posenc_scale</code> <code>bool</code> <p>bool Controls whether the positional encodings are scaled up by a trainable scalar before adding into the embedded features or not. Reference:     'Neural Speech Synthesis with Transformer Network'     https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520</p> <code>False</code> <code>init_alpha</code> <code>float</code> <p>float The initial value of the alpha used for positional encoding scaling. Only effective when posenc_scale is True.</p> <code>1.0</code> <code>max_len</code> <code>int</code> <p>int The maximum length of the input feature sequences.</p> <code>5000</code> <code>dropout</code> <code>float</code> <p>float The dropout rate for the Dropout layer after adding the positional encoding to the input</p> <code>0.0</code> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def module_init(\n    self,\n    posenc_type: str = \"mix\",\n    d_model: int = 512,\n    emb_scale: bool = False,\n    emb_layernorm: bool = False,\n    posenc_scale: bool = False,\n    init_alpha: float = 1.0,\n    max_len: int = 5000,\n    dropout: float = 0.0,\n):\n    \"\"\"Positional Encoding with maximum length max_len.\n\n    Args:\n        posenc_type: str\n            The type of positional encoding (must be either 'mix' or 'sep').\n            For the 'mix' type, sin is applied to the odd dimensions and cos is applied to the even dimensions.\n            The equations are as below:\n                PE(pos, 2i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                PE(pos, 2i + 1) = cos(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                Reference:\n                    'Attention Is All You Need'\n                    https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n            For the 'sep' type, sin is applied to the first half of dimensions and cos is applied to the second half\n            of dimensions. The equations are as below:\n                PE(pos, i) = sin(pos / 10000^{2i / d_model}), i \u2208 {0, ..., d_model / 2 - 1}\n                PE(pos, i) = cos(pos / 10000^{2i / d_model}), i \u2208 {d_model / 2, ..., d_model - 1}\n                Reference:\n                    'Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition'\n                    https://ieeexplore.ieee.org/abstract/document/8462506/\n        d_model: int\n            The dimension of the hidden feature vectors of the Transformer layers.\n        emb_scale: bool\n            Controls whether the embedding vectors are scaled up by sqrt(d_model) before adding into the positional\n            encoding or not.\n            References:\n                Section 3.4 in 'Attention Is All You Need'\n                https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n            In most cases, we don't recommend you to turn it on especially when you don't have a large training set\n            (e.g. LibriSpeech-train_clean_100) because it may make your model hard to converge. Please consider it\n            only when you want to emphasize the embedded features over the positional encodings.\n        emb_layernorm: bool\n            Controls whether the embedding vectors are normalized by LayerNorm before adding into the positional\n            encoding or not.\n        posenc_scale: bool\n            Controls whether the positional encodings are scaled up by a trainable scalar before adding into the\n            embedded features or not.\n            Reference:\n                'Neural Speech Synthesis with Transformer Network'\n                https://ojs.aaai.org/index.php/AAAI/article/view/4642/4520\n        init_alpha: float\n            The initial value of the alpha used for positional encoding scaling.\n            Only effective when posenc_scale is True.\n        max_len: int\n            The maximum length of the input feature sequences.\n        dropout: float\n            The dropout rate for the Dropout layer after adding the positional encoding to the input\n    \"\"\"\n\n    assert posenc_type in [\n        \"mix\",\n        \"sep\",\n    ], f\"The type of PositionalEncoding layer must be either 'mix' or 'sep', but got type={posenc_type}!\"\n    assert (\n        d_model % 2 == 0\n    ), f\"Cannot apply sin/cos positional encoding to the vectors with odd dimensions (got d_model={d_model:d}).\"\n\n    self.posenc_type = posenc_type\n    self.d_model = d_model\n    self.emb_scale = emb_scale\n    if emb_layernorm:\n        self.emb_layernorm = torch.nn.LayerNorm(d_model)\n\n    self.init_alpha = (\n        init_alpha if isinstance(init_alpha, float) else float(init_alpha)\n    )\n    if posenc_scale:\n        self.alpha = torch.nn.Parameter(torch.tensor(self.init_alpha))\n\n    # positional encoding matrix\n    self.update_posenc(max_len)\n\n    # positional encoding Dropout layer\n    self.dropout = torch.nn.Dropout(p=dropout)\n</code></pre>"},{"location":"reference/module/transformer/pos_enc/#module.transformer.pos_enc.PositionalEncoding.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Make sure that the scalar value is not influenced by different model initialization methods.</p> Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"Make sure that the scalar value is not influenced by different model\n    initialization methods.\"\"\"\n    if hasattr(self, \"alpha\"):\n        self.alpha.data = torch.tensor(self.init_alpha)\n</code></pre>"},{"location":"reference/module/transformer/pos_enc/#module.transformer.pos_enc.PositionalEncoding.update_posenc","title":"<code>update_posenc(max_len)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>max_len</code> <code>int</code> required Source code in <code>speechain/module/transformer/pos_enc.py</code> <pre><code>def update_posenc(self, max_len: int):\n    \"\"\"\n\n    Args:\n        max_len:\n\n    \"\"\"\n\n    # positional encoding calculation\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, self.d_model, 2, dtype=torch.float)\n        * (math.log(10000.0) / self.d_model)\n    )\n    posenc = torch.zeros(max_len, self.d_model)\n\n    # 'mix' positional encoding: sine functions and cosine functions mix up with each other\n    if self.posenc_type == \"mix\":\n        posenc[:, 0::2] = torch.sin(position / div_term)\n        posenc[:, 1::2] = torch.cos(position / div_term)\n    # 'sep' positional encoding: sine functions and cosine functions occupy the positional encoding separately\n    elif self.posenc_type == \"sep\":\n        div_term_ext = torch.exp(\n            torch.arange(self.d_model, self.d_model * 2, 2, dtype=torch.float)\n            * (math.log(10000.0) / self.d_model)\n        )\n        posenc[:, : int(self.d_model / 2)] = torch.sin(position / div_term)\n        posenc[:, int(self.d_model / 2) :] = torch.cos(position / div_term_ext)\n\n    # posenc = posenc.unsqueeze(0) does not put posenc into the buffer\n    # here register_buffer() allows posenc to be automatically put onto GPUs as a buffer member\n    self.register_buffer(\"posenc\", posenc.unsqueeze(0))\n</code></pre>"},{"location":"reference/module/vocoder/","title":"vocoder","text":""},{"location":"reference/module/vocoder/hifigan/","title":"hifigan","text":"<p>HiFi-GAN Vocoder Module</p> <p>This module provides a HiFi-GAN vocoder implementation compatible with SpeechBrain's pretrained weights from HuggingFace Hub.</p> <p>The implementation uses weight normalization as in the SpeechBrain checkpoint.</p> Authors <p>Speechain Authors</p>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.HiFiGAN","title":"<code>HiFiGAN</code>","text":"<p>               Bases: <code>Module</code></p> <p>HiFi-GAN Generator Module.</p> <p>This implementation is compatible with SpeechBrain's pretrained weights. It uses weight normalization and the same architecture as the SpeechBrain model.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <p>Number of input channels (mel spectrogram bins)</p> <code>80</code> <code>out_channels</code> <p>Number of output channels (1 for mono audio)</p> <code>1</code> <code>resblock_type</code> <p>Type of residual block (\"1\" or \"2\")</p> <code>'1'</code> <code>resblock_dilation_sizes</code> <p>Dilation sizes for residual blocks</p> <code>[[1, 3, 5], [1, 3, 5], [1, 3, 5]]</code> <code>resblock_kernel_sizes</code> <p>Kernel sizes for residual blocks</p> <code>[3, 7, 11]</code> <code>upsample_kernel_sizes</code> <p>Kernel sizes for upsampling layers</p> <code>[16, 16, 4, 4]</code> <code>upsample_initial_channel</code> <p>Initial number of channels after first conv</p> <code>512</code> <code>upsample_factors</code> <p>Upsampling factors for each layer</p> <code>[8, 8, 2, 2]</code> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>class HiFiGAN(nn.Module):\n    \"\"\"\n    HiFi-GAN Generator Module.\n\n    This implementation is compatible with SpeechBrain's pretrained weights.\n    It uses weight normalization and the same architecture as the SpeechBrain model.\n\n    Args:\n        in_channels: Number of input channels (mel spectrogram bins)\n        out_channels: Number of output channels (1 for mono audio)\n        resblock_type: Type of residual block (\"1\" or \"2\")\n        resblock_dilation_sizes: Dilation sizes for residual blocks\n        resblock_kernel_sizes: Kernel sizes for residual blocks\n        upsample_kernel_sizes: Kernel sizes for upsampling layers\n        upsample_initial_channel: Initial number of channels after first conv\n        upsample_factors: Upsampling factors for each layer\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        resblock_type=\"1\",\n        resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n        resblock_kernel_sizes=[3, 7, 11],\n        upsample_kernel_sizes=[16, 16, 4, 4],\n        upsample_initial_channel=512,\n        upsample_factors=[8, 8, 2, 2],\n    ):\n        super().__init__()\n\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_factors)\n\n        # Initial convolution - named 'conv_pre' to match SpeechBrain\n        self.conv_pre = weight_norm(\n            nn.Conv1d(in_channels, upsample_initial_channel, 7, 1, padding=3)\n        )\n\n        # Select residual block type\n        resblock = ResBlock1 if resblock_type == \"1\" else ResBlock2\n\n        # Upsampling layers - named 'ups' to match SpeechBrain\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_factors, upsample_kernel_sizes)):\n            self.ups.append(\n                weight_norm(\n                    nn.ConvTranspose1d(\n                        upsample_initial_channel // (2**i),\n                        upsample_initial_channel // (2 ** (i + 1)),\n                        k,\n                        u,\n                        padding=(k - u) // 2,\n                    )\n                )\n            )\n\n        # Residual blocks - named 'resblocks' to match SpeechBrain\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel // (2 ** (i + 1))\n            for j, (k, d) in enumerate(\n                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n            ):\n                self.resblocks.append(resblock(ch, k, d))\n\n        # Final convolution - named 'conv_post' to match SpeechBrain\n        self.conv_post = weight_norm(nn.Conv1d(ch, out_channels, 7, 1, padding=3))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of HiFi-GAN generator.\n\n        Args:\n            x: Input mel spectrogram tensor of shape (batch, mel_channels, time)\n\n        Returns:\n            Audio waveform tensor of shape (batch, 1, time * product(upsample_factors))\n        \"\"\"\n        x = self.conv_pre(x)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, 0.1)\n            x = self.ups[i](x)\n\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i * self.num_kernels + j](x)\n                else:\n                    xs += self.resblocks[i * self.num_kernels + j](x)\n            x = xs / self.num_kernels\n\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        \"\"\"Remove weight normalization from all layers.\"\"\"\n\n        for up_layer in self.ups:\n            remove_weight_norm(up_layer)\n        for resblock in self.resblocks:\n            resblock.remove_weight_norm()\n        remove_weight_norm(self.conv_pre)\n        remove_weight_norm(self.conv_post)\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name=\"speechbrain/tts-hifigan-ljspeech\",\n        cache_dir=None,\n        device=None,\n    ):\n        \"\"\"\n        Load pretrained HiFi-GAN from HuggingFace Hub.\n\n        Args:\n            model_name: HuggingFace model name or path\n            cache_dir: Directory to cache downloaded files\n            device: Device to load model onto\n\n        Returns:\n            Loaded HiFiGAN model\n        \"\"\"\n        from huggingface_hub import hf_hub_download\n\n        if cache_dir is None:\n            cache_dir = os.path.expanduser(\"~/.cache/speechain/vocoders\")\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        # Download checkpoint\n        ckpt_path = hf_hub_download(\n            repo_id=model_name,\n            filename=\"generator.ckpt\",\n            cache_dir=cache_dir,\n        )\n\n        print(f\"Loading HiFi-GAN from {ckpt_path}\")\n\n        # Create model with default config\n        model = cls(**HIFIGAN_DEFAULT_CONFIG)\n\n        # Load checkpoint\n        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n\n        # Handle different checkpoint formats\n        if \"model\" in checkpoint:\n            state_dict = checkpoint[\"model\"]\n        elif \"generator\" in checkpoint:\n            state_dict = checkpoint[\"generator\"]\n        else:\n            state_dict = checkpoint\n\n        # Remap keys from SpeechBrain format to our format\n        # SpeechBrain uses: conv_pre.conv.weight_g -&gt; conv_pre.weight_g\n        new_state_dict = {}\n        for key, value in state_dict.items():\n            # Remove the extra .conv. part from SpeechBrain keys\n            new_key = key.replace(\".conv.\", \".\")\n            new_state_dict[new_key] = value\n\n        # Try to load with remapped keys\n        try:\n            model.load_state_dict(new_state_dict, strict=True)\n            print(\"Loaded HiFi-GAN checkpoint successfully\")\n        except RuntimeError as e:\n            print(f\"Warning: Strict loading failed: {e}\")\n            # Try with strict=False as fallback\n            model.load_state_dict(new_state_dict, strict=False)\n            print(\"Loaded HiFi-GAN checkpoint with strict=False\")\n\n        if device is not None:\n            model = model.to(device)\n\n        model.eval()\n        return model\n\n    def decode_batch(self, mel):\n        \"\"\"\n        Decode mel spectrogram to waveform.\n\n        This method is compatible with SpeechBrain's interface.\n\n        Args:\n            mel: Mel spectrogram tensor of shape (batch, time, mel_channels)\n                 Note: SpeechBrain uses (batch, time, channels) format\n\n        Returns:\n            Audio waveform tensor\n        \"\"\"\n        # Transpose from (batch, time, channels) to (batch, channels, time)\n        if mel.dim() == 3 and mel.size(-1) == self.in_channels:\n            mel = mel.transpose(1, 2)\n\n        with torch.no_grad():\n            audio = self.forward(mel)\n\n        return audio.squeeze(1)  # Remove channel dimension\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.HiFiGAN.decode_batch","title":"<code>decode_batch(mel)</code>","text":"<p>Decode mel spectrogram to waveform.</p> <p>This method is compatible with SpeechBrain's interface.</p> <p>Parameters:</p> Name Type Description Default <code>mel</code> <p>Mel spectrogram tensor of shape (batch, time, mel_channels)  Note: SpeechBrain uses (batch, time, channels) format</p> required <p>Returns:</p> Type Description <p>Audio waveform tensor</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>def decode_batch(self, mel):\n    \"\"\"\n    Decode mel spectrogram to waveform.\n\n    This method is compatible with SpeechBrain's interface.\n\n    Args:\n        mel: Mel spectrogram tensor of shape (batch, time, mel_channels)\n             Note: SpeechBrain uses (batch, time, channels) format\n\n    Returns:\n        Audio waveform tensor\n    \"\"\"\n    # Transpose from (batch, time, channels) to (batch, channels, time)\n    if mel.dim() == 3 and mel.size(-1) == self.in_channels:\n        mel = mel.transpose(1, 2)\n\n    with torch.no_grad():\n        audio = self.forward(mel)\n\n    return audio.squeeze(1)  # Remove channel dimension\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.HiFiGAN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of HiFi-GAN generator.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input mel spectrogram tensor of shape (batch, mel_channels, time)</p> required <p>Returns:</p> Type Description <p>Audio waveform tensor of shape (batch, 1, time * product(upsample_factors))</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of HiFi-GAN generator.\n\n    Args:\n        x: Input mel spectrogram tensor of shape (batch, mel_channels, time)\n\n    Returns:\n        Audio waveform tensor of shape (batch, 1, time * product(upsample_factors))\n    \"\"\"\n    x = self.conv_pre(x)\n\n    for i in range(self.num_upsamples):\n        x = F.leaky_relu(x, 0.1)\n        x = self.ups[i](x)\n\n        xs = None\n        for j in range(self.num_kernels):\n            if xs is None:\n                xs = self.resblocks[i * self.num_kernels + j](x)\n            else:\n                xs += self.resblocks[i * self.num_kernels + j](x)\n        x = xs / self.num_kernels\n\n    x = F.leaky_relu(x)\n    x = self.conv_post(x)\n    x = torch.tanh(x)\n\n    return x\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.HiFiGAN.from_pretrained","title":"<code>from_pretrained(model_name='speechbrain/tts-hifigan-ljspeech', cache_dir=None, device=None)</code>  <code>classmethod</code>","text":"<p>Load pretrained HiFi-GAN from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <p>HuggingFace model name or path</p> <code>'speechbrain/tts-hifigan-ljspeech'</code> <code>cache_dir</code> <p>Directory to cache downloaded files</p> <code>None</code> <code>device</code> <p>Device to load model onto</p> <code>None</code> <p>Returns:</p> Type Description <p>Loaded HiFiGAN model</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    model_name=\"speechbrain/tts-hifigan-ljspeech\",\n    cache_dir=None,\n    device=None,\n):\n    \"\"\"\n    Load pretrained HiFi-GAN from HuggingFace Hub.\n\n    Args:\n        model_name: HuggingFace model name or path\n        cache_dir: Directory to cache downloaded files\n        device: Device to load model onto\n\n    Returns:\n        Loaded HiFiGAN model\n    \"\"\"\n    from huggingface_hub import hf_hub_download\n\n    if cache_dir is None:\n        cache_dir = os.path.expanduser(\"~/.cache/speechain/vocoders\")\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Download checkpoint\n    ckpt_path = hf_hub_download(\n        repo_id=model_name,\n        filename=\"generator.ckpt\",\n        cache_dir=cache_dir,\n    )\n\n    print(f\"Loading HiFi-GAN from {ckpt_path}\")\n\n    # Create model with default config\n    model = cls(**HIFIGAN_DEFAULT_CONFIG)\n\n    # Load checkpoint\n    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n\n    # Handle different checkpoint formats\n    if \"model\" in checkpoint:\n        state_dict = checkpoint[\"model\"]\n    elif \"generator\" in checkpoint:\n        state_dict = checkpoint[\"generator\"]\n    else:\n        state_dict = checkpoint\n\n    # Remap keys from SpeechBrain format to our format\n    # SpeechBrain uses: conv_pre.conv.weight_g -&gt; conv_pre.weight_g\n    new_state_dict = {}\n    for key, value in state_dict.items():\n        # Remove the extra .conv. part from SpeechBrain keys\n        new_key = key.replace(\".conv.\", \".\")\n        new_state_dict[new_key] = value\n\n    # Try to load with remapped keys\n    try:\n        model.load_state_dict(new_state_dict, strict=True)\n        print(\"Loaded HiFi-GAN checkpoint successfully\")\n    except RuntimeError as e:\n        print(f\"Warning: Strict loading failed: {e}\")\n        # Try with strict=False as fallback\n        model.load_state_dict(new_state_dict, strict=False)\n        print(\"Loaded HiFi-GAN checkpoint with strict=False\")\n\n    if device is not None:\n        model = model.to(device)\n\n    model.eval()\n    return model\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.HiFiGAN.remove_weight_norm","title":"<code>remove_weight_norm()</code>","text":"<p>Remove weight normalization from all layers.</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>def remove_weight_norm(self):\n    \"\"\"Remove weight normalization from all layers.\"\"\"\n\n    for up_layer in self.ups:\n        remove_weight_norm(up_layer)\n    for resblock in self.resblocks:\n        resblock.remove_weight_norm()\n    remove_weight_norm(self.conv_pre)\n    remove_weight_norm(self.conv_post)\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.ResBlock1","title":"<code>ResBlock1</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual Block Type 1 for HiFi-GAN.</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>class ResBlock1(nn.Module):\n    \"\"\"Residual Block Type 1 for HiFi-GAN.\"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super().__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    nn.Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[i],\n                        padding=get_padding(kernel_size, dilation[i]),\n                    )\n                )\n                for i in range(len(dilation))\n            ]\n        )\n\n        self.convs2 = nn.ModuleList(\n            [\n                weight_norm(\n                    nn.Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=1,\n                        padding=get_padding(kernel_size, 1),\n                    )\n                )\n                for _ in range(len(dilation))\n            ]\n        )\n\n    def forward(self, x):\n        for c1, c2 in zip(self.convs1, self.convs2):\n            xt = F.leaky_relu(x, 0.1)\n            xt = c1(xt)\n            xt = F.leaky_relu(xt, 0.1)\n            xt = c2(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs1:\n            remove_weight_norm(l)\n        for l in self.convs2:\n            remove_weight_norm(l)\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.ResBlock2","title":"<code>ResBlock2</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual Block Type 2 for HiFi-GAN.</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>class ResBlock2(nn.Module):\n    \"\"\"Residual Block Type 2 for HiFi-GAN.\"\"\"\n\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super().__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    nn.Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,\n                        1,\n                        dilation=dilation[i],\n                        padding=get_padding(kernel_size, dilation[i]),\n                    )\n                )\n                for i in range(len(dilation))\n            ]\n        )\n\n    def forward(self, x):\n        for c in self.convs:\n            xt = F.leaky_relu(x, 0.1)\n            xt = c(xt)\n            x = xt + x\n        return x\n\n    def remove_weight_norm(self):\n        for l in self.convs:\n            remove_weight_norm(l)\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.get_padding","title":"<code>get_padding(kernel_size, dilation=1)</code>","text":"<p>Calculate padding for a convolution layer.</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>def get_padding(kernel_size, dilation=1):\n    \"\"\"Calculate padding for a convolution layer.\"\"\"\n    return int((kernel_size * dilation - dilation) / 2)\n</code></pre>"},{"location":"reference/module/vocoder/hifigan/#module.vocoder.hifigan.load_hifigan_vocoder","title":"<code>load_hifigan_vocoder(checkpoint_path=None, model_name='speechbrain/tts-hifigan-ljspeech', device=None)</code>","text":"<p>Load HiFi-GAN vocoder.</p> <p>This function provides a simple interface to load the vocoder either from a local checkpoint or from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <p>Path to local checkpoint (optional)</p> <code>None</code> <code>model_name</code> <p>HuggingFace model name for download</p> <code>'speechbrain/tts-hifigan-ljspeech'</code> <code>device</code> <p>Device to load model onto</p> <code>None</code> <p>Returns:</p> Type Description <p>Loaded HiFiGAN model</p> Source code in <code>speechain/module/vocoder/hifigan.py</code> <pre><code>def load_hifigan_vocoder(\n    checkpoint_path=None,\n    model_name=\"speechbrain/tts-hifigan-ljspeech\",\n    device=None,\n):\n    \"\"\"\n    Load HiFi-GAN vocoder.\n\n    This function provides a simple interface to load the vocoder\n    either from a local checkpoint or from HuggingFace Hub.\n\n    Args:\n        checkpoint_path: Path to local checkpoint (optional)\n        model_name: HuggingFace model name for download\n        device: Device to load model onto\n\n    Returns:\n        Loaded HiFiGAN model\n    \"\"\"\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        print(f\"Loading HiFi-GAN from local path: {checkpoint_path}\")\n        model = HiFiGAN(**HIFIGAN_DEFAULT_CONFIG)\n        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n\n        if \"model\" in checkpoint:\n            state_dict = checkpoint[\"model\"]\n        elif \"generator\" in checkpoint:\n            state_dict = checkpoint[\"generator\"]\n        else:\n            state_dict = checkpoint\n\n        # Remap keys from SpeechBrain format\n        new_state_dict = {}\n        for key, value in state_dict.items():\n            new_key = key.replace(\".conv.\", \".\")\n            new_state_dict[new_key] = value\n\n        try:\n            model.load_state_dict(new_state_dict, strict=True)\n            print(\"Loaded HiFi-GAN checkpoint successfully from local path\")\n        except RuntimeError as e:\n            print(f\"Warning: Strict loading from local path failed: {e}\")\n            # Try with strict=False as fallback\n            model.load_state_dict(new_state_dict, strict=False)\n            print(\"Loaded HiFi-GAN checkpoint from local path with strict=False\")\n\n        if device is not None:\n            model = model.to(device)\n\n        model.eval()\n        return model\n    else:\n        return HiFiGAN.from_pretrained(model_name, device=device)\n</code></pre>"},{"location":"reference/module/vocoder/test_hifigan/","title":"test_hifigan","text":""},{"location":"reference/optim_sche/","title":"optim_sche","text":""},{"location":"reference/optim_sche/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler","title":"<code>OptimScheduler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>OptimScheduler is the base class of all OptimScheduler objects that combine the roles of traditional optimizers and schedulers together. Its main job is optimizing the target model parameters and scheduling the learning rate during training.</p> <p>In this toolkit, we combine traditional optimizers and schedulers into a single class: OptimScheduler. Each OptimScheduler object has one built-in member optimizer (torch.optim.Optimizer) which is initialized automatically by <code>optim_type</code> and <code>optim_conf</code> given in your configuration.</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>class OptimScheduler(ABC):\n    \"\"\"OptimScheduler is the base class of all OptimScheduler objects that combine the\n    roles of traditional optimizers and schedulers together. Its main job is optimizing\n    the target model parameters and scheduling the learning rate during training.\n\n    In this toolkit, we combine traditional optimizers and schedulers into a single class: OptimScheduler. Each\n    OptimScheduler object has one built-in member optimizer (torch.optim.Optimizer) which is initialized automatically\n    by `optim_type` and `optim_conf` given in your configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        optim_type: str,\n        optim_conf: Dict[str, Any],\n        model: Model,\n        distributed: bool = False,\n        optim_loss: str = None,\n        updated_modules: List[str] = None,\n        step_per_update: int = 1,\n        use_amp: bool = True,\n        accum_grad: int = 1,\n        ft_factor: float = 1.0,\n        grad_clip: float = 1.0,\n        grad_norm_type: float = 2.0,\n        **sche_conf,\n    ):\n        \"\"\"This initialization function initializes the general part shared by all\n        OptimScheduler subclasses. At the end of this function, an interface function\n        `sche_init()` is called to initialize the customized part of each OptimScheduler\n        subclass.\n\n        Args:\n            # --- Arguments received from exp_cfg --- #\n            model: speechain.model.abs.Model\n                The pointer to the model whose parameters will be optimized by the built-in `torch.optim.Optimizer`.\n            distributed: bool = False\n                Whether the model to be optimized is distributed to multiple GPUs.\n                If True, gradient accumulation will be done asynchronously in the DDP mode to speed up training.\n            use_amp: bool = True\n                Whether the Automatic Mixed Precision (AMP) technique is used during back-propagation.\n                If True, a built-in `torch.cuda.amp.GradScaler` will be initialized to calculate the gradients.\n            accum_grad: int = 1\n                The number of steps to accumulate gradients before optimization.\n                The larger this argument is, the larger your virtual batches will be.\n            ft_factor: float = 1.0\n                The finetuning factor used to scale down the learning rates during training.\n            # --- Arguments received from train_cfg --- #\n            optim_type: str\n                The optimizer query used to pick up the target Optimizer subclass from `torch.optim`\n            optim_conf: Dict\n                The optimizer configuration used to initialize the optimizer\n            optim_loss: str = None\n                The name of the target loss used in this _OptimScheduler_ object to calculate the gradients.\n                If not given, the loss named `loss` will be used for optimization.\n            updated_modules: str or List[str] = None\n                This argument allows you to update only a part of parameters of the built-in model pointer.\n                `updated_modules` indicate the names of your target modules (first-level module in the nested module\n                tree) in the built-in model pointer.\n                Its value can be either a string (only one target module) or a list (multiple target modules).\n                If not given, the entire model will be updated.\n            step_per_update: int = 1\n                The optimization interval for the built-in optimizer.\n                It means that the parameter optimization will be done once every `step_per_update` steps.\n            **sche_conf:\n                The arguments used to initialize the customized part of this OptimScheduler.\n                Mainly used to decide the learning rate scheduling strategy.\n        \"\"\"\n        # initialize the general part of the scheduler\n        assert (isinstance(accum_grad, int) and accum_grad &gt;= 1) and (\n            isinstance(step_per_update, int) and step_per_update &gt;= 1\n        ), (\n            f\"Both of accum_grad and step_per_update should be an integer equal to or larger than 1, \"\n            f\"but got accum_grad={accum_grad} and step_per_update={step_per_update}.\"\n        )\n        self.model = model\n        self.distributed = distributed\n\n        # gradient-related arguments (loaded from exp_cfg)\n        self.accum_grad = accum_grad\n        self.grad_clip = grad_clip\n        self.grad_norm_type = grad_norm_type\n        self.ft_factor = ft_factor\n\n        # optimization-related arguments (loaded from train_cfg)\n        assert isinstance(optim_loss, str) or optim_loss is None, (\n            \"Your input optim_loss must be a single string or None! If it's not given, the loss named 'loss' will be \"\n            \"used for optimization; If it's given as a string, the loss whose name matches your given string will be \"\n            \"used for optimization.\"\n        )\n        self.optim_loss = optim_loss\n        self.step_per_update = step_per_update\n\n        # specific parameters are updated\n        if updated_modules is not None:\n            self.updated_modules = (\n                updated_modules\n                if isinstance(updated_modules, List)\n                else [updated_modules]\n            )\n            _updated_modules = [\n                self.model.__getattr__(module).parameters()\n                for module in self.updated_modules\n            ]\n            params = itertools.chain(*_updated_modules)\n        # all parameters of the model are returned\n        else:\n            self.updated_modules = None\n            params = self.model.parameters()\n\n        # initialize the optimizer part\n        optim_class = import_class(\"torch.optim.\" + optim_type)\n        self.optimizer = optim_class(params=params, **optim_conf)\n\n        # Initialize the gradient scaler for AMP training\n        self.scaler = GradScaler() if use_amp else None\n\n        # initialize the customized part of the scheduler\n        self.sche_init(**sche_conf)\n\n    @abstractmethod\n    def sche_init(self, **sche_conf):\n        \"\"\"This abstract interface function is the customized initialization function\n        which decides how the learning rate is scheduled as the training goes. This\n        interface is mandatory to be overridden.\n\n        Args:\n            **sche_conf: Dict\n                The arguments used to initialize the customized part of this OptimScheduler.\n                For more details about the learning rate scheduling strategy, please refer to the docstring of\n                `sche_init()` of your target OptimScheduler subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def step(\n        self,\n        losses: Dict[str, torch.Tensor],\n        time_func,\n        optim_name: str,\n        step_num: int,\n        epoch_num: int,\n        logger=None,\n    ):\n        \"\"\"This function optimizes the target parameters of the built-in model pointer\n        with the input training losses.\n\n        Args:\n            losses: Dict[str, torch.Tensor]\n                The training loss Dict received from the `criterion_forward()` of the bulit-in model pointer.\n            time_func:\n                The context function used to record the consumed time during gradient back-propagation and parameter\n                optimization.\n            optim_name: str\n                The name of the OptimScheduler object. This argument is used to identify the recorded consumed time\n                information.\n            step_num: int\n                The number of the current training step.\n                This argument is used to update the learning rate for the current step by `self.update_lr()`.\n            logger:\n                Lazily passed logger object. Used to record logging information during optimization.\n        \"\"\"\n        # --- 0. Initial Preparation Part --- #\n        # get the real step number based on accum_grad\n        real_step = (step_num - 1) // self.accum_grad + 1\n\n        # context function used when doing the loss backward for efficient gradient accumulation in the DDP mode\n        backward_context = (\n            self.model.no_sync\n            if self.distributed and step_num % self.accum_grad != 0\n            else nullcontext\n        )\n\n        # --- 1. Loss Backward Part --- #\n        with time_func([\"loss_backward_time\", optim_name]):\n            # back-propagate the loss only when the real step number meets the updating interval\n            if real_step % self.step_per_update == 0:\n                # pick up the target training loss\n                if self.optim_loss is None:\n                    assert \"loss\" in losses.keys(), (\n                        \"In this toolkit when optim_loss is set to None, the optimizer will automatically optimize \"\n                        \"the input loss named 'loss'. Therefore, please name one training loss in the returned Dict \"\n                        \"of your criterion_forward() implementation as 'loss'.\"\n                    )\n                    loss = losses[\"loss\"]\n                else:\n                    loss = losses[self.optim_loss]\n\n                with backward_context():\n                    # average the loss for accumulation\n                    loss /= self.accum_grad\n                    # backward the loss in either the amp mode or the normal mode\n                    (\n                        self.scaler.scale(loss).backward()\n                        if self.scaler is not None\n                        else loss.backward()\n                    )\n\n        # --- 2. Model Optimization Part --- #\n        with time_func([\"optim_time\", optim_name]):\n            # do optimization only when the real step number meets the updating interval\n            if real_step % self.step_per_update == 0:\n                # update the learning rate for the current step (scaled by the finetuning factor)\n                curr_lr = self.update_lr(real_step=real_step, epoch_num=epoch_num)\n                for param_group in self.optimizer.param_groups:\n                    param_group[\"lr\"] = self.ft_factor * curr_lr\n\n                # update the model parameters if the accumulation interval is met\n                if step_num % self.accum_grad == 0:\n                    # unscale the gradients in advance to enable gradient clipping in the amp setting\n                    # refer: https://pytorch.org/docs/1.10/notes/amp_examples.html#working-with-unscaled-gradients\n                    if self.scaler is not None:\n                        self.scaler.unscale_(self.optimizer)\n\n                    # apply the gradient clipping right before updating the target parameters\n                    grad_norm = torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(),\n                        max_norm=self.grad_clip,\n                        norm_type=self.grad_norm_type,\n                    )\n\n                    # optimize the target parameters only when the values of gradients are not infinite\n                    if not torch.isfinite(grad_norm):\n                        if logger is not None:\n                            logger.info(\n                                f\"The grad_norm in the no.{real_step} real step is infinite! \"\n                                \"So, the parameters are not updated in this step.\"\n                            )\n                        if self.scaler is not None:\n                            self.scaler.step(self.optimizer)\n                            self.scaler.update()\n                    else:\n                        if self.scaler is not None:\n                            self.scaler.step(self.optimizer)\n                            self.scaler.update()\n                        else:\n                            self.optimizer.step()\n\n                    # Turn the gradients of the target parameters of this optimizer to zero right after optimization\n                    self.optimizer.zero_grad()\n\n    @abstractmethod\n    def update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n        \"\"\"This abstract interface function generates the learning rate by the input\n        step number.\n\n        Args:\n            real_step: int\n                The number of the real step for parameter optimization. Due to the existence of `self.accum_grad`,\n                parameter optimization may not be done at each training step. The real step number here means the\n                training steps where parameter optimization is done.\n\n        Returns: float\n            The learning rate used for parameter optimization in the current training step.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_lr(self):\n        \"\"\"This function returns the current learning rate of the built-in\n        `torch.optim.Optimizer` member.\n\n        Returns: float\n            The value of the learning rates obtained from `self.optimizer.param_groups`.\n        \"\"\"\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    def state_dict(self) -&gt; Dict:\n        \"\"\"This function returns the current status of the OptimScheduler object for\n        checkpoint storage.\n\n        Returns: Dict\n            The status Dict containing the current status of the built-in `torch.optim.Optimizer` and the built-in\n            `torch.cuda.amp.GradScaler` (if had).\n        \"\"\"\n        return dict(\n            optimizer=self.optimizer.state_dict(),\n            scaler=self.scaler.state_dict() if self.scaler is not None else None,\n        )\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"This function loads the existing checkpoint information into the\n        _OptimScheduler_ object as the starting status.\n\n        Args:\n            state_dict: Dict\n                The status information loaded from the existing checkpoint.\n        \"\"\"\n        # load the optimizer\n        self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n        # load the gradient scaler\n        if state_dict[\"scaler\"] is not None:\n            self.scaler.load_state_dict(state_dict[\"scaler\"])\n\n    def __repr__(self):\n        \"\"\"This function returns the description string of the _OptimScheduler_ object.\n        There is a general description part shared by all the _OptimScheduler_\n        subclasses.\n\n        In this function, an interface hook function `extra_repr_fn()` will be called to generate the specific\n        description part of each _OptimScheduler_ subclass.\n\n        Returns: str\n            The description string for the OptimScheduler object.\n        \"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"optimizer={self.optimizer.__class__.__name__}, \"\n            f\"optim_loss={self.optim_loss}, \"\n            f\"updated_modules={self.updated_modules}, \" + self.extra_repr_fn() + \")\"\n        )\n\n    def extra_repr_fn(self) -&gt; str:\n        \"\"\"This interface hook function returns the specific part of the description\n        string of the OptimScheduler object. The original implementation in the base\n        class returns an empty string.\n\n        In principle, this interface hook function must be overridden by each _OptimScheduler_ subclass.\n        But there won't be any errors if you don't override it in your implementation.\n\n        Returns: str\n            The specific part of the description string of the OptimScheduler object.\n        \"\"\"\n        return \"\"\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.__init__","title":"<code>__init__(optim_type, optim_conf, model, distributed=False, optim_loss=None, updated_modules=None, step_per_update=1, use_amp=True, accum_grad=1, ft_factor=1.0, grad_clip=1.0, grad_norm_type=2.0, **sche_conf)</code>","text":"<p>This initialization function initializes the general part shared by all OptimScheduler subclasses. At the end of this function, an interface function <code>sche_init()</code> is called to initialize the customized part of each OptimScheduler subclass.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>speechain.model.abs.Model The pointer to the model whose parameters will be optimized by the built-in <code>torch.optim.Optimizer</code>.</p> required <code>distributed</code> <code>bool</code> <p>bool = False Whether the model to be optimized is distributed to multiple GPUs. If True, gradient accumulation will be done asynchronously in the DDP mode to speed up training.</p> <code>False</code> <code>use_amp</code> <code>bool</code> <p>bool = True Whether the Automatic Mixed Precision (AMP) technique is used during back-propagation. If True, a built-in <code>torch.cuda.amp.GradScaler</code> will be initialized to calculate the gradients.</p> <code>True</code> <code>accum_grad</code> <code>int</code> <p>int = 1 The number of steps to accumulate gradients before optimization. The larger this argument is, the larger your virtual batches will be.</p> <code>1</code> <code>ft_factor</code> <code>float</code> <p>float = 1.0 The finetuning factor used to scale down the learning rates during training.</p> <code>1.0</code> <code>optim_type</code> <code>str</code> <p>str The optimizer query used to pick up the target Optimizer subclass from <code>torch.optim</code></p> required <code>optim_conf</code> <code>Dict[str, Any]</code> <p>Dict The optimizer configuration used to initialize the optimizer</p> required <code>optim_loss</code> <code>str</code> <p>str = None The name of the target loss used in this OptimScheduler object to calculate the gradients. If not given, the loss named <code>loss</code> will be used for optimization.</p> <code>None</code> <code>updated_modules</code> <code>List[str]</code> <p>str or List[str] = None This argument allows you to update only a part of parameters of the built-in model pointer. <code>updated_modules</code> indicate the names of your target modules (first-level module in the nested module tree) in the built-in model pointer. Its value can be either a string (only one target module) or a list (multiple target modules). If not given, the entire model will be updated.</p> <code>None</code> <code>step_per_update</code> <code>int</code> <p>int = 1 The optimization interval for the built-in optimizer. It means that the parameter optimization will be done once every <code>step_per_update</code> steps.</p> <code>1</code> <code>**sche_conf</code> <p>The arguments used to initialize the customized part of this OptimScheduler. Mainly used to decide the learning rate scheduling strategy.</p> <code>{}</code> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def __init__(\n    self,\n    optim_type: str,\n    optim_conf: Dict[str, Any],\n    model: Model,\n    distributed: bool = False,\n    optim_loss: str = None,\n    updated_modules: List[str] = None,\n    step_per_update: int = 1,\n    use_amp: bool = True,\n    accum_grad: int = 1,\n    ft_factor: float = 1.0,\n    grad_clip: float = 1.0,\n    grad_norm_type: float = 2.0,\n    **sche_conf,\n):\n    \"\"\"This initialization function initializes the general part shared by all\n    OptimScheduler subclasses. At the end of this function, an interface function\n    `sche_init()` is called to initialize the customized part of each OptimScheduler\n    subclass.\n\n    Args:\n        # --- Arguments received from exp_cfg --- #\n        model: speechain.model.abs.Model\n            The pointer to the model whose parameters will be optimized by the built-in `torch.optim.Optimizer`.\n        distributed: bool = False\n            Whether the model to be optimized is distributed to multiple GPUs.\n            If True, gradient accumulation will be done asynchronously in the DDP mode to speed up training.\n        use_amp: bool = True\n            Whether the Automatic Mixed Precision (AMP) technique is used during back-propagation.\n            If True, a built-in `torch.cuda.amp.GradScaler` will be initialized to calculate the gradients.\n        accum_grad: int = 1\n            The number of steps to accumulate gradients before optimization.\n            The larger this argument is, the larger your virtual batches will be.\n        ft_factor: float = 1.0\n            The finetuning factor used to scale down the learning rates during training.\n        # --- Arguments received from train_cfg --- #\n        optim_type: str\n            The optimizer query used to pick up the target Optimizer subclass from `torch.optim`\n        optim_conf: Dict\n            The optimizer configuration used to initialize the optimizer\n        optim_loss: str = None\n            The name of the target loss used in this _OptimScheduler_ object to calculate the gradients.\n            If not given, the loss named `loss` will be used for optimization.\n        updated_modules: str or List[str] = None\n            This argument allows you to update only a part of parameters of the built-in model pointer.\n            `updated_modules` indicate the names of your target modules (first-level module in the nested module\n            tree) in the built-in model pointer.\n            Its value can be either a string (only one target module) or a list (multiple target modules).\n            If not given, the entire model will be updated.\n        step_per_update: int = 1\n            The optimization interval for the built-in optimizer.\n            It means that the parameter optimization will be done once every `step_per_update` steps.\n        **sche_conf:\n            The arguments used to initialize the customized part of this OptimScheduler.\n            Mainly used to decide the learning rate scheduling strategy.\n    \"\"\"\n    # initialize the general part of the scheduler\n    assert (isinstance(accum_grad, int) and accum_grad &gt;= 1) and (\n        isinstance(step_per_update, int) and step_per_update &gt;= 1\n    ), (\n        f\"Both of accum_grad and step_per_update should be an integer equal to or larger than 1, \"\n        f\"but got accum_grad={accum_grad} and step_per_update={step_per_update}.\"\n    )\n    self.model = model\n    self.distributed = distributed\n\n    # gradient-related arguments (loaded from exp_cfg)\n    self.accum_grad = accum_grad\n    self.grad_clip = grad_clip\n    self.grad_norm_type = grad_norm_type\n    self.ft_factor = ft_factor\n\n    # optimization-related arguments (loaded from train_cfg)\n    assert isinstance(optim_loss, str) or optim_loss is None, (\n        \"Your input optim_loss must be a single string or None! If it's not given, the loss named 'loss' will be \"\n        \"used for optimization; If it's given as a string, the loss whose name matches your given string will be \"\n        \"used for optimization.\"\n    )\n    self.optim_loss = optim_loss\n    self.step_per_update = step_per_update\n\n    # specific parameters are updated\n    if updated_modules is not None:\n        self.updated_modules = (\n            updated_modules\n            if isinstance(updated_modules, List)\n            else [updated_modules]\n        )\n        _updated_modules = [\n            self.model.__getattr__(module).parameters()\n            for module in self.updated_modules\n        ]\n        params = itertools.chain(*_updated_modules)\n    # all parameters of the model are returned\n    else:\n        self.updated_modules = None\n        params = self.model.parameters()\n\n    # initialize the optimizer part\n    optim_class = import_class(\"torch.optim.\" + optim_type)\n    self.optimizer = optim_class(params=params, **optim_conf)\n\n    # Initialize the gradient scaler for AMP training\n    self.scaler = GradScaler() if use_amp else None\n\n    # initialize the customized part of the scheduler\n    self.sche_init(**sche_conf)\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.__repr__","title":"<code>__repr__()</code>","text":"<p>This function returns the description string of the OptimScheduler object. There is a general description part shared by all the OptimScheduler subclasses.</p> <p>In this function, an interface hook function <code>extra_repr_fn()</code> will be called to generate the specific description part of each OptimScheduler subclass.</p> <p>str</p> Type Description <p>The description string for the OptimScheduler object.</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def __repr__(self):\n    \"\"\"This function returns the description string of the _OptimScheduler_ object.\n    There is a general description part shared by all the _OptimScheduler_\n    subclasses.\n\n    In this function, an interface hook function `extra_repr_fn()` will be called to generate the specific\n    description part of each _OptimScheduler_ subclass.\n\n    Returns: str\n        The description string for the OptimScheduler object.\n    \"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"optimizer={self.optimizer.__class__.__name__}, \"\n        f\"optim_loss={self.optim_loss}, \"\n        f\"updated_modules={self.updated_modules}, \" + self.extra_repr_fn() + \")\"\n    )\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.extra_repr_fn","title":"<code>extra_repr_fn()</code>","text":"<p>This interface hook function returns the specific part of the description string of the OptimScheduler object. The original implementation in the base class returns an empty string.</p> <p>In principle, this interface hook function must be overridden by each OptimScheduler subclass. But there won't be any errors if you don't override it in your implementation.</p> <p>str</p> Type Description <code>str</code> <p>The specific part of the description string of the OptimScheduler object.</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def extra_repr_fn(self) -&gt; str:\n    \"\"\"This interface hook function returns the specific part of the description\n    string of the OptimScheduler object. The original implementation in the base\n    class returns an empty string.\n\n    In principle, this interface hook function must be overridden by each _OptimScheduler_ subclass.\n    But there won't be any errors if you don't override it in your implementation.\n\n    Returns: str\n        The specific part of the description string of the OptimScheduler object.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.get_lr","title":"<code>get_lr()</code>","text":"<p>This function returns the current learning rate of the built-in <code>torch.optim.Optimizer</code> member.</p> <p>float</p> Type Description <p>The value of the learning rates obtained from <code>self.optimizer.param_groups</code>.</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def get_lr(self):\n    \"\"\"This function returns the current learning rate of the built-in\n    `torch.optim.Optimizer` member.\n\n    Returns: float\n        The value of the learning rates obtained from `self.optimizer.param_groups`.\n    \"\"\"\n    return self.optimizer.param_groups[0][\"lr\"]\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>This function loads the existing checkpoint information into the OptimScheduler object as the starting status.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>Dict The status information loaded from the existing checkpoint.</p> required Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]):\n    \"\"\"This function loads the existing checkpoint information into the\n    _OptimScheduler_ object as the starting status.\n\n    Args:\n        state_dict: Dict\n            The status information loaded from the existing checkpoint.\n    \"\"\"\n    # load the optimizer\n    self.optimizer.load_state_dict(state_dict[\"optimizer\"])\n    # load the gradient scaler\n    if state_dict[\"scaler\"] is not None:\n        self.scaler.load_state_dict(state_dict[\"scaler\"])\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.sche_init","title":"<code>sche_init(**sche_conf)</code>  <code>abstractmethod</code>","text":"<p>This abstract interface function is the customized initialization function which decides how the learning rate is scheduled as the training goes. This interface is mandatory to be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>**sche_conf</code> <p>Dict The arguments used to initialize the customized part of this OptimScheduler. For more details about the learning rate scheduling strategy, please refer to the docstring of <code>sche_init()</code> of your target OptimScheduler subclass.</p> <code>{}</code> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>@abstractmethod\ndef sche_init(self, **sche_conf):\n    \"\"\"This abstract interface function is the customized initialization function\n    which decides how the learning rate is scheduled as the training goes. This\n    interface is mandatory to be overridden.\n\n    Args:\n        **sche_conf: Dict\n            The arguments used to initialize the customized part of this OptimScheduler.\n            For more details about the learning rate scheduling strategy, please refer to the docstring of\n            `sche_init()` of your target OptimScheduler subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.state_dict","title":"<code>state_dict()</code>","text":"<p>This function returns the current status of the OptimScheduler object for checkpoint storage.</p> <p>Dict</p> Type Description <code>Dict</code> <p>The status Dict containing the current status of the built-in <code>torch.optim.Optimizer</code> and the built-in</p> <code>Dict</code> <p><code>torch.cuda.amp.GradScaler</code> (if had).</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def state_dict(self) -&gt; Dict:\n    \"\"\"This function returns the current status of the OptimScheduler object for\n    checkpoint storage.\n\n    Returns: Dict\n        The status Dict containing the current status of the built-in `torch.optim.Optimizer` and the built-in\n        `torch.cuda.amp.GradScaler` (if had).\n    \"\"\"\n    return dict(\n        optimizer=self.optimizer.state_dict(),\n        scaler=self.scaler.state_dict() if self.scaler is not None else None,\n    )\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.step","title":"<code>step(losses, time_func, optim_name, step_num, epoch_num, logger=None)</code>","text":"<p>This function optimizes the target parameters of the built-in model pointer with the input training losses.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor] The training loss Dict received from the <code>criterion_forward()</code> of the bulit-in model pointer.</p> required <code>time_func</code> <p>The context function used to record the consumed time during gradient back-propagation and parameter optimization.</p> required <code>optim_name</code> <code>str</code> <p>str The name of the OptimScheduler object. This argument is used to identify the recorded consumed time information.</p> required <code>step_num</code> <code>int</code> <p>int The number of the current training step. This argument is used to update the learning rate for the current step by <code>self.update_lr()</code>.</p> required <code>logger</code> <p>Lazily passed logger object. Used to record logging information during optimization.</p> <code>None</code> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>def step(\n    self,\n    losses: Dict[str, torch.Tensor],\n    time_func,\n    optim_name: str,\n    step_num: int,\n    epoch_num: int,\n    logger=None,\n):\n    \"\"\"This function optimizes the target parameters of the built-in model pointer\n    with the input training losses.\n\n    Args:\n        losses: Dict[str, torch.Tensor]\n            The training loss Dict received from the `criterion_forward()` of the bulit-in model pointer.\n        time_func:\n            The context function used to record the consumed time during gradient back-propagation and parameter\n            optimization.\n        optim_name: str\n            The name of the OptimScheduler object. This argument is used to identify the recorded consumed time\n            information.\n        step_num: int\n            The number of the current training step.\n            This argument is used to update the learning rate for the current step by `self.update_lr()`.\n        logger:\n            Lazily passed logger object. Used to record logging information during optimization.\n    \"\"\"\n    # --- 0. Initial Preparation Part --- #\n    # get the real step number based on accum_grad\n    real_step = (step_num - 1) // self.accum_grad + 1\n\n    # context function used when doing the loss backward for efficient gradient accumulation in the DDP mode\n    backward_context = (\n        self.model.no_sync\n        if self.distributed and step_num % self.accum_grad != 0\n        else nullcontext\n    )\n\n    # --- 1. Loss Backward Part --- #\n    with time_func([\"loss_backward_time\", optim_name]):\n        # back-propagate the loss only when the real step number meets the updating interval\n        if real_step % self.step_per_update == 0:\n            # pick up the target training loss\n            if self.optim_loss is None:\n                assert \"loss\" in losses.keys(), (\n                    \"In this toolkit when optim_loss is set to None, the optimizer will automatically optimize \"\n                    \"the input loss named 'loss'. Therefore, please name one training loss in the returned Dict \"\n                    \"of your criterion_forward() implementation as 'loss'.\"\n                )\n                loss = losses[\"loss\"]\n            else:\n                loss = losses[self.optim_loss]\n\n            with backward_context():\n                # average the loss for accumulation\n                loss /= self.accum_grad\n                # backward the loss in either the amp mode or the normal mode\n                (\n                    self.scaler.scale(loss).backward()\n                    if self.scaler is not None\n                    else loss.backward()\n                )\n\n    # --- 2. Model Optimization Part --- #\n    with time_func([\"optim_time\", optim_name]):\n        # do optimization only when the real step number meets the updating interval\n        if real_step % self.step_per_update == 0:\n            # update the learning rate for the current step (scaled by the finetuning factor)\n            curr_lr = self.update_lr(real_step=real_step, epoch_num=epoch_num)\n            for param_group in self.optimizer.param_groups:\n                param_group[\"lr\"] = self.ft_factor * curr_lr\n\n            # update the model parameters if the accumulation interval is met\n            if step_num % self.accum_grad == 0:\n                # unscale the gradients in advance to enable gradient clipping in the amp setting\n                # refer: https://pytorch.org/docs/1.10/notes/amp_examples.html#working-with-unscaled-gradients\n                if self.scaler is not None:\n                    self.scaler.unscale_(self.optimizer)\n\n                # apply the gradient clipping right before updating the target parameters\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(),\n                    max_norm=self.grad_clip,\n                    norm_type=self.grad_norm_type,\n                )\n\n                # optimize the target parameters only when the values of gradients are not infinite\n                if not torch.isfinite(grad_norm):\n                    if logger is not None:\n                        logger.info(\n                            f\"The grad_norm in the no.{real_step} real step is infinite! \"\n                            \"So, the parameters are not updated in this step.\"\n                        )\n                    if self.scaler is not None:\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                else:\n                    if self.scaler is not None:\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                    else:\n                        self.optimizer.step()\n\n                # Turn the gradients of the target parameters of this optimizer to zero right after optimization\n                self.optimizer.zero_grad()\n</code></pre>"},{"location":"reference/optim_sche/abs/#optim_sche.abs.OptimScheduler.update_lr","title":"<code>update_lr(real_step, epoch_num)</code>  <code>abstractmethod</code>","text":"<p>This abstract interface function generates the learning rate by the input step number.</p> <p>Parameters:</p> Name Type Description Default <code>real_step</code> <code>int</code> <p>int The number of the real step for parameter optimization. Due to the existence of <code>self.accum_grad</code>, parameter optimization may not be done at each training step. The real step number here means the training steps where parameter optimization is done.</p> required <p>float</p> Type Description <code>float</code> <p>The learning rate used for parameter optimization in the current training step.</p> Source code in <code>speechain/optim_sche/abs.py</code> <pre><code>@abstractmethod\ndef update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n    \"\"\"This abstract interface function generates the learning rate by the input\n    step number.\n\n    Args:\n        real_step: int\n            The number of the real step for parameter optimization. Due to the existence of `self.accum_grad`,\n            parameter optimization may not be done at each training step. The real step number here means the\n            training steps where parameter optimization is done.\n\n    Returns: float\n        The learning rate used for parameter optimization in the current training step.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optim_sche/exp/","title":"exp","text":""},{"location":"reference/optim_sche/exp/#optim_sche.exp.ExponentDecayLr","title":"<code>ExponentDecayLr</code>","text":"<p>               Bases: <code>OptimScheduler</code></p> <p>ExponentDecayLr is a class that inherits from OptimScheduler and implements an exponential decay learning rate scheduler.</p> <p>It updates the learning rate based on the epoch number using the provided decay factor.</p> Source code in <code>speechain/optim_sche/exp.py</code> <pre><code>class ExponentDecayLr(OptimScheduler):\n    \"\"\"ExponentDecayLr is a class that inherits from OptimScheduler and implements an\n    exponential decay learning rate scheduler.\n\n    It updates the learning rate based on the epoch number using the provided decay\n    factor.\n    \"\"\"\n\n    def sche_init(self, decay_factor: float = 0.999):\n        \"\"\"Initializes the exponential decay learning rate scheduler with the given\n        decay factor.\n\n        Args:\n            decay_factor (float):\n                The decay factor that will be used to exponentially decay the learning rate. Defaults to 0.999.\n        \"\"\"\n        self.decay_factor = decay_factor\n\n    def update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n        \"\"\"Updates the learning rate based on the current epoch number and the decay\n        factor.\n\n        Args:\n            real_step (int):\n                The current step in the optimization process.\n                Not used in this implementation, but required for compatibility with the base class.\n            epoch_num (int):\n                The current epoch number.\n\n        Returns:\n            float: The updated learning rate.\n        \"\"\"\n        return self.get_lr() * pow(self.decay_factor, epoch_num - 1)\n\n    def extra_repr_fn(self) -&gt; str:\n        \"\"\"Returns a string representation of the ExponentDecayLr object, including the\n        decay factor.\n\n        Returns:\n            str: A string containing the class name and the decay factor value.\n        \"\"\"\n        return f\"decay_factor={self.decay_factor}\"\n</code></pre>"},{"location":"reference/optim_sche/exp/#optim_sche.exp.ExponentDecayLr.extra_repr_fn","title":"<code>extra_repr_fn()</code>","text":"<p>Returns a string representation of the ExponentDecayLr object, including the decay factor.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the class name and the decay factor value.</p> Source code in <code>speechain/optim_sche/exp.py</code> <pre><code>def extra_repr_fn(self) -&gt; str:\n    \"\"\"Returns a string representation of the ExponentDecayLr object, including the\n    decay factor.\n\n    Returns:\n        str: A string containing the class name and the decay factor value.\n    \"\"\"\n    return f\"decay_factor={self.decay_factor}\"\n</code></pre>"},{"location":"reference/optim_sche/exp/#optim_sche.exp.ExponentDecayLr.sche_init","title":"<code>sche_init(decay_factor=0.999)</code>","text":"<p>Initializes the exponential decay learning rate scheduler with the given decay factor.</p> <p>Parameters:</p> Name Type Description Default <code>decay_factor</code> <code>float</code> <p>The decay factor that will be used to exponentially decay the learning rate. Defaults to 0.999.</p> <code>0.999</code> Source code in <code>speechain/optim_sche/exp.py</code> <pre><code>def sche_init(self, decay_factor: float = 0.999):\n    \"\"\"Initializes the exponential decay learning rate scheduler with the given\n    decay factor.\n\n    Args:\n        decay_factor (float):\n            The decay factor that will be used to exponentially decay the learning rate. Defaults to 0.999.\n    \"\"\"\n    self.decay_factor = decay_factor\n</code></pre>"},{"location":"reference/optim_sche/exp/#optim_sche.exp.ExponentDecayLr.update_lr","title":"<code>update_lr(real_step, epoch_num)</code>","text":"<p>Updates the learning rate based on the current epoch number and the decay factor.</p> <p>Parameters:</p> Name Type Description Default <code>real_step</code> <code>int</code> <p>The current step in the optimization process. Not used in this implementation, but required for compatibility with the base class.</p> required <code>epoch_num</code> <code>int</code> <p>The current epoch number.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The updated learning rate.</p> Source code in <code>speechain/optim_sche/exp.py</code> <pre><code>def update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n    \"\"\"Updates the learning rate based on the current epoch number and the decay\n    factor.\n\n    Args:\n        real_step (int):\n            The current step in the optimization process.\n            Not used in this implementation, but required for compatibility with the base class.\n        epoch_num (int):\n            The current epoch number.\n\n    Returns:\n        float: The updated learning rate.\n    \"\"\"\n    return self.get_lr() * pow(self.decay_factor, epoch_num - 1)\n</code></pre>"},{"location":"reference/optim_sche/noam/","title":"noam","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/optim_sche/noam/#optim_sche.noam.Noamlr","title":"<code>Noamlr</code>","text":"<p>               Bases: <code>OptimScheduler</code></p> <p>The OptimScheduler where the scheduling contains a LR warmup stage and a LR decay stage. In the warmup stage, the learning rate increases linearly to the peak. In the decay stage, the learning rate decreases in the level of square root.</p> <p>This OptimScheduler is mainly used for Transformer-based models.</p> Source code in <code>speechain/optim_sche/noam.py</code> <pre><code>class Noamlr(OptimScheduler):\n    \"\"\"The OptimScheduler where the scheduling contains a LR warmup stage and a LR decay\n    stage. In the warmup stage, the learning rate increases linearly to the peak. In the\n    decay stage, the learning rate decreases in the level of square root.\n\n    This OptimScheduler is mainly used for Transformer-based models.\n    \"\"\"\n\n    def sche_init(self, d_model: int = None, warmup_steps: int = 4000):\n        \"\"\"The learning rate calculation is different depending on whether d_model is\n        given or not.\n\n        If d_model is given, the learning rate would be:\n            (d_model ** -0.5) * min(step ** -0.5, real_step * warmup_steps ** -1.5)\n        This calculation method is the original method proposed in 'Attention is all you need'.\n\n        If d_model is not given, the learning rate would be:\n            (optimizer.lr * warmup_steps ** 0.5) * min(real_step ** -0.5, step * warmup_steps ** -1.5)\n        This calculation method makes sure that the learning rate reaches the maximum (optimizer.lr) right after\n        all the warmup steps are finished.\n\n        Args:\n            d_model: int\n                The dimension of the hidden vectors of your Transformer model.\n            warmup_steps: int\n                The number of warming up steps.\n\n        Returns:\n            A list of names of your customized member variables.\n        \"\"\"\n        # para recording\n        self.d_model = d_model\n        self.init_lr = (\n            d_model**-0.5 if d_model is not None else self.get_lr() * warmup_steps**0.5\n        )\n        self.warmup_steps = warmup_steps\n\n    def update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n        \"\"\"\n\n        Args:\n            real_step: int\n                The number of the current training step.\n                Will be different from self.step_num when self.accum_grad is layer than 1.\n\n        \"\"\"\n        # the learning rate of the current step for the optimizer\n        return self.init_lr * min(\n            real_step**-0.5, real_step * (self.warmup_steps**-1.5)\n        )\n\n    def extra_repr_fn(self) -&gt; str:\n        return f\"d_model={self.d_model}, \" f\"warmup_steps={self.warmup_steps}\"\n</code></pre>"},{"location":"reference/optim_sche/noam/#optim_sche.noam.Noamlr.sche_init","title":"<code>sche_init(d_model=None, warmup_steps=4000)</code>","text":"<p>The learning rate calculation is different depending on whether d_model is given or not.</p> <p>If d_model is given, the learning rate would be:     (d_model ** -0.5) * min(step ** -0.5, real_step * warmup_steps ** -1.5) This calculation method is the original method proposed in 'Attention is all you need'.</p> <p>If d_model is not given, the learning rate would be:     (optimizer.lr * warmup_steps ** 0.5) * min(real_step ** -0.5, step * warmup_steps ** -1.5) This calculation method makes sure that the learning rate reaches the maximum (optimizer.lr) right after all the warmup steps are finished.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>int The dimension of the hidden vectors of your Transformer model.</p> <code>None</code> <code>warmup_steps</code> <code>int</code> <p>int The number of warming up steps.</p> <code>4000</code> <p>Returns:</p> Type Description <p>A list of names of your customized member variables.</p> Source code in <code>speechain/optim_sche/noam.py</code> <pre><code>def sche_init(self, d_model: int = None, warmup_steps: int = 4000):\n    \"\"\"The learning rate calculation is different depending on whether d_model is\n    given or not.\n\n    If d_model is given, the learning rate would be:\n        (d_model ** -0.5) * min(step ** -0.5, real_step * warmup_steps ** -1.5)\n    This calculation method is the original method proposed in 'Attention is all you need'.\n\n    If d_model is not given, the learning rate would be:\n        (optimizer.lr * warmup_steps ** 0.5) * min(real_step ** -0.5, step * warmup_steps ** -1.5)\n    This calculation method makes sure that the learning rate reaches the maximum (optimizer.lr) right after\n    all the warmup steps are finished.\n\n    Args:\n        d_model: int\n            The dimension of the hidden vectors of your Transformer model.\n        warmup_steps: int\n            The number of warming up steps.\n\n    Returns:\n        A list of names of your customized member variables.\n    \"\"\"\n    # para recording\n    self.d_model = d_model\n    self.init_lr = (\n        d_model**-0.5 if d_model is not None else self.get_lr() * warmup_steps**0.5\n    )\n    self.warmup_steps = warmup_steps\n</code></pre>"},{"location":"reference/optim_sche/noam/#optim_sche.noam.Noamlr.update_lr","title":"<code>update_lr(real_step, epoch_num)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>real_step</code> <code>int</code> <p>int The number of the current training step. Will be different from self.step_num when self.accum_grad is layer than 1.</p> required Source code in <code>speechain/optim_sche/noam.py</code> <pre><code>def update_lr(self, real_step: int, epoch_num: int) -&gt; float:\n    \"\"\"\n\n    Args:\n        real_step: int\n            The number of the current training step.\n            Will be different from self.step_num when self.accum_grad is layer than 1.\n\n    \"\"\"\n    # the learning rate of the current step for the optimizer\n    return self.init_lr * min(\n        real_step**-0.5, real_step * (self.warmup_steps**-1.5)\n    )\n</code></pre>"},{"location":"reference/pyscripts/","title":"pyscripts","text":""},{"location":"reference/pyscripts/empty_file_checker/","title":"empty_file_checker","text":""},{"location":"reference/pyscripts/folder_summarizer/","title":"folder_summarizer","text":""},{"location":"reference/pyscripts/model_para_renamer/","title":"model_para_renamer","text":""},{"location":"reference/pyscripts/phn_duaration_visualizer/","title":"phn_duaration_visualizer","text":""},{"location":"reference/pyscripts/text_dist_visualizer/","title":"text_dist_visualizer","text":""},{"location":"reference/pyscripts/wavlen_dist_visualizer/","title":"wavlen_dist_visualizer","text":""},{"location":"reference/tokenizer/","title":"tokenizer","text":""},{"location":"reference/tokenizer/abs/","title":"abs","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/tokenizer/abs/#tokenizer.abs.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Tokenizer is the base class of all the Tokenizer objects in this toolkit. It on-the-fly transforms text data between strings and tensors.</p> <p>For data storage and visualization, the text data should be in the form of strings which is not friendly for model forward calculation. For model forward calculation, the text data is better to be in the form of vectors (<code>torch.tensor</code> or <code>numpy.ndarray</code>).</p> Source code in <code>speechain/tokenizer/abs.py</code> <pre><code>class Tokenizer(ABC):\n    \"\"\"Tokenizer is the base class of all the _Tokenizer_ objects in this toolkit. It\n    on-the-fly transforms text data between strings and tensors.\n\n    For data storage and visualization, the text data should be in the form of strings which is not friendly for model\n    forward calculation. For model forward calculation, the text data is better to be in the form of vectors\n    (`torch.tensor` or `numpy.ndarray`).\n    \"\"\"\n\n    def __init__(self, token_path: str = None, copy_path: str = None, **tokenizer_conf):\n        \"\"\"\n        This function registers some shared member variables for all _Tokenizer_ subclasses:\n        1. `self.idx2token`: the mapping Dict from the token index to token string.\n        2. `self.token2idx`: the mapping Dict from the token string to token index.\n        3. `self.vocab_size`: the number of tokens in the given vocabulary.\n        4. `self.sos_eos_idx`: the index of the joint &lt;sos/eos&gt; token used as the beginning and end of a sentence.\n        5. `self.ignore_idx`: the index of the blank token used for either CTC blank modeling or ignored token for\n            encoder-decoder ASR&amp;TTS models.\n        6. `self.unk_idx`: the index of the unknown token.\n\n        Args:\n            token_path: str\n                The path where the token vocabulary is placed.\n            copy_path: str = None\n                The path where you want to paste the given token vocabulary as a backup.\n                If not given, no backup will be saved.\n            **tokenizer_conf:\n                The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization.\n        \"\"\"\n        # The vocab in token_path has the higher priority than the backup one in copy_path for vocabulary initialization\n        if token_path is not None:\n            token_vocab = os.path.join(parse_path_args(token_path), \"vocab\")\n\n        # if token_path is not given or vocab does not exist, use the backup one in copy_path\n        if token_path is None or not os.path.exists(token_vocab):\n            assert copy_path is not None, \"Please give copy_path for vocabulary backup!\"\n            token_vocab = os.path.join(parse_path_args(copy_path), \"token_vocab\")\n\n        # register token-related variables\n        self.idx2token = load_idx2data_file(token_vocab, do_separate=False)\n        self.token2idx = dict(map(reversed, self.idx2token.items()))\n        self.vocab_size = len(self.token2idx)\n        self.sos_eos_idx = self.token2idx[\"&lt;sos/eos&gt;\"]\n        self.ignore_idx = self.token2idx[\"&lt;blank&gt;\"]\n        self.unk_idx = self.token2idx[\"&lt;unk&gt;\"]\n        if \"&lt;space&gt;\" in self.token2idx.keys():\n            self.space_idx = self.token2idx[\"&lt;space&gt;\"]\n        else:\n            self.space_idx = None\n\n        # save the backup if copy_path is given\n        if copy_path is not None:\n            np.savetxt(\n                os.path.join(copy_path, \"token_vocab\"),\n                list(self.token2idx.keys()),\n                fmt=\"%s\",\n            )\n\n        # call the hook function for customized initialization\n        self.tokenizer_init_fn(\n            token_path=token_path, copy_path=copy_path, **tokenizer_conf\n        )\n\n    def tokenizer_init_fn(\n        self, token_path: str, copy_path: str = None, **tokenizer_conf\n    ):\n        \"\"\"This hook interface function initializes the customized part of a _Tokenizer_\n        subclass if had. This interface is not mandatory to be overridden.\n\n        Args:\n            copy_path: str = None\n                The path where you want to paste the given tokenizer model as a backup.\n                If not given, no backup will be saved.\n            **tokenizer_conf:\n                The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization.\n                For more details, please refer to the docstring of your target Tokenizer subclass.\n        \"\"\"\n        pass\n\n    def tensor2text(self, tensor: torch.LongTensor) -&gt; str:\n        \"\"\"This functions decodes a text tensor into a human-friendly string.\n\n        The default implementation transforms each token index in the input tensor to the token string by `\n        self.idx2token`. If the token index is `self.unk_idx`, an asterisk (*) will be used to represent an unknown\n        token in the string.\n\n        This interface is not mandatory to be overridden. If your _Tokenizer_ subclass uses some third-party packages\n        to decode the input tensor rather than the built-in `self.idx2token`, please override this function.\n\n        Args:\n            tensor: torch.LongTensor\n                1D integer torch.Tensor that contains the token indices of the sentence to be decoded.\n\n        Returns:\n            The string of the decoded sentence.\n        \"\"\"\n        token_list = []\n        for idx in tensor.tolist():\n            if idx in [self.sos_eos_idx, self.ignore_idx]:\n                continue\n            # the space tokens will be replaced by a blank\n            elif self.space_idx is not None and idx == self.space_idx:\n                token_list.append(\" \")\n            # the unknown tokens will be replaced by a star symbol '*'\n            elif idx == self.unk_idx:\n                token_list.append(\"*\")\n            else:\n                token_list.append(self.idx2token[idx])\n        return \"\".join(token_list)\n\n    @abstractmethod\n    def text2tensor(\n        self,\n        text: str,\n        no_sos: bool = False,\n        no_eos: bool = False,\n        return_tensor: bool = True,\n    ) -&gt; torch.LongTensor or List:\n        \"\"\"This functions encodes a text string into a model-friendly tensor. This\n        interface is mandatory to be overridden. By default, this function will attach\n        two &lt;sos/eos&gt; at the beginning and end of the returned token id sequence.\n\n        Args:\n            text: str\n                the input text string to be encoded\n            no_sos: bool = False\n                Whether to remove the &lt;sos/eos&gt; at the beginning of the token id sequence.\n            no_eos: bool = False\n                Whether to remove the &lt;sos/eos&gt; at the end of the token id sequence.\n            return_tensor: bool = True\n                Whether to return the tokenization results as a tensor. If False, a List will be returned.\n\n        Returns: torch.LongTensor\n            The tensor of the encoded sentence\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/tokenizer/abs/#tokenizer.abs.Tokenizer.__init__","title":"<code>__init__(token_path=None, copy_path=None, **tokenizer_conf)</code>","text":"<p>This function registers some shared member variables for all Tokenizer subclasses: 1. <code>self.idx2token</code>: the mapping Dict from the token index to token string. 2. <code>self.token2idx</code>: the mapping Dict from the token string to token index. 3. <code>self.vocab_size</code>: the number of tokens in the given vocabulary. 4. <code>self.sos_eos_idx</code>: the index of the joint  token used as the beginning and end of a sentence. 5. <code>self.ignore_idx</code>: the index of the blank token used for either CTC blank modeling or ignored token for     encoder-decoder ASR&amp;TTS models. 6. <code>self.unk_idx</code>: the index of the unknown token. <p>Parameters:</p> Name Type Description Default <code>token_path</code> <code>str</code> <p>str The path where the token vocabulary is placed.</p> <code>None</code> <code>copy_path</code> <code>str</code> <p>str = None The path where you want to paste the given token vocabulary as a backup. If not given, no backup will be saved.</p> <code>None</code> <code>**tokenizer_conf</code> <p>The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization.</p> <code>{}</code> Source code in <code>speechain/tokenizer/abs.py</code> <pre><code>def __init__(self, token_path: str = None, copy_path: str = None, **tokenizer_conf):\n    \"\"\"\n    This function registers some shared member variables for all _Tokenizer_ subclasses:\n    1. `self.idx2token`: the mapping Dict from the token index to token string.\n    2. `self.token2idx`: the mapping Dict from the token string to token index.\n    3. `self.vocab_size`: the number of tokens in the given vocabulary.\n    4. `self.sos_eos_idx`: the index of the joint &lt;sos/eos&gt; token used as the beginning and end of a sentence.\n    5. `self.ignore_idx`: the index of the blank token used for either CTC blank modeling or ignored token for\n        encoder-decoder ASR&amp;TTS models.\n    6. `self.unk_idx`: the index of the unknown token.\n\n    Args:\n        token_path: str\n            The path where the token vocabulary is placed.\n        copy_path: str = None\n            The path where you want to paste the given token vocabulary as a backup.\n            If not given, no backup will be saved.\n        **tokenizer_conf:\n            The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization.\n    \"\"\"\n    # The vocab in token_path has the higher priority than the backup one in copy_path for vocabulary initialization\n    if token_path is not None:\n        token_vocab = os.path.join(parse_path_args(token_path), \"vocab\")\n\n    # if token_path is not given or vocab does not exist, use the backup one in copy_path\n    if token_path is None or not os.path.exists(token_vocab):\n        assert copy_path is not None, \"Please give copy_path for vocabulary backup!\"\n        token_vocab = os.path.join(parse_path_args(copy_path), \"token_vocab\")\n\n    # register token-related variables\n    self.idx2token = load_idx2data_file(token_vocab, do_separate=False)\n    self.token2idx = dict(map(reversed, self.idx2token.items()))\n    self.vocab_size = len(self.token2idx)\n    self.sos_eos_idx = self.token2idx[\"&lt;sos/eos&gt;\"]\n    self.ignore_idx = self.token2idx[\"&lt;blank&gt;\"]\n    self.unk_idx = self.token2idx[\"&lt;unk&gt;\"]\n    if \"&lt;space&gt;\" in self.token2idx.keys():\n        self.space_idx = self.token2idx[\"&lt;space&gt;\"]\n    else:\n        self.space_idx = None\n\n    # save the backup if copy_path is given\n    if copy_path is not None:\n        np.savetxt(\n            os.path.join(copy_path, \"token_vocab\"),\n            list(self.token2idx.keys()),\n            fmt=\"%s\",\n        )\n\n    # call the hook function for customized initialization\n    self.tokenizer_init_fn(\n        token_path=token_path, copy_path=copy_path, **tokenizer_conf\n    )\n</code></pre>"},{"location":"reference/tokenizer/abs/#tokenizer.abs.Tokenizer.tensor2text","title":"<code>tensor2text(tensor)</code>","text":"<p>This functions decodes a text tensor into a human-friendly string.</p> <p>The default implementation transforms each token index in the input tensor to the token string by <code>self.idx2token</code>. If the token index is <code>self.unk_idx</code>, an asterisk (*) will be used to represent an unknown token in the string.</p> <p>This interface is not mandatory to be overridden. If your Tokenizer subclass uses some third-party packages to decode the input tensor rather than the built-in <code>self.idx2token</code>, please override this function.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>LongTensor</code> <p>torch.LongTensor 1D integer torch.Tensor that contains the token indices of the sentence to be decoded.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string of the decoded sentence.</p> Source code in <code>speechain/tokenizer/abs.py</code> <pre><code>def tensor2text(self, tensor: torch.LongTensor) -&gt; str:\n    \"\"\"This functions decodes a text tensor into a human-friendly string.\n\n    The default implementation transforms each token index in the input tensor to the token string by `\n    self.idx2token`. If the token index is `self.unk_idx`, an asterisk (*) will be used to represent an unknown\n    token in the string.\n\n    This interface is not mandatory to be overridden. If your _Tokenizer_ subclass uses some third-party packages\n    to decode the input tensor rather than the built-in `self.idx2token`, please override this function.\n\n    Args:\n        tensor: torch.LongTensor\n            1D integer torch.Tensor that contains the token indices of the sentence to be decoded.\n\n    Returns:\n        The string of the decoded sentence.\n    \"\"\"\n    token_list = []\n    for idx in tensor.tolist():\n        if idx in [self.sos_eos_idx, self.ignore_idx]:\n            continue\n        # the space tokens will be replaced by a blank\n        elif self.space_idx is not None and idx == self.space_idx:\n            token_list.append(\" \")\n        # the unknown tokens will be replaced by a star symbol '*'\n        elif idx == self.unk_idx:\n            token_list.append(\"*\")\n        else:\n            token_list.append(self.idx2token[idx])\n    return \"\".join(token_list)\n</code></pre>"},{"location":"reference/tokenizer/abs/#tokenizer.abs.Tokenizer.text2tensor","title":"<code>text2tensor(text, no_sos=False, no_eos=False, return_tensor=True)</code>  <code>abstractmethod</code>","text":"<p>This functions encodes a text string into a model-friendly tensor. This interface is mandatory to be overridden. By default, this function will attach two  at the beginning and end of the returned token id sequence. <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>str the input text string to be encoded</p> required <code>no_sos</code> <code>bool</code> <p>bool = False Whether to remove the  at the beginning of the token id sequence. <code>False</code> <code>no_eos</code> <code>bool</code> <p>bool = False Whether to remove the  at the end of the token id sequence. <code>False</code> <code>return_tensor</code> <code>bool</code> <p>bool = True Whether to return the tokenization results as a tensor. If False, a List will be returned.</p> <code>True</code> <p>torch.LongTensor</p> Type Description <code>LongTensor or List</code> <p>The tensor of the encoded sentence</p> Source code in <code>speechain/tokenizer/abs.py</code> <pre><code>@abstractmethod\ndef text2tensor(\n    self,\n    text: str,\n    no_sos: bool = False,\n    no_eos: bool = False,\n    return_tensor: bool = True,\n) -&gt; torch.LongTensor or List:\n    \"\"\"This functions encodes a text string into a model-friendly tensor. This\n    interface is mandatory to be overridden. By default, this function will attach\n    two &lt;sos/eos&gt; at the beginning and end of the returned token id sequence.\n\n    Args:\n        text: str\n            the input text string to be encoded\n        no_sos: bool = False\n            Whether to remove the &lt;sos/eos&gt; at the beginning of the token id sequence.\n        no_eos: bool = False\n            Whether to remove the &lt;sos/eos&gt; at the end of the token id sequence.\n        return_tensor: bool = True\n            Whether to return the tokenization results as a tensor. If False, a List will be returned.\n\n    Returns: torch.LongTensor\n        The tensor of the encoded sentence\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/tokenizer/abs/#tokenizer.abs.Tokenizer.tokenizer_init_fn","title":"<code>tokenizer_init_fn(token_path, copy_path=None, **tokenizer_conf)</code>","text":"<p>This hook interface function initializes the customized part of a Tokenizer subclass if had. This interface is not mandatory to be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>copy_path</code> <code>str</code> <p>str = None The path where you want to paste the given tokenizer model as a backup. If not given, no backup will be saved.</p> <code>None</code> <code>**tokenizer_conf</code> <p>The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization. For more details, please refer to the docstring of your target Tokenizer subclass.</p> <code>{}</code> Source code in <code>speechain/tokenizer/abs.py</code> <pre><code>def tokenizer_init_fn(\n    self, token_path: str, copy_path: str = None, **tokenizer_conf\n):\n    \"\"\"This hook interface function initializes the customized part of a _Tokenizer_\n    subclass if had. This interface is not mandatory to be overridden.\n\n    Args:\n        copy_path: str = None\n            The path where you want to paste the given tokenizer model as a backup.\n            If not given, no backup will be saved.\n        **tokenizer_conf:\n            The arguments used by tokenizer_init_fn() for your customized Tokenizer initialization.\n            For more details, please refer to the docstring of your target Tokenizer subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tokenizer/char/","title":"char","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/tokenizer/char/#tokenizer.char.CharTokenizer","title":"<code>CharTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer implementation that converts the input sentence string into a list of graphemes (characters).</p> Source code in <code>speechain/tokenizer/char.py</code> <pre><code>class CharTokenizer(Tokenizer):\n    \"\"\"Tokenizer implementation that converts the input sentence string into a list of\n    graphemes (characters).\"\"\"\n\n    def text2tensor(\n        self,\n        text: str,\n        no_sos: bool = False,\n        no_eos: bool = False,\n        return_tensor: bool = True,\n    ):\n        \"\"\"\n\n        Args:\n            text:\n            no_sos:\n            no_eos:\n            return_tensor:\n\n        Returns:\n\n        \"\"\"\n        # initialize the tensor as an empty list\n        tokens = []\n        # whether to attach sos at the beginning of the tokens\n        if not no_sos:\n            tokens.append(self.sos_eos_idx)\n        # attach the main body of the text\n        tokens.extend(\n            [\n                self.token2idx[char] if char in self.token2idx.keys() else self.unk_idx\n                for char in text\n            ]\n        )\n        # whether to attach eos at the end of the tokens\n        if not no_eos:\n            tokens.append(self.sos_eos_idx)\n        # turn the token list into a long-type tensor\n        if return_tensor:\n            return torch.LongTensor(tokens)\n        else:\n            return tokens\n</code></pre>"},{"location":"reference/tokenizer/char/#tokenizer.char.CharTokenizer.text2tensor","title":"<code>text2tensor(text, no_sos=False, no_eos=False, return_tensor=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <code>no_sos</code> <code>bool</code> <code>False</code> <code>no_eos</code> <code>bool</code> <code>False</code> <code>return_tensor</code> <code>bool</code> <code>True</code> <p>Returns:</p> Source code in <code>speechain/tokenizer/char.py</code> <pre><code>def text2tensor(\n    self,\n    text: str,\n    no_sos: bool = False,\n    no_eos: bool = False,\n    return_tensor: bool = True,\n):\n    \"\"\"\n\n    Args:\n        text:\n        no_sos:\n        no_eos:\n        return_tensor:\n\n    Returns:\n\n    \"\"\"\n    # initialize the tensor as an empty list\n    tokens = []\n    # whether to attach sos at the beginning of the tokens\n    if not no_sos:\n        tokens.append(self.sos_eos_idx)\n    # attach the main body of the text\n    tokens.extend(\n        [\n            self.token2idx[char] if char in self.token2idx.keys() else self.unk_idx\n            for char in text\n        ]\n    )\n    # whether to attach eos at the end of the tokens\n    if not no_eos:\n        tokens.append(self.sos_eos_idx)\n    # turn the token list into a long-type tensor\n    if return_tensor:\n        return torch.LongTensor(tokens)\n    else:\n        return tokens\n</code></pre>"},{"location":"reference/tokenizer/g2p/","title":"g2p","text":""},{"location":"reference/tokenizer/g2p/#tokenizer.g2p.GraphemeToPhonemeTokenizer","title":"<code>GraphemeToPhonemeTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer implementation that converts the input sentence string into phoneme tokens by the g2p package.</p> <p>References: https://github.com/Kyubyong/g2p</p> Source code in <code>speechain/tokenizer/g2p.py</code> <pre><code>class GraphemeToPhonemeTokenizer(Tokenizer):\n    \"\"\"Tokenizer implementation that converts the input sentence string into phoneme\n    tokens by the g2p package.\n\n    References: https://github.com/Kyubyong/g2p\n    \"\"\"\n\n    def text2tensor(\n        self,\n        text: str or List[str],\n        no_sos: bool = False,\n        no_eos: bool = False,\n        return_tensor: bool = True,\n    ):\n        \"\"\"\n        This text-to-tensor function can take two types of input:\n        1. raw string of the transcript sentence\n        2. structured string of the phonemes dumped in advance\n\n        But we recommend you to feed the type no.2 to this function because if the input it type no.1, the raw string\n        needs to be decoded by g2p_en.G2p in each epoch, which not only consumes a lot of CPU but also slow down the\n        model forward.\n\n        \"\"\"\n        # initialize the tensor as an empty list\n        tokens = []\n        # whether to attach sos at the beginning of the tokens\n        if not no_sos:\n            tokens.append(self.sos_eos_idx)\n\n        # when input text is a dumped phoneme list\n        if isinstance(text, List):\n            tokens += [\n                (\n                    self.token2idx[token]\n                    if token in self.token2idx.keys()\n                    else self.unk_idx\n                )\n                for token in text\n            ]\n        # when input text is a raw string\n        else:\n            # initialize g2p convertor lazily during training\n            if not hasattr(self, \"g2p\"):\n                self.g2p = G2p()\n            phonemes = self.g2p(text)\n            for phn in phonemes:\n                if phn in abnormal_phns:\n                    continue\n                elif phn == \" \":\n                    tokens.append(self.space_idx)\n                elif phn not in self.token2idx.keys():\n                    tokens.append(self.unk_idx)\n                else:\n                    tokens.append(self.token2idx[phn])\n\n        # whether to attach eos at the end of the tokens\n        if not no_eos:\n            tokens.append(self.sos_eos_idx)\n\n        if return_tensor:\n            return torch.LongTensor(tokens)\n        else:\n            return tokens\n</code></pre>"},{"location":"reference/tokenizer/g2p/#tokenizer.g2p.GraphemeToPhonemeTokenizer.text2tensor","title":"<code>text2tensor(text, no_sos=False, no_eos=False, return_tensor=True)</code>","text":"<p>This text-to-tensor function can take two types of input: 1. raw string of the transcript sentence 2. structured string of the phonemes dumped in advance</p> <p>But we recommend you to feed the type no.2 to this function because if the input it type no.1, the raw string needs to be decoded by g2p_en.G2p in each epoch, which not only consumes a lot of CPU but also slow down the model forward.</p> Source code in <code>speechain/tokenizer/g2p.py</code> <pre><code>def text2tensor(\n    self,\n    text: str or List[str],\n    no_sos: bool = False,\n    no_eos: bool = False,\n    return_tensor: bool = True,\n):\n    \"\"\"\n    This text-to-tensor function can take two types of input:\n    1. raw string of the transcript sentence\n    2. structured string of the phonemes dumped in advance\n\n    But we recommend you to feed the type no.2 to this function because if the input it type no.1, the raw string\n    needs to be decoded by g2p_en.G2p in each epoch, which not only consumes a lot of CPU but also slow down the\n    model forward.\n\n    \"\"\"\n    # initialize the tensor as an empty list\n    tokens = []\n    # whether to attach sos at the beginning of the tokens\n    if not no_sos:\n        tokens.append(self.sos_eos_idx)\n\n    # when input text is a dumped phoneme list\n    if isinstance(text, List):\n        tokens += [\n            (\n                self.token2idx[token]\n                if token in self.token2idx.keys()\n                else self.unk_idx\n            )\n            for token in text\n        ]\n    # when input text is a raw string\n    else:\n        # initialize g2p convertor lazily during training\n        if not hasattr(self, \"g2p\"):\n            self.g2p = G2p()\n        phonemes = self.g2p(text)\n        for phn in phonemes:\n            if phn in abnormal_phns:\n                continue\n            elif phn == \" \":\n                tokens.append(self.space_idx)\n            elif phn not in self.token2idx.keys():\n                tokens.append(self.unk_idx)\n            else:\n                tokens.append(self.token2idx[phn])\n\n    # whether to attach eos at the end of the tokens\n    if not no_eos:\n        tokens.append(self.sos_eos_idx)\n\n    if return_tensor:\n        return torch.LongTensor(tokens)\n    else:\n        return tokens\n</code></pre>"},{"location":"reference/tokenizer/sp/","title":"sp","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/tokenizer/sp/#tokenizer.sp.SentencePieceTokenizer","title":"<code>SentencePieceTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Tokenizer implementation that converts the input sentence string into subword tokens, i.e., combinations of graphemes, by the sentencepiece package.</p> <p>References: https://github.com/google/sentencepiece</p> Source code in <code>speechain/tokenizer/sp.py</code> <pre><code>class SentencePieceTokenizer(Tokenizer):\n    \"\"\"Tokenizer implementation that converts the input sentence string into subword\n    tokens, i.e., combinations of graphemes, by the sentencepiece package.\n\n    References: https://github.com/google/sentencepiece\n    \"\"\"\n\n    def tokenizer_init_fn(self, token_path: str, copy_path: str = None, **kwargs):\n        \"\"\"Initialize the sentencepiece tokenizer model.\n\n        Args:\n            copy_path: str = None\n                The path where you want to paste the given tokenizer model as a backup.\n                If not given, no backup will be saved.\n            token_path: str\n                The path of your specified sentencepiece tokenizer model file.\n                If not given, the model will automatically selected in the same folder as the given token_vocab\n        \"\"\"\n        # The model in token_path token_model has the highest priority for token_model initialization\n        if token_path is not None:\n            token_model = os.path.join(parse_path_args(token_path), \"model\")\n\n        # if token_path is not given or model does not exist, use the backup on in copy_path\n        if token_path is None or not os.path.exists(token_model):\n            assert (\n                copy_path is not None\n            ), \"Please give copy_path for SentencePiece model backup!\"\n            token_model = os.path.join(parse_path_args(copy_path), \"token_model\")\n\n        # initialize the tokenizer model by the sentencepiece package\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.load(token_model)\n\n        # save the backup if copy_path is given\n        if copy_path is not None:\n            try:\n                shutil.copy(src=token_model, dst=os.path.join(copy_path, \"token_model\"))\n            except shutil.SameFileError:\n                pass\n\n    def tensor2text(self, tensor: torch.LongTensor or List):\n        \"\"\"\n\n        Args:\n            tensor:\n\n        Returns:\n\n        \"\"\"\n        if isinstance(tensor, torch.Tensor):\n            tensor = tensor.tolist()\n        text = self.sp_model.decode_ids(\n            [t for t in tensor if t not in [self.sos_eos_idx, self.ignore_idx]]\n        )\n        return text\n\n    def text2tensor(\n        self,\n        text: str,\n        no_sos: bool = False,\n        no_eos: bool = False,\n        return_tensor: bool = True,\n    ):\n        \"\"\"\n\n        Args:\n            text:\n            no_sos:\n            no_eos:\n            return_tensor:\n\n        Returns:\n\n        \"\"\"\n        # initialize the tensor as an empty list\n        tokens = []\n        # whether to attach sos at the beginning of the tokens\n        if not no_sos:\n            tokens.append(self.sos_eos_idx)\n        # attach the main body of the text\n        tokens.extend(self.sp_model.encode_as_ids(text))\n        # whether to attach eos at the end of the tokens\n        if not no_eos:\n            tokens.append(self.sos_eos_idx)\n        # turn the token list into a long-type tensor\n        if return_tensor:\n            return torch.LongTensor(tokens)\n        else:\n            return tokens\n</code></pre>"},{"location":"reference/tokenizer/sp/#tokenizer.sp.SentencePieceTokenizer.tensor2text","title":"<code>tensor2text(tensor)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>LongTensor or List</code> required <p>Returns:</p> Source code in <code>speechain/tokenizer/sp.py</code> <pre><code>def tensor2text(self, tensor: torch.LongTensor or List):\n    \"\"\"\n\n    Args:\n        tensor:\n\n    Returns:\n\n    \"\"\"\n    if isinstance(tensor, torch.Tensor):\n        tensor = tensor.tolist()\n    text = self.sp_model.decode_ids(\n        [t for t in tensor if t not in [self.sos_eos_idx, self.ignore_idx]]\n    )\n    return text\n</code></pre>"},{"location":"reference/tokenizer/sp/#tokenizer.sp.SentencePieceTokenizer.text2tensor","title":"<code>text2tensor(text, no_sos=False, no_eos=False, return_tensor=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <code>no_sos</code> <code>bool</code> <code>False</code> <code>no_eos</code> <code>bool</code> <code>False</code> <code>return_tensor</code> <code>bool</code> <code>True</code> <p>Returns:</p> Source code in <code>speechain/tokenizer/sp.py</code> <pre><code>def text2tensor(\n    self,\n    text: str,\n    no_sos: bool = False,\n    no_eos: bool = False,\n    return_tensor: bool = True,\n):\n    \"\"\"\n\n    Args:\n        text:\n        no_sos:\n        no_eos:\n        return_tensor:\n\n    Returns:\n\n    \"\"\"\n    # initialize the tensor as an empty list\n    tokens = []\n    # whether to attach sos at the beginning of the tokens\n    if not no_sos:\n        tokens.append(self.sos_eos_idx)\n    # attach the main body of the text\n    tokens.extend(self.sp_model.encode_as_ids(text))\n    # whether to attach eos at the end of the tokens\n    if not no_eos:\n        tokens.append(self.sos_eos_idx)\n    # turn the token list into a long-type tensor\n    if return_tensor:\n        return torch.LongTensor(tokens)\n    else:\n        return tokens\n</code></pre>"},{"location":"reference/tokenizer/sp/#tokenizer.sp.SentencePieceTokenizer.tokenizer_init_fn","title":"<code>tokenizer_init_fn(token_path, copy_path=None, **kwargs)</code>","text":"<p>Initialize the sentencepiece tokenizer model.</p> <p>Parameters:</p> Name Type Description Default <code>copy_path</code> <code>str</code> <p>str = None The path where you want to paste the given tokenizer model as a backup. If not given, no backup will be saved.</p> <code>None</code> <code>token_path</code> <code>str</code> <p>str The path of your specified sentencepiece tokenizer model file. If not given, the model will automatically selected in the same folder as the given token_vocab</p> required Source code in <code>speechain/tokenizer/sp.py</code> <pre><code>def tokenizer_init_fn(self, token_path: str, copy_path: str = None, **kwargs):\n    \"\"\"Initialize the sentencepiece tokenizer model.\n\n    Args:\n        copy_path: str = None\n            The path where you want to paste the given tokenizer model as a backup.\n            If not given, no backup will be saved.\n        token_path: str\n            The path of your specified sentencepiece tokenizer model file.\n            If not given, the model will automatically selected in the same folder as the given token_vocab\n    \"\"\"\n    # The model in token_path token_model has the highest priority for token_model initialization\n    if token_path is not None:\n        token_model = os.path.join(parse_path_args(token_path), \"model\")\n\n    # if token_path is not given or model does not exist, use the backup on in copy_path\n    if token_path is None or not os.path.exists(token_model):\n        assert (\n            copy_path is not None\n        ), \"Please give copy_path for SentencePiece model backup!\"\n        token_model = os.path.join(parse_path_args(copy_path), \"token_model\")\n\n    # initialize the tokenizer model by the sentencepiece package\n    self.sp_model = spm.SentencePieceProcessor()\n    self.sp_model.load(token_model)\n\n    # save the backup if copy_path is given\n    if copy_path is not None:\n        try:\n            shutil.copy(src=token_model, dst=os.path.join(copy_path, \"token_model\"))\n        except shutil.SameFileError:\n            pass\n</code></pre>"},{"location":"reference/utilbox/","title":"utilbox","text":""},{"location":"reference/utilbox/data_loading_util/","title":"data_loading_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.11</p>"},{"location":"reference/utilbox/data_loading_util/#utilbox.data_loading_util.get_file_birthtime","title":"<code>get_file_birthtime(file_path, readable_time=False)</code>","text":"<p>Get the creation time of a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path of the file.</p> required <code>readable_time</code> <code>bool</code> <p>If True, the output is a string in a readable format. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>float or str: The creation time of the file. If readable_time is True, a string in a readable format is returned.</p> Source code in <code>speechain/utilbox/data_loading_util.py</code> <pre><code>def get_file_birthtime(file_path: str, readable_time: bool = False):\n    \"\"\"Get the creation time of a file.\n\n    Args:\n        file_path (str):\n            Path of the file.\n        readable_time (bool, optional):\n            If True, the output is a string in a readable format. Defaults to False.\n\n    Returns:\n        float or str: The creation time of the file. If readable_time is True, a string in a readable format is returned.\n    \"\"\"\n    stat = os.stat(file_path)\n\n    try:\n        creation_time = stat.st_birthtime\n    except AttributeError:\n        # If on Linux, get the last modified time of file's metadata, as creation time is not directly accessible.\n        creation_time = stat.st_mtime\n\n    if readable_time:\n        # Convert the creation time to a readable format\n        creation_time = time.ctime(creation_time)\n    return creation_time\n</code></pre>"},{"location":"reference/utilbox/data_loading_util/#utilbox.data_loading_util.load_idx2data_file","title":"<code>load_idx2data_file(file_path, data_type=str, separator=' ', do_separate=True)</code>","text":"<p>Load a dictionary from a file or a list of files containing key-value pairs, where the key is the index of a data instance and the value is the target data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str or List[str]</code> <p>Absolute path of the file(s) to be loaded.</p> required <code>data_type</code> <code>type</code> <p>The data type of the values in the returned dictionary. It should be a Python built-in data type. Defaults to str.</p> <code>str</code> <code>separator</code> <code>str</code> <p>The separator between the data instance index and the data value in each line of the file. Defaults to ' '.</p> <code>' '</code> <code>do_separate</code> <code>bool</code> <p>Whether to separate each row by the given separator. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing key-value pairs, where the key is the index of a data instance and</p> <code>Dict[str, Any]</code> <p>the value is the target data.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the given file does not exist, this error is raised.</p> Source code in <code>speechain/utilbox/data_loading_util.py</code> <pre><code>def load_idx2data_file(\n    file_path: str or List[str],\n    data_type: type = str,\n    separator: str = \" \",\n    do_separate: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Load a dictionary from a file or a list of files containing key-value pairs,\n    where the key is the index of a data instance and the value is the target data.\n\n    Args:\n        file_path (str or List[str]):\n            Absolute path of the file(s) to be loaded.\n        data_type (type, optional):\n            The data type of the values in the returned dictionary.\n            It should be a Python built-in data type. Defaults to str.\n        separator (str, optional):\n            The separator between the data instance index and the data value in each line of the file.\n            Defaults to ' '.\n        do_separate (bool, optional):\n            Whether to separate each row by the given separator. Defaults to True.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing key-value pairs, where the key is the index of a data instance and\n        the value is the target data.\n\n    Raises:\n        AssertionError: If the given file does not exist, this error is raised.\n    \"\"\"\n\n    def load_single_file(_file_path: str):\n        assert os.path.exists(_file_path), f\"{_file_path} doesn't exist!\"\n\n        # non-json metadata file\n        if not _file_path.endswith(\".json\"):\n            # str -&gt; (n,) List. First read the content of the given file one line a time.\n            with open(_file_path, mode=\"r\") as f:\n                data = f.readlines()\n            # (n,) List -&gt; (n, 2) List or (n,) List. Then, the index and sentence are separated by the first blank\n            data = [\n                (\n                    row.replace(\"\\n\", \"\").split(separator, 1)\n                    if do_separate\n                    else row.replace(\"\\n\", \"\")\n                )\n                for row in data\n            ]\n\n            if isinstance(data[0], List) and len(data[0]) &gt; 1:\n                # (n, 2) List -&gt; Dict[str, data_type]\n                return {key: data_type(value) for key, value in data}\n            else:\n                # (n,) List -&gt; Dict[str, data_type]\n                return {\n                    key: data_type(value[0] if isinstance(value, List) else value)\n                    for key, value in enumerate(data)\n                }\n        # .json metadata file\n        else:\n            # str -&gt; (n,) np.ndarray. First read the content of the .json file into a string.\n            with open(_file_path, mode=\"r\") as f:\n                json_data = f.read()\n                data = json.loads(json_data)\n\n            for key, value in data.items():\n                if isinstance(value, List):\n                    data[key] = [data_type(i) for i in value]\n                elif isinstance(value, Dict):\n                    data[key] = {k: data_type(v) for k, v in value}\n                else:\n                    data[key] = data_type(value)\n            return data\n\n    if not isinstance(file_path, List):\n        file_path = [file_path]\n    else:\n        assert isinstance(file_path, List)\n\n    # data file reading, List[str] -&gt; List[Dict[str, str]]\n    idx2data_dict = [load_single_file(parse_path_args(f_p)) for f_p in file_path]\n    # data Dict combination, List[Dict[str, str]] -&gt; Dict[str, str]\n    idx2data_dict = {\n        key: value\n        for _idx2data_dict in idx2data_dict\n        for key, value in _idx2data_dict.items()\n    }\n    # sort the key-value items in the dict by their key names\n    idx2data_dict = dict(sorted(idx2data_dict.items(), key=lambda x: x[0]))\n    return idx2data_dict\n</code></pre>"},{"location":"reference/utilbox/data_loading_util/#utilbox.data_loading_util.read_data_by_path","title":"<code>read_data_by_path(data_path, return_tensor=False, return_sample_rate=False)</code>","text":"<p>This function reads data from the file in the specified path, considering the file format and extension.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path where the data file to be read is placed.</p> required <code>return_tensor</code> <code>bool</code> <p>Whether to return data as torch.Tensor. Defaults to False.</p> <code>False</code> <code>return_sample_rate</code> <code>bool</code> <p>Whether to return the sample rate. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray or Tensor</code> <p>np.ndarray or torch.Tensor: Array-like data. If return_tensor is False, the data type will be numpy.ndarray; otherwise, the data type will be torch.Tensor.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the file format is not supported, this error is raised.</p> Source code in <code>speechain/utilbox/data_loading_util.py</code> <pre><code>def read_data_by_path(\n    data_path: str, return_tensor: bool = False, return_sample_rate: bool = False\n) -&gt; np.ndarray or torch.Tensor:\n    \"\"\"This function reads data from the file in the specified path, considering the\n    file format and extension.\n\n    Args:\n        data_path (str):\n            The path where the data file to be read is placed.\n        return_tensor (bool, optional):\n            Whether to return data as torch.Tensor. Defaults to False.\n        return_sample_rate (bool, optional):\n            Whether to return the sample rate. Defaults to False.\n\n    Returns:\n        np.ndarray or torch.Tensor:\n            Array-like data. If return_tensor is False, the data type will be numpy.ndarray;\n            otherwise, the data type will be torch.Tensor.\n\n    Raises:\n        NotImplementedError: If the file format is not supported, this error is raised.\n    \"\"\"\n    # get the folder directory and data file name\n    folder_path, data_file = os.path.dirname(data_path), os.path.basename(data_path)\n    sample_rate = None\n    # ':' means that the data is stored in a compressed chunk file\n    if \":\" in data_file:\n        assert len(data_file.split(\":\")) == 2\n        chunk_file, data_idx = data_file.split(\":\")\n        chunk_path = os.path.join(folder_path, chunk_file)\n\n        # read data by its extension\n        chunk_ext = chunk_file.split(\".\")[-1].lower()\n        if chunk_ext == \"npz\":\n            data = np.load(chunk_path)[data_idx]\n        elif chunk_ext == \"hdf5\":\n            with h5py.File(chunk_path, \"r\") as reader:\n                data = np.array(reader[data_idx])\n        else:\n            raise NotImplementedError\n\n    # without ':' means that the data is stored in an individual file\n    else:\n        # read data by its extension\n        data_ext = data_file.split(\".\")[-1].lower()\n        if data_ext == \"npy\":\n            data = np.load(data_path)\n        elif data_ext == \"npz\":\n            npz_dict = np.load(data_path)\n            data, sample_rate = npz_dict[\"feat\"], npz_dict[\"sample_rate\"]\n        elif data_ext in [\"wav\", \"flac\"]:\n            # There are 3 ways to extract waveforms from the disk, no large difference in loaded values.\n            # The no.2 method by librosa consumes a little more time than the others.\n            # Among them, torchaudio.load() directly gives torch.Tensor.\n            # 1. soundfile.read(self.src_data[index], always_2d=True, dtype='float32')[0]\n            # 2. librosa.core.load(self.src_data[index], sr=self.sample_rate)[0].reshape(-1, 1)\n            # 3. torchaudio.load(self.src_data[index], channels_first=False, normalize=False)[0]\n            data, sample_rate = sf.read(data_path, always_2d=True, dtype=\"float32\")\n        else:\n            raise NotImplementedError(f\"unknown file extension: {data_ext}!\")\n\n    if return_tensor:\n        data = torch.tensor(data)\n\n    if return_sample_rate:\n        return data, sample_rate\n    else:\n        return data\n</code></pre>"},{"location":"reference/utilbox/data_loading_util/#utilbox.data_loading_util.read_idx2data_file_to_dict","title":"<code>read_idx2data_file_to_dict(path_dict)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path_dict</code> <code>Dict[str, str or List[str]]</code> <p>Dict[str, str or List[str] The path dictionary of the 'idx2XXX' files to be read. In each key-value item, the key is the data name and the value is the path of the target 'idx2XXX' files. Multiple file paths can be given in a list.</p> required <p>(Dict[str, str], List[str])</p> Type Description <code>(Dict[str, str], List[str])</code> <p>Both the result dictionary and the data index list will be returned.</p> Source code in <code>speechain/utilbox/data_loading_util.py</code> <pre><code>def read_idx2data_file_to_dict(\n    path_dict: Dict[str, str or List[str]],\n) -&gt; (Dict[str, str], List[str]):\n    \"\"\"\n\n    Args:\n        path_dict: Dict[str, str or List[str]\n            The path dictionary of the 'idx2XXX' files to be read. In each key-value item, the key is the data name and\n            the value is the path of the target 'idx2XXX' files. Multiple file paths can be given in a list.\n\n    Returns: (Dict[str, str], List[str])\n        Both the result dictionary and the data index list will be returned.\n\n    \"\"\"\n    # --- 1. Transformation from path to Dict --- #\n    # preprocess Dict[str, str] into Dict[str, List[str]]\n    path_dict = {\n        key: [value] if isinstance(value, str) else value\n        for key, value in path_dict.items()\n    }\n\n    # loop each kind of information\n    output_dict = {key: load_idx2data_file(value) for key, value in path_dict.items()}\n\n    # for data_name in path_dict.keys():\n    #     # data file reading, List[str] -&gt; List[Dict[str, str]]\n    #     output_dict[data_name] = [load_idx2data_file(_data_file) for _data_file in path_dict[data_name]]\n    #     # data Dict combination, List[Dict[str, str]] -&gt; Dict[str, str]\n    #     output_dict[data_name] = {key: value for _data_dict in output_dict[data_name]\n    #                               for key, value in _data_dict.items()}\n    #     # sort the key-value items in the dict by their key names\n    #     output_dict[data_name] = dict(sorted(output_dict[data_name].items(), key=lambda x: x[0]))\n\n    # --- 2. Dict Key Mismatch Checking --- #\n    # combine the key lists of all data sources\n    dict_keys = [set(data_dict.keys()) for data_dict in output_dict.values()]\n\n    # get the intersection of the list of key sets\n    key_intsec = dict_keys[0]\n    for i in range(1, len(dict_keys)):\n        key_intsec &amp;= dict_keys[i]\n\n    # remove the redundant key-value items from self.main_data\n    for data_name in output_dict.keys():\n        key_set = set(output_dict[data_name].keys())\n\n        # delete the redundant key-value pairs\n        redundant_keys = key_set.difference(key_intsec)\n        if len(redundant_keys) &gt; 0:\n            warnings.warn(\n                f\"There are {len(redundant_keys)} redundant keys that exist in main_data[{data_name}] but others! \"\n                f\"Please check your data_cfg to examine whether there is a problem.\"\n            )\n            for redund_key in redundant_keys:\n                output_dict[data_name].pop(redund_key)\n\n    return output_dict, sorted(key_intsec)\n</code></pre>"},{"location":"reference/utilbox/data_loading_util/#utilbox.data_loading_util.search_file_in_subfolder","title":"<code>search_file_in_subfolder(curr_query, tgt_match_fn=None, return_name=False, return_sorted=True)</code>","text":"<p>Search for files in a directory and its subdirectories that satisfy a certain condition.</p> <p>Parameters:</p> Name Type Description Default <code>curr_query</code> <code>str</code> <p>Path of the directory to search in.</p> required <code>tgt_match_fn</code> <code>callable</code> <p>A function that takes a file name and returns a boolean value. If provided, only files that satisfy this condition are returned. If not provided, all files will be returned. Defaults to None.</p> <code>None</code> <code>return_name</code> <code>bool</code> <p>If True, return file names instead of file paths. Defaults to False.</p> <code>False</code> <code>return_sorted</code> <code>bool</code> <p>If True, the output list will be sorted in lexicographical order. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths or file names (depending on return_name) that satisfy tgt_match_fn.</p> Source code in <code>speechain/utilbox/data_loading_util.py</code> <pre><code>def search_file_in_subfolder(\n    curr_query: str,\n    tgt_match_fn: callable = None,\n    return_name: bool = False,\n    return_sorted: bool = True,\n):\n    \"\"\"Search for files in a directory and its subdirectories that satisfy a certain\n    condition.\n\n    Args:\n        curr_query (str):\n            Path of the directory to search in.\n        tgt_match_fn (callable, optional):\n            A function that takes a file name and returns a boolean value.\n            If provided, only files that satisfy this condition are returned.\n            If not provided, all files will be returned. Defaults to None.\n        return_name (bool, optional):\n            If True, return file names instead of file paths. Defaults to False.\n        return_sorted (bool, optional):\n            If True, the output list will be sorted in lexicographical order. Defaults to True.\n\n    Returns:\n        list: A list of file paths or file names (depending on return_name) that satisfy tgt_match_fn.\n    \"\"\"\n    candidates = []\n    curr_query = parse_path_args(curr_query)\n\n    # If the input query is a file path\n    if os.path.isfile(curr_query):\n        dir_name, node_name = os.path.dirname(curr_query), os.path.basename(curr_query)\n        if tgt_match_fn is None or tgt_match_fn(node_name):\n            # Skip the symbolic link and only return existing files\n            if not os.path.islink(curr_query):\n                return (\n                    candidates + [curr_query]\n                    if not return_name\n                    else candidates + [node_name]\n                )\n        else:\n            raise RuntimeError(\n                f\"The file at the path {curr_query} does not satisfy the provided condition!\"\n            )\n\n    # If the input query is a directory path\n    for node_name in os.listdir(curr_query):\n        node_path = os.path.join(curr_query, node_name)\n        if os.path.isdir(node_path):\n            # Recursively search subdirectories\n            candidates = candidates + search_file_in_subfolder(\n                node_path,\n                tgt_match_fn,\n                return_name=return_name,\n                return_sorted=return_sorted,\n            )\n        elif tgt_match_fn is None or tgt_match_fn(node_name):\n            # Skip the symbolic link and only return existing files\n            if not os.path.islink(node_path):\n                candidates = (\n                    candidates + [node_path]\n                    if not return_name\n                    else candidates + [node_name]\n                )\n\n    return sorted(candidates) if return_sorted else candidates\n</code></pre>"},{"location":"reference/utilbox/data_saving_util/","title":"data_saving_util","text":""},{"location":"reference/utilbox/data_saving_util/#utilbox.data_saving_util.save_data_by_format","title":"<code>save_data_by_format(file_format, save_path, file_name_list, file_content_list, group_ids=None, sample_rate=None)</code>","text":"<p>Save data in the specified format to disk.</p> <p>Parameters:</p> Name Type Description Default <code>file_format</code> <code>str</code> <p>The format of the output files. It can be one of 'npy', 'npz', 'wav' or 'flac'.</p> required <code>save_path</code> <code>str</code> <p>The directory where the files will be saved.</p> required <code>file_name_list</code> <code>List[str]</code> <p>A list of strings with the names of the files to be saved.</p> required <code>file_content_list</code> <code>List</code> <p>A list with the content of the files to be saved.</p> required <code>group_ids</code> <code>List[str] or str</code> <p>A list of strings with the group ids of the files. If provided, it will be used to create a subdirectory for each group of files inside the save_path. Defaults to None.</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio files, required for 'wav' and 'flac' formats. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dict[str, str]: A dictionary that maps the original file names to their corresponding file paths in disk.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the file format is not supported.</p> Notes <ul> <li>The content of the files can be of any type as long as it can be converted to numpy arrays (using the   to_cpu() function if necessary).</li> <li>If sample_rate is not None, it will be saved along with the data for 'npz', 'wav' and 'flac' formats.</li> </ul> Source code in <code>speechain/utilbox/data_saving_util.py</code> <pre><code>def save_data_by_format(\n    file_format: str,\n    save_path: str,\n    file_name_list: Union[List[str] or str],\n    file_content_list: Union[\n        List[Union[np.ndarray, torch.Tensor]], np.ndarray, torch.Tensor\n    ],\n    group_ids: List[str] or str = None,\n    sample_rate: int = None,\n):\n    \"\"\"Save data in the specified format to disk.\n\n    Args:\n        file_format (str):\n            The format of the output files. It can be one of 'npy', 'npz', 'wav' or 'flac'.\n        save_path (str):\n            The directory where the files will be saved.\n        file_name_list (List[str]):\n            A list of strings with the names of the files to be saved.\n        file_content_list (List):\n            A list with the content of the files to be saved.\n        group_ids (List[str] or str, optional):\n            A list of strings with the group ids of the files. If provided, it will be used to create a subdirectory\n            for each group of files inside the save_path. Defaults to None.\n        sample_rate (int, optional):\n            The sample rate of the audio files, required for 'wav' and 'flac' formats. Defaults to None.\n\n    Returns:\n        Dict[str, str]:\n            A dictionary that maps the original file names to their corresponding file paths in disk.\n\n    Raises:\n        NotImplementedError:\n            If the file format is not supported.\n\n    Notes:\n        - The content of the files can be of any type as long as it can be converted to numpy arrays (using the\n          to_cpu() function if necessary).\n        - If sample_rate is not None, it will be saved along with the data for 'npz', 'wav' and 'flac' formats.\n    \"\"\"\n\n    # record all the data file paths with their names\n    name2file_path = {}\n\n    if not isinstance(file_name_list, List):\n        file_name_list = [file_name_list]\n    if not isinstance(file_content_list, List):\n        file_content_list = [file_content_list]\n    assert len(file_name_list) == len(file_content_list)\n\n    if isinstance(group_ids, str):\n        group_ids = [group_ids for _ in file_name_list]\n\n    # loop over each file name and content pair\n    for i, (name, content) in enumerate(zip(file_name_list, file_content_list)):\n        # convert content to numpy array if it is a PyTorch tensor\n        if isinstance(content, torch.Tensor):\n            content = to_cpu(content, tgt=\"numpy\")\n\n        # if group_ids is not None, create subdirectory for this file's group\n        if group_ids is not None:\n            file_save_path = os.path.join(save_path, group_ids[i])\n        else:\n            file_save_path = save_path\n\n        # make sure the saving folder exists\n        os.makedirs(file_save_path, exist_ok=True)\n        file_path = os.path.join(file_save_path, f\"{name}.{file_format}\")\n        # check the existence of the target file to make sure that data saving won't fail due to the system errors that cannot be captured by Python\n        while not os.path.exists(file_path):\n            # save the file in the specified format\n            if file_format == \"npy\":\n                np.save(file_path, content.astype(np.float32))\n\n            elif file_format == \"npz\":\n                assert (\n                    sample_rate is not None\n                ), \"sample_rate must be provided for npz format\"\n                np.savez(\n                    file_path, feat=content.astype(np.float32), sample_rate=sample_rate\n                )\n\n            elif file_format == \"wav\":\n                assert (\n                    sample_rate is not None\n                ), \"sample_rate must be provided for wav format\"\n                sf.write(\n                    file=file_path,\n                    data=content,\n                    samplerate=sample_rate,\n                    format=\"WAV\",\n                    subtype=sf.default_subtype(\"WAV\"),\n                )\n\n            elif file_format == \"flac\":\n                assert (\n                    sample_rate is not None\n                ), \"sample_rate must be provided for flac format\"\n                sf.write(\n                    file=file_path,\n                    data=content,\n                    samplerate=sample_rate,\n                    format=\"FLAC\",\n                    subtype=sf.default_subtype(\"FLAC\"),\n                )\n\n            else:\n                raise NotImplementedError(\n                    f\"File format '{file_format}' is not supported. \"\n                    f\"Please use one of the supported formats: 'npy', 'npz', 'wav', 'flac'\"\n                )\n\n        # map the original file names to their corresponding file paths in disk\n        name2file_path[name] = file_path\n\n    return name2file_path\n</code></pre>"},{"location":"reference/utilbox/dump_util/","title":"dump_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.11</p>"},{"location":"reference/utilbox/dump_util/#utilbox.dump_util.en_text_process","title":"<code>en_text_process(input_text, txt_format)</code>","text":"<p>The function that processes the text strings for TTS datasets to the specified text format. Currently, available text formats:     punc:         Letter: lowercase         Punctuation: single quotes, commas, periods, hyphens     no-punc:         Letter: lowercase         Punctuation: single quotes</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>str Unprocessed raw sentence from the TTS datasets</p> required <code>txt_format</code> <code>str</code> <p>str The text format you want the processed sentence to have</p> required <p>Returns:</p> Type Description <code>str</code> <p>Processed sentence string by your specified text format.</p> Source code in <code>speechain/utilbox/dump_util.py</code> <pre><code>def en_text_process(input_text: str, txt_format: str) -&gt; str:\n    \"\"\"\n    The function that processes the text strings for TTS datasets to the specified text format.\n    Currently, available text formats:\n        punc:\n            Letter: lowercase\n            Punctuation: single quotes, commas, periods, hyphens\n        no-punc:\n            Letter: lowercase\n            Punctuation: single quotes\n\n    Args:\n        input_text: str\n            Unprocessed raw sentence from the TTS datasets\n        txt_format: str\n            The text format you want the processed sentence to have\n\n    Returns:\n        Processed sentence string by your specified text format.\n\n    \"\"\"\n\n    def is_punc(input_char: str):\n        return not (input_char.isalpha() or input_char == \" \")\n\n    # 1st stage: turn capital letters into their lower cases\n    input_text = input_text.lower()\n\n    # 2nd stage: convert non-English letters into English counterparts\n    input_text = input_text.replace(\"\u00e8\", \"e\")\n    input_text = input_text.replace(\"\u00e9\", \"e\")\n    input_text = input_text.replace(\"\u00ea\", \"e\")\n    input_text = input_text.replace(\"\u00e2\", \"a\")\n    input_text = input_text.replace(\"\u00e0\", \"a\")\n    input_text = input_text.replace(\"\u00fc\", \"u\")\n    input_text = input_text.replace(\"\u00f1\", \"n\")\n    input_text = input_text.replace(\"\u00f4\", \"o\")\n    input_text = input_text.replace(\"\u00e6\", \"ae\")\n    input_text = input_text.replace(\"\u0153\", \"oe\")\n\n    # 3rd stage: convert all kinds of the quotes into half-angle single quotes '\u2019'\n    input_text = input_text.replace(\"\u2019\", \"'\")\n    input_text = input_text.replace(\"\u2018\", \"'\")\n    input_text = input_text.replace(\"\u201c\", \"'\")\n    input_text = input_text.replace(\"\u201d\", \"'\")\n    input_text = input_text.replace('\"', \"'\")\n    input_text = input_text.replace(\"''\", \"'\")\n\n    # 4th stage: process colons and semicolons\n    input_text = input_text.replace(\n        \":'\", \",\"\n    )  # for the colons followed by a quote, turn them into commas\n    input_text = input_text.replace(\":\", \",\")\n    input_text = input_text.replace(\";\", \".\")\n\n    # 5th stage: process double-hyphens and em dashes\n    input_text = input_text.replace(\"--\", \"-\")\n    input_text = input_text.replace(\"\u2014\", \"-\")\n    input_text = input_text.replace(\"\u00af\", \"-\")\n    input_text = input_text.replace(\"-\", \",\")\n    input_text = input_text.replace(\"/\", \".\")\n\n    # 7th stage: replace all the punctuation marks other than ',', '.', '\\'', '!', '?' by a space\n    _input_text_tmp = []\n    for char in input_text:\n        if not char.isalpha() and char not in [\",\", \".\", \"'\", \"!\", \"?\"]:\n            _input_text_tmp.append(\" \")\n            continue\n        _input_text_tmp.append(char)\n    input_text = \"\".join(_input_text_tmp)\n\n    # deal with single quotations by different cases\n    _input_text_tmp = []\n    for idx, char in enumerate(input_text):\n        # save all the non-quotation characters\n        if char != \"'\":\n            _input_text_tmp.append(char)\n        # remove the quotations at the beginning or end\n        elif idx == 0 or idx == len(input_text) - 1:\n            continue\n        # remove the quotations not surrounded by letters on both sides\n        elif not input_text[idx - 1].isalpha() or not input_text[idx + 1].isalpha():\n            # if a quotation is surrounded by a letter on the left and a blank on the right, turn it into a comma\n            if input_text[idx - 1].isalpha() and input_text[idx + 1] == \" \":\n                _input_text_tmp.append(\",\")\n            # non-letter and non-blank character -&gt; punctuation marks\n            # turn the quotations surrounded by two punctuation marks into a blank\n            elif is_punc(input_text[idx - 1]) and is_punc(input_text[idx + 1]):\n                _input_text_tmp.append(\" \")\n            # in other cases, remove it\n            else:\n                continue\n        # save the intra-word quotations\n        else:\n            _input_text_tmp.append(char)\n    input_text = \"\".join(_input_text_tmp)\n\n    # 8th stage: question and exclamation marks\n    # remove duplicated questions\n    input_text = re.sub(\"([.,!?]\\s*)+!\", \"!\", input_text)\n    input_text = re.sub(\n        \"([.,!?]\\s*)+\\?\", \"?\", input_text\n    )  # remove duplicated exclamations\n    # remove duplicated periods\n    input_text = re.sub(\"([.,!?]\\s*)+\\.\", \".\", input_text)\n    # remove duplicated commas\n    input_text = re.sub(\"([.,!?]\\s*)+,\", \",\", input_text)\n\n    # remove the blanks and punctuation marks at the beginning\n    while input_text.startswith(\" \") or is_punc(input_text[0]):\n        input_text = \"\".join(input_text[1:])\n    # remove the blanks at the end\n    while input_text.endswith(\" \"):\n        input_text = \"\".join(input_text[:-1])\n\n    # remove useless blanks\n    _input_text_tmp = []\n    for idx, char in enumerate(input_text):\n        if char == \" \":\n            # remove consecutive blanks and replace them by a single blank\n            if input_text[idx + 1] == \" \":\n                continue\n            # remove the blanks surrounded by letters on the left and punctuations on the right\n            elif _input_text_tmp[-1].isalpha() and is_punc(input_text[idx + 1]):\n                continue\n        elif (is_punc(char) and char != \"'\") and idx &lt; len(input_text) - 1:\n            # add a space between punctuation marks on the left and letters on the right\n            if input_text[idx + 1].isalpha():\n                _input_text_tmp.append(f\"{char} \")\n                continue\n            # only retain the last one of consecutive punctuation marks\n            elif is_punc(input_text[idx + 1]):\n                continue\n        _input_text_tmp.append(char)\n    input_text = \"\".join(_input_text_tmp)\n\n    # remain all the punctuation marks\n    if txt_format == \"punc\":\n        return input_text\n\n    # remove all the punctuation marks other than single-quotations\n    elif txt_format == \"no-punc\":\n        # remove all the punctuation symbols other than single quotes\n        return \"\".join(\n            [char for char in input_text if char.isalpha() or char in [\"'\", \" \"]]\n        )\n\n    else:\n        raise ValueError(\n            f\"txt_format must be one of 'punc' or 'no-punc'. But got {txt_format}!\"\n        )\n</code></pre>"},{"location":"reference/utilbox/eval_util/","title":"eval_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/eval_util/#utilbox.eval_util.get_word_edit_alignment","title":"<code>get_word_edit_alignment(hypo, real)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hypo</code> <code>str</code> required <code>real</code> <code>str</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/eval_util.py</code> <pre><code>def get_word_edit_alignment(hypo: str, real: str) -&gt; (int, int, int, str):\n    \"\"\"\n\n    Args:\n        hypo:\n        real:\n\n    Returns:\n\n    \"\"\"\n    # calculate the alignment between the hypothesis words and real words\n    # Note that split(\" \") is not equivalent to split() here\n    # because split(\" \") will give an extra '' at the end of the list if the string ends with a \" \"\n    # while split() doesn't\n    hypo_word_list, real_word_list = hypo.split(), real.split()\n    opcodes = edit_distance.SequenceMatcher(\n        a=hypo_word_list, b=real_word_list\n    ).get_opcodes()\n\n    insertion, deletion, substitution = 0, 0, 0\n    hypo_words, real_words, word_ops = [], [], []\n    # loop each editing operation\n    for i, op in enumerate(opcodes):\n        # insertion operation\n        if op[0] == \"insert\":\n            insertion += 1\n            word_ops.append(\"I\")\n            hypo_words.append(\" \")\n            real_words.append(real_word_list[op[3]])\n        # deletion operation\n        elif op[0] == \"delete\":\n            deletion += 1\n            word_ops.append(\"D\")\n            hypo_words.append(hypo_word_list[op[1]])\n            real_words.append(\" \")\n        # substitution operation\n        elif op[0] == \"replace\":\n            substitution += 1\n            word_ops.append(\"S\")\n            hypo_words.append(hypo_word_list[op[1]])\n            real_words.append(real_word_list[op[3]])\n        # equal condition\n        else:\n            word_ops.append(\" \")\n            hypo_words.append(hypo_word_list[op[1]])\n            real_words.append(real_word_list[op[3]])\n\n    align_table = get_table_strings(\n        contents=[hypo_words, word_ops, real_words],\n        first_col=[\"Hypothesis\", \"Alignment\", \"Reference\"],\n    )\n\n    return insertion, deletion, substitution, align_table\n</code></pre>"},{"location":"reference/utilbox/feat_util/","title":"feat_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.12</p>"},{"location":"reference/utilbox/feat_util/#utilbox.feat_util.convert_wav_to_logmel","title":"<code>convert_wav_to_logmel(wav, n_mels, hop_length, win_length, sr=16000, n_fft=None, preemphasis=None, pre_stft_norm=None, window='hann', center=True, mag_spec=False, fmin=0.0, fmax=None, clamp=1e-10, logging=True, log_base=10.0, htk=False, norm='slaney', delta_order=0, delta_N=2)</code>","text":"<p>For the details about the arguments and returns, please refer to ${SPEECHAIN_ROOT}/speechain/module/frontend/speech2mel.py.</p> Source code in <code>speechain/utilbox/feat_util.py</code> <pre><code>def convert_wav_to_logmel(\n    wav: np.ndarray,\n    n_mels: int,\n    hop_length: int or float,\n    win_length: int or float,\n    sr: int = 16000,\n    n_fft: int = None,\n    preemphasis: float = None,\n    pre_stft_norm: str = None,\n    window: str = \"hann\",\n    center: bool = True,\n    mag_spec: bool = False,\n    fmin: float = 0.0,\n    fmax: float = None,\n    clamp: float = 1e-10,\n    logging: bool = True,\n    log_base: float or None = 10.0,\n    htk: bool = False,\n    norm: str or None = \"slaney\",\n    delta_order: int = 0,\n    delta_N: int = 2,\n) -&gt; np.ndarray:\n    \"\"\"For the details about the arguments and returns, please refer to\n    ${SPEECHAIN_ROOT}/speechain/module/frontend/speech2mel.py.\"\"\"\n    # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n    hop_length = int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n    win_length = int(win_length * sr) if isinstance(win_length, float) else win_length\n\n    # if n_fft is not given, it will be initialized to the window length\n    if n_fft is None:\n        n_fft = win_length\n\n    # --- 1. Waveform -&gt; Linear Spectrogram --- #\n    linear_spec = convert_wav_to_stft(\n        wav=wav,\n        hop_length=hop_length,\n        win_length=win_length,\n        sr=sr,\n        n_fft=n_fft,\n        preemphasis=preemphasis,\n        pre_stft_norm=pre_stft_norm,\n        window=window,\n        center=center,\n        mag_spec=mag_spec,\n        clamp=clamp,\n    )\n\n    # --- 2. Linear Spectrogram -&gt; Mel Spectrogram --- #\n    mel_spec = librosa.feature.melspectrogram(\n        S=linear_spec.transpose(1, 0),\n        sr=sr,\n        n_fft=n_fft,\n        n_mels=n_mels,\n        hop_length=hop_length,\n        win_length=win_length,\n        fmin=fmin,\n        fmax=fmax,\n        window=window,\n        center=center,\n        power=1 if mag_spec else 2,\n        htk=htk,\n        norm=norm,\n    )\n    # take the logarithm operation\n    if logging:\n        # pre-log clamping for numerical stability\n        mel_spec = np.maximum(mel_spec, clamp)\n\n        # go through the logarithm operation for the mel-fbanks\n        mel_spec = np.log(mel_spec)\n        if log_base is not None:\n            mel_spec /= math.log(log_base)\n\n    # --- 3. Log-Mel Spectrogram -&gt; Log-Mel Spectrogram + Deltas --- #\n    return feat_derivation(mel_spec, delta_order, delta_N).transpose(1, 0)\n</code></pre>"},{"location":"reference/utilbox/feat_util/#utilbox.feat_util.convert_wav_to_mfcc","title":"<code>convert_wav_to_mfcc(wav, hop_length, win_length, num_ceps=None, n_mfcc=20, sr=16000, n_fft=None, n_mels=80, preemphasis=None, pre_stft_norm=None, window='hann', center=True, fmin=0.0, fmax=None, clamp=1e-10, logging=True, log_base=10.0, htk=False, norm='slaney', delta_order=0, delta_N=2)</code>","text":"<p>For the details about the arguments and returns, please refer to ${SPEECHAIN_ROOT}/speechain/utilbox/feat_util.convert_wav_to_logmel() and librosa.feature.mfcc.</p> Source code in <code>speechain/utilbox/feat_util.py</code> <pre><code>def convert_wav_to_mfcc(\n    wav: np.ndarray,\n    hop_length: int or float,\n    win_length: int or float,\n    num_ceps: int = None,\n    n_mfcc: int = 20,\n    sr: int = 16000,\n    n_fft: int = None,\n    n_mels: int = 80,\n    preemphasis: float = None,\n    pre_stft_norm: str = None,\n    window: str = \"hann\",\n    center: bool = True,\n    fmin: float = 0.0,\n    fmax: float = None,\n    clamp: float = 1e-10,\n    logging: bool = True,\n    log_base: float or None = 10.0,\n    htk: bool = False,\n    norm: str or None = \"slaney\",\n    delta_order: int = 0,\n    delta_N: int = 2,\n) -&gt; np.ndarray:\n    \"\"\"For the details about the arguments and returns, please refer to\n    ${SPEECHAIN_ROOT}/speechain/utilbox/feat_util.convert_wav_to_logmel() and\n    librosa.feature.mfcc.\"\"\"\n    # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n    hop_length = int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n    win_length = int(win_length * sr) if isinstance(win_length, float) else win_length\n\n    # if n_fft is not given, it will be initialized to the window length\n    if n_fft is None:\n        n_fft = win_length\n\n    # update n_mfcc if num_ceps is too large\n    if num_ceps is not None and num_ceps &gt; n_mfcc - 1:\n        n_mfcc = num_ceps + 1\n\n    # --- 1. Waveform -&gt; Log-Mel Power Spectrogram --- #\n    # log-mel power spectrogram is returned for MFCC calculation\n    mel_spec = convert_wav_to_logmel(\n        wav=wav,\n        n_mels=n_mels,\n        hop_length=hop_length,\n        win_length=win_length,\n        sr=sr,\n        n_fft=n_fft,\n        preemphasis=preemphasis,\n        pre_stft_norm=pre_stft_norm,\n        window=window,\n        center=center,\n        mag_spec=False,\n        fmin=fmin,\n        fmax=fmax,\n        clamp=clamp,\n        logging=logging,\n        log_base=log_base,\n        htk=htk,\n        norm=norm,\n    )\n\n    # --- 2. Log-Mel Power Spectrogram -&gt; MFCC --- #\n    # remove the first mel cepstral coefficient\n    # mfcc = librosa.feature.mfcc(S=mel_spec.transpose(1, 0), sr=sr, n_mfcc=n_mfcc)[1:, :]\n    mfcc = librosa.feature.mfcc(S=mel_spec.transpose(1, 0), sr=sr, n_mfcc=n_mfcc)\n    if num_ceps is not None:\n        mfcc = mfcc[:num_ceps, :]\n\n    # --- 3. MFCC -&gt; MFCC + Deltas --- #\n    return feat_derivation(mfcc, delta_order, delta_N).transpose(1, 0)\n</code></pre>"},{"location":"reference/utilbox/feat_util/#utilbox.feat_util.convert_wav_to_pitch","title":"<code>convert_wav_to_pitch(wav, hop_length=256, sr=22050, f0min=80, f0max=400, continuous_f0=True, return_tensor=False)</code>","text":"<p>The function that converts a waveform to a pitch contour by dio &amp; stonemask of pyworld.</p> <p>Parameters:</p> Name Type Description Default <code>wav</code> <code>ndarray or Tensor</code> <p>(n_sample, 1) or (n_sample,) The waveform to be processed.</p> required <code>hop_length</code> <code>int or float</code> <p>int = 256     The value of the argument 'hop_length' given to pyworld.dio()</p> <code>256</code> <code>sr</code> <code>int</code> <p>int = 22050 The value of the argument 'fs' given to pyworld.dio()</p> <code>22050</code> <code>f0min</code> <code>int</code> <p>int = 80 The value of the argument 'f0min' given to pyworld.dio()</p> <code>80</code> <code>f0max</code> <code>int</code> <p>int = 400 The value of the argument 'f0max' given to pyworld.dio()</p> <code>400</code> <code>continuous_f0</code> <code>bool</code> <p>bool = True Whether to make the calculated pitch values continuous over time.</p> <code>True</code> <code>return_tensor</code> <code>bool</code> <p>bool Whether to return the pitch in torch.Tensor. If False, np.ndarray will be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>f0</code> <code>ndarray or Tensor</code> <p>(n_frame,)</p> Source code in <code>speechain/utilbox/feat_util.py</code> <pre><code>def convert_wav_to_pitch(\n    wav: np.ndarray or torch.Tensor,\n    hop_length: int or float = 256,\n    sr: int = 22050,\n    f0min: int = 80,\n    f0max: int = 400,\n    continuous_f0: bool = True,\n    return_tensor: bool = False,\n) -&gt; np.ndarray or torch.Tensor:\n    \"\"\"The function that converts a waveform to a pitch contour by dio &amp; stonemask of\n    pyworld.\n\n    Args:\n        wav: (n_sample, 1) or (n_sample,)\n            The waveform to be processed.\n        hop_length: int = 256\n                The value of the argument 'hop_length' given to pyworld.dio()\n        sr: int = 22050\n            The value of the argument 'fs' given to pyworld.dio()\n        f0min: int = 80\n            The value of the argument 'f0min' given to pyworld.dio()\n        f0max: int = 400\n            The value of the argument 'f0max' given to pyworld.dio()\n        continuous_f0: bool = True\n            Whether to make the calculated pitch values continuous over time.\n        return_tensor: bool\n            Whether to return the pitch in torch.Tensor. If False, np.ndarray will be returned.\n\n    Returns:\n        f0: (n_frame,)\n    \"\"\"\n    if isinstance(hop_length, float):\n        hop_length = int(hop_length * sr)\n\n    # datatype checking\n    if isinstance(wav, torch.Tensor):\n        wav = to_native(wav, tgt=\"numpy\").astype(np.float64)\n    elif isinstance(wav, np.ndarray):\n        wav = wav.astype(np.float64)\n    else:\n        raise TypeError(\n            f\"wav should be either a torch.Tensor or a np.ndarray, but got type(wav)={type(wav)}!\"\n        )\n\n    # dimension checking\n    if wav.shape[-1] == 1:\n        wav = wav.squeeze(-1)\n    if len(wav.shape) &gt; 2:\n        raise RuntimeError(\n            \"convert_wav_to_pitch doesn't support batch_level pitch extraction!\"\n        )\n\n    f0, timeaxis = pyworld.dio(\n        wav, sr, f0_floor=f0min, f0_ceil=f0max, frame_period=1000 * hop_length / sr\n    )\n    f0 = pyworld.stonemask(wav, f0, timeaxis, sr)\n\n    # borrowed from https://github.com/espnet/espnet/blob/master/espnet2/tts/feats_extract/dio.py#L125\n    if continuous_f0:\n        # padding start and end of f0 sequence\n        start_f0, end_f0 = f0[f0 != 0][0], f0[f0 != 0][-1]\n        start_idx, end_idx = (\n            np.where(f0 == start_f0)[0][0],\n            np.where(f0 == end_f0)[0][-1],\n        )\n        f0[:start_idx], f0[end_idx:] = start_f0, end_f0\n\n        # get non-zero frame index\n        nonzero_idxs = np.where(f0 != 0)[0]\n\n        # perform linear interpolation\n        interp_fn = interp1d(\n            nonzero_idxs,\n            f0[nonzero_idxs],\n            bounds_error=False,\n            fill_value=(start_f0, end_f0),\n        )\n        f0 = interp_fn(np.arange(0, f0.shape[0]))\n\n    if return_tensor:\n        f0 = torch.tensor(f0, dtype=torch.float32)\n    else:\n        f0 = f0.astype(np.float32)\n    return f0\n</code></pre>"},{"location":"reference/utilbox/feat_util/#utilbox.feat_util.convert_wav_to_stft","title":"<code>convert_wav_to_stft(wav, hop_length, win_length, sr=16000, n_fft=None, preemphasis=None, pre_stft_norm=None, window='hann', center=True, mag_spec=False, clamp=1e-10, logging=False, log_base=None)</code>","text":"<p>For the details about the arguments and returns, please refer to ${SPEECHAIN_ROOT}/speechain/module/frontend/speech2linear.py.</p> Source code in <code>speechain/utilbox/feat_util.py</code> <pre><code>def convert_wav_to_stft(\n    wav: np.ndarray,\n    hop_length: int or float,\n    win_length: int or float,\n    sr: int = 16000,\n    n_fft: int = None,\n    preemphasis: float = None,\n    pre_stft_norm: str = None,\n    window: str = \"hann\",\n    center: bool = True,\n    mag_spec: bool = False,\n    clamp: float = 1e-10,\n    logging: bool = False,\n    log_base: float or None = None,\n) -&gt; np.ndarray:\n    \"\"\"For the details about the arguments and returns, please refer to\n    ${SPEECHAIN_ROOT}/speechain/module/frontend/speech2linear.py.\"\"\"\n    # if hop_length and win_length are given in the unit of seconds, turn them into the corresponding time steps\n    hop_length = int(hop_length * sr) if isinstance(hop_length, float) else hop_length\n    win_length = int(win_length * sr) if isinstance(win_length, float) else win_length\n\n    # if n_fft is not given, it will be initialized to the window length\n    if n_fft is None:\n        n_fft = win_length\n\n    # --- 1. Pre-emphasis --- #\n    if preemphasis is not None:\n        wav = preemphasize_wav(wav, preemphasis)\n\n    # --- 2. Waveform Pre-Normalization --- #\n    # normalization for audio signals before STFT\n    if pre_stft_norm is not None:\n        if pre_stft_norm == \"mean_std\":\n            wav = (wav - wav.mean(axis=0)) / wav.std(axis=0)\n        elif pre_stft_norm == \"min_max\":\n            wav_min, wav_max = np.amin(wav, axis=1, keepdims=True), np.amax(\n                wav, axis=1, keepdims=True\n            )\n            wav = (wav - wav_min) / (wav_max - wav_min) * 2 - 1\n        else:\n            raise ValueError\n\n    # --- 3. STFT Processing --- #\n    stft_results = librosa.stft(\n        wav.squeeze(-1) if len(wav.shape) == 2 else wav,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=win_length,\n        window=window,\n        center=center,\n    )\n    linear_spec = np.abs(stft_results) ** 2 + np.angle(stft_results) ** 2\n\n    # --- 4. STFT Post-Processing --- #\n    # convert the energy spectrogram to the magnitude spectrogram if specified\n    if mag_spec:\n        linear_spec = np.sqrt(linear_spec)\n\n    # take the logarithm operation\n    if logging:\n        # pre-log clamping for numerical stability\n        linear_spec = np.maximum(linear_spec, clamp)\n        linear_spec = np.log(linear_spec)\n        if log_base is not None:\n            linear_spec /= math.log(log_base)\n\n    return linear_spec.transpose(1, 0)\n</code></pre>"},{"location":"reference/utilbox/humanfriendly/","title":"humanfriendly","text":"<p>Copied from the main module of the <code>humanfriendly</code> package.</p>"},{"location":"reference/utilbox/humanfriendly/#utilbox.humanfriendly.format_size","title":"<code>format_size(num_bytes, keep_width=False, binary=False)</code>","text":"<p>Format a byte count as a human readable file size.</p> <p>:param num_bytes: The size to format in bytes (an integer). :param keep_width: :data:<code>True</code> if trailing zeros should not be stripped,                    :data:<code>False</code> if they can be stripped. :param binary: :data:<code>True</code> to use binary multiples of bytes (base-2),                :data:<code>False</code> to use decimal multiples of bytes (base-10). :returns: The corresponding human readable file size (a string).</p> <p>This function knows how to format sizes in bytes, kilobytes, megabytes, gigabytes, terabytes and petabytes. Some examples:</p> <p>from humanfriendly import format_size format_size(0) '0 bytes' format_size(1) '1 byte' format_size(5) '5 bytes' format_size(1000) '1 KB' format_size(1024, binary=True) '1 KiB' format_size(1000 ** 3 * 4) '4 GB'</p> Source code in <code>speechain/utilbox/humanfriendly.py</code> <pre><code>def format_size(num_bytes, keep_width=False, binary=False):\n    \"\"\"Format a byte count as a human readable file size.\n\n    :param num_bytes: The size to format in bytes (an integer).\n    :param keep_width: :data:`True` if trailing zeros should not be stripped,\n                       :data:`False` if they can be stripped.\n    :param binary: :data:`True` to use binary multiples of bytes (base-2),\n                   :data:`False` to use decimal multiples of bytes (base-10).\n    :returns: The corresponding human readable file size (a string).\n\n    This function knows how to format sizes in bytes, kilobytes, megabytes,\n    gigabytes, terabytes and petabytes. Some examples:\n\n    &gt;&gt;&gt; from humanfriendly import format_size\n    &gt;&gt;&gt; format_size(0)\n    '0 bytes'\n    &gt;&gt;&gt; format_size(1)\n    '1 byte'\n    &gt;&gt;&gt; format_size(5)\n    '5 bytes'\n    &gt; format_size(1000)\n    '1 KB'\n    &gt; format_size(1024, binary=True)\n    '1 KiB'\n    &gt;&gt;&gt; format_size(1000 ** 3 * 4)\n    '4 GB'\n    \"\"\"\n    for unit in reversed(disk_size_units):\n        if num_bytes &gt;= unit.binary.divider and binary:\n            number = round_number(\n                float(num_bytes) / unit.binary.divider, keep_width=keep_width\n            )\n            return pluralize(number, unit.binary.symbol, unit.binary.symbol)\n        elif num_bytes &gt;= unit.decimal.divider and not binary:\n            number = round_number(\n                float(num_bytes) / unit.decimal.divider, keep_width=keep_width\n            )\n            return pluralize(number, unit.decimal.symbol, unit.decimal.symbol)\n    return pluralize(num_bytes, \"byte\")\n</code></pre>"},{"location":"reference/utilbox/humanfriendly/#utilbox.humanfriendly.pluralize","title":"<code>pluralize(count, singular, plural=None)</code>","text":"<p>Combine a count with the singular or plural form of a word.</p> <p>:param count: The count (a number). :param singular: The singular form of the word (a string). :param plural: The plural form of the word (a string or :data:<code>None</code>). :returns: The count and singular or plural word concatenated (a string).</p> <p>See :func:<code>pluralize_raw()</code> for the logic underneath :func:<code>pluralize()</code>.</p> Source code in <code>speechain/utilbox/humanfriendly.py</code> <pre><code>def pluralize(count, singular, plural=None):\n    \"\"\"Combine a count with the singular or plural form of a word.\n\n    :param count: The count (a number).\n    :param singular: The singular form of the word (a string).\n    :param plural: The plural form of the word (a string or :data:`None`).\n    :returns: The count and singular or plural word concatenated (a string).\n\n    See :func:`pluralize_raw()` for the logic underneath :func:`pluralize()`.\n    \"\"\"\n    return \"%s %s\" % (count, pluralize_raw(count, singular, plural))\n</code></pre>"},{"location":"reference/utilbox/humanfriendly/#utilbox.humanfriendly.pluralize_raw","title":"<code>pluralize_raw(count, singular, plural=None)</code>","text":"<p>Select the singular or plural form of a word based on a count.</p> <p>:param count: The count (a number). :param singular: The singular form of the word (a string). :param plural: The plural form of the word (a string or :data:<code>None</code>). :returns: The singular or plural form of the word (a string).</p> <p>When the given count is exactly 1.0 the singular form of the word is selected, in all other cases the plural form of the word is selected.</p> <p>If the plural form of the word is not provided it is obtained by concatenating the singular form of the word with the letter \"s\". Of course this will not always be correct, which is why you have the option to specify both forms.</p> Source code in <code>speechain/utilbox/humanfriendly.py</code> <pre><code>def pluralize_raw(count, singular, plural=None):\n    \"\"\"Select the singular or plural form of a word based on a count.\n\n    :param count: The count (a number).\n    :param singular: The singular form of the word (a string).\n    :param plural: The plural form of the word (a string or :data:`None`).\n    :returns: The singular or plural form of the word (a string).\n\n    When the given count is exactly 1.0 the singular form of the word is\n    selected, in all other cases the plural form of the word is selected.\n\n    If the plural form of the word is not provided it is obtained by\n    concatenating the singular form of the word with the letter \"s\". Of course\n    this will not always be correct, which is why you have the option to\n    specify both forms.\n    \"\"\"\n    if not plural:\n        plural = singular + \"s\"\n    return singular if float(count) == 1.0 else plural\n</code></pre>"},{"location":"reference/utilbox/humanfriendly/#utilbox.humanfriendly.round_number","title":"<code>round_number(count, keep_width=False)</code>","text":"<p>Round a floating point number to two decimal places in a human friendly format.</p> <p>:param count: The number to format. :param keep_width: :data:<code>True</code> if trailing zeros should not be stripped,                    :data:<code>False</code> if they can be stripped. :returns: The formatted number as a string. If no decimal places are           required to represent the number, they will be omitted.</p> <p>The main purpose of this function is to be used by functions like :func:<code>format_length()</code>, :func:<code>format_size()</code> and :func:<code>format_timespan()</code>.</p> <p>Here are some examples:</p> <p>from humanfriendly import round_number round_number(1) '1' round_number(math.pi) '3.14' round_number(5.001) '5'</p> Source code in <code>speechain/utilbox/humanfriendly.py</code> <pre><code>def round_number(count, keep_width=False):\n    \"\"\"Round a floating point number to two decimal places in a human friendly format.\n\n    :param count: The number to format.\n    :param keep_width: :data:`True` if trailing zeros should not be stripped,\n                       :data:`False` if they can be stripped.\n    :returns: The formatted number as a string. If no decimal places are\n              required to represent the number, they will be omitted.\n\n    The main purpose of this function is to be used by functions like\n    :func:`format_length()`, :func:`format_size()` and\n    :func:`format_timespan()`.\n\n    Here are some examples:\n\n    &gt;&gt;&gt; from humanfriendly import round_number\n    &gt;&gt;&gt; round_number(1)\n    '1'\n    &gt;&gt;&gt; round_number(math.pi)\n    '3.14'\n    &gt;&gt;&gt; round_number(5.001)\n    '5'\n    \"\"\"\n    text = \"%.2f\" % float(count)\n    if not keep_width:\n        text = re.sub(\"0+$\", \"\", text)\n        text = re.sub(r\"\\.$\", \"\", text)\n    return text\n</code></pre>"},{"location":"reference/utilbox/import_util/","title":"import_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/import_util/#utilbox.import_util.get_idle_gpu","title":"<code>get_idle_gpu(ngpu=1, id_only=False)</code>","text":"<p>Find idle GPUs for distributed learning.</p> Source code in <code>speechain/utilbox/import_util.py</code> <pre><code>def get_idle_gpu(ngpu: int = 1, id_only: bool = False) -&gt; List[GPU]:\n    \"\"\"Find idle GPUs for distributed learning.\"\"\"\n    sorted_gpus = sorted(getGPUs(), key=lambda g: g.memoryUtil)\n    if len(sorted_gpus) &lt; ngpu:\n        warnings.warn(\n            f\"Your machine doesn't have enough GPUs ({len(sorted_gpus)}) as you specified ({ngpu})! \"\n            f\"Currently, only {len(sorted_gpus)} GPUs are used.\"\n        )\n    sorted_gpus = sorted_gpus[:ngpu]\n\n    if id_only:\n        return [gpu.id for gpu in sorted_gpus]\n    else:\n        return sorted_gpus\n</code></pre>"},{"location":"reference/utilbox/import_util/#utilbox.import_util.get_idle_port","title":"<code>get_idle_port()</code>","text":"<p>Find an idle port to used for distributed learning.</p> Source code in <code>speechain/utilbox/import_util.py</code> <pre><code>def get_idle_port() -&gt; str:\n    \"\"\"Find an idle port to used for distributed learning.\"\"\"\n    pscmd = \"netstat -ntl |grep -v Active| grep -v Proto|awk '{print $4}'|awk -F: '{print $NF}'\"\n    procs = os.popen(pscmd).read()\n    procarr = procs.split(\"\\n\")\n    tt = str(random.randint(15000, 30000))\n    if tt not in procarr:\n        return tt\n    else:\n        return get_idle_port()\n</code></pre>"},{"location":"reference/utilbox/import_util/#utilbox.import_util.parse_path_args","title":"<code>parse_path_args(input_path)</code>","text":"<p>This function parses the input path string into valid path before its later usage.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>str If input_path starts with '/', no parsing will be done; Otherwise, this function will return its absolute path in the toolkit root, i.e., SPEECHAIN_ROOT.</p> required <p>str</p> Type Description <code>str</code> <p>Parsed valid absolute path.</p> Source code in <code>speechain/utilbox/import_util.py</code> <pre><code>def parse_path_args(input_path: str) -&gt; str:\n    \"\"\"This function parses the input path string into valid path before its later\n    usage.\n\n    Args:\n        input_path: str\n            If input_path starts with '/', no parsing will be done;\n            Otherwise, this function will return its absolute path in the toolkit root, i.e., SPEECHAIN_ROOT.\n\n    Returns: str\n        Parsed valid absolute path.\n    \"\"\"\n\n    # do nothing for the absolute path\n    if input_path.startswith(\"/\"):\n        return input_path\n\n    # turn the general relative path into its absolute value\n    elif input_path.startswith(\".\"):\n        return os.path.abspath(input_path)\n\n    # turn the in-toolkit relative path into its absolute value\n    else:\n        assert \"SPEECHAIN_ROOT\" in os.environ.keys(), (\n            \"SPEECHAIN_ROOT doesn't exist in your environmental variables! \"\n            \"Please move to the toolkit root and execute envir_preparation.sh there to build the toolkit environment!\"\n        )\n        return os.path.join(os.environ[\"SPEECHAIN_ROOT\"], input_path)\n</code></pre>"},{"location":"reference/utilbox/log_util/","title":"log_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/log_util/#utilbox.log_util.distributed_zero_first","title":"<code>distributed_zero_first(distributed, rank)</code>","text":"<p>Decorator to make all other processes in distributed training wait for the master process to do something. Have no influence on the single-GPU training case.</p> <p>However, this ContextManager function will cause an extra GPU memory consumption for each process in the multi-GPU training setting. These memory occupations are neither allocated memory nor reserved memory, which may be the CUDA context memory. I haven't found any effective solutions to release them so far.</p> Source code in <code>speechain/utilbox/log_util.py</code> <pre><code>@contextmanager\ndef distributed_zero_first(distributed: bool, rank: int):\n    \"\"\"Decorator to make all other processes in distributed training wait for the master\n    process to do something. Have no influence on the single-GPU training case.\n\n    However, this ContextManager function will cause an extra GPU memory consumption for\n    each process in the multi-GPU training setting. These memory occupations are neither\n    allocated memory nor reserved memory, which may be the CUDA context memory. I\n    haven't found any effective solutions to release them so far.\n    \"\"\"\n    if distributed and rank != 0:\n        torch.distributed.barrier()\n    yield\n    if distributed and rank == 0:\n        torch.distributed.barrier()\n</code></pre>"},{"location":"reference/utilbox/log_util/#utilbox.log_util.logger_stdout_file","title":"<code>logger_stdout_file(log_path, file_name=None, distributed=False, rank=0, name_candidate=1000)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>log_path</code> required <code>file_name</code> <code>str</code> <code>None</code> <code>rank</code> <code>int</code> <code>0</code> <code>log_candidate</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/log_util.py</code> <pre><code>def logger_stdout_file(\n    log_path,\n    file_name: str = None,\n    distributed: bool = False,\n    rank: int = 0,\n    name_candidate: int = 1000,\n):\n    \"\"\"\n\n    Args:\n        log_path:\n        file_name:\n        rank:\n        log_candidate:\n\n    Returns:\n\n    \"\"\"\n\n    # initialize the logger, use time.time() makes sure that we always get unique logger\n    rootLogger = logging.getLogger(str(time.time()))\n    rootLogger.setLevel(logging.INFO)\n\n    # initialize the file handler\n    logFormatter = logging.Formatter(\n        \"[ %(asctime)s | %(levelname)s ] %(message)s\", \"%d/%m/%Y %H:%M:%S\"\n    )\n    if not os.path.exists(log_path):\n        os.makedirs(log_path, exist_ok=True)\n\n    # return empty logger if no specified file\n    if file_name is None:\n        return rootLogger\n    else:\n        # looping all the candidate names\n        result_log = None\n        for i in range(name_candidate):\n            result_log = os.path.join(\n                log_path, f\"{file_name}.log\" if i == 0 else f\"{file_name}{i}.log\"\n            )\n            # non-existing file is the target\n            if not os.path.exists(result_log):\n                break\n\n    # the logger is only functional for single-GPU training or master process of the multi-GPU training\n    if not distributed or rank == 0:\n        # initialize the file handler for writing the info to the disk\n        fileHandler = logging.FileHandler(result_log)\n        fileHandler.setFormatter(logFormatter)\n        rootLogger.addHandler(fileHandler)\n\n        # we don't initialize the console handler because showing the info on the console may have some problems\n        # consoleHandler = logging.StreamHandler(sys.stdout)\n        # consoleHandler.setFormatter(logFormatter)\n        # rootLogger.addHandler(consoleHandler)\n\n    # For the non-master multi-GPU training processes or testing processes, an empty logger will be returned\n    return rootLogger\n</code></pre>"},{"location":"reference/utilbox/log_util/#utilbox.log_util.model_summary","title":"<code>model_summary(model)</code>","text":"<p>Return the information summary of the model for logging.</p> <p>Codes borrowed from https://github.com/espnet/espnet/blob/a2abaf11c81e58653263d6cc8f957c0dfd9677e7/espnet2/torch_utils/model_summary.py#L48</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/log_util.py</code> <pre><code>def model_summary(model: Model) -&gt; str:\n    \"\"\"Return the information summary of the model for logging.\n\n    Codes borrowed from\n    https://github.com/espnet/espnet/blob/a2abaf11c81e58653263d6cc8f957c0dfd9677e7/espnet2/torch_utils/model_summary.py#L48\n\n    Args:\n        model:\n\n    Returns:\n    \"\"\"\n\n    def get_human_readable_count(number: int) -&gt; str:\n        \"\"\"Return human_readable_count\n        Originated from:\n        https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/memory.py\n        Abbreviates an integer number with K, M, B, T for thousands, millions,\n        billions and trillions, respectively.\n        Examples:\n            &gt;&gt;&gt; get_human_readable_count(123)\n            '123  '\n            &gt;&gt;&gt; get_human_readable_count(1234)  # (one thousand)\n            '1 K'\n            &gt;&gt;&gt; get_human_readable_count(2e6)   # (two million)\n            '2 M'\n            &gt;&gt;&gt; get_human_readable_count(3e9)   # (three billion)\n            '3 B'\n            &gt;&gt;&gt; get_human_readable_count(4e12)  # (four trillion)\n            '4 T'\n            &gt;&gt;&gt; get_human_readable_count(5e15)  # (more than trillion)\n            '5,000 T'\n        Args:\n            number: a positive integer number\n        Return:\n            A string formatted according to the pattern described above.\n        \"\"\"\n        assert number &gt;= 0\n        labels = [\" \", \"K\", \"M\", \"B\", \"T\"]\n        num_digits = int(np.floor(np.log10(number)) + 1 if number &gt; 0 else 1)\n        num_groups = int(np.ceil(num_digits / 3))\n        # don't abbreviate beyond trillions\n        num_groups = min(num_groups, len(labels))\n        shift = -3 * (num_groups - 1)\n        return f\"{number * (10 ** shift):.2f} {labels[num_groups - 1]}\"\n\n    def to_bytes(dtype) -&gt; int:\n        # torch.float16 -&gt; 16\n        return int(str(dtype)[-2:]) // 8\n\n    message = \"Model structure:\\n\"\n    message += str(model)\n    tot_params = sum(p.numel() for p in model.parameters())\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    percent_trainable = \"{:.1f}\".format(num_params * 100.0 / tot_params)\n    tot_params = get_human_readable_count(tot_params)\n    num_params = get_human_readable_count(num_params)\n    message += \"\\n\\nModel summary:\\n\"\n    message += f\"    Class Name: {model.__class__.__name__}\\n\"\n    message += f\"    Total Number of model parameters: {tot_params}\\n\"\n    message += (\n        f\"    Number of trainable parameters: {num_params} ({percent_trainable}%)\\n\"\n    )\n    num_bytes = humanfriendly.format_size(\n        sum(\n            p.numel() * to_bytes(p.dtype) for p in model.parameters() if p.requires_grad\n        )\n    )\n    message += f\"    Size: {num_bytes}\\n\"\n    dtype = next(iter(model.parameters())).dtype\n    message += f\"    Type: {dtype}\"\n    return message\n</code></pre>"},{"location":"reference/utilbox/md_util/","title":"md_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/md_util/#utilbox.md_util.get_list_strings","title":"<code>get_list_strings(content_dict, header_bold=True)</code>","text":"<p>Return the .md string for making a list.</p> <p>Parameters:</p> Name Type Description Default <code>content_dict</code> <code>Dict</code> <p>Dict The main body of the list. Each key-value item corresponds to a row. The key is the header name and the value is the content.</p> required <code>header_bold</code> <code>bool</code> <p>bool Controls whether the header names are bolded.</p> <code>True</code> <p>Returns:</p> Type Description <p>The well-structured .md list string.</p> Source code in <code>speechain/utilbox/md_util.py</code> <pre><code>def get_list_strings(content_dict: Dict, header_bold: bool = True):\n    \"\"\"Return the .md string for making a list.\n\n    Args:\n        content_dict: Dict\n            The main body of the list. Each key-value item corresponds to a row.\n            The key is the header name and the value is the content.\n        header_bold: bool\n            Controls whether the header names are bolded.\n\n    Returns:\n        The well-structured .md list string.\n    \"\"\"\n\n    list_strings = \"\"\n    for header, content in content_dict.items():\n        list_strings += (\n            f\"* {f'**{header}:**' if header_bold else f'{header}:'} {content}\\n\"\n        )\n\n    return list_strings\n</code></pre>"},{"location":"reference/utilbox/md_util/#utilbox.md_util.get_table_strings","title":"<code>get_table_strings(contents, first_col=None, first_col_bold=True, headers=None, header_bold=True)</code>","text":"<p>Return the .md string for making a table.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>List[List] or List</code> <p>List[List] or List The main body of the table. Each list element corresponds to a row.</p> required <code>first_col</code> <code>List</code> <p>List The values of the first column. If not given, no first column will be added.</p> <code>None</code> <code>first_col_bold</code> <code>bool</code> <p>bool Controls whether the values of the first column is bolded.</p> <code>True</code> <code>headers</code> <code>List</code> <p>List The values of the table headers. If not given, no header will be added.</p> <code>None</code> <code>header_bold</code> <code>bool</code> <p>bool Controls whether the values of the header is bolded.</p> <code>True</code> <p>Returns:</p> Type Description <p>The well-structured .md table string.</p> Source code in <code>speechain/utilbox/md_util.py</code> <pre><code>def get_table_strings(\n    contents: List[List] or List,\n    first_col: List = None,\n    first_col_bold: bool = True,\n    headers: List = None,\n    header_bold: bool = True,\n):\n    \"\"\"Return the .md string for making a table.\n\n    Args:\n        contents: List[List] or List\n            The main body of the table. Each list element corresponds to a row.\n        first_col: List\n            The values of the first column. If not given, no first column will be added.\n        first_col_bold: bool\n            Controls whether the values of the first column is bolded.\n        headers: List\n            The values of the table headers. If not given, no header will be added.\n        header_bold: bool\n            Controls whether the values of the header is bolded.\n\n    Returns:\n        The well-structured .md table string.\n    \"\"\"\n    if not isinstance(contents[0], List):\n        contents = [contents]\n    if first_col is not None:\n        assert len(first_col) == len(\n            contents\n        ), \"The lengths of first_col and contents don't match!\"\n    if headers is not None:\n        if first_col is not None:\n            assert (\n                len(headers) == len(contents[0]) + 1\n            ), \"The lengths of headers and contents don't match!\"\n        else:\n            assert len(headers) == len(\n                contents[0]\n            ), \"The lengths of headers and contents don't match!\"\n\n    table_strings = \"\"\n    if headers is not None:\n        table_strings += (\n            \"|\" + \"|\".join([f\"**{h}**\" if header_bold else h for h in headers]) + \"|\\n\"\n        )\n    else:\n        if first_col is None:\n            table_strings += \"|\"\n        table_strings += \"|\" + \"|\".join([\"\" for _ in range(len(contents[0]))]) + \"|\\n\"\n\n    table_strings += \"|---|\" + \"\".join([\"---|\" for _ in contents[0]]) + \"\\n\"\n\n    for i in range(len(contents)):\n        if first_col is None:\n            table_strings += \"|\"\n        else:\n            table_strings += (\n                f\"|{f'**{first_col[i]}**' if first_col_bold else first_col[i]}|\"\n            )\n        table_strings += \"|\".join(contents[i]) + \"|\\n\"\n\n    return table_strings\n</code></pre>"},{"location":"reference/utilbox/regex_util/","title":"regex_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/utilbox/sb_util/","title":"sb_util","text":""},{"location":"reference/utilbox/sb_util/#utilbox.sb_util.SpeechBrainWrapper","title":"<code>SpeechBrainWrapper</code>","text":"<p>               Bases: <code>object</code></p> <p>A wrapper class for the vocoder forward function of the speechbrain package.</p> <p>This wrapper is not implemented as a Module because we don't want it to be in the computational graph of a TTS model.</p> Before wrapping <p>feat -&gt; vocoder -&gt; wav</p> <p>After wrapping:     feat, feat_len -&gt; SpeechBrainWrapper(vocoder) -&gt; wav, wav_len</p> Source code in <code>speechain/utilbox/sb_util.py</code> <pre><code>class SpeechBrainWrapper(object):\n    \"\"\"A wrapper class for the vocoder forward function of the speechbrain package.\n\n    This wrapper is not implemented as a Module because we don't want it to be in the computational graph of a TTS model.\n\n    Before wrapping:\n        feat -&gt; vocoder -&gt; wav\n    After wrapping:\n        feat, feat_len -&gt; SpeechBrainWrapper(vocoder) -&gt; wav, wav_len\n    \"\"\"\n\n    def __init__(self, vocoder: HiFiGAN):\n        self.vocoder = vocoder\n\n    def __call__(self, feat: torch.Tensor, feat_len: torch.Tensor):\n        # feat is (batch, time, channels), need to transpose to (batch, channels, time)\n        wav = self.vocoder.decode_batch(feat.transpose(-2, -1))\n        # wav output is (batch, time) after decode_batch\n        # add channel dimension back and transpose: (batch, 1, time) -&gt; (batch, time, 1)\n        wav = wav.unsqueeze(-1)\n        # the lengths of the shorter utterances in the batch are estimated by their feature lengths\n        wav_len = (feat_len * (wav.size(1) / feat.size(1))).long()\n        # make sure that the redundant parts are set to silence\n        for i in range(len(wav_len)):\n            wav[i][wav_len[i] :] = 0\n        return wav[:, : wav_len.max()], wav_len\n</code></pre>"},{"location":"reference/utilbox/spk_util/","title":"spk_util","text":""},{"location":"reference/utilbox/spk_util/#utilbox.spk_util.extract_spk_feat","title":"<code>extract_spk_feat(spk2wav_dict, gpu_id, spk_emb_model, save_path=None, batch_size=10)</code>","text":"<p>Extract speaker features using a specified speaker embedding model and save them.</p> <p>Parameters:</p> Name Type Description Default <code>spk2wav_dict</code> <code>Dict</code> <p>A dictionary mapping unique IDs to waveform file paths.</p> required <code>gpu_id</code> <code>int</code> <p>The GPU device ID to use. Set to -1 for CPU.</p> required <code>spk_emb_model</code> <code>str</code> <p>The speaker embedding model to use (either 'ecapa' or 'xvector').</p> required <code>save_path</code> <code>str</code> <p>The path to save the extracted speaker features. If not given, the extracted features will be stored in memory. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size for processing. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>Tuple[Dict, Dict]: - A dictionary mapping unique IDs to the corresponding extracted speaker features. - A dictionary mapping speaker IDs to the corresponding average speaker features.</p> Source code in <code>speechain/utilbox/spk_util.py</code> <pre><code>def extract_spk_feat(\n    spk2wav_dict: Dict[str, Dict[str, str]],\n    gpu_id: int,\n    spk_emb_model: str,\n    save_path: str = None,\n    batch_size: int = 10,\n) -&gt; Tuple[Dict, Dict]:\n    \"\"\"Extract speaker features using a specified speaker embedding model and save them.\n\n    Args:\n        spk2wav_dict (Dict):\n            A dictionary mapping unique IDs to waveform file paths.\n        gpu_id (int):\n            The GPU device ID to use. Set to -1 for CPU.\n        spk_emb_model (str):\n            The speaker embedding model to use (either 'ecapa' or 'xvector').\n        save_path (str, optional):\n            The path to save the extracted speaker features. If not given, the extracted features will be stored\n            in memory. Defaults to None.\n        batch_size (int, optional):\n            The batch size for processing. Defaults to 10.\n\n    Returns:\n        Tuple[Dict, Dict]:\n            - A dictionary mapping unique IDs to the corresponding extracted speaker features.\n            - A dictionary mapping speaker IDs to the corresponding average speaker features.\n    \"\"\"\n\n    def proc_curr_batch():\n        \"\"\"Process the current batch of waveforms and extract speaker features.\"\"\"\n        idx_list, wav_list = [i[0] for i in curr_batch], [i[1] for i in curr_batch]\n        wav_len = torch.LongTensor([w.size(0) for w in wav_list]).to(device)\n        max_wav_len = wav_len.max().item()\n\n        # Pad feature vectors into a matrix\n        wav_matrix = torch.zeros((wav_len.size(0), max_wav_len), device=device)\n        for i in range(len(wav_list)):\n            wav_matrix[i][: wav_len[i]] = wav_list[i]\n\n        spk_feat = speechbrain_model.encode_batch(\n            wavs=wav_matrix, wav_lens=wav_len / max_wav_len\n        )\n        if save_path is None:\n            idx2spk_feat.update(\n                dict(zip(idx_list, [to_cpu(s_f, tgt=\"numpy\") for s_f in spk_feat]))\n            )\n        else:\n            idx2spk_feat.update(\n                save_data_by_format(\n                    file_format=\"npy\",\n                    save_path=save_path,\n                    group_ids=spk_id,\n                    file_name_list=idx_list,\n                    file_content_list=[to_cpu(s_f, tgt=\"numpy\") for s_f in spk_feat],\n                )\n            )\n\n        # refresh the current batch\n        return []\n\n    # initialize the speaker embedding model and downloading path for speechbrain API\n    download_dir = parse_path_args(\"datasets/spk_emb_models\")\n    if spk_emb_model == \"ecapa\":\n        speechbrain_args = dict(\n            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n            savedir=os.path.join(download_dir, \"spkrec-ecapa-voxceleb\"),\n        )\n    elif spk_emb_model == \"xvector\":\n        speechbrain_args = dict(\n            source=\"speechbrain/spkrec-xvect-voxceleb\",\n            savedir=os.path.join(download_dir, \"spkrec-xvect-voxceleb\"),\n        )\n    else:\n        raise ValueError(\n            f\"Unknown speaker embedding model ({spk_emb_model})! \"\n            f\"Currently, spk_emb_model should be one of ['ecapa', 'xvector'].\"\n        )\n\n    device = f\"cuda:{gpu_id}\" if gpu_id &gt;= 0 else \"cpu\"\n    speechbrain_args.update(run_opts=dict(device=device))\n    speechbrain_model = EncoderClassifier.from_hparams(**speechbrain_args)\n\n    idx2spk_feat, spk2aver_spk_feat, resamplers = {}, {}, {}\n    # loop each speaker\n    for spk_id, wav_dict in tqdm(spk2wav_dict.items()):\n        # a batch contains only waveforms for a single speaker\n        curr_batch = []\n        # loop each waveform file for each speaker\n        for wav_idx, wav_path in wav_dict.items():\n            # Collect the data into the current batch\n            wav, sample_rate = read_data_by_path(\n                wav_path, return_tensor=True, return_sample_rate=True\n            )\n            wav = wav.squeeze(-1).to(device)\n            if sample_rate &gt; 16000:\n                if sample_rate not in resamplers.keys():\n                    resamplers[sample_rate] = torchaudio.transforms.Resample(\n                        orig_freq=sample_rate, new_freq=16000\n                    ).to(device)\n                wav = resamplers[sample_rate](wav)\n\n            elif sample_rate &lt; 16000:\n                raise RuntimeError\n\n            curr_batch.append([wav_idx, wav])\n            # Process the batch if it meets the given size\n            if len(curr_batch) == batch_size:\n                # refresh the current batch\n                curr_batch = proc_curr_batch()\n\n        # Process the remaining incomplete batch\n        if len(curr_batch) != 0:\n            curr_batch = proc_curr_batch()\n\n        # calculate the average speaker embedding for each speaker\n        spk_feat_list, failed_idx_list = [], []\n        # loop each waveform file for each speaker and check the saved data\n        for wav_idx in wav_dict.keys():\n            if isinstance(idx2spk_feat[wav_idx], str):\n                try:\n                    spk_feat_list.append(read_data_by_path(idx2spk_feat[wav_idx]))\n                except ValueError:\n                    # record the waveform index whose speaker embedding dumping failed\n                    failed_idx_list.append(wav_idx)\n            else:\n                spk_feat_list.append(idx2spk_feat[wav_idx])\n\n        # keep looping until no failed waveforms remain\n        while len(failed_idx_list) != 0:\n            # loop each failed waveform\n            for wav_idx in failed_idx_list:\n                wav, sample_rate = read_data_by_path(\n                    wav_dict[wav_idx], return_tensor=True, return_sample_rate=True\n                )\n                wav = wav.squeeze(-1).to(device)\n                if sample_rate &gt; 16000:\n                    wav = resamplers[sample_rate](wav)\n                curr_batch.append([wav_idx, wav])\n\n            # reprocess the failed waveforms and check again\n            curr_batch = proc_curr_batch()\n            for wav_idx in failed_idx_list:\n                try:\n                    spk_feat_list.append(idx2spk_feat[wav_idx])\n                except ValueError:\n                    # the waveform remains in the failed list if the error happens again\n                    pass\n                else:\n                    # remove the waveform if no error happens\n                    failed_idx_list.remove(wav_idx)\n\n        aver_spk_feat = np.mean(spk_feat_list, axis=0)\n        # Save the average speaker features in memory or to disk\n        if save_path is None:\n            spk2aver_spk_feat[spk_id] = aver_spk_feat\n        else:\n            spk2aver_spk_feat.update(\n                save_data_by_format(\n                    file_format=\"npy\",\n                    save_path=save_path,\n                    group_ids=spk_id,\n                    file_name_list=spk_id,\n                    file_content_list=aver_spk_feat,\n                )\n            )\n    return idx2spk_feat, spk2aver_spk_feat\n</code></pre>"},{"location":"reference/utilbox/tensor_util/","title":"tensor_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/test_humanfriendly/","title":"test_humanfriendly","text":""},{"location":"reference/utilbox/text_util/","title":"text_util","text":""},{"location":"reference/utilbox/train_util/","title":"train_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.07</p>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.float_near_round","title":"<code>float_near_round(input_float)</code>","text":"<p>Round the float number in [X.0, X.5) to X and the float number in (X.5, {X+1}.0] to X+1.</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def float_near_round(input_float: float):\n    \"\"\"Round the float number in [X.0, X.5) to X and the float number in (X.5, {X+1}.0]\n    to X+1.\"\"\"\n    int_part = int(input_float)\n    frac_part = input_float - int_part\n    if frac_part &lt; 0.5:\n        return int_part\n    else:\n        return int_part + 1\n</code></pre>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.get_min_indices_by_freq","title":"<code>get_min_indices_by_freq(freq_dict, shuffle=True, chosen_idx_num=1, freq_weights=None)</code>","text":"<p>Get the specified number of indices with minimum values from the input frequency dictionary, optionally applying frequency weights, and return the updated frequency dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>freq_dict</code> <code>Dict[Any, Union[int, float]]</code> <p>The input frequency dictionary containing the values to be compared.</p> required <code>shuffle</code> <code>bool</code> <p>If True, shuffle the input frequency dictionary before selecting the minimum indices. Defaults to True.</p> <code>True</code> <code>chosen_idx_num</code> <code>int</code> <p>The number of minimum indices to return. If not provided, all indices will be returned.</p> <code>1</code> <code>freq_weights</code> <code>List[Union[int, float]]</code> <p>The frequency weights to apply when selecting the minimum indices. Should have the same length as the number of indices to return. If not provided, equal weights will be applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List, Dict[Any, Union[int, float]]]</code> <p>Tuple[List[int], Dict[Any, Union[int, float]]]: A tuple containing a list of the selected minimum indices and the updated frequency dictionary.</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def get_min_indices_by_freq(\n    freq_dict: Dict[Any, Union[int, float]],\n    shuffle: bool = True,\n    chosen_idx_num: int = 1,\n    freq_weights: Union[int, float] or List[Union[int, float]] = None,\n) -&gt; Tuple[List, Dict[Any, Union[int, float]]]:\n    \"\"\"Get the specified number of indices with minimum values from the input frequency\n    dictionary, optionally applying frequency weights, and return the updated frequency\n    dictionary.\n\n    Args:\n        freq_dict (Dict[Any, Union[int, float]]):\n            The input frequency dictionary containing the values to be compared.\n        shuffle (bool, optional):\n            If True, shuffle the input frequency dictionary before selecting the minimum indices. Defaults to True.\n        chosen_idx_num (int, optional):\n            The number of minimum indices to return. If not provided, all indices will be returned.\n        freq_weights (List[Union[int, float]], optional):\n            The frequency weights to apply when selecting the minimum indices.\n            Should have the same length as the number of indices to return.\n            If not provided, equal weights will be applied.\n\n    Returns:\n        Tuple[List[int], Dict[Any, Union[int, float]]]:\n            A tuple containing a list of the selected minimum indices and the updated frequency dictionary.\n    \"\"\"\n\n    # Ensure the frequency weights have the same length as the number of indices to return\n    if freq_weights is not None:\n        if not isinstance(freq_weights, List):\n            freq_weights = [freq_weights]\n        assert len(freq_weights) == chosen_idx_num\n    else:\n        freq_weights = [1 for _ in range(chosen_idx_num)]\n\n    min_indices = []\n\n    # Select the minimum indices based on the frequency weights\n    for i in range(chosen_idx_num):\n        index_freq_list = list(freq_dict.items())\n        if shuffle:\n            random.shuffle(index_freq_list)\n\n        # Sort the indices based on their values in the input frequency dictionary\n        sorted_indices = [idx for idx, _ in sorted(index_freq_list, key=lambda x: x[1])]\n\n        # Append the last index (minimum value) to the list of minimum indices\n        min_indices.append(sorted_indices[0])\n\n        # Update the input frequency dictionary by adding the frequency weight to the value of the selected index\n        freq_dict[sorted_indices[0]] += freq_weights[i]\n\n    return min_indices, freq_dict\n</code></pre>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.make_mask_from_len","title":"<code>make_mask_from_len(data_len, max_len=None, mask_type=torch.bool, return_3d=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_len</code> <code>Tensor</code> <p>(batch_size,) The length of each sequence in the batch</p> required <code>max_len</code> <code>int</code> <p>int The max length of the mask matrix. Could be larger than the real length of data_len</p> <code>None</code> <code>mask_type</code> <code>dtype</code> <p>torch.dtype The value type of the mask matric.</p> <code>bool</code> <code>return_3d</code> <code>bool</code> <p>bool Whether to return a 3d mask tensors. If True, the returned mask tensor will be 3d (batch_size, 1, max_len) If False, the returned mask tensor will be 2d (batch_size, max_len)</p> <code>True</code> <p>Returns:</p> Type Description <p>The corresponding mask for this batch.</p> <p>The parts at the end of the shorter sequence will be False or 0.0.</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def make_mask_from_len(\n    data_len: torch.Tensor,\n    max_len: int = None,\n    mask_type: torch.dtype = torch.bool,\n    return_3d: bool = True,\n):\n    \"\"\"\n\n    Args:\n        data_len: (batch_size,)\n            The length of each sequence in the batch\n        max_len: int\n            The max length of the mask matrix. Could be larger than the real length of data_len\n        mask_type: torch.dtype\n            The value type of the mask matric.\n        return_3d: bool\n            Whether to return a 3d mask tensors.\n            If True, the returned mask tensor will be 3d (batch_size, 1, max_len)\n            If False, the returned mask tensor will be 2d (batch_size, max_len)\n\n    Returns:\n        The corresponding mask for this batch.\n        The parts at the end of the shorter sequence will be False or 0.0.\n\n    \"\"\"\n    batch_size = data_len.size(0)\n    if max_len is None:\n        max_len = data_len.max()\n    else:\n        assert (\n            max_len &gt;= data_len.max()\n        ), \"max_len should be larger than the maximum of data_len!\"\n\n    if return_3d:\n        mask = torch.zeros((batch_size, 1, max_len), dtype=mask_type)\n        for i in range(data_len.size(0)):\n            mask[i, :, : data_len[i]] = 1.0\n    else:\n        mask = torch.zeros((batch_size, max_len), dtype=mask_type)\n        for i in range(data_len.size(0)):\n            mask[i, : data_len[i]] = 1.0\n\n    if data_len.is_cuda:\n        mask = mask.cuda(data_len.device)\n    return mask\n</code></pre>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.recur_criterion_init","title":"<code>recur_criterion_init(input_conf, criterion_class)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_conf</code> <code>Dict</code> required <code>criterion_class</code> <code>Criterion</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def recur_criterion_init(input_conf: Dict, criterion_class: Criterion):\n    \"\"\"\n\n    Args:\n        input_conf:\n        criterion_class:\n\n    Returns:\n\n    \"\"\"\n    #\n    leaf_flags = [not isinstance(value, Dict) for value in input_conf.values()]\n\n    #\n    if sum(leaf_flags) == len(input_conf):\n        return {\n            key: recur_criterion_init(value, criterion_class)\n            for key, value in input_conf.items()\n        }\n    #\n    elif sum(leaf_flags) == 0:\n        return criterion_class(**input_conf)\n    else:\n        raise RuntimeError\n</code></pre>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.spk2tensor","title":"<code>spk2tensor(spk_list, spk2idx_dict)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>spk_list</code> <code>List[str]</code> required <code>spk2idx_dict</code> <code>Dict</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def spk2tensor(spk_list: List[str], spk2idx_dict: Dict) -&gt; torch.LongTensor:\n    \"\"\"\n\n    Args:\n        spk_list:\n        spk2idx_dict:\n\n    Returns:\n\n    \"\"\"\n    # turn the speaker id strings into the tensors\n    return torch.LongTensor(\n        [spk2idx_dict[spk] if spk in spk2idx_dict.keys() else 0 for spk in spk_list]\n    )\n</code></pre>"},{"location":"reference/utilbox/train_util/#utilbox.train_util.text2tensor_and_len","title":"<code>text2tensor_and_len(text_list, text2tensor_func, ignore_idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text_list</code> <code>List[str or List[str]]</code> required <code>text2tensor_func</code> required <code>ignore_idx</code> <code>int</code> required <p>Returns:</p> Source code in <code>speechain/utilbox/train_util.py</code> <pre><code>def text2tensor_and_len(\n    text_list: List[str or List[str]], text2tensor_func, ignore_idx: int\n) -&gt; (torch.LongTensor, torch.LongTensor):\n    \"\"\"\n\n    Args:\n        text_list:\n        text2tensor_func:\n        ignore_idx:\n\n    Returns:\n\n    \"\"\"\n    for i in range(len(text_list)):\n        text_list[i] = text2tensor_func(text_list[i])\n    text_len = torch.LongTensor([len(t) for t in text_list])\n\n    text = torch.full(\n        (text_len.size(0), text_len.max().item()), ignore_idx, dtype=text_len.dtype\n    )\n    for i in range(text_len.size(0)):\n        text[i][: text_len[i]] = text_list[i]\n    return text, text_len\n</code></pre>"},{"location":"reference/utilbox/type_util/","title":"type_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.08</p>"},{"location":"reference/utilbox/type_util/#utilbox.type_util.str2dict","title":"<code>str2dict(input_str)</code>","text":"<p>Examples:</p> <p>input string:     a:{b:12.3,c:{d:123,e:{g:xyz}}},g:xyz</p> <p>output dict:     a:         b: 12.3         c:             d: 123             e:                 g:xyz     g: xyz</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>str The input string to be parsed into the corresponding nested dict</p> required <p>Dict</p> Type Description <code>Dict or str</code> <p>The nested dict after parsing</p> Source code in <code>speechain/utilbox/type_util.py</code> <pre><code>def str2dict(input_str: str) -&gt; Dict or str:\n    \"\"\"\n    Examples:\n        input string:\n            a:{b:12.3,c:{d:123,e:{g:xyz}}},g:xyz\n\n        output dict:\n            a:\n                b: 12.3\n                c:\n                    d: 123\n                    e:\n                        g:xyz\n            g: xyz\n\n    Args:\n        input_str: str\n            The input string to be parsed into the corresponding nested dict\n\n    Returns: Dict\n        The nested dict after parsing\n\n    \"\"\"\n\n    def recur_dict_init(unproc_string: str):\n        assert (not unproc_string.startswith(\"{\")) and (not unproc_string.endswith(\"}\"))\n\n        # there is no commas inside\n        if \",\" not in unproc_string:\n            assert \":\" in unproc_string and unproc_string.count(\":\") == 1\n            key, value = unproc_string.split(\":\")[0], unproc_string.split(\":\")[-1]\n\n            # continue the recursion for the register matches in the Dict\n            if value.startswith(\"match_\"):\n                return {key: recur_dict_init(match_dict[value])}\n            elif value.startswith(\"list_match_\"):\n                return {key: str2list(list_match_dict[value])}\n            else:\n                # convert the digital string into a integer\n                if value.isdigit():\n                    value = int(value)\n                # convert the non-integer string into a float number\n                elif \".\" in value and value.replace(\".\", \"\").isdigit():\n                    value = float(value)\n                # convert the string in the str2list format into a List\n                elif value.startswith(\"[\") and value.endswith(\"]\"):\n                    value = str2list(value)\n                else:\n                    try:\n                        value = str2bool(value)\n                    except ValueError:\n                        pass\n                return {key: value}\n\n        # there are commas inside\n        else:\n            # remove the brackets at the beginning and the end\n            proc_list = unproc_string.split(\",\")\n            proc_list = [recur_dict_init(ele) for ele in proc_list]\n            proc_dict = proc_list[0]\n            for i in range(1, len(proc_list)):\n                proc_dict.update(proc_list[i])\n            return proc_dict\n\n    # remove the blanks\n    input_str = input_str.replace(\" \", \"\")\n    # remove the line breaks\n    input_str = input_str.replace(\"\\n\", \"\")\n    # remove the tabs\n    input_str = input_str.replace(\"\\t\", \"\")\n\n    # if the input string is not given in the specified format, directly return it because it may be a path.\n    if \"{\" not in input_str and \"}\" not in input_str:\n        if input_str == \"\":\n            return dict()\n        if \":\" not in input_str:\n            return input_str\n        else:\n            return recur_dict_init(input_str)\n\n    # if only a pair of braces is input, return an empty dict\n    if input_str == \"{}\":\n        return dict()\n\n    # input string checking\n    assert (not input_str.startswith(\"{\")) or (not input_str.endswith(\"}\")), (\n        \"If you want the framework to automatically convert your input string into a Dict, \"\n        \"please don't surround it by a pair of braces '{}'.\"\n    )\n    assert input_str.count(\"{\") == input_str.count(\n        \"}\"\n    ), \"The number of left braces '{' doesn't match that of right braces '}'.\"\n\n    # register all the smallest {}-surrounded sub-strings into a nested Dict\n    match_dict, match_num = {}, 0\n    while True:\n        regex_matches = regex_brace.findall(input_str)\n        if len(regex_matches) == 0:\n            break\n\n        for match in regex_matches:\n            # remove the brackets and register the match sub-string\n            match_dict[f\"match_{match_num}\"] = match[1:-1]\n            input_str = input_str.replace(match, f\"match_{match_num}\", 1)\n            match_num += 1\n\n    list_match_dict, list_match_num = {}, 0\n    for key in match_dict.keys():\n        list_matches = regex_square_bracket_large.findall(match_dict[key])\n        for list_match in list_matches:\n            # remove the brackets and register the match sub-string\n            list_match_dict[f\"list_match_{list_match_num}\"] = list_match\n            match_dict[key] = match_dict[key].replace(\n                list_match, f\"list_match_{list_match_num}\", 1\n            )\n            list_match_num += 1\n\n    return recur_dict_init(input_str)\n</code></pre>"},{"location":"reference/utilbox/type_util/#utilbox.type_util.str2list","title":"<code>str2list(input_str)</code>","text":"<p>Examples:</p> <p>input string:     [a,[1,2,[1.1,2.2,3.3],[h,i,j,k]],c,[d,e,[f,g,[h,i,j,k]]]]</p> <p>output list:     - 'a'     - - 1       - 2       - - 1.1         - 2.2         - 3.3       - - 'h'         - 'i'         - 'j'         - 'k'     - 'c'     - - 'd'       - 'e'       - - 'f'         - 'g'         - - 'h'           - 'i'           - 'j'           - 'k'</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>str The input string to be parsed into the corresponding nested list</p> required <p>List</p> Type Description <code>List</code> <p>The nested list after parsing</p> Source code in <code>speechain/utilbox/type_util.py</code> <pre><code>def str2list(input_str: str) -&gt; List:\n    \"\"\"\n    Examples:\n        input string:\n            [a,[1,2,[1.1,2.2,3.3],[h,i,j,k]],c,[d,e,[f,g,[h,i,j,k]]]]\n\n        output list:\n            - 'a'\n            - - 1\n              - 2\n              - - 1.1\n                - 2.2\n                - 3.3\n              - - 'h'\n                - 'i'\n                - 'j'\n                - 'k'\n            - 'c'\n            - - 'd'\n              - 'e'\n              - - 'f'\n                - 'g'\n                - - 'h'\n                  - 'i'\n                  - 'j'\n                  - 'k'\n\n    Args:\n        input_str: str\n            The input string to be parsed into the corresponding nested list\n\n    Returns: List\n        The nested list after parsing\n\n    \"\"\"\n\n    def cast_single_string(single_string: str):\n        # convert the digital string into a integer\n        if single_string.isdigit():\n            return int(single_string)\n        # convert the non-integer string into a float number\n        elif \".\" in single_string and single_string.replace(\".\", \"\").isdigit():\n            return float(single_string)\n\n        # for other strings, remain the same type\n        else:\n            try:\n                return str2bool(single_string)\n            except ValueError:\n                return single_string\n\n    def recur_list_init(unproc_string: str):\n        assert (not unproc_string.startswith(\"[\")) and (not unproc_string.endswith(\"]\"))\n\n        # there is no commas inside\n        if \",\" not in unproc_string:\n            # continue the recursion for the register matches in the Dict\n            if unproc_string.startswith(\"match_\"):\n                return recur_list_init(match_dict[unproc_string])\n            # cast the leaf string into its corresponding type\n            else:\n                return cast_single_string(unproc_string)\n\n        # there are commas inside\n        else:\n            # remove the brackets at the beginning and the end\n            proc_list = unproc_string.split(\",\")\n            return [recur_list_init(ele) for ele in proc_list]\n\n    # remove the blanks\n    input_str = input_str.replace(\" \", \"\")\n    # remove the line breaks\n    input_str = input_str.replace(\"\\n\", \"\")\n    # remove the tabs\n    input_str = input_str.replace(\"\\t\", \"\")\n\n    # for the input string in the form of 'X,X,X', directly return a list\n    if \"[\" not in input_str and \"]\" not in input_str:\n        return [cast_single_string(s) for s in input_str.split(\",\")]\n\n    # input string checking\n    assert input_str.startswith(\"[\") and input_str.endswith(\"]\"), (\n        \"If you want the framework to automatically convert your input string into a list, \"\n        \"please surround it by a pair of square brackets '[]'.\"\n    )\n    assert input_str.count(\"[\") == input_str.count(\n        \"]\"\n    ), \"The number of left square brackets '[' doesn't match that of right square brackets ']'.\"\n\n    # register all the smallest []-surrounded sub-strings into a nested Dict\n    match_dict, match_num = {}, 0\n    while True:\n        regex_matches = regex_square_bracket.findall(input_str)\n        if len(regex_matches) == 0:\n            break\n\n        for match in regex_matches:\n            # remove the brackets and register the match sub-string\n            match_dict[f\"match_{match_num}\"] = match[1:-1]\n            input_str = input_str.replace(match, f\"match_{match_num}\", 1)\n            match_num += 1\n\n    return recur_list_init(input_str)\n</code></pre>"},{"location":"reference/utilbox/yaml_util/","title":"yaml_util","text":"<p>Author: Heli Qi Affiliation: NAIST Date: 2022.09</p>"},{"location":"reference/utilbox/yaml_util/#utilbox.yaml_util.load_yaml","title":"<code>load_yaml(yaml_file)</code>","text":"<p>Loads a YAML file and returns its content as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>str or IO</code> <p>The path to the YAML file or a file-like object.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>The parsed YAML content as a dictionary.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the input YAML file does not exist.</p> Source code in <code>speechain/utilbox/yaml_util.py</code> <pre><code>def load_yaml(yaml_file) -&gt; Dict:\n    \"\"\"Loads a YAML file and returns its content as a dictionary.\n\n    Args:\n        yaml_file (str or IO): The path to the YAML file or a file-like object.\n\n    Returns:\n        Dict: The parsed YAML content as a dictionary.\n\n    Raises:\n        AssertionError: If the input YAML file does not exist.\n    \"\"\"\n    # turn the input file path into file IO stream\n    if isinstance(yaml_file, str):\n        assert os.path.exists(\n            yaml_file\n        ), f\"Your input .yaml file {yaml_file} doesn't exist!\"\n        yaml_file = open(yaml_file, mode=\"r\", encoding=\"utf-8\")\n\n    # parse the yaml file with !-prefixed representers\n    ruamel_yaml = ruamel.yaml.YAML()\n    yaml_config = reform_config_dict(ruamel_yaml.load(yaml_file))\n\n    # modify the value of each item in yaml_config in-place if there is an !-prefixed representer\n    yaml_config = remove_representer(yaml_config, yaml_config)\n\n    return yaml_config\n</code></pre>"},{"location":"reference/utilbox/yaml_util/#utilbox.yaml_util.reform_config_dict","title":"<code>reform_config_dict(input_config)</code>","text":"<p>Recursively reformat a configuration dictionary to convert rumel.yaml data types into normal Python data types.</p> <p>Parameters:</p> Name Type Description Default <code>input_config</code> <code>Dict or List or ScalarFloat or PlainScalarString</code> <p>The input configuration dictionary.</p> required <p>Returns:</p> Type Description <p>Dict or List or float or str: The reformatted configuration dictionary with normal Python data types.</p> Source code in <code>speechain/utilbox/yaml_util.py</code> <pre><code>def reform_config_dict(input_config):\n    \"\"\"Recursively reformat a configuration dictionary to convert rumel.yaml data types\n    into normal Python data types.\n\n    Args:\n        input_config (Dict or List or ScalarFloat or PlainScalarString):\n            The input configuration dictionary.\n\n    Returns:\n        Dict or List or float or str: The reformatted configuration dictionary with normal Python data types.\n    \"\"\"\n    if isinstance(input_config, Dict):\n        return {\n            str(key): reform_config_dict(value) for key, value in input_config.items()\n        }\n    elif isinstance(input_config, List):\n        return [reform_config_dict(item) for item in input_config]\n    else:\n        # Convert rumel.yaml data type into normal Python data type\n        if isinstance(input_config, ScalarFloat):\n            input_config = float(input_config)\n        elif isinstance(input_config, PlainScalarString):\n            input_config = str(input_config)\n        return input_config\n</code></pre>"},{"location":"reference/utilbox/yaml_util/#utilbox.yaml_util.remove_representer","title":"<code>remove_representer(parent_node, reference, curr_key=None)</code>","text":"<p>Recursively removes representers from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>parent_node</code> <code>Dict or List</code> <p>The parent node in the configuration dictionary.</p> required <code>reference</code> <code>Dict</code> <p>The reference dictionary for resolving references.</p> required <code>curr_key</code> <code>str</code> <p>The current key within the parent node. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dict or List or any: The updated configuration dictionary with removed representers.</p> Source code in <code>speechain/utilbox/yaml_util.py</code> <pre><code>def remove_representer(parent_node, reference, curr_key=None):\n    \"\"\"Recursively removes representers from a configuration dictionary.\n\n    Args:\n        parent_node (Dict or List):\n            The parent node in the configuration dictionary.\n        reference (Dict):\n            The reference dictionary for resolving references.\n        curr_key (str, optional):\n            The current key within the parent node. Defaults to None.\n\n    Returns:\n        Dict or List or any: The updated configuration dictionary with removed representers.\n    \"\"\"\n\n    def get_reference_value(_ref: str):\n        # Get the reference key\n        ref_key = _ref[1:-1]\n        if \"[\" in ref_key and \"]\" in ref_key:\n            assert ref_key.count(\"[\") == ref_key.count(\n                \"]\"\n            ) and not has_nested_structure(ref_key)\n\n            left_indices = [idx for idx, char in enumerate(ref_key) if char == \"[\"]\n            right_indices = [idx for idx, char in enumerate(ref_key) if char == \"]\"]\n            ref_key_main = ref_key[: left_indices[0]]\n            ref_key_indices = [\n                int(ref_key[left_idx + 1 : right_idx])\n                for left_idx, right_idx in zip(left_indices, right_indices)\n            ]\n\n            reference_value = reference[ref_key_main]\n            for idx in ref_key_indices:\n                assert not hasattr(\n                    ref_key_indices, \"tag\"\n                ), f\"{ref_key_indices} should be clarified before {ref_key}!\"\n                reference_value = reference_value[idx]\n        else:\n            assert not hasattr(\n                reference[ref_key], \"tag\"\n            ), f\"{reference[ref_key]} should be clarified before {ref_key}!\"\n            reference_value = reference[ref_key]\n\n        return reference_value\n\n    child_node = parent_node[curr_key] if curr_key is not None else parent_node\n\n    if isinstance(child_node, Dict):\n        return {\n            key: remove_representer(child_node, reference, key)\n            for key, value in child_node.items()\n        }\n    elif isinstance(child_node, List):\n        return [\n            remove_representer(child_node, reference, i)\n            for i, item in enumerate(child_node)\n        ]\n    else:\n        # For the item with an !-prefixed representer\n        if hasattr(child_node, \"tag\"):\n            # For '!ref' representer\n            if child_node.tag.value == \"!ref\":\n                ref_string = child_node.value\n                # Standalone '!ref' without &lt;&gt; reference or the anchor points where the reference is already done\n                if regex_angle_bracket.search(ref_string) is None:\n                    parent_node[curr_key] = parent_node[curr_key].value\n                # '!ref' with &lt;&gt; reference\n                else:\n                    # &lt;&gt; reference-only without any additional information, retain the data type of the reference\n                    if regex_angle_bracket.fullmatch(ref_string):\n                        # get the reference key\n                        parent_node[curr_key] = get_reference_value(ref_string)\n                    # reference with additional information, data type will be str\n                    else:\n                        ref_matches = regex_angle_bracket.findall(ref_string)\n\n                        # loop each match surrounded by &lt;&gt;\n                        for ref in ref_matches:\n                            # get the reference key\n                            parent_node[curr_key].value = parent_node[\n                                curr_key\n                            ].value.replace(ref, str(get_reference_value(ref)))\n                        # assign the value to input_config after looping\n                        parent_node[curr_key] = parent_node[curr_key].value\n\n            # turn the string '(x, x, ..., x)' into tuple by '!tuple'\n            elif child_node.tag.value == \"!tuple\":\n                parent_node[curr_key] = tuple(\n                    [\n                        int(i) if i.isnumeric() else i\n                        for i in parent_node[curr_key]\n                        .value[1:-1]\n                        .replace(\" \", \"\")\n                        .split(\",\")\n                    ]\n                )\n\n            # turn the string '[x, x, ..., x]' into list by '!list'\n            elif child_node.tag.value == \"!list\":\n                parent_node[curr_key] = [\n                    int(i) if i.isnumeric() else i\n                    for i in parent_node[curr_key]\n                    .value[1:-1]\n                    .replace(\" \", \"\")\n                    .split(\",\")\n                ]\n\n            # turn the numerical value into string by '!str'\n            elif child_node.tag.value == \"!str\":\n                parent_node[curr_key] = str(parent_node[curr_key].value)\n\n        return parent_node[curr_key]\n</code></pre>"}]}