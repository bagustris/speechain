{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SpeeChain","text":"<p>A Machine Speech Chain Toolkit for ASR, TTS and Both</p> <p>SpeeChain is an open-source PyTorch-based speech and language processing toolkit produced by the AHC lab at Nara Institute of Science and Technology (NAIST).  This toolkit is designed to simplify the pipeline of the research on the machine speech chain,  i.e. the joint model of automatic speech recognition (ASR) and text-to-speech synthesis (TTS). </p> <p>SpeeChain is currently in beta. Contribution to this toolkit is warmly welcomed anywhere anytime! </p> <p>If you find our toolkit helpful for your research, we sincerely hope that you can give us a star\u2b50!  Anytime when you encounter problems when using our toolkit, please don't hesitate to leave us an issue!</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Machine Speech Chain</li> <li>Toolkit Characteristics</li> <li>Get a Quick Start</li> </ol>"},{"location":"#machine-speech-chain","title":"Machine Speech Chain","text":"<ul> <li>Offline TTS\u2192ASR Chain</li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"#toolkit-characteristics","title":"Toolkit Characteristics","text":"<ul> <li>Data Processing:<ul> <li>On-the-fly Log-Mel Spectrogram Extraction  </li> <li>On-the-fly SpecAugment  </li> <li>On-the-fly Feature Normalization  </li> </ul> </li> <li>Model Training:<ul> <li>Multi-GPU Model Distribution based on torch.nn.parallel.DistributedDataParallel </li> <li>Real-time status reporting by online Tensorboard and offline Matplotlib </li> <li>Real-time learning dynamics visualization (attention visualization, spectrogram visualization)  </li> </ul> </li> <li>Data Loading:<ul> <li>On-the-fly mixture of multiple datasets in a single dataloader.  </li> <li>On-the-fly data selection for each dataloader to filter the undesired data samples.  </li> <li>Multi-dataloader batch generation to form training batches by multiple datasets.   </li> </ul> </li> <li>Optimization:<ul> <li>Model training can be done by multiple optimizers. Each optimizer is responsible for a specific part -  model parameters.  </li> <li>Gradient accumulation for mimicking the large-batch gradients by the ones on several small batches.  </li> <li>Easy-to-set finetuning factor to scale down the learning rates without any modification of the scheduler configuration.  </li> </ul> </li> <li>Model Evaluation:<ul> <li>Multi-level .md evaluation reports (overall-level, group-level model, and sample-level) without any - yout misplacement.  </li> <li>Histogram visualization for the distribution of evaluation metrics.  </li> <li>TopN bad case analysis for better model diagnosis.</li> </ul> </li> </ul> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"#get-a-quick-start","title":"Get a Quick Start","text":"<p>We recommend you first install Anaconda into your machine before using our toolkit.  After the installation of Anaconda, please follow the steps below to deploy our toolkit on your machine:</p> <ol> <li>Find a path with enough disk memory space. (e.g. at least 500GB if you want to use LibriSpeech or LibriTTS datasets).  </li> <li>Clone our toolkit by <code>git clone https://github.com/bagustris/SpeeChain.git</code>.  </li> <li>Go to the root path of our toolkit by <code>cd SpeeChain</code>.  </li> <li>Run <code>source envir_preparation.sh</code> to build the environment for SpeeChain toolkit. After execution, a virtual environment named <code>speechain</code> will be created and two environmental variables <code>SPEECHAIN_ROOT</code> and <code>SPEECHAIN_PYTHON</code> will be initialized in your <code>~/.bashrc</code>. Note: It must be executed in the root path <code>SpeeChain</code> and by the command <code>source</code> rather than <code>./envir_preparation.sh</code>.  </li> <li>Run <code>conda activate speechain</code> in your terminal to examine the installation of Conda environment.  If the environment <code>speechain</code> is not successfully activated, please run <code>conda env create -f environment.yaml</code>, <code>conda activate speechain</code> and <code>pip install -e ./</code> to manually install it.  </li> <li> <p>Run <code>echo ${SPEECHAIN_ROOT}</code> and <code>echo ${SPEECHAIN_PYTHON}</code> in your terminal to examine the environmental variables. If either one is empty, please manually add them into your <code>~/.bashrc</code> by <code>export SPEECHAIN_ROOT=xxx</code> or <code>export SPEECHAIN_PYTHON=xxx</code> and then activate them by <code>source ~/.bashrc</code>. </p> <ul> <li> <p><code>SPEECHAIN_ROOT</code> should be the absolute path of the <code>SpeeChain</code> folder you have just cloned (i.e. <code>/xxx/SpeeChain</code> where <code>/xxx/</code> is the parent directory); </p> </li> <li> <p><code>SPEECHAIN_PYTHON</code> should be the absolute path of the python compiler in the folder of <code>speechain</code> environment (i.e. <code>/xxx/anaconda3/envs/speechain/bin/python3.X</code> where <code>/xxx/</code> is where your <code>anaconda3</code> is placed and <code>X</code> depends on <code>environment.yaml</code>).  </p> </li> </ul> </li> <li> <p>Read the handbook and start your journey in SpeeChain!  </p> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/","title":"Automatic Speech Recognition (ASR)","text":"<p>\ud83d\udc46Back to the recipe README.md</p>"},{"location":"asr/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Available Backbones</li> <li>Pretrained Models for Reproducibility</li> <li>Training an ASR model</li> <li>Creating your own ASR model</li> </ol>"},{"location":"asr/#available-backbones","title":"Available Backbones","text":"Dataset Subset Configuration WER w/o. LM WER w. LM librispeech train-clean-100 100-bpe5k_transformer-wide_lr2e-3  8.40% / 21.92%   5.50% / 15.56%  100-bpe5k_conformer-small_lr2e-3  8.50% / 23.50%   5.45% / 16.74%  100-bpe5k_conformer-medium_lr2e-3  7.87% / 21.36%   5.30% / 15.57%  100-bpe5k_conformer-large_lr2e-3  7.30% / 20.24%   5.33% / 15.15%  train-clean-460 460-bpe5k_transformer-large  % / %   % / %  460-bpe5k_conformer-large  % / %   % / %  train-960 960-bpe5k_transformer-large  % / %   % / %  960-bpe5k_conformer-large  % / %   % / %  libritts_librispeech train-960 960-bpe5k_transformer-large  % / %   % / %  <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#pretrained-models-for-reproducibility","title":"Pretrained Models for Reproducibility","text":"<p>For reproducibility of our ASR model configuration files in <code>${SPEECHAIN_ROOT}/recipes/asr/</code>, we provide the following pretrained models to ensure consistent performance: 1. SentencePiece tokenizer models (GoogleDrive).     * Please download tokenizer model and vocabulary to where your dataset is dumped. The default path is <code>${SPEECHAIN_ROOT}/datasets</code>.  Note: If your dataset is dumped outside SpeeChain, please replace <code>${SPEECHAIN_ROOT}/datasets</code> in the following commands by your place.    * LibriSpeech:      1. train-clean-100:            ```           # Download BPE model           gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=1OQGKJqpEykl6hQwx-3xuWeaEdU7kDVTS</p> <pre><code>      # Download BPE vocabulary\n      gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=1lGhDi2NI_ukI5Z9Z-MjOHoUEz5YGcJX9\n      ```\n 2. **train-clean-460:** \n      ```\n      # Download BPE model\n      gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=1WAmcHvilnzB7r0kgND91LLReeX_NW7A4\n\n      # Download BPE vocabulary\n      gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=1isfQn75l-Szu0WPGBr1ZaR3pb20SDa2F\n      ```\n 3. **train-960:** \n      ```\n      # Download BPE model\n      gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=1uFRm1DcPivFqjo1Kiy-uVSFTf3e9y1S4\n\n      # Download BPE vocabulary by \n      gdown -O ${SPEECHAIN_ROOT}/datasets/librispeech/data/sentencepiece/train-clean-100/bpe5k/no-punc https://drive.google.com/uc?id=13ERNAA_5glzjvJkmUW7edf94IPfAejiL\n      ```\n</code></pre> <ol> <li>Transformer-based language models (GoogleDrive).</li> <li>Please download both LM model and configuration file. The default path is <code>${SPEECHAIN_ROOT}/recipes/lm</code>.  Note: If you want to store model files outside SpeeChain, please replace <code>${SPEECHAIN_ROOT}/recipes/lm</code> in the following commands by your place. Also, change the <code>lm_cfg_path</code> and <code>lm_model_path</code> arguments in each ASR configuration file.</li> <li>LibriSpeech:<ol> <li> <p>train-clean-100:        ```       # Download LM model       gdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer_gelu/models https://drive.google.com/uc?id=1ZsFKRb8UBpzDjWcjpQcxt4rT4J6CuKAm</p> <p># Download LM configuration   gdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/lm_text/exp/100-bpe5k_transformer_gelu https://drive.google.com/uc?id=1TRtIOu2ptXTKphY77cF3fne9b-i-vNJD   <code>3. **train-960:**</code>   # Download LM model   gdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/train-960_lm_text/exp/960-bpe5k_transformer_gelu/models https://drive.google.com/uc?id=1hMFlaJVojonyBiwaXmC_I7iLxI-kSWit</p> <p># Download LM configuration   gdown -O ${SPEECHAIN_ROOT}/recipes/lm/librispeech/train-960_lm_text/exp/960-bpe5k_transformer_gelu https://drive.google.com/uc?id=1fAMDjPxBDnTp2tiUNTNIMtqGl-vQ2gzz   ```</p> </li> </ol> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#training-an-asr-model","title":"Training an ASR model","text":"<p>Before training an ASR model, ensure that your target datasets are dumped by the scripts in <code>${SPEECHAIN_ROOT}/datasets/{your-target-dataset}</code>. More details on how to dump a dataset can be found here.</p>"},{"location":"asr/#use-an-existing-dataset-with-a-pre-tuned-configuration","title":"Use an existing dataset with a pre-tuned configuration","text":"<ol> <li>locate a .yaml configuration file in <code>${SPEECHAIN_ROOT}/recipes/asr</code>.     Suppose we want to train an ASR model by the configuration <code>${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml</code>.</li> <li>Train and evaluate the ASR model on your target training set    <code>cd ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960    bash run.sh --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb (--ngpu x --gpus x,x)</code> Note: </li> <li>Review the comments on the top of the configuration file to ensure that your computational resources fit the configuration before training the model.       If your resources do not match the configuration, adjust it by <code>--ngpu</code> and <code>--gpus</code> to match your available GPU memory.</li> <li>To save the experimental results outside the toolkit folder <code>${SPEECHAIN_ROOT}</code>,        specify your desired location by appending <code>--train_result_path {your-target-path}</code> to <code>bash run.sh</code>.       In this example, <code>bash run.sh --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --train_result_path /a/b/c</code>       will save results to <code>/a/b/c/960-bpe5k_transformer-wide_ctc_perturb</code>.</li> </ol>"},{"location":"asr/#creating-a-new-configuration-for-a-non-existing-dataset","title":"Creating a new configuration for a non-existing dataset","text":"<ol> <li>Dump your target dataset from the Internet following these instructions.</li> <li>Create a folder for your dumped dataset <code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}</code>:    <code>mkdir ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}    cp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/run.sh ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/run.sh</code> Note: </li> <li>Update the arguments <code>dataset</code> and <code>subset</code> (line no.16 &amp; 17) in <code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/run.sh</code>:       <code>dataset=librispeech -&gt; 'your-new-dataset'       subset='train-960' -&gt; 'your-new-dataset'</code></li> <li>Copy a pre-tuned configuration file into your newly created folder.     Suppose we want to use the configuration <code>${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml</code>:    <code>cd ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}    mkdir ./data_cfg ./exp_cfg    cp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml ./exp_cfg</code> Note: </li> <li> <p>Update the dataset arguments at the beginning of your selected configuration:       ```       # dataset-related       dataset: librispeech -&gt; 'your-new-dataset'       train_set: train-960 -&gt; 'your-target-subset'       valid_set: dev -&gt; 'valid-set-of-new-dataset'</p> <p># tokenizer-related   txt_format: asr   vocab_set: train-960 -&gt; 'your-target-subset'   token_type: sentencepiece   token_num: bpe5k   <code>4. Train the ASR model on your target training set:</code>    cd ${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}    bash run.sh --test false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb (--ngpu x --gpus x,x)    <code>**Note:**     1. `--test false` is used to skip the testing stage.    2. Ensure your computational resources match the configuration before training the model.    3. To save experimental results outside ${SPEECHAIN_ROOT}, specify your desired location by appending --train_result_path {your-target-path} to bash run.sh. 5. Tune the inference hyperparameters on the corresponding validation set</code>    cp ${SPEECHAIN_ROOT}/recipes/asr/librispeech/train-960/data_cfg/test_dev-clean+other.yaml ./data_cfg    mv ./data_cfg/test_dev-clean.yaml ./data_cfg/test_{your-valid-set-name}.yaml    bash run.sh --train false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --data_cfg test_{your-valid-set-name}    <code>**Note:**     1. Update the dataset arguments in `./data_cfg/test_{your-valid-set-name}.yaml`:</code>   dataset: librispeech -&gt; 'your-new-dataset'   valid_dset: &amp;valid_dset dev-clean -&gt; &amp;valid_dset 'valid-set-of-new-dataset'   <code>2. `--train false` is used to skip the training stage.    3. `--data_cfg` switches the data loading configuration from the original one for training in exp_cfg to the one for validation tuning.    4. To access experimental results saved outside `${SPEECHAIN_ROOT}`, append `--train_result_path {your-target-path}` to `bash run.sh`. 6. Evaluate the trained ASR model on the official test sets</code>    bash run.sh --train false --exp_cfg 960-bpe5k_transformer-wide_ctc_perturb --infer_cfg \"{the-best-configuration-you-get-during-validation-tuning}\"    <code>``    **Note:**     1.</code>--train false<code>is used to skip the training stage.    2. There are two ways to specify the optimal</code>infer_cfg<code>tuned on the validation set:   1. Update</code>infer_cfg<code>in</code>${SPEECHAIN_ROOT}/recipes/asr/{your-new-dataset}/{your-target-subset}/exp_cfg/960-bpe5k_transformer-wide_ctc_perturb.yaml<code>.   2. Provide a parsable string as the value for</code>--infer_cfg<code>in the terminal. For example,</code>beam_size:16,ctc_weight:0.2<code>can be converted into a dictionary with two key-value items (</code>beam_size=16<code>and</code>ctc_weight=0.2<code>).     For more details about this syntax, refer to [**here**](https://github.com/ahclab/SpeeChain/blob/main/handbook.md#convertable-arguments-in-the-terminal).    3. To access experimental results saved outside</code>${SPEECHAIN_ROOT}<code>, append</code>--train_result_path {your-target-path}<code>to</code>bash run.sh`.</p> </li> </ol> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"asr/#how-to-create-your-own-asr-model","title":"How to create your own ASR model","text":"<p>The detailed instructions for creating your own ASR model using SpeeChain are coming soon.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/","title":"Datasets","text":"<p>This folder contains all the available datasets in this toolkit.  Each dataset corresponds to a sub-folder and has a uniform file system.  You can easily dump your target dataset to your machine by following the instructions below.  If you want to contribute a new dataset, we would appreciate it if you could follow our file systems and metadata formats.</p> <p>\ud83d\udc46Back to the handbook page</p>"},{"location":"datasets/#table-of-contents","title":"Table of Contents","text":"<ol> <li>File System</li> <li>Metadata Format<ol> <li>idx2wav</li> <li>idx2wav_len</li> <li>idx2feat</li> <li>idx2feat_len</li> <li>idx2text</li> <li>idx2spk</li> <li>idx2spk_feat</li> <li>spk_list</li> <li>idx2gen</li> </ol> </li> <li>How to Dump a Dataset on your Machine</li> <li>How to Extract Speaker Embedding by my own model</li> <li>How to Contribute a New Dataset</li> </ol>"},{"location":"datasets/#file-system","title":"File System","text":"<pre><code>/datasets\n    /pyscripts                  # fixed .py scripts provided by the toolkit\n    data_dumping.sh             # the shared .sh script across all the speech-text datasets. It contains the complete pipeline of data dumping.\n    meta_generator.py           # the abstract .py script used by each dataset to decide their own meta generation logic.\n    meta_post_processor.py      # the abstract .py script used by each dataset to decide their own meta post-processing logic.\n    /{dataset_name}             # root folder of each dataset\n        /data                       # main folder of each dataset (the folder name 'data' is shared across all the datasets)\n            /wav                        # waveform folder (the folder name 'wav' is shared across all the datasets)\n                /{subset_name}              # the name of a subset in this dataset \n                    /wav_{comp_file_ext}        # (optional) the folder that contains the compressed package files of all the waveform data\n                    idx2wav_{comp_file_ext}     # (optional) the file that contains the pairs of index and the absolute address of waveforms in the compressed package files\n                    idx2wav                     # the file that contains the pairs of index and the absolute address of waveform files\n                    idx2wav_len                 # the file that contains the pairs of index and waveform length\n                    idx2{txt_format}_text       # the file that contains the pairs of index and transcript text. (txt_format is the format of processed text which is used to distinguish different idx2text files)\n                    ...                         # (optional) some other metadata files available in the dataset (such as idx2spk, idx2spk_feat, idx2gen, ...)\n            /wav{sample_rate}           # (optional) downsampled waveform folder (sample_rate is the samping rate of waveforms after downsampling)\n                ...                         # same structure as '/wav'\n            /{feat_config}              # (optional) acoustic feature folder by a given configuration (feat_config is the name of the configuration file)\n                /{subset_name}              # the name of a subset in this dataset \n                    /feat_{comp_file_ext}       # (optional) the folder that contains the compressed chunk files of all the acoustic feature data\n                    idx2feat_{comp_file_ext}    # (optional) the file that contains the pairs of index and the absolute address of waveforms in the compressed package files\n                    idx2feat                    # the file that contains the pairs of index and the absolute address of acoustic feature files\n                    idx2feat_len                # the file that contains the pairs of index and feature length\n                    idx2{txt_format}_text       # the file that contains the pairs of index and transcript text. (txt_format is the format of processed text which is used to distinguish different idx2text files)\n                    ...                         # (optional) some metadata files available in the dataset (such as idx2spk, idx2gen, ...)\n            /{token_type}               # token folder of a specific type (token_type is the name of the specified token type)\n                /{src_subset}               # the source subset used for generating the token vocabulary (src_subset is the name of the used source subset)\n                    /{token_config}             # the configuration for generating the vocabulary list.\n                        /{txt_format}               # the format of text data used to generate the token vocabulary\n                            idx2text                    # the file that contains the pairs of index and transcript text after tokenization\n                            idx2text_len                # the file that contains the pairs of index and transcript text length after tokenization\n                            vocab                       # token vocabulary\n                            model                       # (optional) tokenizer model. Used for sentencepiece tokenizers.\n        run.sh                      # the dataset-specific .sh script that controls how the data dumping for the corresponding dataset is going\n        data_download.sh            # the dataset-specific .sh script that downloads the corresponding dataset from the internet\n        meta_generator.py           # the dataset-specific .py script that generates the meta data files of the dataset (idx2wav, idx2spk, ...)\n        meta_post_processor.py      # (optional) the dataset-specific .py script that performs post-processing for the meta data files (needed by some datasets like LibriSpeech)\n</code></pre> <p>The names in the braces({}) mean the undefined names depending on the settings of datasets and configuration.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#metadata-format","title":"Metadata Format","text":"<p>Metadata files are suffix-free .txt files used to access data instances during training.  The data formats of metadata files are uniform for all datasets.  The format uniformity enables the automatic configuration for the data loading part of our toolkit.  The format for each metadata file is shown below.</p>"},{"location":"datasets/#idx2wav","title":"idx2wav","text":"<p>In <code>idx2wav</code>, each line corresponds to a pair of file index and the absolute address of raw waveforms.  Index and address are separated by a blank. </p> <p>For example, </p> <pre><code>103-1240-0000 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0000.flac\n103-1240-0001 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0001.flac\n103-1240-0002 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0002.flac\n103-1240-0003 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0003.flac\n103-1240-0004 ${SPEECHAIN_ROOT}/datasets/librispeech/data/wav/train-clean-100/103/1240/103-1240-0004.flac\n</code></pre> <p>Any audio files that can be processed by <code>soundfile.read()</code> (such as .flac, .wav, ...) are OK in idx2wav.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2wav_len","title":"idx2wav_len","text":"<p>In <code>idx2wav_len</code>, each line corresponds to a pair of file index and file length which are separated by a blank.  The file length means the number of sampling points of the waveform.</p> <p>For example, </p> <pre><code>103-1240-0000 225360\n103-1240-0001 255120\n103-1240-0002 223120\n103-1240-0003 235360\n103-1240-0004 200240\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2feat","title":"idx2feat","text":"<p>In <code>idx2feat</code>, each line corresponds to a pair of file index and absolute address of the acoustic feature.  The index and absolute address are separated by a blank. </p> <p>For example, </p> <pre><code>103-1240-0000 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0000.npz\n103-1240-0001 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0001.npz\n103-1240-0002 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0002.npz\n103-1240-0003 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0003.npz\n103-1240-0004 ${SPEECHAIN_ROOT}/datasets/librispeech/data/{feat_config}/train-clean-100/103-1240-0004.npz\n</code></pre> <p>feat_config is the name of the used feature extraction configuration file. In our toolkit, acoustic feature of a waveform is saved as a .npy file by the NumPy package.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2feat_len","title":"idx2feat_len","text":"<p>In <code>idx2feat_len</code>, each line corresponds to a pair of file index and file length which are separated by a blank.  The file length means the number of time frames in the extracted acoustic features (e.g. log-mel spectrogram or MFCC).</p> <p>For example, </p> <pre><code>103-1240-0000 1408\n103-1240-0001 1594\n103-1240-0002 1394\n103-1240-0003 1471\n103-1240-0004 1251\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2text","title":"idx2text","text":"<p>In <code>idx2text</code>, each line corresponds to a pair of file index and transcript text string which are separated by a blank.  <code>idx2text</code> will be renamed as <code>idx2{txt_format}_text</code> to indicate the text processing format used to generate this file. In our toolkit, there are many available text processing format to generate transcript text strings with different styles.</p> <p>For example, </p> <pre><code>103-1240-0000 chapter one missus rachel lynde is surprised missus rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies eardrops and traversed by a brook\n103-1240-0001 that had its source away back in the woods of the old cuthbert place it was reputed to be an intricate headlong brook in its earlier course through those woods with dark secrets of pool and cascade but by the time it reached lynde's hollow it was a quiet well conducted little stream\n103-1240-0002 for not even a brook could run past missus rachel lynde's door without due regard for decency and decorum it probably was conscious that missus rachel was sitting at her window keeping a sharp eye on everything that passed from brooks and children up\n103-1240-0003 and that if she noticed anything odd or out of place she would never rest until she had ferreted out the whys and wherefores thereof there are plenty of people in avonlea and out of it who can attend closely to their neighbor's business by dint of neglecting their own\n103-1240-0004 but missus rachel lynde was one of those capable creatures who can manage their own concerns and those of other folks into the bargain she was a notable housewife her work was always done and well done she ran the sewing circle\n</code></pre> <p>Note: you don't need to worry about the blanks inside each transcript text string.  Those additional blanks will be ignored in the subsequent processing.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2spk","title":"idx2spk","text":"<p>In <code>idx2spk</code>, each line corresponds to a pair of file index and speaker ID. </p> <p>For example, </p> <pre><code>103-1240-0000 103\n103-1240-0001 103\n103-1240-0002 103\n103-1240-0003 103\n103-1240-0004 103\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2spk_feat","title":"idx2spk_feat","text":"<p>In <code>idx2spk_feat</code>, each line corresponds to a pair of file index and the absolute address of a speaker embedding. </p> <p>For example, </p> <pre><code>1034_121119_000001_000001 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000001_000001.npy\n1034_121119_000002_000001 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000002_000001.npy\n1034_121119_000010_000004 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000010_000004.npy\n1034_121119_000010_000006 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000010_000006.npy\n1034_121119_000012_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000012_000000.npy\n1034_121119_000014_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000014_000000.npy\n1034_121119_000018_000000 ${SPEECHAIN_ROOT}/datasets/libritts/data/wav/train-clean-100/xvector/1034_121119_000018_000000.npy\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#spk_list","title":"spk_list","text":"<p>In spk_list, each line corresponds the string ID of a speaker in the corresponding subset of the dataset.  This file is mainly used for training multi-speaker TTS models.</p> <p>For example, </p> <pre><code>116\n1255\n1272\n1462\n1585\n1630\n1650\n1651\n1673\n1686\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#idx2gen","title":"idx2gen","text":"<p>In idx2gen, each line corresponds to a pair of file index and gender. </p> <p>For example, </p> <pre><code>103-1240-0000 F\n103-1240-0001 F\n103-1240-0002 F\n103-1240-0003 F\n103-1240-0004 F\n</code></pre> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-dump-a-dataset-on-your-machine","title":"How to Dump a Dataset on your Machine","text":"<p>For dumping an existing dataset,     1. Go to the folder of your target dataset <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> (e.g. if you want to dump LibriTTS, please go to <code>${SPEECHAIN_ROOT}/datasets/libritts</code>)    2. Run <code>bash run.sh --help</code> to familiarize yourself with the involved arguments.    3. Run <code>bash run.sh</code> to dump your target dataset (add some arguments if needed).</p> <p>Note:    1. If you already have the decompressed dataset on your disk, please attach the argument <code>--src_path {the-path-of-your-existing-dataset}</code> to the command <code>bash run.sh</code> in the no.3 step above.    Please make sure that <code>src_path</code> is an absolute path starting with a slash '/' and the content of <code>src_path</code> should be exactly the same with the one downloaded from the internet (please see the help message of <code>--src_path</code> in each <code>run.sh</code>).    2. If you want to save the dumped data and metadata files outside the toolkit folder (<code>${SPEECHAIN_ROOT}</code>), please attach the argument <code>--tgt_path {the-path-you-want-to-save-files}</code> to the command <code>bash run.sh</code> in the no.3 step above.    Please make sure that <code>tgt_path</code> is an absolute path starting with a slash '/'.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-extract-speaker-embedding-by-my-own-model","title":"How to Extract Speaker Embedding by my own model","text":"<p>If you want to use the pretrained speaker embedding model on your machine, please 1. Don't give the argument <code>--spk_emb_model</code> when running the command <code>bash run.sh</code> 3. Write your own extraction script. You can use the metadata files <code>idx2wav</code> and <code>idx2wav_len</code> to read and organize the audio files.  Please save all the speaker embedding vectors to a specific folder in the same directory of <code>idx2wav</code> and give a metadata file named <code>idx2spk_feat</code> for data reference. Note:     1. For the file format of <code>idx2spk_feat</code>, please click here for reference.    2. Please keep the same data index with <code>idx2wav</code> in your <code>idx2spk_feat</code>.    3. Each speaker embedding vector should be in the shape of <code>[1, spk_feat_dim]</code>.    4. Speaker embedding vectors could be saved in two ways:       1. save each vector to an individual <code>.npy</code> file       2. save all vectors to a <code>.npz</code> file where the index of each vector is exactly the one in <code>idx2spk_feat</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"datasets/#how-to-contribute-a-new-dataset","title":"How to Contribute a New Dataset","text":"<p>If the dataset that you want to use for your experiments is not included here,  you could make the dumping pipeline of your target dataset by the following instructions: 1. Go to <code>${SPEECHAIN_ROOT}/datasets/</code>. 2. Run <code>bash data_dumping.sh --help</code> to familiarize yourself with the involved arguments. 3. Make a new folder in <code>${SPEECHAIN_ROOT}/datasets/</code> with the name as your target dataset. 4. Make a new data_download.sh in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to download your target dataset from the internet. Please download the dataset into <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}/data/wav</code>. 5. Make a new meta_generator.py in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to extract the metadata files of your target dataset.  Please refer to <code>${SPEECHAIN_ROOT}/datasets/meta_generator.py</code> for instructions of how to override the pipeline of metadata generation. 6. If needed, make a new meta_post_processor.py in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to post-process the extracted metadata files of all the subsets.  (e.g. combine train-clean-100 and train-clean-360 of LibriSpeech into train-clean-460) Please refer to <code>${SPEECHAIN_ROOT}/datasets/meta_post_processor.py</code> for instructions of how to override the pipeline of metadata post-processing. 7. Make a new run.sh in <code>${SPEECHAIN_ROOT}/datasets/{dataset_name}</code> to manipulate the dumping pipeline of your target dataset.  You could refer to the ones in the existing dataset folders as a template.</p> <p>Note: Please keep the same script names (i.e., <code>data_download.sh</code>, <code>meta_generator.py</code>, and <code>meta_post_processor.py</code>) for the compatibility with <code>data_dumping.sh</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"readme/","title":"Readme","text":""},{"location":"readme/#how-to-build-speechain-documentation","title":"How to build speechain documentation","text":"<pre><code># install necessary packages\npip install mkdocs-material mkdocstrings[python]\n\n# build documentation\nmkdocs build\n\n# serve documentation\nmkdocs serve\n\n# deploy to github pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. See the speechain repo for more details.</p>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset","title":"<code>speechain.dataset.speech_text.SpeechTextDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>This Dataset subclass is mainly used by ASR and TTS models. In this subclass, each data instance is made up of an utterance and a sentence as well as the speaker information (speaker ID + speaker embedding feature).</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>class SpeechTextDataset(Dataset):\n    \"\"\"\n    This Dataset subclass is mainly used by ASR and TTS models.\n    In this subclass, each data instance is made up of an utterance and a sentence as well as the speaker information\n    (speaker ID + speaker embedding feature).\n\n    \"\"\"\n\n    def dataset_init_fn(\n        self,\n        use_g2p: bool = False,\n        unk_mask_prob: float = 0.0,\n        use_speed_perturb: bool = False,\n        sample_rate: int = 16000,\n        perturb_range: List[float] = [0.9, 1.0, 1.1],\n        pitch_conf: Dict = None,\n    ):\n        \"\"\"\n\n        Args:\n            # phoneme-related\n            use_g2p: bool = False\n                Whether to process the raw string by G2P. We don't recommend you to turn it on because on-the-fly\n                transformer from string to phoneme list consumes a lot of CPU resources.\n            # waveform-related\n            use_speed_perturb: bool = False\n                Whether to perturb the speed of the waveforms\n            sample_rate: int = 16000\n            perturb_range: List[float] = [0.9, 1.0, 1.1]\n            # pitch-related\n            pitch_conf: Dict = None\n                The configuration given to convert_wav_to_pitch() for pitch extraction.\n                If not given, pitch extraction will not be done on-the-fly.\n\n        \"\"\"\n        # register sampling rate for later check\n        self.sample_rate = sample_rate\n        warnings.warn(\n            f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n            f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n            f\"Please make sure that {sample_rate} is the same with your model! \"\n            f\"If this is not your target sampling rate, \"\n            f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n            f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n        )\n\n        assert (\n            0 &lt;= unk_mask_prob &lt;= 1\n        ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n        self.unk_mask_prob = unk_mask_prob\n\n        # phoneme extraction\n        if use_g2p:\n            self.g2p = G2p()\n\n        if use_speed_perturb:\n            self.perturb_range = perturb_range\n            self.speed_resampler_list = [\n                torchaudio.transforms.Resample(\n                    orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n                )\n                for factor in perturb_range\n            ]\n\n        # pitch extraction\n        if pitch_conf is not None:\n            if \"sr\" in pitch_conf.keys():\n                assert pitch_conf[\"sr\"] == self.sample_rate, (\n                    f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                    f\"given sample_rate ({self.sample_rate})!\"\n                )\n            pitch_conf[\"sr\"] = self.sample_rate\n            self.pitch_extract_fn = partial(\n                convert_wav_to_pitch, return_tensor=True, **pitch_conf\n            )\n\n    @staticmethod\n    def data_len_register_fn(\n        main_data: Dict[str, Dict[str, str]]\n    ) -&gt; Dict[str, int or float] or None:\n        \"\"\"\n\n        Returns:\n            If 'text' is given in main_data, return the number of characters in each sentence.\n            Otherwise, return None\n\n        \"\"\"\n        if \"text\" in main_data.keys():\n            return {key: len(value) for key, value in main_data[\"text\"].items()}\n        else:\n            return None\n\n    def collate_main_data_fn(\n        self, batch_dict: Dict[str, List]\n    ) -&gt; Dict[str, torch.Tensor or List]:\n        \"\"\"\n        The utterances used for training ASR and TTS models may have different lengths, so we need to do the\n        padding operations to make them equal in length.\n\n        The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n        vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n        Args:\n            batch_dict: Dict[str, List]\n            The keys of the input `batch_dict` dictionary should be one of the following:\n                1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n                2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n                3. `text`: a List of text strings.\n                4. `spk_ids`: a List of speaker ID strings.\n                5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n        Returns: Dict[str, torch.Tensor or List]\n            `feat` and `spk_feat` are in the form of three-dimensional `torch.Tensor`;\n            `text` and `spk_ids` are in the form of List of raw strings whose discretization is done in the Model object.\n\n        \"\"\"\n\n        # --- 1. Pad Speech Data and Stack them together --- #\n        if \"feat\" in batch_dict.keys():\n            # para init\n            feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n            batch_size, feat_maxlen, feat_dim = (\n                len(batch_dict[\"feat\"]),\n                feat_len.max().item(),\n                batch_dict[\"feat\"][0].shape[-1],\n            )\n\n            # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n            # overwrite the padding matrix with each feat vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                    feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n                elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                    feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            # update 'feat' and attach 'feat_len' for later model forward\n            batch_dict[\"feat\"] = feat\n            batch_dict[\"feat_len\"] = feat_len\n\n        # --- 2. Pad Pitch Data and Stack them together --- #\n        if \"pitch\" in batch_dict.keys():\n            # para init\n            pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n            batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n            # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n            pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n            # overwrite the padding matrix with each pitch vector\n            for i in range(batch_size):\n                # process feat data based on data type\n                if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                    pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n                elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                    pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n                # only support np.ndarray and torch.Tensor now\n                else:\n                    raise TypeError\n\n            batch_dict[\"pitch\"] = pitch\n            batch_dict[\"pitch_len\"] = pitch_len\n\n        # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n        if \"duration\" in batch_dict.keys():\n            # para init\n            batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n                [len(ele) for ele in batch_dict[\"duration\"]]\n            )\n\n            # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n            duration = torch.zeros(\n                (batch_size, duration_len.max().item()), dtype=torch.float32\n            )\n            # overwrite the padding matrix with each duration vector\n            for i in range(batch_size):\n                # process duration data based on data type\n                if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                    duration[i][: duration_len[i]] = torch.tensor(\n                        batch_dict[\"duration\"][i]\n                    )\n                elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                    duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n                else:\n                    raise TypeError(\n                        f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                    )\n\n            # attach 'duration' and 'duration_len' for model forward\n            batch_dict[\"duration\"] = duration\n            batch_dict[\"duration_len\"] = duration_len\n\n        # --- 4. Stack Speaker Embedding Feature together --- #\n        if \"spk_feat\" in batch_dict.keys():\n            batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n        return batch_dict\n\n    def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n        \"\"\"\n        The function that loads speech-text data from the disk. If the speech is in the form of raw waveforms,\n        the last dimension should be expanded to 1 of raw speech for compatibility with acoustic feature.\n\n        Args:\n            main_data: Dict[str, str]\n                The keys of the input main_data dictionary should be one of the following:\n                    1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                    2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                    TTS models.\n                    3. 'duration': phoneme durations. used for training fastspeech2 model.\n                    4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                    ASR and TTS models.\n                    5. 'spk_feat': speaker embedding features.\n                `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n                in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n                However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n                CPU burden.\n\n        Returns:\n            `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n            `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n\n        \"\"\"\n        assert (\n            \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n        ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n        for key in main_data.keys():\n            if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n                raise RuntimeError(\n                    f\"Unknown data name {key}! \"\n                    f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                    \"'feat' (for paths of raw waveforms or acoustic features), \"\n                    \"'text' (for transcript text data), \"\n                    \"'duration' (for phoneme duration data), \"\n                    \"'spk_ids' (for speaker IDs), \"\n                    \"'spk_feat' (for speaker embedding features).\"\n                )\n\n        # --- 1. Speech Data Extraction --- #\n        if \"feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"feat\"], sample_rate = read_data_by_path(\n                main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n            )\n            # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n            if main_data[\"feat\"].size(0) == 0:\n                return None\n\n            # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n            if sample_rate &gt; self.sample_rate:\n                if not hasattr(self, \"wav_resampler_dict\"):\n                    self.wav_resampler_dict = {\n                        sample_rate: torchaudio.transforms.Resample(\n                            orig_freq=sample_rate, new_freq=self.sample_rate\n                        )\n                    }\n                main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n            # extracted waveforms could not have lower sampling rate than the built-in one\n            elif sample_rate &lt; self.sample_rate:\n                raise RuntimeError(\n                    f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n                )\n\n            # perturb the speed of the extracted speech if specified\n            if hasattr(self, \"speed_resampler_list\"):\n                assert sample_rate == self.sample_rate, (\n                    f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                    f\"waveform ({sample_rate})!\"\n                )\n                resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n                main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                    main_data[\"feat\"].squeeze(-1)\n                ).unsqueeze(-1)\n\n            # extract the pitch from the speech on-the-fly\n            if hasattr(self, \"pitch_extract_fn\"):\n                try:\n                    main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n                # IndexError means all the pitch values are unvoiced (=0.0)\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 2. Transcript Text Extraction --- #\n        if \"text\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"text\"], str\n            ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n            # for the text data in the format of a list\n            if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n                main_data[\"text\"] = main_data[\"text\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"text\"] = main_data[\"text\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"text\"] = [\n                    (\n                        token[1:-1]\n                        if token.startswith(\"'\") and token.endswith(\"'\")\n                        else token\n                    )\n                    for token in main_data[\"text\"]\n                ]\n            # process the raw string by G2P if specified\n            elif hasattr(self, \"g2p\"):\n                phn_list = self.g2p(main_data[\"text\"])\n                main_data[\"text\"] = [\n                    phn if phn != \" \" else \"&lt;space&gt;\"\n                    for phn in phn_list\n                    if phn not in abnormal_phns\n                ]\n\n        # --- 3. Phoneme Duration Extraction --- #\n        if \"duration\" in main_data.keys():\n            # text length is not returned because the text here is just a raw string\n            assert isinstance(\n                main_data[\"duration\"], str\n            ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n            # for the text data in the format of a list\n            if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n                \"]\"\n            ):\n                main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n                # split the text into individual tokens by a comma followed a blank\n                main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n                # remove the single quote marks surrounding each token if needed\n                main_data[\"duration\"] = [\n                    (\n                        float(duration[1:-1])\n                        if duration.startswith(\"'\") and duration.endswith(\"'\")\n                        else float(duration)\n                    )\n                    for duration in main_data[\"duration\"]\n                ]\n            else:\n                raise RuntimeError(\n                    \"The 'duration' string should be surrounded by a pair of square brackets!\"\n                )\n\n        # --- 4. Silence Trimming at the two ends --- #\n        # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n        if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n            main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n        ):\n            # trim both feat and text\n            if \"feat\" in main_data.keys():\n                assert \"duration\" in main_data.keys(), (\n                    \"If you want to trim the silence at two ends of speech, \"\n                    \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n                )\n                front_trim_len, tail_trim_len, total_duration = (\n                    0,\n                    0,\n                    sum(main_data[\"duration\"]),\n                )\n                try:\n                    # sum up all the silence tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        front_trim_len += main_data[\"duration\"][0]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][1:],\n                            main_data[\"duration\"][1:],\n                        )\n                    # sum up all the silence tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        tail_trim_len += main_data[\"duration\"][-1]\n                        main_data[\"text\"], main_data[\"duration\"] = (\n                            main_data[\"text\"][:-1],\n                            main_data[\"duration\"][:-1],\n                        )\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n                # normalize the trimming lengths by the total duration length\n                front_trim_len, tail_trim_len = (\n                    front_trim_len / total_duration,\n                    tail_trim_len / total_duration,\n                )\n                # trim the extra silence in feat (waveforms or acoustic features)\n                feat_start, feat_end = int(\n                    front_trim_len * len(main_data[\"feat\"])\n                ), int(tail_trim_len * len(main_data[\"feat\"]))\n                main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n                if feat_end &gt; 0:\n                    main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n                # also trim the two ends of pitch values if extracted\n                if \"pitch\" in main_data.keys():\n                    pitch_start, pitch_end = int(\n                        front_trim_len * len(main_data[\"pitch\"])\n                    ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                    main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                    if pitch_end &gt; 0:\n                        main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n            # only trim text if feat is not given\n            else:\n                try:\n                    # sum up all the &lt;space&gt; tokens at the beginning\n                    while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][1:]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][1:]\n                    # sum up all the &lt;space&gt; tokens at the end\n                    while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                        main_data[\"text\"] = main_data[\"text\"][:-1]\n                        if \"duration\" in main_data.keys():\n                            main_data[\"duration\"] = main_data[\"duration\"][:-1]\n                # IndexError means the text is full of '&lt;space&gt;'\n                # return None to remove this utterance from the current batch\n                except IndexError:\n                    return None\n\n        # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n        if self.unk_mask_prob &gt; 0:\n            assert \"text\" in main_data.keys() and isinstance(\n                main_data[\"text\"], List\n            ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n            # Get the start and end indices of words based on the positions of space tokens\n            space_indices = [\n                i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n            ]\n            word_start_indices, word_end_indices = [0] + [\n                s_i + 1 for s_i in space_indices\n            ], space_indices + [len(main_data[\"text\"])]\n\n            # Determine which words to mask\n            word_mask_flags = (\n                np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n            )\n\n            _tmp_text, _tmp_duration = [], []\n            for i in range(len(word_mask_flags)):\n                # If the word should be masked, add an '&lt;unk&gt;' token\n                if word_mask_flags[i]:\n                    _tmp_text.append(\"&lt;unk&gt;\")\n                    if \"duration\" in main_data.keys():\n                        _sum_duration = sum(\n                            main_data[\"duration\"][\n                                word_start_indices[i] : word_end_indices[i]\n                            ]\n                        )\n                        _tmp_duration.append(round(_sum_duration, 2))\n\n                # If the word shouldn't be masked, add the original tokens of the word\n                else:\n                    _tmp_text += main_data[\"text\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration += main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n\n                # Add space tokens and their durations between words, except for the last word\n                if i != len(word_mask_flags) - 1:\n                    _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                    if \"duration\" in main_data.keys():\n                        _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n            # Update main_data with the new text and duration information\n            main_data[\"text\"] = _tmp_text\n            if \"duration\" in main_data.keys():\n                main_data[\"duration\"] = _tmp_duration\n\n        # --- 6. Speaker ID Extraction --- #\n        if \"spk_ids\" in main_data.keys():\n            # the speaker ID here is just a raw string\n            assert isinstance(\n                main_data[\"spk_ids\"], str\n            ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n        # --- 7. Speaker Embedding Feature --- #\n        if \"spk_feat\" in main_data.keys():\n            # read the selected data speech feature as a tensor by its path\n            main_data[\"spk_feat\"] = read_data_by_path(\n                main_data[\"spk_feat\"], return_tensor=True\n            )\n\n        return main_data\n\n    def __repr__(self):\n        outputs = f\"{self.__class__.__name__}(sample_rate={self.sample_rate}\"\n        if hasattr(self, \"g2p\"):\n            outputs += \", use_g2p=True\"\n        if hasattr(self, \"speed_resampler_list\"):\n            outputs += f\", speed_perturb_range={self.perturb_range}\"\n        if hasattr(self, \"pitch_extract_fn\"):\n            outputs += \", pitch_extract=True\"\n        if self.unk_mask_prob &gt; 0:\n            outputs += f\", unk_mask_prob={self.unk_mask_prob}\"\n        return outputs + \")\"\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.collate_main_data_fn","title":"<code>collate_main_data_fn(batch_dict)</code>","text":"<p>The utterances used for training ASR and TTS models may have different lengths, so we need to do the padding operations to make them equal in length.</p> <p>The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dict</code> <code>Dict[str, List]</code> <p>Dict[str, List]</p> required <code>The</code> <code>keys of the input `batch_dict` dictionary should be one of the following</code> <ol> <li><code>feat</code>: a List of 2d <code>torch.Tensor</code> with different lengths.</li> <li><code>pitch</code>: a List of 1d <code>torch.Tensor</code> with different lengths.</li> <li><code>text</code>: a List of text strings.</li> <li><code>spk_ids</code>: a List of speaker ID strings.</li> <li><code>spk_feat</code>: a List of 2d <code>torch.Tensor</code> with equal lengths.</li> </ol> required <p>Dict[str, torch.Tensor or List]</p> Type Description <code>Dict[str, Tensor or List]</code> <p><code>feat</code> and <code>spk_feat</code> are in the form of three-dimensional <code>torch.Tensor</code>;</p> <code>Dict[str, Tensor or List]</code> <p><code>text</code> and <code>spk_ids</code> are in the form of List of raw strings whose discretization is done in the Model object.</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def collate_main_data_fn(\n    self, batch_dict: Dict[str, List]\n) -&gt; Dict[str, torch.Tensor or List]:\n    \"\"\"\n    The utterances used for training ASR and TTS models may have different lengths, so we need to do the\n    padding operations to make them equal in length.\n\n    The loaded speech feature vectors will be arranged into a single matrix with 0 padding at the end of short\n    vectors. Text data remains unprocessed strings and the tokenization will be done later in the model.\n\n    Args:\n        batch_dict: Dict[str, List]\n        The keys of the input `batch_dict` dictionary should be one of the following:\n            1. `feat`: a List of 2d `torch.Tensor` with different lengths.\n            2. `pitch`: a List of 1d `torch.Tensor` with different lengths.\n            3. `text`: a List of text strings.\n            4. `spk_ids`: a List of speaker ID strings.\n            5. `spk_feat`: a List of 2d `torch.Tensor` with equal lengths.\n\n    Returns: Dict[str, torch.Tensor or List]\n        `feat` and `spk_feat` are in the form of three-dimensional `torch.Tensor`;\n        `text` and `spk_ids` are in the form of List of raw strings whose discretization is done in the Model object.\n\n    \"\"\"\n\n    # --- 1. Pad Speech Data and Stack them together --- #\n    if \"feat\" in batch_dict.keys():\n        # para init\n        feat_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"feat\"]])\n        batch_size, feat_maxlen, feat_dim = (\n            len(batch_dict[\"feat\"]),\n            feat_len.max().item(),\n            batch_dict[\"feat\"][0].shape[-1],\n        )\n\n        # acoustic feature padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        feat = torch.zeros((batch_size, feat_maxlen, feat_dim), dtype=torch.float32)\n        # overwrite the padding matrix with each feat vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"feat\"][i], np.ndarray):\n                feat[i][: feat_len[i]] = torch.tensor(batch_dict[\"feat\"][i])\n            elif isinstance(batch_dict[\"feat\"][i], torch.Tensor):\n                feat[i][: feat_len[i]] = batch_dict[\"feat\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        # update 'feat' and attach 'feat_len' for later model forward\n        batch_dict[\"feat\"] = feat\n        batch_dict[\"feat_len\"] = feat_len\n\n    # --- 2. Pad Pitch Data and Stack them together --- #\n    if \"pitch\" in batch_dict.keys():\n        # para init\n        pitch_len = torch.LongTensor([ele.shape[0] for ele in batch_dict[\"pitch\"]])\n        batch_size, pitch_maxlen = len(batch_dict[\"pitch\"]), pitch_len.max().item()\n\n        # pitch padding, pitch.dtype needs to match the type of model parameters (torch.float32)\n        pitch = torch.zeros((batch_size, pitch_maxlen), dtype=torch.float32)\n        # overwrite the padding matrix with each pitch vector\n        for i in range(batch_size):\n            # process feat data based on data type\n            if isinstance(batch_dict[\"pitch\"][i], np.ndarray):\n                pitch[i][: pitch_len[i]] = torch.tensor(batch_dict[\"pitch\"][i])\n            elif isinstance(batch_dict[\"pitch\"][i], torch.Tensor):\n                pitch[i][: pitch_len[i]] = batch_dict[\"pitch\"][i]\n            # only support np.ndarray and torch.Tensor now\n            else:\n                raise TypeError\n\n        batch_dict[\"pitch\"] = pitch\n        batch_dict[\"pitch_len\"] = pitch_len\n\n    # --- 3. Separate Phoneme Duration Data into Text Data and Duration Data --- #\n    if \"duration\" in batch_dict.keys():\n        # para init\n        batch_size, duration_len = len(batch_dict[\"duration\"]), torch.LongTensor(\n            [len(ele) for ele in batch_dict[\"duration\"]]\n        )\n\n        # duration padding, feat.dtype needs to match the type of model parameters (torch.float32)\n        duration = torch.zeros(\n            (batch_size, duration_len.max().item()), dtype=torch.float32\n        )\n        # overwrite the padding matrix with each duration vector\n        for i in range(batch_size):\n            # process duration data based on data type\n            if isinstance(batch_dict[\"duration\"][i], (np.ndarray, List)):\n                duration[i][: duration_len[i]] = torch.tensor(\n                    batch_dict[\"duration\"][i]\n                )\n            elif isinstance(batch_dict[\"duration\"][i], torch.Tensor):\n                duration[i][: duration_len[i]] = batch_dict[\"duration\"][i]\n            else:\n                raise TypeError(\n                    f\"{self.__class__.name} only supports np.ndarray and torch.Tensor now!\"\n                )\n\n        # attach 'duration' and 'duration_len' for model forward\n        batch_dict[\"duration\"] = duration\n        batch_dict[\"duration_len\"] = duration_len\n\n    # --- 4. Stack Speaker Embedding Feature together --- #\n    if \"spk_feat\" in batch_dict.keys():\n        batch_dict[\"spk_feat\"] = torch.stack(batch_dict[\"spk_feat\"])\n\n    return batch_dict\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.data_len_register_fn","title":"<code>data_len_register_fn(main_data)</code>  <code>staticmethod</code>","text":"<p>Returns:</p> Type Description <code>Dict[str, int or float] or None</code> <p>If 'text' is given in main_data, return the number of characters in each sentence.</p> <code>Dict[str, int or float] or None</code> <p>Otherwise, return None</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>@staticmethod\ndef data_len_register_fn(\n    main_data: Dict[str, Dict[str, str]]\n) -&gt; Dict[str, int or float] or None:\n    \"\"\"\n\n    Returns:\n        If 'text' is given in main_data, return the number of characters in each sentence.\n        Otherwise, return None\n\n    \"\"\"\n    if \"text\" in main_data.keys():\n        return {key: len(value) for key, value in main_data[\"text\"].items()}\n    else:\n        return None\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.dataset_init_fn","title":"<code>dataset_init_fn(use_g2p=False, unk_mask_prob=0.0, use_speed_perturb=False, sample_rate=16000, perturb_range=[0.9, 1.0, 1.1], pitch_conf=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>use_g2p</code> <code>bool</code> <p>bool = False Whether to process the raw string by G2P. We don't recommend you to turn it on because on-the-fly transformer from string to phoneme list consumes a lot of CPU resources.</p> <code>False</code> <code>use_speed_perturb</code> <code>bool</code> <p>bool = False Whether to perturb the speed of the waveforms</p> <code>False</code> <code>sample_rate</code> <code>int</code> <p>int = 16000</p> <code>16000</code> <code>perturb_range</code> <code>List[float]</code> <p>List[float] = [0.9, 1.0, 1.1]</p> <code>[0.9, 1.0, 1.1]</code> <code>pitch_conf</code> <code>Dict</code> <p>Dict = None The configuration given to convert_wav_to_pitch() for pitch extraction. If not given, pitch extraction will not be done on-the-fly.</p> <code>None</code> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def dataset_init_fn(\n    self,\n    use_g2p: bool = False,\n    unk_mask_prob: float = 0.0,\n    use_speed_perturb: bool = False,\n    sample_rate: int = 16000,\n    perturb_range: List[float] = [0.9, 1.0, 1.1],\n    pitch_conf: Dict = None,\n):\n    \"\"\"\n\n    Args:\n        # phoneme-related\n        use_g2p: bool = False\n            Whether to process the raw string by G2P. We don't recommend you to turn it on because on-the-fly\n            transformer from string to phoneme list consumes a lot of CPU resources.\n        # waveform-related\n        use_speed_perturb: bool = False\n            Whether to perturb the speed of the waveforms\n        sample_rate: int = 16000\n        perturb_range: List[float] = [0.9, 1.0, 1.1]\n        # pitch-related\n        pitch_conf: Dict = None\n            The configuration given to convert_wav_to_pitch() for pitch extraction.\n            If not given, pitch extraction will not be done on-the-fly.\n\n    \"\"\"\n    # register sampling rate for later check\n    self.sample_rate = sample_rate\n    warnings.warn(\n        f\"The waveform sampling rate of {self.__class__.__name__} is set to {sample_rate}. \"\n        f\"All the extracted waveforms will be downsampled into {sample_rate} if needed. \"\n        f\"Please make sure that {sample_rate} is the same with your model! \"\n        f\"If this is not your target sampling rate, \"\n        f\"please change it by the key 'sample_rate' in the item 'dataset_conf' under 'data_cfg'. \"\n        f\"If you want to train Language Models or synthesize speech by text, you can ignore this warning.\"\n    )\n\n    assert (\n        0 &lt;= unk_mask_prob &lt;= 1\n    ), f\"unk_mask_prob should be a float number in [0, 1], but got {unk_mask_prob}!\"\n    self.unk_mask_prob = unk_mask_prob\n\n    # phoneme extraction\n    if use_g2p:\n        self.g2p = G2p()\n\n    if use_speed_perturb:\n        self.perturb_range = perturb_range\n        self.speed_resampler_list = [\n            torchaudio.transforms.Resample(\n                orig_freq=sample_rate, new_freq=int(sample_rate * factor)\n            )\n            for factor in perturb_range\n        ]\n\n    # pitch extraction\n    if pitch_conf is not None:\n        if \"sr\" in pitch_conf.keys():\n            assert pitch_conf[\"sr\"] == self.sample_rate, (\n                f\"The sampling rate in your given 'pitch_conf' ({pitch_conf['sr']}) is different from your \"\n                f\"given sample_rate ({self.sample_rate})!\"\n            )\n        pitch_conf[\"sr\"] = self.sample_rate\n        self.pitch_extract_fn = partial(\n            convert_wav_to_pitch, return_tensor=True, **pitch_conf\n        )\n</code></pre>"},{"location":"reference/#speechain.dataset.speech_text.SpeechTextDataset.extract_main_data_fn","title":"<code>extract_main_data_fn(main_data)</code>","text":"<p>The function that loads speech-text data from the disk. If the speech is in the form of raw waveforms, the last dimension should be expanded to 1 of raw speech for compatibility with acoustic feature.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>Dict</code> <p>Dict[str, str] The keys of the input main_data dictionary should be one of the following:     1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.     2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and     TTS models.     3. 'duration': phoneme durations. used for training fastspeech2 model.     4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the     ASR and TTS models.     5. 'spk_feat': speaker embedding features. <code>spk_ids</code> and <code>spk_feat</code> are designed for multi-speaker TTS model and are not mandatory to be included in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training. However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the CPU burden.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] or None</code> <p><code>feat</code> and <code>spk_feat</code> are in the form of two-dimensional <code>torch.Tensor</code>;</p> <code>Dict[str, Any] or None</code> <p><code>text</code> and <code>spk_ids</code> are in the form of raw strings whose discretization is done in the Model object.</p> Source code in <code>speechain/dataset/speech_text.py</code> <pre><code>def extract_main_data_fn(self, main_data: Dict) -&gt; Dict[str, Any] or None:\n    \"\"\"\n    The function that loads speech-text data from the disk. If the speech is in the form of raw waveforms,\n    the last dimension should be expanded to 1 of raw speech for compatibility with acoustic feature.\n\n    Args:\n        main_data: Dict[str, str]\n            The keys of the input main_data dictionary should be one of the following:\n                1. 'feat': speech features, can be either raw waveforms or acoustic features like log-mel or MFCC.\n                2. 'text': transcript text, in the form of raw string. The tokenization will be done in the ASR and\n                TTS models.\n                3. 'duration': phoneme durations. used for training fastspeech2 model.\n                4. 'spk_ids': speaker ID, in the form of raw string. The speaker discretization will be done in the\n                ASR and TTS models.\n                5. 'spk_feat': speaker embedding features.\n            `spk_ids` and `spk_feat` are designed for multi-speaker TTS model and are not mandatory to be included\n            in `main_data; 'feat' and 'text' are mandatory to be included for ASR and TTS training.\n            However, during model testing, we can choose to only include one of 'feat' and 'text' here to reduce the\n            CPU burden.\n\n    Returns:\n        `feat` and `spk_feat` are in the form of two-dimensional `torch.Tensor`;\n        `text` and `spk_ids` are in the form of raw strings whose discretization is done in the Model object.\n\n    \"\"\"\n    assert (\n        \"feat\" in main_data.keys() or \"text\" in main_data.keys()\n    ), \"Please at least include one of 'feat' and 'text' in a single batch.\"\n    for key in main_data.keys():\n        if key not in [\"feat\", \"text\", \"duration\", \"spk_ids\", \"spk_feat\"]:\n            raise RuntimeError(\n                f\"Unknown data name {key}! \"\n                f\"For {self.__class__.__name__}, the key in 'main_data' must be one of \"\n                \"'feat' (for paths of raw waveforms or acoustic features), \"\n                \"'text' (for transcript text data), \"\n                \"'duration' (for phoneme duration data), \"\n                \"'spk_ids' (for speaker IDs), \"\n                \"'spk_feat' (for speaker embedding features).\"\n            )\n\n    # --- 1. Speech Data Extraction --- #\n    if \"feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"feat\"], sample_rate = read_data_by_path(\n            main_data[\"feat\"], return_sample_rate=True, return_tensor=True\n        )\n        # sometimes the extracted waveform data from an audio file can be empty, skip the current file if that happens\n        if main_data[\"feat\"].size(0) == 0:\n            return None\n\n        # on-the-fly downsampling if extracted sampling rate is larger than the built-in one\n        if sample_rate &gt; self.sample_rate:\n            if not hasattr(self, \"wav_resampler_dict\"):\n                self.wav_resampler_dict = {\n                    sample_rate: torchaudio.transforms.Resample(\n                        orig_freq=sample_rate, new_freq=self.sample_rate\n                    )\n                }\n            main_data[\"feat\"] = self.wav_resampler_dict[sample_rate](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n        # extracted waveforms could not have lower sampling rate than the built-in one\n        elif sample_rate &lt; self.sample_rate:\n            raise RuntimeError(\n                f\"The current waveform has the lower sampling rate than {self.sample_rate}!\"\n            )\n\n        # perturb the speed of the extracted speech if specified\n        if hasattr(self, \"speed_resampler_list\"):\n            assert sample_rate == self.sample_rate, (\n                f\"Your given sample rate ({self.sample_rate}) is different from the real one gotten from the \"\n                f\"waveform ({sample_rate})!\"\n            )\n            resampler_index = torch.randint(len(self.speed_resampler_list), (1,))[0]\n            main_data[\"feat\"] = self.speed_resampler_list[resampler_index](\n                main_data[\"feat\"].squeeze(-1)\n            ).unsqueeze(-1)\n\n        # extract the pitch from the speech on-the-fly\n        if hasattr(self, \"pitch_extract_fn\"):\n            try:\n                main_data[\"pitch\"] = self.pitch_extract_fn(main_data[\"feat\"])\n            # IndexError means all the pitch values are unvoiced (=0.0)\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 2. Transcript Text Extraction --- #\n    if \"text\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"text\"], str\n        ), f\"The 'text' data should be given as a string, but got {main_data['text']}\"\n        # for the text data in the format of a list\n        if main_data[\"text\"].startswith(\"[\") and main_data[\"text\"].endswith(\"]\"):\n            main_data[\"text\"] = main_data[\"text\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"text\"] = main_data[\"text\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"text\"] = [\n                (\n                    token[1:-1]\n                    if token.startswith(\"'\") and token.endswith(\"'\")\n                    else token\n                )\n                for token in main_data[\"text\"]\n            ]\n        # process the raw string by G2P if specified\n        elif hasattr(self, \"g2p\"):\n            phn_list = self.g2p(main_data[\"text\"])\n            main_data[\"text\"] = [\n                phn if phn != \" \" else \"&lt;space&gt;\"\n                for phn in phn_list\n                if phn not in abnormal_phns\n            ]\n\n    # --- 3. Phoneme Duration Extraction --- #\n    if \"duration\" in main_data.keys():\n        # text length is not returned because the text here is just a raw string\n        assert isinstance(\n            main_data[\"duration\"], str\n        ), f\"The 'duration' data should be given as a string, but got {main_data['duration']}\"\n        # for the text data in the format of a list\n        if main_data[\"duration\"].startswith(\"[\") and main_data[\"duration\"].endswith(\n            \"]\"\n        ):\n            main_data[\"duration\"] = main_data[\"duration\"][1:-1]\n            # split the text into individual tokens by a comma followed a blank\n            main_data[\"duration\"] = main_data[\"duration\"].split(\", \")\n            # remove the single quote marks surrounding each token if needed\n            main_data[\"duration\"] = [\n                (\n                    float(duration[1:-1])\n                    if duration.startswith(\"'\") and duration.endswith(\"'\")\n                    else float(duration)\n                )\n                for duration in main_data[\"duration\"]\n            ]\n        else:\n            raise RuntimeError(\n                \"The 'duration' string should be surrounded by a pair of square brackets!\"\n            )\n\n    # --- 4. Silence Trimming at the two ends --- #\n    # trim the silence at two ends of the waveforms if the phoneme sequence starts or ends with spaces\n    if (\"text\" in main_data.keys() and isinstance(main_data[\"text\"], List)) and (\n        main_data[\"text\"][0] == \"&lt;space&gt;\" or main_data[\"text\"][-1] == \"&lt;space&gt;\"\n    ):\n        # trim both feat and text\n        if \"feat\" in main_data.keys():\n            assert \"duration\" in main_data.keys(), (\n                \"If you want to trim the silence at two ends of speech, \"\n                \"please give 'duration' in 'main_data' of the item 'dataset_conf' under 'data_cfg'.\"\n            )\n            front_trim_len, tail_trim_len, total_duration = (\n                0,\n                0,\n                sum(main_data[\"duration\"]),\n            )\n            try:\n                # sum up all the silence tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    front_trim_len += main_data[\"duration\"][0]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][1:],\n                        main_data[\"duration\"][1:],\n                    )\n                # sum up all the silence tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    tail_trim_len += main_data[\"duration\"][-1]\n                    main_data[\"text\"], main_data[\"duration\"] = (\n                        main_data[\"text\"][:-1],\n                        main_data[\"duration\"][:-1],\n                    )\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n            # normalize the trimming lengths by the total duration length\n            front_trim_len, tail_trim_len = (\n                front_trim_len / total_duration,\n                tail_trim_len / total_duration,\n            )\n            # trim the extra silence in feat (waveforms or acoustic features)\n            feat_start, feat_end = int(\n                front_trim_len * len(main_data[\"feat\"])\n            ), int(tail_trim_len * len(main_data[\"feat\"]))\n            main_data[\"feat\"] = main_data[\"feat\"][feat_start:]\n            if feat_end &gt; 0:\n                main_data[\"feat\"] = main_data[\"feat\"][:-feat_end]\n\n            # also trim the two ends of pitch values if extracted\n            if \"pitch\" in main_data.keys():\n                pitch_start, pitch_end = int(\n                    front_trim_len * len(main_data[\"pitch\"])\n                ), int(tail_trim_len * len(main_data[\"pitch\"]))\n                main_data[\"pitch\"] = main_data[\"pitch\"][pitch_start:]\n                if pitch_end &gt; 0:\n                    main_data[\"pitch\"] = main_data[\"pitch\"][:-pitch_end]\n\n        # only trim text if feat is not given\n        else:\n            try:\n                # sum up all the &lt;space&gt; tokens at the beginning\n                while main_data[\"text\"][0] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][1:]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][1:]\n                # sum up all the &lt;space&gt; tokens at the end\n                while main_data[\"text\"][-1] == \"&lt;space&gt;\":\n                    main_data[\"text\"] = main_data[\"text\"][:-1]\n                    if \"duration\" in main_data.keys():\n                        main_data[\"duration\"] = main_data[\"duration\"][:-1]\n            # IndexError means the text is full of '&lt;space&gt;'\n            # return None to remove this utterance from the current batch\n            except IndexError:\n                return None\n\n    # --- 5. Randomly Masking the text data by unknown tokens (After silence trimming for data safety) --- #\n    if self.unk_mask_prob &gt; 0:\n        assert \"text\" in main_data.keys() and isinstance(\n            main_data[\"text\"], List\n        ), \"If you want to activate unk_mask_prob, text must be given in the 'main_date' tag as a token sequence.\"\n\n        # Get the start and end indices of words based on the positions of space tokens\n        space_indices = [\n            i for i, token in enumerate(main_data[\"text\"]) if token == \"&lt;space&gt;\"\n        ]\n        word_start_indices, word_end_indices = [0] + [\n            s_i + 1 for s_i in space_indices\n        ], space_indices + [len(main_data[\"text\"])]\n\n        # Determine which words to mask\n        word_mask_flags = (\n            np.random.rand(len(word_start_indices)) &lt; self.unk_mask_prob\n        )\n\n        _tmp_text, _tmp_duration = [], []\n        for i in range(len(word_mask_flags)):\n            # If the word should be masked, add an '&lt;unk&gt;' token\n            if word_mask_flags[i]:\n                _tmp_text.append(\"&lt;unk&gt;\")\n                if \"duration\" in main_data.keys():\n                    _sum_duration = sum(\n                        main_data[\"duration\"][\n                            word_start_indices[i] : word_end_indices[i]\n                        ]\n                    )\n                    _tmp_duration.append(round(_sum_duration, 2))\n\n            # If the word shouldn't be masked, add the original tokens of the word\n            else:\n                _tmp_text += main_data[\"text\"][\n                    word_start_indices[i] : word_end_indices[i]\n                ]\n                if \"duration\" in main_data.keys():\n                    _tmp_duration += main_data[\"duration\"][\n                        word_start_indices[i] : word_end_indices[i]\n                    ]\n\n            # Add space tokens and their durations between words, except for the last word\n            if i != len(word_mask_flags) - 1:\n                _tmp_text.append(main_data[\"text\"][word_end_indices[i]])\n                if \"duration\" in main_data.keys():\n                    _tmp_duration.append(main_data[\"duration\"][word_end_indices[i]])\n\n        # Update main_data with the new text and duration information\n        main_data[\"text\"] = _tmp_text\n        if \"duration\" in main_data.keys():\n            main_data[\"duration\"] = _tmp_duration\n\n    # --- 6. Speaker ID Extraction --- #\n    if \"spk_ids\" in main_data.keys():\n        # the speaker ID here is just a raw string\n        assert isinstance(\n            main_data[\"spk_ids\"], str\n        ), f\"The 'spk_ids' data should be given as a string, but got {main_data['spk_ids']}\"\n\n    # --- 7. Speaker Embedding Feature --- #\n    if \"spk_feat\" in main_data.keys():\n        # read the selected data speech feature as a tensor by its path\n        main_data[\"spk_feat\"] = read_data_by_path(\n            main_data[\"spk_feat\"], return_tensor=True\n        )\n\n    return main_data\n</code></pre>"},{"location":"tts/","title":"Text-To-Speech Synthesis (TTS)","text":"<p>\ud83d\udc46Back to the recipe README.md</p>"},{"location":"tts/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Available Backbones</li> <li>Preparing Durations for FastSpeech2</li> <li>Training a TTS model</li> </ol>"},{"location":"tts/#available-backbones","title":"Available Backbones","text":"<p>Below is a table of available backbones:</p> Dataset Subset Configuration Audio Samples Link libritts train-clean-100 train-clean-460 train-960 ljspeech vctk <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts/#preparing-durations-for-fastspeech2","title":"Preparing Durations for FastSpeech2","text":"<p>For training a FastSpeech2 model, you need to acquire additional duration data for your target dataset.  Follow these steps: 1. Create a virtual environment for MFA: <code>conda create -n speechain_mfa -c conda-forge montreal-forced-aligner gdown</code>. 2. Activate the <code>speechain_mfa</code> environment: <code>conda activate speechain_mfa</code>. 3. Downsample your target TTS dataset to 16khz. For details, please see how to dump a dataset on your machine. 4. By default, MFA package will store all the temporary files to your user directory. If you lack sufficient space, add <code>export MFA_ROOT_DIR={your-target-directory}</code> to <code>~/.bashrc</code> and run <code>source ~/.bashrc</code>. 5. Navigate to <code>${SPEECHAIN_ROOT}/datasets</code> and run <code>bash mfa_preparation.sh -h</code> for help. Then, add appropriate arguments to <code>bash mfa_preparation.sh</code> to acquire duration data.</p> <p>Note: MFA cannot process duration calculations for multiple datasets concurrently on a single machine (or a single node on a cluster).  Please process each dataset one at a time.</p> <p>\ud83d\udc46Back to the table of contents</p>"},{"location":"tts/#training-an-tts-model","title":"Training an TTS model","text":"<p>To train a TTS model, follow the ASR model training instructions located in <code>recipes/asr</code>.  Make sure to replace the folder names and configuration file names from <code>recipes/asr</code> with their corresponding names in <code>recipes/tts</code>.</p> <p>\ud83d\udc46Back to the table of contents</p>"}]}